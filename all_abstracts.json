[
    {
        "url": "https://arxiv.org/pdf/2406.11781",
        "abstract": "the rise of online multi-modal sharing platforms like tiktok and\nyoutube has enabled personalized recommender systems to incor-\nporate multiple modalities (such as visual, textual, and acoustic)\ninto user representations. however, addressing the challenge of\ndata sparsity in these systems remains a key issue. to address this\nlimitation, recent research has introduced self-supervised learn-\ning techniques to enhance recommender systems. however, these\nmethods often rely on simplistic random augmentation or intu-\nitive cross-view information, which can introduce irrelevant noise\nand fail to accurately align the multi-modal context with user-\nitem interaction modeling. to fill this research gap, we propose\na novel multi-modal graph diffusion model for recommendation\ncalled diffmm. our framework integrates a modality-aware graph\ndiffusion model with a cross-modal contrastive learning paradigm\nto improve modality-aware user representation learning. this inte-\ngration facilitates better alignment between multi-modal feature\ninformation and collaborative relation modeling. our approach\nleverages diffusion models\u2019 generative capabilities to automatically\ngenerate a user-item graph that is aware of different modalities,\nfacilitating the incorporation of useful multi-modal knowledge in\nmodeling user-item interactions. we conduct extensive experiments\non three public datasets, consistently demonstrating the superiority\nof our diffmm over various competitive baselines. for open-sourced\nmodel implementation details, you can access the source codes of\nour proposed framework at: https://github.com/hkuds/diffmm.\nacm reference format:\nyangqin jiang, lianghao xia, wei wei, da luo, kangyi lin, and chao huang.\n2024. diffmm: multi-modal diffusion model for recommendation. in pro-\nceedings of acm conference (conference\u201917). acm, new york, ny, usa,\n11 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.11745",
        "abstract": "to seek reliable information sources for news events, we introduce\na novel task of expert recommendation, which aims to identify\ntrustworthy sources based on their previously quoted statements.\nto achieve this, we built a novel dataset, called newsquote, con-\nsisting of 23,571 quote-speaker pairs sourced from a collection of\nnews articles. we formulate the recommendation task as the re-\ntrieval of experts based on their likelihood of being associated with\na given query. we also propose a multi-layer ranking framework\nemploying large language models to improve the recommenda-\ntion performance. our results show that employing an in-context\nlearning based llm ranker and a multi-layer ranking-based filter\nsignificantly improve both the predictive quality and behavioural\nquality of the recommender system.\nccs concepts\n\u2022information systems \u2192retrieval models and ranking .\nkeywords\nrecommender system, in-context learning, large language model\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.11720",
        "abstract": "re-ranking systems aim to reorder an initial list of documents to\nsatisfy better the information needs associated with a user-provided\nquery. modern re-rankers predominantly rely on neural network\nmodels, which have proven highly effective in representing samples\nfrom various modalities. however, these models typically evalu-\nate query-document pairs in isolation, neglecting the underlying\ndocument distribution that could enhance the quality of the re-\nranked list. to address this limitation, we propose graph neural\nre-ranking (gnrr), a pipeline based on graph neural networks\n(gnns), that enables each query to consider documents distribu-\ntion during inference. our approach models document relationships\nthrough corpus subgraphs and encodes their representations using\ngnns. through extensive experiments, we demonstrate that gnns\neffectively capture cross-document interactions, improving perfor-\nmance on popular ranking metrics. in trec-dl19, we observe a\nrelative improvement of 5.8% in average precision compared to\nour baseline. these findings suggest that integrating the gnn seg-\nment offers significant advantages, especially in scenarios where\nunderstanding the broader context of documents is crucial.\nccs concepts\n\u2022information systems \u2192recommender systems .\nkeywords\ngraph neural networks, neural re-ranking, graph neural re-\nranking\nacm reference format:\nandrea giuseppe di francesco, christian giannetti, nicola tonellotto,\nand fabrizio silvestri. 2018. graph neural re-ranking via corpus graph. in\nproceedings of make sure to enter the correct conference title from your rights\nconfirmation emai (conference acronym \u2019xx). acm, new york, ny, usa,\n8 pages. https://doi.org/xxxxxxx.xxxxxxx\n\u2217these authors contributed equally to this work.\n\u2020sapienza university of rome, rome, italy\n\u2021isti-cnr, pisa, italy\n\u00a7university of pisa\npermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. copyrights for components of this work owned by others than the\nauthor(s) must be honored. abstracting with credit is permitted. to copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. request permissions from permissions@acm.org.\nconference acronym \u2019xx, june 03\u201305, 2018, woodstock, ny\n\u00a92018 copyright held by the owner/author(s). publication rights licensed to acm.\nacm isbn 978-1-4503-xxxx-x/18/06\nhttps://doi.org/xxxxxxx.xxxxxxx1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.11706",
        "abstract": "we develop a method for training small-scale\n(under 100m parameter) neural information re-\ntrieval models with as few as 10 gold relevance\nlabels. the method depends on generating syn-\nthetic queries for documents using a language\nmodel (lm), and the key step is that we auto-\nmatically optimize the lm prompt that is used\nto generate these queries based on training qual-\nity. in experiments with the birco benchmark,\nwe find that models trained with our method\noutperform rankzephyr and are competitive\nwith rankllama, both of which are 7b param-\neter models trained on over 100k labels. these\nfindings point to the power of automatic prompt\noptimization for synthetic dataset generation.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.11678",
        "abstract": "large language models (llms) are increas-\ningly employed in zero-shot documents rank-\ning, yielding commendable results. however,\nseveral significant challenges still persist in\nllms for ranking: (1) llms are constrained\nby limited input length, precluding them from\nprocessing a large number of documents simul-\ntaneously; (2) the output document sequence\nis influenced by the input order of documents,\nresulting in inconsistent ranking outcomes; (3)\nachieving a balance between cost and ranking\nperformance is quite challenging. to tackle\nthese issues, we introduce a novel documents\nranking method called tourrank1, which is in-\nspired by the tournament mechanism. this ap-\nproach alleviates the impact of llm\u2019s limited\ninput length through intelligent grouping, while\nthe tournament-like points system ensures ro-\nbust ranking, mitigating the influence of the\ndocument input sequence. we test tourrank\nwith different llms on the trec dl datasets\nand the beir benchmark. experimental results\nshow that tourrank achieves state-of-the-art\nperformance at a reasonable cost.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.11505",
        "abstract": ". users\u2019 interaction or preference data used in recommender\nsystems carry the risk of unintentionally revealing users\u2019 private at-\ntributes (e.g., gender or race). this risk becomes particularly concerning\nwhen the training data contains user preferences that can be used to\ninfer these attributes, especially if they align with common stereotypes.\nthis major privacy issue allows malicious attackers or other third par-\nties to infer users\u2019 protected attributes. previous efforts to address this\nissue have added or removed parts of users\u2019 preferences prior to or dur-\ning model training to improve privacy, which often leads to decreases in\nrecommendation accuracy. in this work, we introduce sbo, a novel proba-\nbilistic obfuscation method for user preference data designed to improve\nthe accuracy\u2013privacy trade-off for such recommendation scenarios. we\napply sboto three state-of-the-art recommendation models (i.e., bpr,\nmultvae,andlightgcn)andtwopopulardatasets(i.e.,movielens-1m\nand lfm-2b). our experiments reveal that sbooutperforms comparable\napproaches with respect to the accuracy\u2013privacy trade-off. specifically,\nwe can reduce the leakage of users\u2019 protected attributes while maintain-\ning on-par recommendation accuracy.\nkeywords: recommender systems \u00b7privacy \u00b7obfuscation \u00b7debiasing\n\u00b7implicit feedback\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.11424",
        "abstract": "this paper presents an analysis of open-source large language mod-\nels (llms) and their application in retrieval-augmented generation\n(rag) tasks, specific for enterprise-specific data sets scraped from\ntheir websites. with the increasing reliance on llms in natural\nlanguage processing, it is crucial to evaluate their performance, ac-\ncessibility, and integration within specific organizational contexts.\nthis study examines various open-source llms, explores their in-\ntegration into rag frameworks using enterprise-specific data, and\nassesses the performance of different open-source embeddings in\nenhancing the retrieval and generation process. our findings indi-\ncate that open-source llms, combined with effective embedding\ntechniques, can significantly improve the accuracy and efficiency of\nrag systems, offering a viable alternative to proprietary solutions\nfor enterprises.\nccs concepts\n\u2022computing methodologies \u2192natural language generation .\nkeywords\nlarge language models(llms), retrieval augmented generation\n(rag), natural language processing(nlp),information retriever,\nrouge score, cosine similarity, top-k, llama3, mistral,generative\npre-trained transformers(gpt),generative ai(gen ai),cosine sim-\nilarity with groundtruth answer(csga)\n,\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.11323",
        "abstract": "Abstract or Introduction section not found on the first page."
    },
    {
        "url": "https://arxiv.org/pdf/2406.11290",
        "abstract": "utility and topical relevance are critical mea-\nsures in information retrieval (ir), reflect-\ning system and user perspectives, respectively.\nwhile topical relevance has long been empha-\nsized, utility is a higher standard of relevance\nand is more useful for facilitating downstream\ntasks, e.g., in retrieval-augmented genera-\ntion (rag). when we incorporate utility judg-\nments into rag, we realize that the topi-\ncal relevance, utility, and answering in rag\nare closely related to the three types of rel-\nevance that schutz discussed from a philo-\nsophical perspective. they are topical rel-\nevance, interpretational relevance, and mo-\ntivational relevance, respectively. inspired\nby the dynamic iterations of the three types\nof relevance, we propose an iterative util-\nity judgment framework (item) to pro-\nmote each step of the cycle of rag. we\nconducted extensive experiments on multi-\ngrade passage retrieval and factoid question-\nanswering datasets (i.e., trec dl, webap,\nand nq). experimental results demonstrate\nsignificant improvements in utility judgments,\nranking of topical relevance, and answer gen-\neration upon representative baselines, includ-\ning multiple single-shot utility judging ap-\nproaches. our code and benchmark can be\nfound at https://anonymous.4open.\nscience/r/item-b486/ .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.11251",
        "abstract": "in the real world, documents are organized in\ndifferent formats and varied modalities. tradi-\ntional retrieval pipelines require tailored docu-\nment parsing techniques and content extraction\nmodules to prepare input for indexing. this pro-\ncess is tedious, prone to errors, and has infor-\nmation loss. to this end, we propose document\nscreenshot embedding (dse), a novel retrieval\nparadigm that regards document screenshots\nas a unified input format, which does not re-\nquire any content extraction preprocess and\npreserves all the information in a document\n(e.g., text, image and layout). dse leverages\na large vision-language model to directly en-\ncode document screenshots into dense represen-\ntations for retrieval. to evaluate our method,\nwe first craft the dataset of wiki-ss, a 1.3m\nwikipedia web page screenshots as the corpus\nto answer the questions from the natural ques-\ntions dataset. in such a text-intensive document\nretrieval setting, dse shows competitive effec-\ntiveness compared to other text retrieval meth-\nods relying on parsing. for example, dse out-\nperforms bm25 by 17 points in top-1 retrieval\naccuracy. additionally, in a mixed-modality\ntask of slide retrieval, dse significantly out-\nperforms ocr text retrieval methods by over\n15 points in ndcg@10. these experiments\nshow that dse is an effective document re-\ntrieval paradigm for diverse types of documents.\nmodel checkpoints, code, and wiki-ss collec-\ntion will be released at http://tevatron.ai .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.11156",
        "abstract": "sequential recommendation (sr) tasks enhance recommen-\ndation accuracy by capturing the connection between users\u2019\npast interactions and their changing preferences. conven-\ntional models often focus solely on capturing sequential\npatterns within the training data, neglecting the broader\ncontext and semantic information embedded in item titles\nfrom external sources. this limits their predictive power\nand adaptability. recently, large language models (llms)\nhave shown promise in sr tasks due to their advanced un-\nderstanding capabilities and strong generalization abilities.\nresearchers have attempted to enhance llms\u2019 recommen-\ndation performance by incorporating information from sr\nmodels. however, previous approaches have encountered\nproblems such as 1) only influencing llms at the result level;\n2) increased complexity of llms recommendation methods\nleading to reduced interpretability; 3) incomplete understand-\ning and utilization of sr models information by llms.\nto address these problems, we proposes a novel frame-\nwork, delrec, which aims to extract knowledge from sr\nmodels and enable llms to easily comprehend and utilize\nthis supplementary information for more effective sequential\nrecommendations. delrec consists of two main stages: 1) sr\nmodels pattern distilling , focusing on extracting behavioral\npatterns exhibited by sr models using soft prompts through\ntwo well-designed strategies; 2) llms-based sequential rec-\nommendation , aiming to fine-tune llms to effectively use\nthe distilled auxiliary information to perform sr tasks. ex-\ntensive experimental results conducted on three real datasets\nvalidate the effectiveness of the delrec framework.\nccs concepts: \u2022information systems \u2192recommender\nsystems .\npermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. abstracting with credit is permitted. to copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. request permissions from permissions@acm.org.\nconference acronym \u2019xx, june 03\u201305, 2018, woodstock, ny\n\u00a92018 copyright held by the owner/author(s). publication rights licensed\nto acm.\nacm isbn 978-1-4503-xxxx-x/18/06\nhttps://doi.org/xxxxxxx.xxxxxxxkeywords: large language model, sequential recommenda-\ntion, pattern distillation\nacm reference format:\nguohao sun and haoyi zhang. 2018. delrec: distilling sequential\npattern to enhance llm-based recommendation. in proceedings\nof make sure to enter the correct conference title from your rights\nconfirmation emai (conference acronym \u2019xx). acm, new york, ny,\nusa, 12 pages. https://doi.org/xxxxxxx.xxxxxxx\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10517",
        "abstract": "advertising platforms have evolved in estimating lifetime value\n(ltv) to better align with advertisers\u2019 true performance metric\nwhich considers cumulative sum of purchases a customer con-\ntributes over a period. accurate ltv estimation is crucial for the\nprecision of the advertising system and the effectiveness of adver-\ntisements. however, the sparsity of real-world ltv data presents a\nsignificant challenge to ltv predictive model(i.e., pltv), severely\nlimiting the their capabilities. therefore, we propose to utilize ex-\nternal data, in addition to the internal data of advertising platform,\nto expand the size of purchase samples and enhance the ltv pre-\ndiction model of the advertising platform. to tackle the issue of\ndata distribution shift between internal and external platforms,\nwe introduce an adaptive difference siamese network (adsnet),\nwhich employs cross-domain transfer learning to prevent negative\ntransfer. specifically, adsnet is designed to learn information that\nis beneficial to the target domain. we introduce a gain evaluation\nstrategy to calculate information gain, aiding the model in learning\nhelpful information for the target domain and providing the ability\nto reject noisy samples, thus avoiding negative transfer. addition-\nally, we also design a domain adaptation module as a bridge to\nconnect different domains, reduce the distribution distance between\nthem, and enhance the consistency of representation space distri-\nbution. we conduct extensive offline experiments and online a/b\n\u2217corresponding authors\npermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. copyrights for components of this work owned by others than the\nauthor(s) must be honored. abstracting with credit is permitted. to copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. request permissions from permissions@acm.org.\nkdd \u201924, august 25\u201329, 2024, barcelona, spain\n\u00a92024 copyright held by the owner/author(s). publication rights licensed to acm.\nacm isbn 979-8-4007-0490-1/24/08\nhttps://doi.org/10.1145/3637528.3671612tests on a real advertising platform. our proposed adsnet method\noutperforms other methods, improving gini by 2 %. the ablation\nstudy highlights the importance of the gain evaluation strategy in\nnegative gain sample rejection and improving model performance.\nadditionally, adsnet significantly improves long-tail prediction.\nthe online a/b tests confirm adsnet\u2019s efficacy, increasing online\nltv by 3.47 %and gmv by 3.89 %.\nccs concepts\n\u2022information systems \u2192computational advertising ;online\nadvertising .\nkeywords\nlifetime value prediction, adaptive cross-domain transfer learn-\ning, computational advertising\nacm reference format:\nruize wang, hui xu, ying cheng, qi he, xing zhou, rui feng, wei xu, lei\nhuang, and jie jiang. 2024. adsnet: cross-domain ltv prediction with an\nadaptive siamese network in advertising. in proceedings of the 30th acm\nsigkdd conference on knowledge discovery and data mining (kdd \u201924),\naugust 25\u201329, 2024, barcelona, spain. acm, new york, ny, usa, 10 pages.\nhttps://doi.org/10.1145/3637528.3671612\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10450",
        "abstract": "there is a growing interest in utilizing large-scale language\nmodels (llms) to advance next-generation recommender systems\n(recsys), driven by their outstanding language understanding\nand in-context learning capabilities. in this scenario, tokenizing\n(i.e., indexing) users and items becomes essential for ensuring a\nseamless alignment of llms with recommendations. while several\nstudies have made progress in representing users and items through\ntextual contents or latent representations, challenges remain in\nefficiently capturing high-order collaborative knowledge into\ndiscrete tokens that are compatible with llms. additionally, the\nmajority of existing tokenization approaches often face difficulties\nin generalizing effectively to new/unseen users or items that were\nnot in the training corpus. to address these challenges, we propose\na novel framework called tokenrec , which introduces not only\nan effective id tokenization strategy but also an efficient retrieval\nparadigm for llm-based recommendations. specifically, our to-\nkenization strategy, masked vector-quantized (mq) tokenizer,\ninvolves quantizing the masked user/item representations learned\nfrom collaborative filtering into discrete tokens, thus achieving a\nsmooth incorporation of high-order collaborative knowledge and\na generalizable tokenization of users and items for llm-based\nrecsys. meanwhile, our generative retrieval paradigm is designed\nto efficiently recommend top- \ud835\udc3eitems for users to eliminate the need\nfor the time-consuming auto-regressive decoding and beam search\nprocesses used by llms, thus significantly reducing inference\ntime. comprehensive experiments validate the effectiveness of\nthe proposed methods, demonstrating that tokenrec outperforms\ncompetitive benchmarks, including both traditional recommender\nsystems and emerging llm-based recommender systems.\nccs concepts\n\u2022information systems \u2192recommender systems .\n* corresponding author: wenqi fan, department of computing, and department of\nmanagement and marketing, the hong kong polytechnic university.\npermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. copyrights for components of this work owned by others than acm\nmust be honored. abstracting with credit is permitted. to copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. request permissions from permissions@acm.org.\nconference\u201917, july 2017, washington, dc, usa\n\u00a92024 association for computing machinery.\nacm isbn 978-x-xxxx-xxxx-x/yy/mm. . . $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnnkeywords\nrecommender systems, large language models (llms), id tok-\nenization, vector quantization, collaborative filtering (cf).\nacm reference format:\nhaohao qu, wenqi fan*, zihuai zhao, and qing li. 2024. tokenrec: learning\nto tokenize id for llm-based generative recommendations. in proceedings\nof acm conference (conference\u201917). acm, new york, ny, usa, 14 pages.\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10274",
        "abstract": ". in this article we report on an initial exploration to assess\nthe viability of using the general large language models (llms), recently\nmade public, to classify mathematical documents. automated classifi-\ncation would be useful from the applied perspective of improving the\nnavigation of the literature and the more open-ended goal of identifying\nrelations among mathematical results. the mathematical subject clas-\nsification msc 2020, from mathscinet and zbmath, is widely used and\nthere is a significant corpus of ground truth material in the open lit-\nerature. we have evaluated the classification of preprint articles from\narxiv.org according to msc 2020. the experiment used only the title\nand abstract alone \u2014 not the entire paper. since this was early in the use\nof chatbots and the development of their apis, we report here on what\nwas carried out by hand. of course, the automation of the process will\nhave to follow if it is to be generally useful. we found that in about 60%\nof our sample the llm produced a primary classification matching that\nalready reported on arxiv. in about half of those instances, there were\nadditional primary classifications that were not detected. in about 40%\nof our sample, the llm suggested a different classification than what\nwas provided. a detailed examination of these cases, however, showed\nthat the llm-suggested classifications were in most cases betterthan\nthose provided.\nkeywords: large language models \u00b7mathematical documents \u00b7docu-\nment classification\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10262",
        "abstract": ". in two-sided marketplaces such as online flea markets, recom-\nmender systems for providing consumers with personalized item rankings\nplay a key role in promoting transactions between providers and con-\nsumers. meanwhile, two-sided marketplaces face the problem of balanc-\ning consumer satisfaction and fairness among items to stimulate activity\nof item providers. saito and joachims (2022) devised an impact-based\nfair ranking method for maximizing the nash social welfare based on\nfair division; however, this method, which requires solving a large-scale\nconstrained nonlinear optimization problem, is very difficult to apply to\npractical-scale recommender systems. we thus propose a fast solution\nto the impact-based fair ranking problem. we first transform the fair\nranking problem into an unconstrained optimization problem and then\ndesign a gradient ascent method that repeatedly executes the sinkhorn\nalgorithm. experimental results demonstrate that our algorithm pro-\nvides fair rankings of high quality and is about 1000 times faster than\napplication of commercial optimization software.\nkeywords: ranking \u00b7fairness \u00b7optimal transport.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10252",
        "abstract": "this paper introduces autosurvey, a speedy and well-organized methodology for\nautomating the creation of comprehensive literature surveys in rapidly evolving\nfields like artificial intelligence. traditional survey paper creation faces challenges\ndue to the vast volume and complexity of information, prompting the need for\nefficient survey methods. while large language models (llms) offer promise in\nautomating this process, challenges such as context window limitations, parametric\nknowledge constraints, and the lack of evaluation benchmarks remain. autosurvey\naddresses these challenges through a systematic approach that involves initial re-\ntrieval and outline generation, subsection drafting by specialized llms, integration\nand refinement, and rigorous evaluation and iteration. our contributions include a\ncomprehensive solution to the survey problem, a reliable evaluation method, and\nexperimental validation demonstrating autosurvey\u2019s effectiveness.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10250",
        "abstract": ". this paper is concerned with portfolio optimization models\nfor creating high-quality lists of recommended items to balance the ac-\ncuracy and diversity of recommendations. however, the statistics (i.e.,\nexpectation and covariance of ratings) required for mean\u2013variance port-\nfolio optimization are subject to inevitable estimation errors. to remedy\nthis situation, we focus on robust optimization techniques that derive\nreliable solutions to uncertain optimization problems. specifically, we\npropose a robust portfolio optimization model that copes with the un-\ncertainty of estimated statistics based on the cardinality-based uncer-\ntainty sets. this robust portfolio optimization model can be reduced to\na mixed-integer linear optimization problem, which can be solved exactly\nusing mathematical optimization solvers. experimental results using two\npublicly available rating datasets demonstrate that our method can im-\nprove not only the recommendation accuracy but also the diversity of\nrecommendations compared with conventional mean\u2013variance portfolio\noptimization models. notably, our method has the potential to improve\nthe recommendation quality of various rating prediction algorithms.\nkeywords: recommender system \u00b7portfolio optimization \u00b7robust op-\ntimization \u00b7diversity.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10246",
        "abstract": "Abstract or Introduction section not found on the first page."
    },
    {
        "url": "https://arxiv.org/pdf/2406.10245",
        "abstract": "the use of e-learning systems has a long tradition, where students can study online helped by a\nsystem. in this context, the use of recommender systems is relatively new. in our research project, we\ninvestigated various ways to create a recommender system. they all aim at facilitating the learning\nand understanding of a student. we present a common concept of the learning path and its learning\nindicators and embed 5 different recommenders in this context.\nkeywords e-learning \u00b7recommender systems \u00b7learning path personalization\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10244",
        "abstract": "in the rapidly evolving field of artificial intelligence, transformer-\nbased models have gained significant attention in the context of\nsequential recommender systems (srss), demonstrating remark-\nable proficiency in capturing user-item interactions. however, such\nattention-based frameworks result in substantial computational\noverhead and extended inference time. to address this problem,\nthis paper proposes a novel efficient sequential recommendation\nframework glint-ru that leverages dense selective gated recur-\nrent units (gru) module to accelerate the inference speed, which is\na pioneering work to further exploit the potential of efficient gru\nmodules in srss. the gru module lies at the heart of glint-ru,\nplaying a crucial role in substantially reducing both inference time\nand gpu memory usage. through the integration of a dense selec-\ntive gate, our framework adeptly captures both long-term and short-\nterm item dependencies, enabling the adaptive generation of item\nscores. glint-ru further integrates a mixing block, enriching it\nwith global user-item interaction information to bolster recommen-\ndation quality. moreover, we design a gated multi-layer perceptron\n(mlp) for our framework where the information is deeply filtered.\nextensive experiments on three datasets are conducted to high-\nlight the effectiveness and efficiency of glint-ru. our glint-ru\nachieves exceptional inference speed and prediction accuracy, out-\nperforming existing baselines based on recurrent neural network\n(rnn), transformer, mlp and state space model (ssm). these\nresults establish a new standard in sequential recommendation,\nhighlighting the potential of glint-ru as a renewing approach in\nthe realm of recommender systems. the implementation code is\navailable at https://github.com/szhang-cityu/glint-ru.\nccs concepts\n\u2022information systems \u2192recommender systems .\nkeywords\nsequential recommender systems, gated recurrnet units, efficient\nmodel\n\u2217xiangyu zhao is the corresponding author.\npermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. copyrights for components of this work owned by others than the\nauthor(s) must be honored. abstracting with credit is permitted. to copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. request permissions from permissions@acm.org.\nconference\u201917, july 2017, washington, dc, usa\n\u00a92024 copyright held by the owner/author(s). publication rights licensed to acm.\nacm isbn 978-1-4503-xxxx-x/18/06\nhttps://doi.org/xxxxxxx.xxxxxxxacm reference format:\nsheng zhang, maolin wang, and xiangyu zhao. 2024. glint-ru: gated\nlightweight intelligent recurrent units for sequential recommender sys-\ntems. in proceedings of acm conference (conference\u201917). acm, new york,\nny, usa, 9 pages. https://doi.org/xxxxxxx.xxxxxxx\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10239",
        "abstract": "\u2014thispaperproposesnewmethodstoenhanceclick-\nthroughrate(ctr)predictionmodelsusingthedeepinterest\nnetwork(din)model,specificallyappliedtotheadvertising\nsystemofalibaba\u2019staobaoplatform.unliketraditionaldeep\nlearningapproaches,thisresearchfocusesonlocalizeduser\nbehavioractivationfortailoredadtargetingbyleveraging\nextensiveuserbehaviordata.comparedtotraditionalmodels,\nthismethoddemonstratessuperiorabilitytohandlediverseand\ndynamicuserdata,therebyimprovingtheefficiencyofad\nsystemsandincreasingrevenue.\nkeywords-ctrprediction;deepinterestnetwork;e-commerce\nadvertising;deeplearning\ni."
    },
    {
        "url": "https://arxiv.org/pdf/2406.10237",
        "abstract": ".  the complexity of bim software presents significant barriers to the widespread adoption \nof bim and model -based design within the architecture, engineering, and construction (aec) \nsector.  end-users frequently express concerns regarding the additional effort required to create a \nsufficiently detailed bim model when compared with conventional 2d drafting.  this study explor es \nthe potential of sequen tial recommendation systems to accelerate the bim modeling process. by \ntreating bim software commands as recommendable  items, we introduce a novel end -to-end \napproach that predicts the next -best command based on user historical interactions. our framework \nextensively preprocesses real -world , large -scale bim log data, utilizes the transformer architectures \nfrom the latest large language models as the backbone network, and ultimately results in a prototype \nthat provides real -time command suggestions within the bim authoring tool vectorworks. \nsubsequent experiments validated that our proposed model outperf orms the previous study, \ndemonstrating the immense potential of the recommendation system in enhancing design efficiency.  \n1."
    },
    {
        "url": "https://arxiv.org/pdf/2406.10235",
        "abstract": ". recommender systems are a kind of data filtering that guides the user \nto interesting and valuable resources within an extensive dataset. by providing \nsuggestions of products that are expected to match  their preferences. however, \ndue to data  overloading, recommender systems struggle to handle large volumes \nof data reliably and accurately before  offering suggestions. the main purpose of \nthis work is to address the recommender system\u2019s data sparsity  and accuracy \nproblems by using the matrix factorization algorithm of  collaborative filtering \nbased on the dimensional reduction method and,  more preci sely, the nonnegative \nmatrix factorization (nmf) combined  with ontology. we tested the method and \ncompared the results to other  classic methods. the findings showed that the im-\nplemented approach efficiently reduces the sparsity of cf suggestions, improves \ntheir accuracy,  and gives more relevant items as recommendations . \n \nkeywords:  cf, matrix factoring,  nmf,  ontology, sparsity, accuracy . \n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.11612",
        "abstract": "nowadays, the \ufb01elds of code and natural language processing are evolving rapidly.\nin particular, models become better at processing long cont ext windows \u2014 sup-\nported context sizes have increased by orders of magnitude o ver the last few years.\nhowever, there is a shortage of benchmarks for code processi ng that go beyond a\nsingle \ufb01le of context, while the most popular ones are limite d to a single method.\nwith this work, we aim to close this gap by introducing long co de arena, a\nsuite of six benchmarks for code processing tasks that requi re project-wide con-\ntext. these tasks cover different aspects of code processin g: library-based code\ngeneration, ci builds repair, project-level code completi on, commit message gen-\neration, bug localization, and module summarization. for e ach task, we provide a\nmanually veri\ufb01ed dataset for testing, an evaluation suite, and open-source baseline\nsolutions based on popular llms to showcase the usage of the d ataset and to sim-\nplify adoption by other researchers. we publish the benchma rk page on hugging-\nface spaces with the leaderboard, links to huggingface hub f or all the datasets,\nand link to the github repository with baselines: https://huggingface.co/\nspaces/jetbrains-research/long-code-arena .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.11589",
        "abstract": "semantic code search, retrieving code that\nmatches a given natural language query, is\nan important task to improve productivity in\nsoftware engineering. existing code search\ndatasets are problematic: either using unreal-\nistic queries, or with mismatched codes, and\ntypically using one-to-one query-code pair-\ning, which fails to reflect the reality that a\nquery might have multiple valid code matches.\nthis paper introduces cosqa+, pairing high-\nquality queries (reused from cosqa) with mul-\ntiple suitable codes. we collect code candi-\ndates from diverse sources and form candi-\ndate pairs by pairing queries with these codes.\nutilizing the power of large language mod-\nels (llms), we automate pair annotation, fil-\ntering, and code generation for queries with-\nout suitable matches. through extensive ex-\nperiments, cosqa+has demonstrated supe-\nrior quality over cosqa. models trained on\ncosqa+exhibit improved performance. fur-\nthermore, we propose a new metric mean multi-\nchoice reciprocal rank (mmrr), to assess\none-to-n code search performance. we pro-\nvide the code and data at https://github.\ncom/deepsoftwareanalytics/cosqa_plus .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.11331",
        "abstract": ". vision language models (vlms) such as clip are powerful\nmodels; however they can exhibit unwanted biases, making them less\nsafe when deployed directly in applications such as text-to-image, text-\nto-video retrievals, reverse search, or classification tasks. in this work, we\nproposeanovelframeworktogeneratesyntheticcounterfactualimagesto\ncreate a diverse and balanced dataset that can be used to fine-tune clip.\ngiven a set of diverse synthetic base images from text-to-image models,\nwe leverage off-the-shelf segmentation and inpainting models to place\nhumans with diverse visual appearances in context . we show that clip\ntrained on such datasets learns to disentangle the human appearance\nfrom the context of an image, i.e., what makes a doctor is not correlated\nto the person\u2019s visual appearance, like skin color or body type, but to the\ncontext, such as background, the attire they are wearing, or the objects\nthey are holding. we demonstrate that our fine-tuned clip model, cf\u03b1,\nimproves key fairness metrics such as maxskew, minskew, and ndkl\nby 40-66% for image retrieval tasks, while still achieving similar levels of\nperformance in downstream tasks. we show that, by design, our model\nretainsmaximalcompatibilitywiththeoriginalclipmodels,andcanbe\neasily controlled to support different accuracy versus fairness trade-offs\nin a plug-n-play fashion.\nkeywords: bias mitigation \u00b7fairness \u00b7unsupervised de-biasing \u00b7vi-\nsual content-based indexing and retrieval\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10806",
        "abstract": "despite advancements in natural language processing (nlp) and\nthe growing availability of pretrained models, the english language re-\nmains the primary focus of model development. continued pre training on\nlanguage-speci\ufb01c corpora provides a practical solution fo r adapting mod-\nels to other languages. however, the impact of di\ufb00erent pret raining set-\ntings on downstream tasks remains underexplored. this work introduces\nptt5-v2 , investigating the continued pretraining of t5 models for p or-\ntuguese. we \ufb01rst develop a baseline set of settings and pretr ain models\nwith sizes up to 3b parameters. finetuning on three portugue se down-\nstream tasks (assin2 sts, assin2 rte, and tweetsentbr) yiel ds sota\nresults on the latter two. we then explore the e\ufb00ects of di\ufb00er ent pre-\ntraining con\ufb01gurations, including quality \ufb01lters, optimi zation strategies,\nand multi-epoch pretraining. perhaps surprisingly, their impact remains\nsubtle compared to our baseline. we release the ptt5-v2 pretrained\ncheckpoints and and the \ufb01netuned monot5 rerankers on huggin gface.1\n2\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10803",
        "abstract": "a myriad of different large language mod-\nels (llms) face a common challenge in con-\ntextually analyzing table question-answering\ntasks. these challenges are engendered from\n(1) finite context windows for large tables, (2)\nmulti-faceted discrepancies amongst tokeniza-\ntion patterns against cell boundaries, and (3)\nvarious limitations stemming from data con-\nfidentiality in the process of using external\nmodels such as gpt-3.5-turbo . we propose\na cooperative game dubbed \"hiddentables\"\nas a potential resolution to this challenge. in\nessence, \"hiddentables\" is played between the\ncode-generating llm \"solver\" and the \"ora-\ncle\" which evaluates the ability of the llm\nagents to solve table qa tasks. this game\nis based on natural language schemas and im-\nportantly, ensures the security of the underly-\ning data. we provide evidential experiments\non a diverse set of tables that demonstrate an\nllm\u2019s collective inability to generalize and\nperform on complex queries, handle composi-\ntional dependencies, and align natural language\nto programmatic commands when concrete ta-\nble schemas are provided. unlike encoder-\nbased models, we have pushed the boundaries\nof \"hiddentables\" to not be limited by the num-\nber of rows - therefore we exhibit improved ef-\nficiency in prompt and completion tokens. our\ninfrastructure has spawned a new dataset \"pyq-\ntax\" that spans across 116,671 question-table-\nanswer triplets and provides additional fine-\ngrained breakdowns & labels for varying ques-\ntion taxonomies. therefore, in tandem with our\nacademic contributions regarding llms\u2019 defi-\nciency in tableqa tasks, \"hiddentables\" is a\ntactile manifestation of how llms can inter-\nact with massive datasets while ensuring data\nsecurity and minimizing generation costs.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10746",
        "abstract": "contradiction retrieval refers to identifying and extracting documents that explicitly\ndisagree with or refute the content of a query, which is important to many down-\nstream applications like fact checking and data cleaning. to retrieve contradiction\nargument to the query from large document corpora, existing methods such as\nsimilarity search and crossencoder models exhibit significant limitations. the\nformer struggles to capture the essence of contradiction due to its inherent nature\nof favoring similarity, while the latter suffers from computational inefficiency,\nespecially when the size of corpora is large. to address these challenges, we\nintroduce a novel approach: sparse clthat leverages specially trained sentence\nembeddings designed to preserve subtle, contradictory nuances between sentences.\nour method utilizes a combined metric of cosine similarity and a sparsity function\nto efficiently identify and retrieve documents that contradict a given query. this\napproach dramatically enhances the speed of contradiction detection by reducing\nthe need for exhaustive document comparisons to simple vector calculations. we\nvalidate our model using the arguana dataset, a benchmark dataset specifically\ngeared towards contradiction retrieval, as well as synthetic contradictions generated\nfrom the msmarco and hotpotqa datasets using gpt-4. our experiments\ndemonstrate the efficacy of our approach not only in contradiction retrieval with\nmore than 30% accuracy improvements on msmarco and hotpotqa across\ndifferent model architectures but also in applications such as cleaning corrupted\ncorpora to restore high-quality qa retrieval. this paper outlines a promising\ndirection for improving the accuracy and efficiency of contradiction retrieval in\nlarge-scale text corpora.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10593",
        "abstract": "fine-tuning large language models (llms) for\nspecific domain tasks has achieved great suc-\ncess in text-to-sql tasks. however, these fine-\ntuned models often face challenges with multi-\nturn text-to-sql tasks caused by ambiguous\nor unanswerable questions. it is desired to en-\nhance llms to handle multiple types of ques-\ntions in multi-turn text-to-sql tasks. to ad-\ndress this, we propose a novel data augmen-\ntation method, called qda-sql, which gen-\nerates multiple types of multi-turn q&a pairs\nby using llms. in qda-sql, we introduce a\nnovel data augmentation method incorporating\nvalidation and correction mechanisms to han-\ndle complex multi-turn text-to-sql tasks. ex-\nperimental results demonstrate that qda-sql\nenables fine-tuned models to exhibit higher per-\nformance on sql statement accuracy and en-\nhances their ability to handle complex, unan-\nswerable questions in multi-turn text-to-sql\ntasks. the generation script and test set are re-\nleased athttps://github.com/mcxiaoxiao/\nqda-sql .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10402",
        "abstract": ". the number of topics might be the most important param-\neter of a topic model. the topic modelling community has developed a\nset of various procedures to estimate the number of topics in a dataset,\nbut there has not yet been a sufficiently complete comparison of existing\npractices. this study attempts to partially fill this gap by investigating\nthe performance of various methods applied to several topic models on\na number of publicly available corpora. further analysis demonstrates\nthat intrinsic methods are far from being reliable and accurate tools.\nthe number of topics is shown to be a method- and a model-dependent\nquantity,asopposedtobeinganabsolutepropertyofaparticularcorpus.\nwe conclude that other methods for dealing with this problem should be\ndeveloped and suggest some promising directions for further research.\nkeywords: topic modelling \u00b7number of topics \u00b7coherence \u00b7diversity\n\u00b7perplexity \u00b7stability \u00b7entropy\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10295",
        "abstract": "multi-modal large language models (llms) have shown remarkable performance in various natural language processing tasks, including data extraction from documents. however, the accuracy of these models can be significantly affected by document in-plane rotation, also known as skew, a common issue in real-world scenarios for scanned documents. this study investigates the impact of document skew on the data extraction accuracy of three state-of-the-art multi-modal llms: anthropic claude v3 sonnet, gpt-4-turbo, and llava:v1.6. we focus on extracting specific entities from synthetically generated sample documents with varying degrees of skewness. the results demonstrate that document skew adversely affects the data extraction accuracy of all the tested llms, with the severity of the impact varying across models. we identify the safe in-plane rotation angles (sipra) for each model and investigate the effects of skew on model hallucinations. furthermore, we explore existing skew detection and correction mechanisms and discuss their potential limitations. we propose alternative approaches, including developing new multi-modal architectures that are inherently more robust to document skew and incorporating skewing techniques during the pre-training phase of the models. additionally, we highlight the need for more comprehensive testing on a wider range of document quality and conditions to fully understand the challenges and opportunities associated with using multi-modal llms for information extraction in real-world scenarios. keywords: multi-modal large language models, data extraction, document skew, skew correction, document understanding, ocr, computer vision.   i."
    },
    {
        "url": "https://arxiv.org/pdf/2406.10291",
        "abstract": "large language models (llms) have exhibited remarkable performance across\nvarious tasks in natural language processing. nevertheless, challenges still arise\nwhen these tasks demand domain-specific expertise and advanced analytical skills,\nsuch as conducting research surveys on a designated topic. in this research, we\ndevelop researcharena, a benchmark that measures llm agents\u2019 ability to conduct\nacademic surveys, an initial step of academic research process. specifically, we\ndeconstructs the surveying process into three stages 1) information discovery:\nlocating relevant papers, 2) information selection: assessing papers\u2019 importance\nto the topic, and 3) information organization: organizing papers into meaningful\nstructures. in particular, we establish an offline environment comprising 12.0m\nfull-text academic papers and 7.9k survey papers, which evaluates agents\u2019 ability\nto locate supporting materials for composing the survey on a topic, rank the located\npapers based on their impact, and organize these into a hierarchical knowledge\nmind-map. with this benchmark, we conduct preliminary evaluations of existing\ntechniques and find that all llm-based methods under-performing when compared\nto basic keyword-based retrieval techniques, highlighting substantial opportunities\nfor future research.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10289",
        "abstract": "the proliferation of fake news poses a sig-\nnificant threat not only by disseminating mis-\nleading information but also by undermining\nthe very foundations of democracy. the re-\ncent advance of generative artificial intelligence\nhas further exacerbated the challenge of distin-\nguishing genuine news from fabricated stories.\nin response to this challenge, we introduce ve-\nract scan, a novel retrieval-augmented sys-\ntem for fake news detection. this system oper-\nates by extracting the core facts from a given\npiece of news and subsequently conducting an\ninternet-wide search to identify corroborating\nor conflicting reports. then sources\u2019 credibility\nis leveraged for information verification. be-\nsides determining the veracity of news, we also\nprovide transparent evidence and reasoning to\nsupport its conclusions, resulting in the inter-\npretability and trust in the results. in addition to\ngpt-4 turbo, llama-2 13b is also fine-tuned\nfor news content understanding, information\nverification, and reasoning. both implementa-\ntions have demonstrated state-of-the-art accu-\nracy in the realm of fake news detection1.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10253",
        "abstract": "this paper presents the development of a lexicon centered on emerging concepts, focusing on non-technological \ninnovation1. it introduces a four -step methodology that combines human expertise, statistical analysis, and \nmachine learning techniques to establish a model that can be generalized across multiple domains. this process \nincludes the creation of a thematic corpus, the development of a gold standard lexicon , annotation and \npreparation of a training corpus, and finally, the implementation of learning models to identify new terms. the \nresults demonstrate the robustness and relevance of our approach, highlighting its adaptability to various contexts \nand its con tribution to lexical research. the developed methodology promises applicability in conceptual fields.  \n \nkeywords :  semantic lexicon development, conceptual domain modeling, emerging concepts identification, \nknowledge extraction, machine learning annotation, corpus creation.  \nr\u00e9sum\u00e9  \ncet article pr\u00e9sente le d\u00e9veloppement d \u2019un lexique centr\u00e9 sur les concepts \u00e9mergents, en se focalisant sur \nl\u2019innovation non technologique . il d\u00e9crit une m\u00e9thodologie en quatre \u00e9tapes, combinant expertise humaine, \nstatistiques et techniques d \u2019apprentissage automatique, pour \u00e9tablir un mod\u00e8le g\u00e9n\u00e9ralisable \u00e0 plusieurs domaines. \ncette proc\u00e9dure comprend la cr\u00e9ation d \u2019un corpus th\u00e9matique, la constitution d \u2019un lexique de r\u00e9f\u00e9rence, \nl\u2019annotation et la pr\u00e9paration d \u2019un corpus d \u2019entra\u00eenement, et enfin, l \u2019impl\u00e9ment ation de mod\u00e8les d \u2019apprentissage \npour identifier de nouveaux termes. les r\u00e9sultats montrent la robustesse et la pertinence de notre approche, mettant \nen \u00e9vidence sa capacit\u00e9 \u00e0 \u00eatre adapt\u00e9e \u00e0 plusieurs contextes et sa contribution \u00e0 la recherche lexicale. la \nm\u00e9thodologie d\u00e9velopp\u00e9e promet une a pplicabilit\u00e9 dans des domaines conceptuels.  \nmots cl\u00e9s :  d\u00e9veloppement de lexique s\u00e9mantique, mod\u00e9lisation de domaine conceptuel, identification de \nconcepts \u00e9mergents, extraction de connaissances, annotation par apprentissage automatique, cr\u00e9ation de corpus.  \n1."
    },
    {
        "url": "https://arxiv.org/pdf/2406.07979",
        "abstract": "link prediction is a fundamental task in graph learning, inherently\nshaped by the topology of the graph. while traditional heuristics\nare grounded in graph topology, they encounter challenges in gen-\neralizing across diverse graphs. recent research efforts have aimed\nto leverage the potential of heuristics, yet a unified formulation\naccommodating both local and global heuristics remains undiscov-\nered. drawing insights from the fact that both local and global\nheuristics can be represented by adjacency matrix multiplications,\nwe propose a unified matrix formulation to accommodate and gener-\nalize various heuristics. we further propose the heuristic learning\ngraph neural network (hl-gnn) to efficiently implement the for-\nmulation. hl-gnn adopts intra-layer propagation and inter-layer\nconnections, allowing it to reach a depth of around 20 layers with\nlower time complexity than gcn. extensive experiments on the\nplanetoid, amazon, and ogb datasets underscore the effective-\nness and efficiency of hl-gnn. it outperforms existing methods\nby a large margin in prediction performance. additionally, hl-\ngnn is several orders of magnitude faster than heuristic-inspired\nmethods while requiring only a few trainable parameters. the\ncase study further demonstrates that the generalized heuristics and\nlearned weights are highly interpretable. the code is available at\nhttps://github.com/lars-research/hl-gnn.\nccs concepts\n\u2022theory of computation \u2192graph algorithms analysis ;\u2022\ncomputing methodologies \u2192neural networks .\nkeywords\ngraph learning; link prediction; graph neural networks; heuristic\nmethods\n\u2217correspondence is to q. yao at qyaoaa@tsinghua.edu.cn.\npermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. copyrights for components of this work owned by others than the\nauthor(s) must be honored. abstracting with credit is permitted. to copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. request permissions from permissions@acm.org.\nkdd \u201924, august 25\u201329, 2024, barcelona, spain.\n\u00a92024 copyright held by the owner/author(s). publication rights licensed to acm.\nacm isbn 979-8-4007-0490-1/24/08\nhttps://doi.org/10.1145/3637528.3671946acm reference format:\njuzheng zhang, lanning wei, zhen xu, and quanming yao. 2024. heuristic\nlearning with graph neural networks: a unified framework for link\nprediction. in proceedings of the 30th acm sigkdd conference on knowledge\ndiscovery and data mining (kdd \u201924), august 25\u201329, 2024, barcelona, spain.\nacm, new york, ny, usa, 15 pages. https://doi.org/10.1145/3637528.3671946\n1"
    }
]