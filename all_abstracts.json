[
    {
        "url": "https://arxiv.org/pdf/2309.05653",
        "label": 1.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": "hio state university,\u2021hkust,\u25cbuniversity of edinburgh,\u00a701.ai\nyue.149@osu.edu, wenhuchen@uwaterloo.ca\n https://tiger-ai-lab.github.io/mammoth/ \nabstract\nwe introduce mammoth , a series of open-source large language models (llms)\nspecifically t"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09046",
        "label": 1.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": "ations and promote climate actions and sustainable investment deci-\nsions. the codes are available:  https://github.com/yvnminc/exioml \n1 i ntroduction\nthe increase in greenhouse gas (ghg) emissions due to fossil-fuel-driven economic d"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08863",
        "label": 0.0,
        "abstract": "cad (computer-aided design) plays a crucial role in mechanical\nindustry, where large numbers of similar-shaped cad parts are\noften created. efficiently reusing these parts is key to reducing\ndesign and production costs for enterprises. retrieval systems are\nvital for achieving cad reuse, but the complex shapes of cad\nmodels are difficult to accurately describe using text or keywords,\nmaking traditional retrieval methods ineffective. while existing\nrepresentation learning approaches have been developed for cad,\nmanually labeling similar samples in these methods is expensive.\nadditionally, cad models\u2019 unique parameterized data structure\npresents challenges for applying existing 3d shape representation\nlearning techniques directly. in this work, we propose gc-cad, a\nself-supervised contrastive graph neural network-based method for\nmechanical cad retrieval that directly models parameterized cad\nraw files. gc-cad consists of two key modules: structure-aware\nrepresentation learning and constrastive graph learning framework.\nthe method leverages graph neural networks to extract both geo-\nmetric and topological information from cad models, generating\nfeature representations. we then introduce a simple yet effective\ncontrastive graph learning framework approach, enabling the model\nto train without manual labels and generate retrieval-ready repre-\nsentations. experimental results on four datasets including human\nevaluation demonstrate that the proposed method achieves sig-\nnificant accuracy improvements and up to 100 times efficiency\nimprovement over the baseline methods.\nkeywords\nself-supervised learning, graph neural network, computer-aided\ndesign\n1",
        "context": "ng have shown promise for vari-\nous cad tasks, including segmentation [ 26,31] and assembly [ 47],\n3 https://www.cadenas.de/tl_files/cadenas/downloads/pdf/produktflyer/en \n/cadenas_partsolutions_brochure_en.pdf\n4www.cadenas.de/brochure/geosearcharxiv:2406.08863v2  [cs.ir"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08866",
        "label": 0.0,
        "abstract": ". feature alignment serves as the primary mechanism for\nfusing multimodal data. we put forth a feature alignment approach\nthat achieves full integration of multimodal information. this is ac-\ncomplished via an alternating process of shifting and expanding fea-\nture representations across modalities to obtain a consistent uni-\nfied representation in a joint feature space. the proposed technique\ncan reliably capture high-level interplay between features originating\nfrom distinct modalities. consequently, substantial gains in multi-\nmodal learning performance are attained. additionally, we demon-\nstrate the superiority of our approach over other prevalent multi-\nmodal fusion schemes on a range of tasks. extensive experimental\nevaluation conducted on multimodal datasets comprising time series,\nimage, and text demonstrates that our method achieves state-of-the-\nart results.\n1",
        "context": "m and shift is all you need\njiahao qina;*\naxi\u2019an jiaotong-liverpool university\norcid id: jiahao qin  https://orcid.org/https://orcid.org/0000-0002-0551-4647 \nabstract. feature alignment serves as the primary mechanism for\nfusing multimodal data. we put fort"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08827",
        "label": 1.0,
        "abstract": "it has been shown that the effectiveness of graph convolutional\nnetwork (gcn) for recommendation is attributed to the spectral\ngraph filtering. most gcn-based methods consist of a graph filter\nor followed by a low-rank mapping optimized based on supervised\ntraining. however, we show two limitations suppressing the power\nof graph filtering: (1) lack of generality. due to the varied noise\ndistribution, graph filters fail to denoise sparse data where noise is\nscattered across all frequencies, while supervised training results\nin worse performance on dense data where noise is concentrated\nin middle frequencies that can be removed by graph filters without\ntraining. (2) lack of expressive power. we theoretically show that\nlinear gcn (lgcn) that is effective on collaborative filtering (cf)\ncannot generate arbitrary embeddings, implying the possibility that\noptimal data representation might be unreachable.\nto tackle the first limitation, we show close relation between\nnoise distribution and the sharpness of spectrum where a sharper\nspectral distribution is more desirable causing data noise to be\nseparable from important features without training. based on this\nobservation, we propose a generalized graph normalization ( g2n)\nwith hyperparameters adjusting the sharpness of spectral distri-\nbution in order to redistribute data noise to assure that it can be\nremoved by graph filtering without training. as for the second\nlimitation, we propose an individualized graph filter (igf) adapt-\ning to the different confidence levels of the user preference that\ninteractions can reflect, which is proved to be able to generate ar-\nbitrary embeddings. by simplifying lgcn, we further propose a\nsimplified graph filtering for cf (sgfcf)1which only requires the\ntop-\ud835\udc3esingular values for recommendation. finally, experimental\nresults on four datasets with different density settings demonstrate\nthe effectiveness and efficiency of our proposed methods.\n1https://github.com/tanatosuu/sgfcf\npermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. copyrights for components of this work owned by others than acm\nmust be honored. abstracting with credit is permitted. to copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. request permissions from permissions@acm.org.\nconference acronym \u2019xx, june 03\u201305, 2018, woodstock, ny\n\u00a92018 association for computing machinery.\nacm isbn 978-1-4503-xxxx-x/18/06. . . $15.00\nhttps://doi.org/xxxxxxx.xxxxxxxccs concepts\n\u2022information systems \u2192recommender systems .\nkeywords\nrecommender system, collaborative filtering, graph convolu-\ntional network\nacm reference format:\nshaowen peng, xin liu, kazunari sugiyama, and tsunenori mine. 2018.\nhow powerful is graph filtering for recommendation. in proceedings of\nmake sure to enter the correct conference title from your rights confirmation\nemai (conference acronym \u2019xx). acm, new york, ny, usa, 12 pages. https:\n//doi.org/xxxxxxx.xxxxxxx\n1",
        "context": "h different density settings demonstrate\nthe effectiveness and efficiency of our proposed methods.\n1 https://github.com/tanatosuu/sgfcf \npermission to make digital or hard copies of all or part of this work for personal or\nclassroom use"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08920",
        "label": 0.0,
        "abstract": "novel view acoustic synthesis (nv as) aims to render binaural audio at any target\nviewpoint, given a mono audio emitted by a sound source at a 3d scene. existing\nmethods have proposed nerf-based implicit models to exploit visual cues as a\ncondition for synthesizing binaural audio. however, in addition to low efficiency\noriginating from heavy nerf rendering, these methods all have a limited ability of\ncharacterizing the entire scene environment such as room geometry, material prop-\nerties, and the spatial relation between the listener and sound source. to address\nthese issues, we propose a novel audio-visual gaussian splatting (a v-gs) model.\nto obtain a material-aware and geometry-aware condition for audio synthesis, we\nlearn an explicit point-based scene representation with an audio-guidance param-\neter on locally initialized gaussian points, taking into account the space relation\nfrom the listener and sound source. to make the visual scene model audio adaptive,\nwe propose a point densification and pruning strategy to optimally distribute the\ngaussian points, with the per-point contribution in sound propagation (e.g., more\npoints needed for texture-less wall surfaces as they affect sound path diversion).\nextensive experiments validate the superiority of our a v-gs over existing alterna-\ntives on the real-world rwas and simulation-based soundspaces datasets. project\npage: https://surrey-uplab.github.io/research/avgs/\n1",
        "context": "ting alterna-\ntives on the real-world rwas and simulation-based soundspaces datasets. project\npage:  https://surrey-uplab.github.io/research/avgs/ \n1 introduction\nnovel view synthesis [ 25,1,14] allows to generate images for any target viewpoints,"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08863",
        "label": 0.0,
        "abstract": "cad (computer-aided design) plays a crucial role in mechanical\nindustry, where large numbers of similar-shaped cad parts are\noften created. efficiently reusing these parts is key to reducing\ndesign and production costs for enterprises. retrieval systems are\nvital for achieving cad reuse, but the complex shapes of cad\nmodels are difficult to accurately describe using text or keywords,\nmaking traditional retrieval methods ineffective. while existing\nrepresentation learning approaches have been developed for cad,\nmanually labeling similar samples in these methods is expensive.\nadditionally, cad models\u2019 unique parameterized data structure\npresents challenges for applying existing 3d shape representation\nlearning techniques directly. in this work, we propose gc-cad, a\nself-supervised contrastive graph neural network-based method for\nmechanical cad retrieval that directly models parameterized cad\nraw files. gc-cad consists of two key modules: structure-aware\nrepresentation learning and constrastive graph learning framework.\nthe method leverages graph neural networks to extract both geo-\nmetric and topological information from cad models, generating\nfeature representations. we then introduce a simple yet effective\ncontrastive graph learning framework approach, enabling the model\nto train without manual labels and generate retrieval-ready repre-\nsentations. experimental results on four datasets including human\nevaluation demonstrate that the proposed method achieves sig-\nnificant accuracy improvements and up to 100 times efficiency\nimprovement over the baseline methods.\nkeywords\nself-supervised learning, graph neural network, computer-aided\ndesign\n1",
        "context": "ng have shown promise for vari-\nous cad tasks, including segmentation [ 26,31] and assembly [ 47],\n3 https://www.cadenas.de/tl_files/cadenas/downloads/pdf/produktflyer/en \n/cadenas_partsolutions_brochure_en.pdf\n4www.cadenas.de/brochure/geosearcharxiv:2406.08863v2  [cs.ir"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09021",
        "label": 0.0,
        "abstract": "the diversity of recommendation is equally crucial as accuracy in\nimproving user experience. existing studies, e.g., determinantal\npoint process (dpp) and maximal marginal relevance (mmr), em-\nploy a greedy paradigm to iteratively select items that optimize both\naccuracy and diversity. however, prior methods typically exhibit\nquadratic complexity, limiting their applications to the re-ranking\nstage and are not applicable to other recommendation stages with a\nlarger pool of candidate items, such as the pre-ranking andranking\nstages. in this paper, we propose contextual distillation model\n(cdm), an efficient recommendation model that addresses diver-\nsification, suitable for the deployment in all stages of industrial\nrecommendation pipelines. specifically, cdm utilizes the candidate\nitems in the same user request as context to enhance the diversifi-\ncation of the results. we propose a contrastive context encoder that\nemploys attention mechanisms to model both positive and negative\ncontexts. for the training of cdm, we compare each target item\nwith its context embedding and utilize the knowledge distillation\nframework to learn the win probability of each target item under the\nmmr algorithm, where the teacher is derived from mmr outputs.\nduring inference, ranking is performed through a linear combina-\ntion of the recommendation and student model scores, ensuring\nboth diversity and efficiency. we perform offline evaluations on\ntwo industrial datasets and conduct online a/btest of cdm on the\nshort-video platform kuaishou . the considerable enhancements\n\u2217equal contributions.\n\u2020corresponding author.\npermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. copyrights for components of this work owned by others than the\nauthor(s) must be honored. abstracting with credit is permitted. to copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. request permissions from permissions@acm.org.\nkdd \u201924, august 25\u201329, 2024, barcelona, spain\n\u00a92024 copyright held by the owner/author(s). publication rights licensed to acm.\nacm isbn 979-8-4007-0490-1/24/08. . . $15.00\nhttps://doi.org/10.1145/3637528.3671514observed in both recommendation quality and diversity, as shown\nby metrics, provide strong superiority for the effectiveness of cdm.\nccs concepts\n\u2022information systems \u2192recommender systems .\nkeywords\nrecommender system, knowledge distillation, diversified recom-\nmendation\nacm reference format:\nfan li, xu si, shisong tang, dingmin wang, kunyan han, bing han, guorui\nzhou, yang song, and hechang chen. 2024. contextual distillation model\nfor diversified recommendation. in proceedings of the 30th acm sigkdd\nconference on knowledge discovery and data mining (kdd \u201924), august\n25\u201329, 2024, barcelona, spain. acm, new york, ny, usa, 10 pages. https:\n//doi.org/10.1145/3637528.3671514\n1",
        "context": "ch as the determinantal point process (dpp) [ 5]\n1https://www.tiktok.com/\n2https://www.douyin.com/\n3 https://www.kuaishou.com/new-recoarxiv:2406.09021v1   [cs.ir]  13 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09215",
        "label": 0.0,
        "abstract": "recommender systems aim to predict personalized rankings based on user pref-\nerence data. with the rise of language models (lms), lm-based recommenders\nhave been widely explored due to their extensive world knowledge and power-\nful reasoning abilities. most of the lm-based recommenders convert historical\ninteractions into language prompts, pairing with a positive item as the target re-\nsponse and fine-tuning lm with a language modeling loss. however, the current\nobjective fails to fully leverage preference data and is not optimized for personal-\nized ranking tasks, which hinders the performance of lm-based recommenders.\ninspired by the current advancement of direct preference optimization (dpo)\nin human preference alignment and the success of softmax loss in recommenda-\ntions, we propose softmax-dpo ( s-dpo ) to instill ranking information into the\nlm to help lm-based recommenders distinguish preferred items from negatives,\nrather than solely focusing on positives. specifically, we incorporate multiple\nnegatives in user preference data and devise an alternative version of dpo loss\ntailored for lm-based recommenders, connected to softmax sampling strategies.\ntheoretically, we bridge s-dpo with the softmax loss over negative sampling and\nfind that it has a side effect of mining hard negatives, which assures its excep-\ntional capabilities in recommendation tasks. empirically, extensive experiments\nconducted on three real-world datasets demonstrate the superiority of s-dpo to\neffectively model user preference and further boost recommendation performance\nwhile mitigating the data likelihood decline issue of dpo. our codes are available\nathttps://github.com/chenyuxin1999/s-dpo .\n1",
        "context": "on performance\nwhile mitigating the data likelihood decline issue of dpo. our codes are available\nat https://github.com/chenyuxin1999/s-dpo  .\n1 introduction\nrecommender systems aim to predict personalized rankings based on user preference "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08922",
        "label": 0.0,
        "abstract": "with the launch of chatgpt, large language\nmodels (llms) have attracted global attention.\nin the realm of article writing, llms have wit-\nnessed extensive utilization, giving rise to con-\ncerns related to intellectual property protection,\npersonal privacy, and academic integrity. in re-\nsponse, ai-text detection has emerged to distin-\nguish between human and machine-generated\ncontent. however, recent research indicates\nthat these detection systems often lack robust-\nness and struggle to effectively differentiate\nperturbed texts. currently, there is a lack of\nsystematic evaluations regarding detection per-\nformance in real-world applications, and a com-\nprehensive examination of perturbation tech-\nniques and detector robustness is also absent.\nto bridge this gap, our work simulates real-\nworld scenarios in both informal and profes-\nsional writing, exploring the out-of-the-box per-\nformance of current detectors. additionally, we\nhave constructed 12 black-box text perturba-\ntion methods to assess the robustness of current\ndetection models across various perturbation\ngranularities. furthermore, through adversarial\nlearning experiments, we investigate the impact\nof perturbation data augmentation on the robust-\nness of ai-text detectors. we have released our\ncode and data at https://github.com/zhouy\ning20/ai-text-detector-evaluation .\n1",
        "context": " before their\n1https://copyleaks.com/ai-content-detector\n2https://github.com/declipsonator/gptzzzs\n3 https://github.com/obaskly/aitextdetectionby \npassarxiv:2406.08922v1  [cs.cl]  13 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08587",
        "label": 1.0,
        "abstract": "computer science (cs) stands as a testament to the intricacies of human intelli-\ngence, profoundly advancing the development of artificial intelligence and modern\nsociety. however, the current community of large language models (llms) overly\nfocuses on benchmarks for analyzing specific foundational skills (e.g. mathematics\nand code generation), neglecting an all-round evaluation of the computer science\nfield. to bridge this gap, we introduce cs-bench, the first bilingual (chinese-\nenglish) benchmark dedicated to evaluating the performance of llms in computer\nscience. cs-bench comprises approximately 5k meticulously curated test samples,\ncovering 26 subfields across 4 key areas of computer science, encompassing var-\nious task forms and divisions of knowledge and reasoning. utilizing cs-bench,\nwe conduct a comprehensive evaluation of over 30 mainstream llms, revealing\nthe relationship between cs performance and model scales. we also quantita-\ntively analyze the reasons for failures in existing llms and highlight directions\nfor improvements, including knowledge supplementation and cs-specific reason-\ning. further cross-capability experiments show a high correlation between llms\u2019\ncapabilities in computer science and their abilities in mathematics and coding.\nmoreover, expert llms specialized in mathematics and coding also demonstrate\nstrong performances in several cs subfields. looking ahead, we envision cs-\nbench serving as a cornerstone for llm applications in the cs field and paving\nnew avenues in assessing llms\u2019 diverse reasoning capabilities. the cs-bench\ndata and evaluation code are available at https://github.com/csbench/csbench.\n1",
        "context": "essing llms\u2019 diverse reasoning capabilities. the cs-bench\ndata and evaluation code are available at  https://github.com/csbench/csbench. \n1 introduction\nserving as the cornerstone of the modern information revolution, the significance of"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08931",
        "label": 0.0,
        "abstract": "advent of modern deep learning techniques has given rise to ad-\nvancements in the field of speech emotion recognition (ser).\nhowever, most systems prevalent in the field fail to general-\nize to speakers not seen during training. this study focuses\non handling challenges of multilingual ser, specifically on un-\nseen speakers. we introduce camulenet, a novel architecture\nleveraging co-attention based fusion and multitask learning to\naddress this problem. additionally, we benchmark pretrained\nencoders of whisper, hubert, wav2vec2.0, and wavlm us-\ning 10-fold leave-speaker-out cross-validation on five exist-\ning multilingual benchmark datasets: iemocap, ra vdess,\ncrema-d, emodb and cafe and, release a novel dataset for\nser on the hindi language (bhavvani). camulenet shows an\naverage improvement of approximately 8% over all benchmarks\non unseen speakers determined by our cross-validation strategy.\nindex terms : speaker emotion recognition, co-attention, mul-\ntitask learning, new dataset\n1.",
        "context": "itecture through a multitask\nsetup to improve performance on unseen speakers\u2019 emotion\nrecognition.\n1 https://github.com/arnav10goel/camulenetarxiv:2406.08931v1   [cs.cl]  13 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08973",
        "label": 0.0,
        "abstract": "following the success of the in-context learning paradigm in large-scale language\nand computer vision models, the recently emerging field of in-context reinforce-\nment learning is experiencing a rapid growth. however, its development has\nbeen held back by the lack of challenging benchmarks, as all the experiments\nhave been carried out in simple environments and on small-scale datasets. we\npresent xland-100b , a large-scale dataset for in-context reinforcement learning\nbased on the xland-minigrid environment, as a first step to alleviate this prob-\nlem. it contains complete learning histories for nearly 30,000different tasks,\ncovering 100b transitions and 2.5b episodes. it took 50,000gpu hours to col-\nlect the dataset, which is beyond the reach of most academic labs. along with\nthe dataset, we provide the utilities to reproduce or expand it even further. with\nthis substantial effort, we aim to democratize research in the rapidly growing\nfield of in-context reinforcement learning and provide a solid foundation for fur-\nther scaling. the code is open-source and available under apache 2.0 licence at\nhttps://github.com/dunno-lab/xland-minigrid-datasets .\n1",
        "context": "foundation for fur-\nther scaling. the code is open-source and available under apache 2.0 licence at\n https://github.com/dunno-lab/xland-minigrid-datasets  .\n1 introduction\nin-context learning, i.e. the ability to learn new tasks purely based on examples "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08598",
        "label": 1.0,
        "abstract": "the rapid advancement of large language models (llms) necessitates robust\nand challenging benchmarks. leaderboards like chatbot arena rank llms based\non how well their responses align with human preferences. however, many tasks\nsuch as those related to emotional intelligence, creative writing, or persuasiveness,\nare highly subjective and often lack majoritarian human agreement. judges may\nhave irreconcilable disagreements about what constitutes a better response. to\naddress the challenge of ranking llms on highly subjective tasks, we propose\na novel benchmarking framework, the language model council (lmc) . the\nlmc operates through a democratic process to: 1) formulate a test set through\nequal participation, 2) administer the test among council members, and 3) evaluate\nresponses as a collective jury. we deploy a council of 20 newest llms on an\nopen-ended emotional intelligence task: responding to interpersonal dilemmas.\nour results show that the lmc produces rankings that are more separable, robust,\nand less biased than those from any individual llm judge, and is more consistent\nwith a human-established leaderboard compared to other benchmarks.\n1",
        "context": "rry2,\n1predibase,2bocconi university\njustin@predibase.com\n{flor.plaza, amanda.cercas}@unibocconi.it\n https://www.llm-council.com \nabstract\nthe rapid advancement of large language models (llms) necessitates robust\nand challenging "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08979",
        "label": 1.0,
        "abstract": "the latest breakthroughs in large language\nmodels (llms), e.g., chatdev, have catalyzed\nprofound transformations, particularly through\nmulti-agent collaboration for software devel-\nopment. llm agents can collaborate in teams\nlike humans, and follow the waterfall model\nto sequentially work on requirements analysis,\ndevelopment, review, testing, and other phases\nto perform autonomous software generation.\nhowever, for an agent team, each phase in a\nsingle development process yields only one pos-\nsible outcome. this results in the completion\nof only one development chain, thereby losing\nthe opportunity to explore multiple potential\ndecision paths within the solution space. con-\nsequently, this may lead to obtaining subop-\ntimal results. to address this challenge, we\nintroduce cross-teamcollaboration (ctc),\na scalable multi-team framework that enables\norchestrated teams to jointly propose various\ndecisions and communicate with their insights\nin a cross-team collaboration environment for\nsuperior content generation. experimental re-\nsults in software development reveal a notable\nincrease in quality compared to state-of-the-\nart baselines, underscoring the efficacy of our\nframework. the significant improvements in\nstory generation demonstrate the promising\ngeneralization ability of our framework across\nvarious domains. we anticipate that our work\nwill guide llm agents towards a cross-team\nparadigm and contribute to their significant\ngrowth in but not limited to software devel-\nopment. the code and data will be available at\nhttps://github.com/openbmb/chatdev .\n1",
        "context": "ificant\ngrowth in but not limited to software devel-\nopment. the code and data will be available at\n https://github.com/openbmb/chatdev  .\n1 introduction\nin the field of software development, the complex-\nity is profound, requiring a sy"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09043",
        "label": 0.0,
        "abstract": "crosswords are a form of word puzzle that re-\nquire a solver to demonstrate a high degree\nof proficiency in natural language understand-\ning, wordplay, reasoning, and world knowl-\nedge, along with adherence to character and\nlength constraints. in this paper we tackle the\nchallenge of solving crosswords with large\nlanguage models (llms). we demonstrate\nthat the current generation of state-of-the art\n(sota) language models show significant com-\npetence at deciphering cryptic crossword clues,\nand outperform previously reported sota re-\nsults by a factor of 2-3 in relevant benchmarks.\nwe also develop a search algorithm that builds\noff this performance to tackle the problem of\nsolving full crossword grids with llms for\nthe very first time, achieving an accuracy of\n93% on new york times crossword puzzles.\ncontrary to previous work in this area which\nconcluded that llms lag human expert perfor-\nmance significantly, our research suggests this\ngap is a lot narrower.\n1",
        "context": "length constraints, which is\na critical component of solving crossword puzzles,\n1example taken from  https://lovattspuzzles.com  .\n1arxiv:2406.09043v2  [cs.cl]  14 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09009",
        "label": 1.0,
        "abstract": "the transformer model has shown leading performance in time se-\nries forecasting. nevertheless, in some complex scenarios, it tends\nto learn low-frequency features in the data and overlook high-\nfrequency features, showing a frequency bias. this bias prevents\nthe model from accurately capturing important high-frequency\ndata features. in this paper, we undertake empirical analyses to\nunderstand this bias and discover that frequency bias results from\nthe model disproportionately focusing on frequency features with\nhigher energy. based on our analysis, we formulate this bias and\npropose fredformer , a transformer-based framework designed\nto mitigate frequency bias by learning features equally across dif-\nferent frequency bands. this approach prevents the model from\noverlooking lower amplitude features important for accurate fore-\ncasting. extensive experiments show the effectiveness of our pro-\nposed approach, which can outperform other baselines in differ-\nent real-world time-series datasets. furthermore, we introduce a\nlightweight variant of the fredformer with an attention matrix\napproximation, which achieves comparable performance but with\nmuch fewer parameters and lower computation costs. the code is\navailable at: https://github.com/chenzrg/fredformer\nccs concepts\n\u2022computing methodologies \u2192artificial intelligence ;neural\nnetworks .\nkeywords\ntime series forecasting, deep learning\nacm reference format:\nxihao piao, zheng chen, taichi murayama, yasuko matsubara, and yasushi\nsakurai. 2024. fredformer: frequency debiased transformer for time\nseries forecasting . in proceedings of the 30th acm sigkdd conference\non knowledge discovery and data mining (kdd \u201924), august 25\u201329, 2024,\npermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. copyrights for components of this work owned by others than the\nauthor(s) must be honored. abstracting with credit is permitted. to copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. request permissions from permissions@acm.org.\nkdd \u201924, august 25\u201329, 2024, barcelona, spain\n\u00a92024 copyright held by the owner/author(s). publication rights licensed to acm.\nacm isbn 979-8-4007-0490-1/24/08\nhttps://doi.org/10.1145/3637528.3671928barcelona, spain. acm, new york, ny, usa, 17 pages. https://doi.org/10.\n1145/3637528.3671928\n1",
        "context": "e performance but with\nmuch fewer parameters and lower computation costs. the code is\navailable at:  https://github.com/chenzrg/fredformer \nccs concepts\n\u2022computing methodologies \u2192artificial intelligence ;neural\nnetworks .\nkeywords\ntime ser"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08680",
        "label": 0.0,
        "abstract": "automatically assessing classroom discussion quality is be-\ncoming increasingly feasible with the help of new nlp ad-\nvancements such as large language models (llms). in this\nwork, we examine how the assessment performance of 2\nllms interacts with 3 factors that may affect performance:\ntask formulation, context length, and few-shot examples.\nwe also explore the computational efficiency and predic-\ntive consistency of the 2 llms. our results suggest that\nthe 3 aforementioned factors do affect the performance of\nthe tested llms and there is a relation between consistency\nand performance. we recommend a llm-based assessment\napproach that has a good balance in terms of predictive per-\nformance, computational efficiency, and consistency.\nkeywords\nclassroom discussion, large language models, scoring\n1.",
        "context": "\nhigh-performance approaches. to support reproducibility,\nwe also make our source code available at  https://github. \ncom/nhattlm95/llm_for_classroom_discussion .\n2. related work\nresearchers have measured classroom di"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08707",
        "label": NaN,
        "abstract": "multimodal large language models (mllms) are trained on a large amount of\ntext-image data. while most mllms are trained on caption-like data only, alayrac\net al. [2022] showed that additionally training them on interleaved sequences of text\nand images can lead to the emergence of in-context learning capabilities. however,\nthe dataset they used, m3w, is not public and is only in english. there have\nbeen attempts to reproduce their results but the released datasets are english-only.\nin contrast, current multilingual and multimodal datasets are either composed\nof caption-like only or medium-scale or fully private data. this limits mllm\nresearch for the 7,000 other languages spoken in the world. we therefore introduce\nmoscar, to the best of our knowledge the first large-scale multilingual and\nmultimodal document corpus crawled from the web. it covers 163 languages,\n315m documents, 214b tokens and 1.2b images. we carefully conduct a set of\nfiltering and evaluation steps to make sure moscar is sufficiently safe, diverse and\nof good quality. we additionally train two types of multilingual model to prove the\nbenefits of moscar: (1) a model trained on a subset of moscar and captioning\ndata and (2) a model train on captioning data only. the model additionally trained\non moscar shows a strong boost in few-shot learning performance across various\nmultilingual image-text tasks and benchmarks, confirming previous findings for\nenglish-only mllms. the dataset can be accessed here.2\n1",
        "context": "rain mllms are either private [alayrac et al., 2022],\n\u2217correspondence to matthieu.futeral@inria.fr\n2 https://oscar-project.github.io/documentation/versions/moscar/ \npreprint. under review.arxiv:2406.08707v1  [cs.cl]  13 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09031",
        "label": 1.0,
        "abstract": "graph pooling has gained attention for its ability to obtain effective node and graph representations\nfor various downstream tasks. despite the recent surge in graph pooling approaches, there is a lack of\nstandardized experimental settings and fair benchmarks to evaluate their performance. to address\nthis issue, we have constructed a comprehensive benchmark that includes 15 graph pooling methods\nand 21 different graph datasets. this benchmark systematically assesses the performance of graph\npooling methods in three dimensions, i.e., effectiveness, robustness, and generalizability. we first\nevaluate the performance of these graph pooling approaches across different tasks including graph\nclassification, graph regression and node classification. then, we investigate their performance under\npotential noise attacks and out-of-distribution shifts in real-world scenarios. we also involve detailed\nefficiency analysis and parameter analysis. extensive experiments validate the strong capability and\napplicability of graph pooling approaches in various scenarios, which can provide valuable insights\nand guidance for deep geometric learning research. the source code of our benchmark is available at\nhttps://github.com/goose315/graph_pooling_benchmark .\nkeywords graph pooling; benchmark; graph neural networks; graph machine learning\n1",
        "context": "and guidance for deep geometric learning research. the source code of our benchmark is available at\n https://github.com/goose315/graph_pooling_benchmark  .\nkeywords graph pooling; benchmark; graph neural networks; graph machine learning\n1 introduction\nr"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09052",
        "label": 1.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": "88.5% and 46.6%\naccuracy on mnist and fashionmnist datasets, respectively.\nour code is available at  https://github.com/2younis/dfgr \ni. i ntroduction\nrecent advances in neural networks and deep learning have\nbeen reported to surpass"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08726",
        "label": 0.0,
        "abstract": "in this position paper, we explore standard language ideolo gy in\nlanguagegeneratedbylargelanguagemodels(llms).first, weout-\nlinehowstandardlanguageideologyisre\ufb02ectedandreinfor ced in\nllms. we then present a taxonomy of open problems regarding\nstandard language ideology in ai-generated language with i mpli-\ncations for minoritized language communities. we introduc e the\nconcept of standard ai-generated language ideology, the pr ocess\nbywhich ai-generated language regards standard american e ng-\nlish(sae)asalinguisticdefaultandreinforcesalinguist icbiasthat\nsae is the most \u201cappropriate\u201d language. finally, we discuss ten-\nsions that remain, including re\ufb02ecting on what desirable sy stem\nbehaviorlookslike,aswellasadvantagesanddrawbacksofg ener-\native ai tools imitating\u2014or often not\u2014di\ufb00erent english lan guage\nvarieties.throughout,wediscussstandardlanguageideol ogyasa\nmanifestation of existing global power structures in and th rough\nai-generated language before ending with questions to move to-\nwards alternative, moreemancipatorydigital futures.\n1",
        "context": "hierarchybetweenlanguagevarie ties.in\nthis paper,weillustratehowand in whatways standard langu age\n1 https://www.demandsage.com/chatgpt-statistics/ \npermission to make digital or hard copies of all or part of thi s work for personal or\nclassroomuse"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08757",
        "label": 0.0,
        "abstract": "accurately identifying and organizing textual content is crucial for the automation\nof document processing in the field of form understanding. existing datasets, such\nas funsd and xfund, support entity classification and relationship prediction\ntasks but are typically limited to local and entity-level annotations. this limitation\noverlooks the hierarchically structured representation of documents, constraining\ncomprehensive understanding of complex forms. to address this issue, we present\nthe srfund, a hierarchically structured multi-task form understanding bench-\nmark. srfund provides refined annotations on top of the original funsd and\nxfund datasets, encompassing five tasks: (1) word to text-line merging , (2)\ntext-line to entity merging , (3) entity category classification , (4) item table lo-\ncalization , and (5) entity-based full-document hierarchical structure recovery .\nwe meticulously supplemented the original dataset with missing annotations at\nvarious levels of granularity and added detailed annotations for multi-item table\nregions within the forms. additionally, we introduce global hierarchical struc-\nture dependencies for entity relation prediction tasks, surpassing traditional local\nkey-value associations. the srfund dataset includes eight languages including\nenglish, chinese, japanese, german, french, spanish, italian, and portuguese ,\nmaking it a powerful tool for cross-lingual form understanding. extensive exper-\nimental results demonstrate that the srfund dataset presents new challenges\nand significant opportunities in handling diverse layouts and global hierarchical\nstructures of forms, thus providing deep insights into the field of form understand-\ning. the original dataset and implementations of baseline methods are available at\nhttps://sprateam-ustc.github.io/srfund .\n1",
        "context": "litates information dissemination\n\u2217corresponding author.\n2https://www.irs.gov/pub/irs-pdf/p55b.pdf\n3 https://www.pitneybowes.com/content/dam/pitneybowes/us/en/shipping-index/ \n23-mktc-03596-2023_global_parcel_shipping_index_ebook-web.pdf\npreprint. under review.arxiv:2406.087"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08796",
        "label": 1.0,
        "abstract": "instruction tuning has emerged as a powerful\ntechnique, significantly boosting zero-shot per-\nformance on unseen tasks. while recent work\nhas explored cross-lingual generalization by ap-\nplying instruction tuning to multilingual mod-\nels, previous studies have primarily focused\non english, with a limited exploration of non-\nenglish tasks. for an in-depth exploration of\ncross-lingual generalization in instruction tun-\ning, we perform instruction tuning individu-\nally for two distinct language meta-datasets.\nsubsequently, we assess the performance on\nunseen tasks in a language different from the\none used for training. to facilitate this inves-\ntigation, we introduce a novel non-english\nmeta-dataset named \"korani\" (korean natu-\nral instruction), comprising 51 korean bench-\nmarks. moreover, we design cross-lingual tem-\nplates to mitigate discrepancies in language\nand instruction-format of the template between\ntraining and inference within the cross-lingual\nsetting. our experiments reveal consistent im-\nprovements through cross-lingual generaliza-\ntion in both english and korean, outperforming\nbaseline by average scores of 20.7% and 13.6%,\nrespectively. remarkably, these enhancements\nare comparable to those achieved by monolin-\ngual instruction tuning and even surpass them\nin some tasks. the result underscores the sig-\nnificance of relevant data acquisition across lan-\nguages over linguistic congruence with unseen\ntasks during instruction tuning1.\n1",
        "context": "i et al., 2022;\nsanh et al., 2022; wang et al., 2022; chung et al.,\n\u2217indicates equal contribution.\n1 https://github.com/chlee0801/ \nkorani-instruction-tuning2022; luccioni et al., 2022; ouyang et al., 2022;\nzhong et al., 2021). sev"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09056",
        "label": 0.0,
        "abstract": "the proliferation of large language models (llms) has significantly enhanced text generation capabilities\nacross various industries. however, these models\u2019 ability to generate human-like text poses substantial\nchallenges in discerning between human and artificial intelligence (ai) authorship. despite the effectiveness\nof existing ai-generated text detectors, their development is hindered by the lack of comprehensive, publicly\navailable benchmarks. current benchmarks are limited to specific scenarios, such as question answering\nand text polishing, and predominantly focus on english texts, failing to capture the diverse applications and\nlinguistic nuances of llms. to address these limitations, this paper constructs a comprehensive bilingual\nbenchmark in both chinese and english to evaluate mainstream ai-generated text detectors. we categorize\nllm text generation into five distinct operations: creation, updating, deletion, rewriting, and translation\n(cudrt ), encompassing all current llms activities. we also establish a robust benchmark evaluation\nframework to support scalable and reproducible experiments. for each cudrt category, we have developed\nextensive datasets to thoroughly assess detector performance. by employing the latest mainstream llms\nspecific to each language, our datasets provide a thorough evaluation environment. extensive experimental\nresults offer critical insights for optimizing ai-generated text detectors and suggest future research directions\nto improve detection accuracy and generalizability across various scenarios. source code is available at\ngithub\u2020.\nk e y w o r d s\nai-generated text, large language models, llms operations, text detection, benchmarking\n1",
        "context": " to produce text content whose authenticity\nis challenging to discern (derner & batisti \u02c7c, 2023).\n\u2020 https://github.com/taozhen1110/cudrt  benchmark\n* these authors contributed equally to this work.currently, there are numerous methods fo"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09105",
        "label": 1.0,
        "abstract": "large vision-language models (lvlms) have demonstrated outstanding per-\nformance in various general multimodal applications such as image recogni-\ntion and visual reasoning, and have also shown promising potential in special-\nized domains. however, the application potential of lvlms in the insurance\ndomain\u2014characterized by rich application scenarios and abundant multimodal\ndata\u2014has not been effectively explored. there is no systematic review of multi-\nmodal tasks in the insurance domain, nor a benchmark specifically designed to\nevaluate the capabilities of lvlms in insurance. this gap hinders the development\nof lvlms within the insurance domain. in this paper, we systematically review and\ndistill multimodal tasks for four representative types of insurance: auto insurance,\nproperty insurance, health insurance, and agricultural insurance. we propose ins-\nmmbench, the first comprehensive lvlms benchmark tailored for the insurance\ndomain. ins-mmbench comprises a total of 2.2k thoroughly designed multiple-\nchoice questions, covering 12 meta-tasks and 22 fundamental tasks. furthermore,\nwe evaluate multiple representative lvlms, including closed-source models such\nas gpt-4o and open-source models like blip-2. this evaluation not only validates\nthe effectiveness of our benchmark but also provides an in-depth performance\nanalysis of current lvlms on various multimodal tasks in the insurance domain.\nwe hope that ins-mmbench will facilitate the further application of lvlms in\nthe insurance domain and inspire interdisciplinary development. our dataset and\nevaluation code are available at https://github.com/fdu-ins/ins-mmbench .\n1",
        "context": " domain and inspire interdisciplinary development. our dataset and\nevaluation code are available at  https://github.com/fdu-ins/ins-mmbench  .\n1 introduction\nin recent years, large language models (llms) have demonstrated remarkably powerfu"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09112",
        "label": 0.0,
        "abstract": "the goal for classification is to correctly assign labels to unseen samples. however, most\nmethods misclassify samples with unseen labels and assign them to one of the known classes.\nopen-set classification (osc) algorithms aim to maximize both closed and open-set recog-\nnition capabilities. recent studies showed the utility of such algorithms on small-scale\ndata sets, but limited experimentation makes it difficult to assess their performances in\nreal-world problems. here, we provide a comprehensive comparison of various osc algo-\nrithms, including training-based (softmax, garbage, eos) and post-processing methods\n(maximum softmax scores, maximum logit scores, openmax, evm, proser), the lat-\nter are applied on features from the former. we perform our evaluation on three large-scale\nprotocols that mimic real-world challenges, where we train on known and negative open-set\nsamples, and test on known and unknown instances. our results show that eos helps to\nimprove performance of almost all post-processing algorithms. particularly, openmax and\nproser are able to exploit better-trained networks, demonstrating the utility of hybrid\nmodels. however, while most algorithms work well on negative test samples \u2013 samples of\nopen-set classes seen during training \u2013 they tend to perform poorly when tested on samples\nof previously unseen unknown classes, especially in challenging conditions.\nkeywords: open-set classification, large-scale evaluation, image classification, deep learn-\ning, reproducible research\n1.",
        "context": "chitectures are created to perform\n\u00a9bisgin, palechor, suter, and g\u00a8 unther.\nlicense: cc-by 4.0, see  https://creativecommons.org/licenses/by/4.0/  .arxiv:2406.09112v1  [cs.cv]  13 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09155",
        "label": 1.0,
        "abstract": "large language models (llms) have demonstrated remarkable capabilities, revo-\nlutionizing the integration of ai in daily life applications. however, they are prone\nto hallucinations, generating claims that contradict established facts, deviating from\nprompts, and producing inconsistent responses when the same prompt is presented\nmultiple times. addressing these issues is challenging due to the lack of com-\nprehensive and easily assessable benchmark datasets. most existing datasets are\nsmall and rely on multiple-choice questions, which are inadequate for evaluating\nthe generative prowess of llms. to measure hallucination in llms, this paper\nintroduces a comprehensive benchmark dataset comprising over 75,000 prompts\nacross eight domains. these prompts are designed to elicit definitive, concise,\nand informative answers. the dataset is divided into two segments: one publicly\navailable for testing and assessing llm performance and a hidden segment for\nbenchmarking various llms. in our experiments, we tested six llms\u2014gpt-3.5,\nllama 2, llama 3, gemini, mixtral, and zephyr\u2014revealing that overall factual\nhallucination ranges from 59% to 82% on the public dataset and 57% to 76% in the\nhidden benchmark. prompt misalignment hallucination ranges from 6% to 95% in\nthe public dataset and 17% to 94% in the hidden counterpart. average consistency\nranges from 21% to 61% and 22% to 63%, respectively. domain-wise analysis\nshows that llm performance significantly deteriorates when asked for specific\nnumeric information while performing moderately with person, location, and date\nqueries. our dataset demonstrates its efficacy and serves as a comprehensive\nbenchmark for llm performance evaluation. our dataset and llms responses are\navailable at https://github.com/ashikiut/defan.\n1",
        "context": "rehensive\nbenchmark for llm performance evaluation. our dataset and llms responses are\navailable at  https://github.com/ashikiut/defan. \n1 introduction\nthe domain of generative artificial intelligence (ai) has witnessed a paradigm shift"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09181",
        "label": 1.0,
        "abstract": "with the rapid development of ai-generated content (aigc) technology, the\nproduction of realistic fake facial images and videos that deceive human visual\nperception has become possible. consequently, various face forgery detection\ntechniques have been proposed to identify such fake facial content. however,\nevaluating the effectiveness and generalizability of these detection techniques re-\nmains a significant challenge. to address this, we have constructed a large-scale\nevaluation benchmark called deepfacegen, aimed at quantitatively assessing the\neffectiveness of face forgery detection and facilitating the iterative development\nof forgery detection technology. deepfacegen consists of 776 ,990real face\nimage/video samples and 773 ,812face forgery image/video samples, generated\nusing 34mainstream face generation techniques. during the construction pro-\ncess, we carefully consider important factors such as content diversity, fairness\nacross ethnicities, and availability of comprehensive labels, in order to ensure\nthe versatility and convenience of deepfacegen. subsequently, deepfacegen is\nemployed in this study to evaluate and analyze the performance of 13mainstream\nface forgery detection techniques from various perspectives. through extensive\nexperimental analysis, we derive significant findings and propose potential direc-\ntions for future research. the code and dataset for deepfacegen are available at\nhttps://github.com/hengruilou/deepfacegen.\n1",
        "context": "e potential direc-\ntions for future research. the code and dataset for deepfacegen are available at\n https://github.com/hengruilou/deepfacegen. \n1 introduction\nin recent years, aigc [ 1] technology has experienced rapid development, significant"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09215",
        "label": 0.0,
        "abstract": "recommender systems aim to predict personalized rankings based on user pref-\nerence data. with the rise of language models (lms), lm-based recommenders\nhave been widely explored due to their extensive world knowledge and power-\nful reasoning abilities. most of the lm-based recommenders convert historical\ninteractions into language prompts, pairing with a positive item as the target re-\nsponse and fine-tuning lm with a language modeling loss. however, the current\nobjective fails to fully leverage preference data and is not optimized for personal-\nized ranking tasks, which hinders the performance of lm-based recommenders.\ninspired by the current advancement of direct preference optimization (dpo)\nin human preference alignment and the success of softmax loss in recommenda-\ntions, we propose softmax-dpo ( s-dpo ) to instill ranking information into the\nlm to help lm-based recommenders distinguish preferred items from negatives,\nrather than solely focusing on positives. specifically, we incorporate multiple\nnegatives in user preference data and devise an alternative version of dpo loss\ntailored for lm-based recommenders, connected to softmax sampling strategies.\ntheoretically, we bridge s-dpo with the softmax loss over negative sampling and\nfind that it has a side effect of mining hard negatives, which assures its excep-\ntional capabilities in recommendation tasks. empirically, extensive experiments\nconducted on three real-world datasets demonstrate the superiority of s-dpo to\neffectively model user preference and further boost recommendation performance\nwhile mitigating the data likelihood decline issue of dpo. our codes are available\nathttps://github.com/chenyuxin1999/s-dpo .\n1",
        "context": "on performance\nwhile mitigating the data likelihood decline issue of dpo. our codes are available\nat https://github.com/chenyuxin1999/s-dpo  .\n1 introduction\nrecommender systems aim to predict personalized rankings based on user preference "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09260",
        "label": 1.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": "tions for ship-based autonomous uav landing and navigation.\nsource code : fdcl-gwu/tnn-mo , video :  https://youtu.be/zg_zvvs8xw8 \ni. introduction\nunmanned aerial vehicles (uavs) have seen a surge in usage across a multitude of in"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09321",
        "label": 1.0,
        "abstract": "jailbreak attacks aim to induce large language models (llms) to generate harmful responses\nfor forbidden instructions, presenting severe misuse threats to llms. up to now, research into\njailbreak attacks and defenses is emerging, however, there is (surprisingly) no consensus on how\nto evaluate whether a jailbreak attempt is successful. in other words, the methods to assess the\nharmfulness of an llm\u2019s response are varied, such as manual annotation or prompting gpt-4\nin specific ways. each approach has its own set of strengths and weaknesses, impacting their\nalignment with human values, as well as the time and financial cost. this diversity in evaluation\npresents challenges for researchers in choosing suitable evaluation methods and conducting fair\ncomparisons across different jailbreak attacks and defenses.\nin this paper, we conduct a comprehensive analysis of jailbreak evaluation methodologies,\ndrawing from nearly ninety jailbreak research released between may 2023 and april 2024. our\nstudy introduces a systematic taxonomy of jailbreak evaluators, offering in-depth insights into\ntheir strengths and weaknesses, along with the current status of their adaptation. moreover,\nto facilitate subsequent research, we propose jailbreakeval (https://github.com/thuccslab/\njailbreakeval ), a user-friendly toolkit focusing on the evaluation of jailbreak attempts. it includes\nvarious well-known evaluators out-of-the-box, so that users can obtain evaluation results with only\na single command. jailbreakeval also allows users to customize their own evaluation workflow\nin a unified framework with the ease of development and comparison. in summary, we regard\njailbreakeval to be a catalyst that simplifies the evaluation process in jailbreak research and\nfosters an inclusive standard for jailbreak evaluation within the community.\n1",
        "context": " status of their adaptation. moreover,\nto facilitate subsequent research, we propose jailbreakeval ( https://github.com/thuccslab/ \njailbreakeval ), a user-friendly toolkit focusing on the evaluation of jailbreak attempts. it inclu"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09326",
        "label": 1.0,
        "abstract": "recently, artificial intelligence techniques for education have been received increas-\ning attentions, while it still remains an open problem to design the effective music\ninstrument instructing systems. although key presses can be directly derived from\nsheet music, the transitional movements among key presses require more extensive\nguidance in piano performance. in this work, we construct a piano-hand motion\ngeneration benchmark to guide hand movements and fingerings for piano playing.\nto this end, we collect an annotated dataset, pianomotion10m , consisting of\n116 hours of piano playing videos from a bird\u2019s-eye view with 10 million annotated\nhand poses. we also introduce a powerful baseline model that generates hand\nmotions from piano audios through a position predictor and a position-guided\ngesture generator. furthermore, a series of evaluation metrics are designed to\nassess the performance of the baseline model, including motion similarity, smooth-\nness, positional accuracy of left and right hands, and overall fidelity of movement\ndistribution. despite that piano key presses with respect to music scores or audios\nare already accessible, pianomotion10m aims to provide guidance on piano\nfingering for instruction purposes. the dataset and source code can be accessed at\nhttps://agnjason.github.io/pianomotion-page .\n1",
        "context": "uidance on piano\nfingering for instruction purposes. the dataset and source code can be accessed at\n https://agnjason.github.io/pianomotion-page  .\n1 introduction\nthe process of learning has been significantly improved with artificial intelligen"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09388",
        "label": 0.0,
        "abstract": "vision and language models (vlms) such as clip have\nshowcased remarkable zero-shot recognition abilities yet\nface challenges in visio-linguistic compositionality, partic-\nularly in linguistic comprehension and fine-grained image-\ntext alignment. this paper explores the intricate relation-\nship between compositionality and recognition \u2013 two piv-\notal aspects of vlm capability. we conduct a compre-\nhensive evaluation of existing vlms, covering both pre-\ntraining approaches aimed at recognition and the fine-\ntuning methods designed to improve compositionality. our\nevaluation employs 12 benchmarks for compositionality,\nalong with 21 zero-shot classification and two retrieval\nbenchmarks for recognition. in our analysis from 274 clip\nmodel checkpoints, we reveal patterns and trade-offs that\nemerge between compositional understanding and recog-\nnition accuracy. ultimately, this necessitates strategic ef-\nforts towards developing models that improve both capabil-\nities, as well as the meticulous formulation of benchmarks\nfor compositionality. we open our evaluation framework at\nhttps://github.com/ytaek-oh/vl_compo .\n1.",
        "context": " the meticulous formulation of benchmarks\nfor compositionality. we open our evaluation framework at\n https://github.com/ytaek-oh/vl_compo  .\n1. introduction\nthe advent of vision and language models (vlms) like\nclip [26] has significantly "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09401",
        "label": 1.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": "xisting benchmarks and in-the-wild evaluation. codes, datasets, and benchmarks\nwill be available at  https://github.com/openrobotlab/embodiedscan  .\npreprint. under review.arxiv:2406.09401v1  [cs.cv]  13 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09406",
        "label": 1.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": "2jiaming hu2afshin dehghan2amir zamir1\n1swiss federal institute of technology lausanne (epfl)2apple\n https://4m.epfl.ch \nimage\nmetadata\norig.res.:512x512\ncolorfulness:35%\ncontrast:45%\nbrightness:60%\nsaturation:40% \n\u2026\nrgb"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09410",
        "label": 1.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": "both obd\nand sgg tasks. the rsg dataset and sai-oriented toolkit will be made publicly available at  https://linlin-dev.github.io/project/rsg. \nindex terms \u2014scene graph generation (sgg) benchmark, large-size satellite imagery, object detection"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09411",
        "label": 1.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": "o <fwang598@usc.edu; xingyuf2@seas.upenn.edu>.\n\u2020equal contribution; alphabetic order.\nproject page:  https://huggingface.co/datasets/muirbench/muirbench \npreprint. under review.arxiv:2406.09411v1  [cs.cv]  13 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09979",
        "label": 1.0,
        "abstract": "large language models (llms) excel in natural language\ntasks but face limitations due to static training datasets, result-\ning in outdated or contextually shallow responses. retrieval-\naugmented generation (rag) addresses this by integrating\nreal-time external knowledge, enhancing model accuracy and\ncredibility, especially for knowledge-intensive tasks. how-\never, rag-enhanced llms struggle with long contexts, caus-\ning them to \u201dchoke\u201d on information overload, compromising\nresponse quality. recent rag applications use hierarchical\ndata structures for storing documents, organized at various\nlevels of summarization and information density. in this con-\ntext, we introduce hiro (hierarchical information retrieval\noptimization), a novel querying approach for rag appli-\ncations using hierarchical structures for storing documents.\nhiro employs dfs-based recursive similarity score calcula-\ntion and branch pruning to minimize the context returned to\nthe llm without informational loss. hiro outperforms ex-\nisting querying mechanisms on the narrativeqa dataset by\nan absolute performance gain of 10.85%.\ncode \u2014 https://github.com/krishgoel/hiro",
        "context": "ng querying mechanisms on the narrativeqa dataset by\nan absolute performance gain of 10.85%.\ncode \u2014  https://github.com/krishgoel/hiro \nintroduction\nthe advent of large language models (llms) has brought\nabout a significant transformat"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09464",
        "label": 0.0,
        "abstract": "large language models have taken the cognitive sci-\nence world by storm. it is perhaps timely now to take\nstock of the various research paradigms that have been\nused to make scientific inferences about \u201ccognition\u201d in\nthese models or about human cognition. we review sev-\neral emerging research paradigms\u2014gpt-ology, llms-\nas-computational-models, and \u201csilicon sampling\u201d\u2014 and\nreview recent papers that have used llms under these\nparadigms. in doing so, we discuss their claims as well\nas challenges to scientific inference under these vari-\nous paradigms. we highlight several outstanding is-\nsues about llms that have to be addressed to push\nour science forward: closed-source vs open-sourced mod-\nels; (the lack of visibility of) training data; and repro-\nducibility in llm research, including forming conven-\ntions on new task \u201chyperparameters\u201d like instructions\nand prompts.\nkeywords: large language models; cognitive science",
        "context": " some researchers have been using llms to\nsimulate human behavior (argyle et al., 2023; dillion et\n1 https://cogscillm.com/ \n2this name is inspired by earlier work on studying how\nbert, then the most successful nlp model, wo"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09671",
        "label": 0.0,
        "abstract": "",
        "context": "fairness of high-stakes educational exams. the paper\u2019s research materials are\npublicly available at  https://github.com/nabormendonca/gpt-4v-enade-cs-2021. \nccs concepts: \u2022applied computing \u2192e-learning ;\u2022computing methodologies \u2192natural language\nprocessing"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09818",
        "label": 1.0,
        "abstract": "to handle the vast amounts of qualitative data\nproduced in corporate climate communica-\ntion, stakeholders increasingly rely on retrieval\naugmented generation (rag) systems. how-\never, a significant gap remains in evaluating\ndomain-specific information retrieval \u2013 the ba-\nsis for answer generation. to address this chal-\nlenge, this work simulates the typical tasks of a\nsustainability analyst by examining 30 sustain-\nability reports with 16 detailed climate-related\nquestions. as a result, we obtain a dataset\nwith over 8.5k unique question-source-answer\npairs labeled by different levels of relevance.\nfurthermore, we develop a use case with the\ndataset to investigate the integration of expert\nknowledge into information retrieval with em-\nbeddings. although we show that incorpo-\nrating expert knowledge works, we also out-\nline the critical limitations of embeddings in\nknowledge-intensive downstream domains like\nclimate change communication.1 2\n1",
        "context": "\npotential impacts on business operations.3\n1all the data and code for this project is available on\n https://github.com/anomized-for-submission  .\n2we thank the expert annotators for their work on this\nproject (anonymized for submission).\n3for "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09520",
        "label": 0.0,
        "abstract": ": the use of generative artificial intelligence (genai) in academia is a subjective and hotly \ndebated topic. currently, there are no agreed guidelines towards the usage of genai systems in \nhigher education (he) and, thus, it is still unclear how to make effective use of the technology for \nteaching and learning practice. this paper provides an overview of the current state of research on \ngenai for teaching and learning in he. to this end, this study conducted a systematic review of \nrelevant studies indexed by scopus, using the preferred reporting items for systematic reviews and \nmeta-analyses (prisma) guidelines. the search criteria revealed a total of 625 research papers, of \nwhich 355 met the final inclusion criteria. the findings from the review showed the current state \nand the future trends in documents, citations, document sources/authors, keywords, and co-\nauthorship. the research gaps identified suggest that while some authors have looked at \nunderstanding the detection of ai-generated text, it may be beneficial to understand how genai can \nbe incorporated into supp orting the educational curriculum for assessments, teaching, and learning \ndelivery. furthermore, there is a need for additional interdisciplinary, multidimensional studies in he \nthrough collaboration. this will strengthen the awareness and understanding of students, tutors, \nand other stakeholders, which will be instrumental in formulating guidelines, frameworks, and \npolicies for genai usage. \n \n \ncitation: ogunleye, b.; zakariyyah, \nk.i.; ajao, o.; olayinka, o.; sharma, \nh. a systematic review of generative \nai for teaching and learning practice. \neduc. sci. 2024 , 14, 636. https:// \ndoi.org/10.3390 /educsci14060636  \n \nacademic editor: bracha kramarski \n \nreceived: 28 march 2024  \nrevised: 6 june 2024  \naccepted: 11 june 2024  \npublished: 13 june 2024  \n \n \n \ncopyright: \u00a9 2024  by the authors. \nlicensee mdpi, basel, switzerland. \nthis article is an open access article \ndistributed under the terms and \nconditions of the creative comm ons \nattribution (cc by) license (https:// \ncreativecomm ons.org/licenses/by/ \n4.0/). keywords: artificial intelligence; generative ai; higher education; prisma; systematic literature \nreview; teaching and learning; topic modelling \n \n \n \n1.",
        "context": "10.3390 /educsci14060636                                                                             https://www.mdpi.com/journal/education "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09988",
        "label": 1.0,
        "abstract": ". the state of an object reflects its current status or condition\nand is important for a robot\u2019s task planning and manipulation. how-\never, detecting an object\u2019s state and generating a state-sensitive plan\nfor robots is challenging. recently, pre-trained large language models\n(llms) and vision-language models (vlms) have shown impressive\ncapabilities in generating plans. however, to the best of our knowledge,\nthereishardlyanyinvestigationonwhetherllmsorvlmscanalsogen-\nerate object state-sensitive plans. to study this, we introduce an object\nstate-sensitive agent (ossa), a task-planning agent empowered by pre-\ntrained neural networks. we propose two methods for ossa: (i) a modu-\nlarmodelconsistingofapre-trainedvisionprocessingmodule(densecap-\ntioning model, dcm) and a natural language processing model (llm),\nand (ii) a monolithic model consisting only of a vlm. to quantitatively\nevaluate the performances of the two methods, we use tabletop scenarios\nwhere the task is to clear the table. we contribute a multimodal bench-\nmark dataset that takes object states into consideration. our results\nshow that both methods can be used for object state-sensitive tasks, but\nthe monolithic approach outperforms the modular approach. the code\nfor ossa is available at https://github.com/xiao-wen-sun/ossa\nkeywords: objectstateidentification \u00b7artificialintelligence \u00b7robotics\n\u00b7language models \u00b7multimodality\n1",
        "context": "ks, but\nthe monolithic approach outperforms the modular approach. the code\nfor ossa is available at  https://github.com/xiao-wen-sun/ossa \nkeywords: objectstateidentification \u00b7artificialintelligence \u00b7robotics\n\u00b7language models \u00b7multimodali"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10144",
        "label": 1.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": " provide\nan open source implementation of our approach as well as pretrained models and datasets at\n https://github.com/jean-kouagou/enhancedrulelearning  .\n1. introduction\nknowledge graphs are gaining increasing attention due\nto their wide range of appl"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10228",
        "label": 1.0,
        "abstract": "the swift progress of multi-modal large models\n(mllms) has showcased their impressive ability to tackle\ntasks blending vision and language. yet, most current models\nand benchmarks cater to scenarios with a narrow scope of\nvisual and textual contexts. these models often fall short\nwhen faced with complex comprehension tasks, which in-\nvolve navigating through a plethora of irrelevant and poten-\ntially misleading information in both text and image forms.\nto bridge this gap, we introduce a new, more demanding task\nknown as interleaved image-text comprehension (iitc).\nthis task challenges models to discern and disregard super-\nfluous elements in both images and text to accurately answer\nquestions and to follow intricate instructions to pinpoint\nthe relevant image. in support of this task, we further craft\na new vega dataset, tailored for the iitc task on scien-\ntific content, and devised a subtask, image-text association\n(ita), to refine image-text correlation skills. our evaluation\nof four leading closed-source models, as well as various\nopen-source models using vega, underscores the rigorous\nnature of iitc. even the most advanced models, such as\ngemini-1.5-pro and gpt4v , only achieved modest success.\nby employing a multi-task, multi-scale post-training strat-\negy, we have set a robust baseline for mllms on the iitc\ntask, attaining an 85.8%accuracy rate in image associa-\ntion and a 0.508rouge score. these results validate the\neffectiveness of our dataset in improving mllms capabili-\nties for nuanced image-text comprehension. project page:\nhttps://zhourax.github.io/vega/\n*equal contribution\n\u2020corresponding author\n\u2660project leader1.",
        "context": "f our dataset in improving mllms capabili-\nties for nuanced image-text comprehension. project page:\n https://zhourax.github.io/vega/ \n*equal contribution\n\u2020corresponding author\n\u2660project leader1. introduction\nthe swift advancement of m"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10227",
        "label": 0.0,
        "abstract": "graphical user interface (gui) automation holds significant promise for enhancing\nhuman productivity by assisting with computer tasks. existing task formulations\nprimarily focus on simple tasks that can be specified by a single, language-only\ninstruction, such as \u201cinsert a new slide.\u201d in this work, we introduce videogui , a\nnovel multi-modal benchmark designed to evaluate gui assistants on visual-centric\ngui tasks. sourced from high-quality web instructional videos, our benchmark\nfocuses on tasks involving professional and novel software ( e.g., adobe pho-\ntoshop or stable diffusion webui) and complex activities ( e.g., video editing).\nvideogui evaluates gui assistants through a hierarchical process, allowing for\nidentification of the specific levels at which they may fail: (i) high-level planning:\nreconstruct procedural subtasks from visual conditions without language descrip-\ntions; (ii) middle-level planning: generate sequences of precise action narrations\nbased on visual state ( i.e.,screenshot) and goals; (iii) atomic action execution:\nperform specific actions such as accurately clicking designated elements. for each\nlevel, we design evaluation metrics across individual dimensions to provide clear\nsignals, such as individual performance in clicking, dragging, typing, and scrolling\nfor atomic action execution. our evaluation on videogui reveals that even the\nsota large multimodal model gpt4o performs poorly on visual-centric gui tasks,\nespecially for high-level planning.\n1",
        "context": "yang2, lijuan wang2, mike zheng shou1b\n1show lab, national university of singapore2microsoft gen ai\n https://showlab.github.io/videogui/ \nabstract\ngraphical user interface (gui) automation holds significant promise for enhancing\nhuman pr"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10221",
        "label": 0.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": " present time-extended stories with a rich variety in terms of scenes, characters and interactions.\n https://shortfilmdataset.github.io \nabstract\nrecent advances in vision-language models have significantly propelled video\nunderstanding"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10210",
        "label": 1.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": "3hilit segev1eran hirsch1royi rassin1gal chechik1,2\n1bar-ilan university2nvidia3tel-aviv university\n https://make-it-count-paper.github.io \nabstract\ndespite the unprecedented success of text-to-image diffusion models, controlling\nthe numbe"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10163",
        "label": 0.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": "niversity5university of chinese academy of sciences\n6sensetime research7stepfun8westlake university\n https://buaacyw.github.io/mesh-anything/ \n3d gs\n nerf\n text condition: a c\n ommode\nimage\n image\npoint cloud\npoint cloud\ndense mesh\n dense mes"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10157",
        "label": 0.0,
        "abstract": ": minigolf, a game with countless court layouts, and complex ball mo-\ntion, constitutes a compelling real-world testbed for the study of embodied intel-\nligence. as it not only challenges spatial and kinodynamic reasoning but also re-\nquires reflective and corrective capacities to address erroneously designed courses.\nwe introduce robogolf , a framework that perceives dual-camera visual inputs\nwith nested vlm-empowered closed-loop control and reflective equilibrium loop.\nextensive experiments demonstrate the effectiveness of robogolf on challenging\nminigolf courts including those that are impossible to finish. experiment videos\nare available at https://jity16.github.io/robogolf/ .\nkeywords: reflective equilibrium, closed-loop control, real-world minigolf, vi-\nsion language model\n1",
        "context": "uazhe xu2,3\n1universitaet hamburg2tsinghua university3shanghai qi zhi institute\n*equal contribution  https://jity16.github.io/robogolf/ \nabstract: minigolf, a game with countless court layouts, and complex ball mo-\ntion, constitutes a c"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10100",
        "label": 1.0,
        "abstract": "remote sensing large multi-modal models (rslmms) are developing rapidly and\nshowcase significant capabilities in remote sensing imagery (rsi) comprehension.\nhowever, due to the limitations of existing datasets, rslmms have shortcomings\nin understanding the rich semantic relations among objects in complex remote\nsensing scenes. to unlock rslmms\u2019 complex comprehension ability, we propose\na large-scale instruction tuning dataset fit-rs, containing 1,800,851 instruction\nsamples. fit-rs covers common interpretation tasks and innovatively introduces\nseveral complex comprehension tasks of escalating difficulty, ranging from re-\nlation reasoning to image-level scene graph generation. based on fit-rs, we\nbuild the fit-rsfg benchmark. furthermore, we establish a new benchmark to\nevaluate the fine-grained relation comprehension capabilities of lmms, named\nfit-rsrc. based on combined instruction data, we propose skysensegpt, which\nachieves outstanding performance on both public datasets and fit-rsfg, sur-\npassing existing rslmms. we hope the fit-rs dataset can enhance the relation\ncomprehension capability of rslmms and provide a large-scale fine-grained\ndata source for the remote sensing community. the dataset will be available at\nhttps://github.com/luo-z13/skysensegpt .\n1",
        "context": "e-scale fine-grained\ndata source for the remote sensing community. the dataset will be available at\n https://github.com/luo-z13/skysensegpt  .\n1 introduction\nbenefiting from the recent rapid advancements and evolution of large language mode"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10079",
        "label": 1.0,
        "abstract": "video understanding is a pivotal task in the digital era, yet the dynamic and multi-\nevent nature of videos makes them labor-intensive and computationally demanding\nto process. thus, localizing a specific event given a semantic query has gained\nimportance in both user-oriented applications like video search and academic\nresearch into video foundation models. a significant limitation in current research\nis that semantic queries are typically in natural language that depicts the semantics\nof the target event. this setting overlooks the potential for multimodal semantic\nqueries composed of images and texts. to address this gap, we introduce a new\nbenchmark, icq, for localizing events in videos with multimodal queries, along\nwith a new evaluation dataset icq-highlight. our new benchmark aims to evaluate\nhow well models can localize an event given a multimodal semantic query that\nconsists of a reference image, which depicts the event, and a refinement text to\nadjust the images\u2019 semantics. to systematically benchmark model performance,\nwe include 4 styles of reference images and 5 types of refinement texts, allowing us\nto explore model performance across different domains. we propose 3 adaptation\nmethods that tailor existing models to our new setting and evaluate 10 sota\nmodels, ranging from specialized to large-scale foundation models. we believe\nthis benchmark is an initial step toward investigating multimodal queries in video\nevent localization.2.\n1",
        "context": "ations like video search and recommendation, tasks\n\u2217equal contribution\n2our project is available at  https://icq-benchmark.github.io/ \npreprint. under review.arxiv:2406.10079v1  [cs.cv]  14 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10057",
        "label": 0.0,
        "abstract": "with the development of multimodal large language\nmodels (mllms) technology, its general capabilities are\nincreasingly powerful. to evaluate the various abilities of\nmllms, numerous evaluation systems have emerged. but\nnow there is still a lack of a comprehensive method to evalu-\nate mllms in the tasks related to flowcharts, which are very\nimportant in daily life and work. we propose the first com-\nprehensive method, flowce, to assess mllms across var-\nious dimensions for tasks related to flowcharts. it encom-\npasses evaluating mllms\u2019 abilities in reasoning, local-\nization recognition, information extraction, logical veri-\nfication, and summarization on flowcharts. however, we\nfind that even the gpt4o model achieves only a score of\n56.63. among open-source models, phi-3-vision obtained\nthe highest score of 49.97. we hope that flowce can\ncontribute to future research on mllms for tasks based\non flowcharts. https://github.com/360ailab-\nnlp/flowce\n1.",
        "context": "9.97. we hope that flowce can\ncontribute to future research on mllms for tasks based\non flowcharts.  https://github.com/360ailab- \nnlp/flowce\n1. introduction\nin the modern work environment, flowcharts have become\na widely used gra"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09422",
        "label": 0.0,
        "abstract": "networked computing power is a critical utility in the era of artificial intelligence. this paper\npresents a novel physical infrastructure finance (pinfi) protocol designed to facilitate the distribu-\ntion of computing power within networks in a decentralized manner. addressing the core challenges\nof coordination, pricing, and liquidity in decentralized physical infrastructure networks (depin),\nthe pinfi protocol introduces a distinctive dynamic pricing mechanism. it enables providers to al-\nlocate excess computing resources to a \u201cdissipative\u201d pinfi liquidity pool, distinct from traditional\ndefi liquidity pools, ensuring seamless access for clients at equitable, market-based prices. this\napproach significantly reduces the costs of accessing computing power, potentially to as low as 1%\ncompared to existing services, while simultaneously enhancing security and dependability. the\npinfi protocol is poised to transform the dynamics of supply and demand in computing power\nnetworks, setting a new standard for efficiency and accessibility.\n1",
        "context": "quidity pools, enhancing both the security and stability\nof these pools.\n1see the uptime history at  https://status.openai.com/uptime \n2please refer to their websites, akash network, nosana, io.net, render network\n1arxiv:2406.09422v1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09454",
        "label": 0.0,
        "abstract": "multi-modal learning has transformed generative ai, particularly in vision-\nlanguage modeling. advances such as the multi-modal gpt-4v and open-source\nprojects like llav a have enabled robust conversational agents capable of zero-shot\ntask completions. however, extending these technologies in the biomedical field\nintroduces unique challenges. recent initiatives like llav a-med have begun\nto tailor instruction-tuning to biomedical contexts using extensive datasets like\npmc-15m. our research contributes three significant advancements: (i) we intro-\nduce a new instruct dataset enriched with medical image-text pairs derived from\nclaude3-opus and llama3 70b, (ii) we propose an innovative image encoding\nstrategy that employs hierarchical representations to enhance fine-grained biomed-\nical visual comprehension, and (iii) we develop the llama3-med model, which\nachieves state-of-the-art zero-shot performance on biomedical visual question an-\nswering benchmarks, improving performance by over 10% on average compared\nto prior methods. these advancements provide more precise and reliable tools for\nmedical professionals, effectively bridging gaps in current multi-modal conversa-\ntional assistants and fostering further innovations in medical ai. codes available at\nhttps://github.com/standardmodelbio/llama3-med.git .\n1",
        "context": "dal conversa-\ntional assistants and fostering further innovations in medical ai. codes available at\n https://github.com/standardmodelbio/llama3-med.git  .\n1 introduction\nthe integration of multimodal data through auto-regression has emerged as a transf"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09688",
        "label": 1.0,
        "abstract": "controllable text generation (ctg) seeks to\ncraft texts adhering to specific attributes, tradi-\ntionally employing learning-based techniques\nsuch as training, fine-tuning, or prefix-tuning\nwith attribute-specific datasets. these ap-\nproaches, while effective, demand extensive\ncomputational and data resources. in contrast,\nsome proposed learning-free alternatives cir-\ncumvent learning but often yield inferior re-\nsults, exemplifying the fundamental machine\nlearning trade-off between computational ex-\npense and model efficacy. to overcome these\nlimitations, we propose freectrl, a learning-\nfree approach that dynamically adjusts the\nweights of selected feedforward neural network\n(ffn) vectors to steer the outputs of large lan-\nguage models (llms). freectrl hinges on the\nprinciple that the weights of different ffn vec-\ntors influence the likelihood of different tokens\nappearing in the output. by identifying and\nadaptively adjusting the weights of attribute-\nrelated ffn vectors, freectrl can control the\noutput likelihood of attribute keywords in the\ngenerated content. extensive experiments on\nsingle- and multi-attribute control reveal that\nthe learning-free freectrl outperforms other\nlearning-free and learning-based methods, suc-\ncessfully resolving the dilemma between learn-\ning costs and model performance1.\n1",
        "context": "groups\nbased on their dependency on a learning process:\n*corresponding author\n1code is available at  https://github.com/zijian678/ \nfreectrl\nlearning costperformanceoptimum region\nlearning-free\nlearning-based\nours0.81.0figure 1: tr"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09760",
        "label": 1.0,
        "abstract": "human alignment in large language models (llms) is an active area of research.\na recent groundbreaking work, direct preference optimization (dpo), has greatly\nsimplified the process from past work in reinforcement learning from human\nfeedback (rlhf) by bypassing the reward learning stage in rlhf. dpo, after\ntraining, provides an implicit reward model. in this work, we make a novel\nobservation that this implicit reward model can by itself be used in a bootstrapping\nfashion to further align the llm. our approach is to use the rewards from a current\nllm model to construct a preference dataset, which is then used in subsequent\ndpo rounds. we incorporate refinements that debias the length of the responses\nand improve the quality of the preference dataset to further improve our approach.\nour approach, named self-alignment with dpoimplicit rewards (dice), shows\ngreat improvements in alignment and achieves superior performance than gemini\npro on alpacaeval 2, reaching 27.55% length-controlled win rate against gpt-4\nturbo, but with only 8b parameters and no external feedback. our code is available\nathttps://github.com/sail-sg/dice .\n1",
        "context": " against gpt-4\nturbo, but with only 8b parameters and no external feedback. our code is available\nat https://github.com/sail-sg/dice  .\n1 introduction\ndirect preference optimization (dpo) [ 17] presents a compelling alternative to re"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09455",
        "label": 1.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": "o, shibo hao, yemin shi, zhengzhong liu,\neric p. xing, zhiting hu\nmaitrix.org, uc san diego, mbzuai\n https://world-model.ai  *equal contribution\nfigure 1:\npandora simulates future world states (videos) under action control ("
    },
    {
        "url": "https://arxiv.org/pdf/2406.09486",
        "label": 0.0,
        "abstract": "model-based offline reinforcement learning (rl)\nis a promising approach that leverages existing\ndata effectively in many real-world applications,\nespecially those involving high-dimensional in-\nputs like images and videos. to alleviate the\ndistribution shift issue in offline rl, existing\nmodel-based methods heavily rely on the uncer-\ntainty of learned dynamics. however, the model\nuncertainty estimation becomes significantly bi-\nased when observations contain complex distrac-\ntors with non-trivial dynamics. to address this\nchallenge, we propose a new approach - sepa-\nrated model-based offline policy optimization\n(semopo) - decomposing latent states into en-\ndogenous and exogenous parts via conservative\nsampling and estimating model uncertainty on the\nendogenous states only. we provide a theoret-\nical guarantee of model uncertainty and perfor-\nmance bound of semopo. to assess the efficacy,\nwe construct the low-quality vision deep data-\ndriven datasets for rl (lqv-d4rl), where the\ndata are collected by non-expert policy and the\nobservations include moving distractors. exper-\nimental results show that our method substan-\ntially outperforms all baseline methods, and fur-\nther analytical experiments validate the critical\ndesigns in our method. the project website is\nhttps://sites.google.com/view/semopo.\n1school of artificial intelligence, nanjing university, china\n2national key laboratory for novel software technology, nanjing\nuniversity, china3school of mathematical sciences, center for\nstatistical science, peking university, beijing, china4school of\ncyberspace science and technology, beijing institute of tech-\nnology, beijing, china. correspondence to: de-chuan zhan\n<zhandc@nju.edu.cn >.\nproceedings of the 41stinternational conference on machine\nlearning , vienna, austria. pmlr 235, 2024. copyright 2024 by\nthe author(s).1.",
        "context": "ur-\nther analytical experiments validate the critical\ndesigns in our method. the project website is\n https://sites.google.com/view/semopo. \n1school of artificial intelligence, nanjing university, china\n2national key laboratory for novel so"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09496",
        "label": 1.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": "and\ndisproportionately contribute to erasure based on region. the dataset and code are\navailable at  https://github.com/oxai/world-wide-dishes/  .\n\u2217joint first author.\n\u2020work done in affiliation with the oxford artificial intelligence society.\n\u2021"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09520",
        "label": 0.0,
        "abstract": ": the use of generative artificial intelligence (genai) in academia is a subjective and hotly \ndebated topic. currently, there are no agreed guidelines towards the usage of genai systems in \nhigher education (he) and, thus, it is still unclear how to make effective use of the technology for \nteaching and learning practice. this paper provides an overview of the current state of research on \ngenai for teaching and learning in he. to this end, this study conducted a systematic review of \nrelevant studies indexed by scopus, using the preferred reporting items for systematic reviews and \nmeta-analyses (prisma) guidelines. the search criteria revealed a total of 625 research papers, of \nwhich 355 met the final inclusion criteria. the findings from the review showed the current state \nand the future trends in documents, citations, document sources/authors, keywords, and co-\nauthorship. the research gaps identified suggest that while some authors have looked at \nunderstanding the detection of ai-generated text, it may be beneficial to understand how genai can \nbe incorporated into supp orting the educational curriculum for assessments, teaching, and learning \ndelivery. furthermore, there is a need for additional interdisciplinary, multidimensional studies in he \nthrough collaboration. this will strengthen the awareness and understanding of students, tutors, \nand other stakeholders, which will be instrumental in formulating guidelines, frameworks, and \npolicies for genai usage. \n \n \ncitation: ogunleye, b.; zakariyyah, \nk.i.; ajao, o.; olayinka, o.; sharma, \nh. a systematic review of generative \nai for teaching and learning practice. \neduc. sci. 2024 , 14, 636. https:// \ndoi.org/10.3390 /educsci14060636  \n \nacademic editor: bracha kramarski \n \nreceived: 28 march 2024  \nrevised: 6 june 2024  \naccepted: 11 june 2024  \npublished: 13 june 2024  \n \n \n \ncopyright: \u00a9 2024  by the authors. \nlicensee mdpi, basel, switzerland. \nthis article is an open access article \ndistributed under the terms and \nconditions of the creative comm ons \nattribution (cc by) license (https:// \ncreativecomm ons.org/licenses/by/ \n4.0/). keywords: artificial intelligence; generative ai; higher education; prisma; systematic literature \nreview; teaching and learning; topic modelling \n \n \n \n1.",
        "context": "10.3390 /educsci14060636                                                                             https://www.mdpi.com/journal/education "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09622",
        "label": 0.0,
        "abstract": "generic face image quality assessment (gfiqa) evalu-\nates the perceptual quality of facial images, which is crucial\nin improving image restoration algorithms and selecting\nhigh-quality face images for downstream tasks. we present\na novel transformer-based method for gfiqa, which is\naided by two unique mechanisms. first, a \u201c dual-set\ndegradation representation learning\u201d (dsl) mechanism\nuses facial images with both synthetic and real degrada-\ntions to decouple degradation from content, ensuring gen-\neralizability to real-world scenarios. this self-supervised\nmethod learns degradation features on a global scale, pro-\nviding a robust alternative to conventional methods that use\nlocal patch information in degradation learning. second,\nour transformer leverages facial landmarks to emphasize\nvisually salient parts of a face image in evaluating its per-\nceptual quality. we also introduce a balanced and diverse\ncomprehensive generic face iqa (cgfiqa-40k) dataset\nof 40k images carefully designed to overcome the biases, in\nparticular the imbalances in skin tone and gender represen-\ntation, in existing datasets. extensive analysis and evalua-\ntion demonstrate the robustness of our method, marking a\nsignificant improvement over prior methods.\n1.",
        "context": " quality assessment presents\nsignificant challenges. the inherent complexity of human\nproject page:  https://dsl-fiqa.github.io \n\u2020 part of the work done during internship at snap research.\n* co-corresponding authors\n\u2666project lea"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09675",
        "label": 0.0,
        "abstract": "with the recent advancements in graph neural networks (gnns), spectral gnns\nhave received increasing popularity by virtue of their specialty in capturing graph\nsignals in the frequency domain, demonstrating promising capability in specific\ntasks. however, few systematic studies have been conducted to assess their spectral\ncharacteristics. this emerging family of models also varies in terms of design and\nsettings, leading to difficulties in comparing their performance and deciding on the\nsuitable model for specific scenarios, especially for large-scale tasks. in this work,\nwe extensively benchmark spectral gnns with a focus on the frequency perspective.\nwe analyze and categorize over 30 gnns with 27 corresponding filters. then,\nwe implement these spectral models within a unified framework with dedicated\ngraph computations and efficient training schemes. thorough experiments are\nconducted on the spectral models with inclusive metrics on effectiveness and\nefficiency, offering practical guidelines on evaluating and selecting spectral gnns\nwith desirable performance. our implementation enables application on larger\ngraphs with comparable performance and less overhead, which is available at:\nhttps://github.com/gdmnl/spectral-gnn-benchmark .\n1",
        "context": " application on larger\ngraphs with comparable performance and less overhead, which is available at:\n https://github.com/gdmnl/spectral-gnn-benchmark  .\n1 introduction\ngraph neural networks (gnns) have emerged as powerful tools in a wide range of gra"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09684",
        "label": 1.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": "e best performance in terms of\naccuracy, time efficiency and robustness. data and code available\nat  https://github.com/pcwhy/xml-intrusiondetection.git \ni. i ntroduction\nmachine learning (ml) has emerged as a transformative tool\nin the field of intrusi"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09723",
        "label": 1.0,
        "abstract": "gradient regularization (gr), which aims to pe-\nnalize the gradient norm atop the loss function,\nhas shown promising results in training modern\nover-parameterized deep neural networks. how-\never, can we trust this powerful technique? this\npaper reveals that gr can cause performance de-\ngeneration in adaptive optimization scenarios, par-\nticularly with learning rate warmup. our empiri-\ncal and theoretical analyses suggest this is due to\ngr inducing instability and divergence in gradi-\nent statistics of adaptive optimizers at the initial\ntraining stage. inspired by the warmup heuristic,\nwe propose three gr warmup strategies, each re-\nlaxing the regularization effect to a certain extent\nduring the warmup course to ensure the accurate\nand stable accumulation of gradients. with exper-\niments on vision transformer family, we confirm\nthe three gr warmup strategies can effectively\ncircumvent these issues, thereby largely improv-\ning the model performance. meanwhile, we note\nthat scalable models tend to rely more on the gr\nwarmup, where the performance can be improved\nby up to 3% on cifar10 compared to baseline gr.\ncode is available at https://github.com/zhaoyang-\n0204/gnp.\n1.",
        "context": "he performance can be improved\nby up to 3% on cifar10 compared to baseline gr.\ncode is available at  https://github.com/zhaoyang- \n0204/gnp.\n1. introduction\nadvancements in computational hardware have catalyzed\nthe design of moder"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09770",
        "label": 0.0,
        "abstract": "solving multi-objective optimization problems for large deep neural networks is\na challenging task due to the complexity of the loss landscape and the expensive\ncomputational cost of training and evaluating models. efficient pareto front approx-\nimation of large models enables multi-objective optimization for various tasks such\nas multi-task learning and trade-off analysis. existing algorithms for learning pareto\nset, including (1) evolutionary, hypernetworks, and hypervolume-maximization\nmethods, are computationally expensive and have restricted scalability to large\nmodels; (2) scalarization algorithms, where a separate model is trained for each\nobjective ray, which is inefficient for learning the entire pareto set and fails to\ncapture the objective trade-offs effectively. inspired by the recent success of model\nmerging, we propose a practical and scalable approach to pareto set learning prob-\nlem via mixture of experts (moe) based model fusion. by ensembling the weights\nof specialized single-task models, the moe module can effectively capture the\ntrade-offs between multiple objectives and closely approximate the entire pareto\nset of large neural networks. once the routers are learned and a preference vec-\ntor is set, the moe module can be unloaded, thus no additional computational\ncost is introduced during inference. we conduct extensive experiments on vision\nand language tasks using large-scale models such as clip-vit and gpt-2. the\nexperimental results demonstrate that our method efficiently approximates the\nentire pareto front of large models. using only hundreds of trainable parameters\nof the moe routers, our method even has lower memory usage compared to linear\nscalarization and algorithms that learn a single pareto optimal solution, and are\nscalable to both the number of objectives and the size of the model. our method\nsignificantly reduces the computational burden of learning the pareto set, for exam-\nple, in the two-task case, it can be achieved in just a few minutes. code is available\nat:https://github.com/tanganke/pareto_set_learning\n1",
        "context": "for exam-\nple, in the two-task case, it can be achieved in just a few minutes. code is available\nat: https://github.com/tanganke/pareto_set_learning \n1 introduction\nmulti-objective optimization problems (moops) are ubiquitous in machine learning, wh"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09831",
        "label": 0.0,
        "abstract": "federated learning (fl) offers a compelling framework for training large language models\n(llms) while addressing data privacy and decentralization challenges. this paper surveys recent advance-\nments in the federated learning of large language models, with a particular focus on machine unlearning\u2014a\ncrucial aspect for complying with privacy regulations like the right to be forgotten. machine unlearning in\nthe context of federated llms involves systematically and securely removing individual data contributions\nfrom the learned model without retraining from scratch. we explore various strategies that enable effective\nunlearning, such as perturbation techniques, model decomposition, and incremental learning, highlighting\ntheir implications for maintaining model performance and data privacy. furthermore, we examine case\nstudies and experimental results from recent literature to assess the effectiveness and efficiency of these\napproaches in real-world scenarios. our survey reveals a growing interest in developing more robust and\nscalable federated unlearning methods, suggesting a vital area for future research in the intersection of ai\nethics and distributed machine learning technologies.\nindex terms federated learning (fl), large language models (llms), swarm intelligence, efficiency,\npre-trained models, privacy and security\ni.",
        "context": ".\nthis work is licensed under a creative commons attribution 4.0 license. for more information, see  https://creativecommons.org/licenses/by/4.0/ \nvolume , 1arxiv:2406.09831v1  [cs.lg]  14 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09838",
        "label": 1.0,
        "abstract": "real-time detection and prediction of extreme weather protect human lives and\ninfrastructure. traditional methods rely on numerical threshold setting and manual\ninterpretation of weather heatmaps with geographic information systems (gis),\nwhich can be slow and error-prone. our research redefines extreme weather\nevents detection (ewed) by framing it as a visual question answering (vqa)\nproblem, thereby introducing a more precise and automated solution. leveraging\nvision-language models (vlm) to simultaneously process visual and textual data,\nwe offer an effective aid to enhance the analysis process of weather heatmaps.\nour initial assessment of general-purpose vlms (e.g., gpt-4-vision) on ewed\nrevealed poor performance, characterized by low accuracy and frequent halluci-\nnations due to inadequate color differentiation andinsufficient meteorological\nknowledge . to address these challenges, we introduce climateiqa , the first mete-\norological vqa dataset, which includes 8,760 wind gust heatmaps and 254,040\nquestion-answer pairs covering four question types, both generated from the latest\nclimate reanalysis data. we also propose sparse position and outline tracking\n(spot) , an innovative technique that leverages opencv and k-means clustering\nto capture and depict color contours in heatmaps, providing climateiqa with\nmore accurate color spatial location information. finally, we present climate-zoo ,\nthe first meteorological vlm collection, which adapts vlms to meteorological\napplications using the climateiqa dataset. experiment results demonstrate that\nmodels from climate-zoo substantially outperform state-of-the-art general vlms,\nachieving an accuracy increase from 0% to over 90% in ewed verification. the\ndatasets and models in this study are publicly available for future climate science\nresearch: https://github.com/alexjjjchen/climate-zoo .\n1",
        "context": ". the\ndatasets and models in this study are publicly available for future climate science\nresearch:  https://github.com/alexjjjchen/climate-zoo  .\n1 introduction\nmeteorology is essential for disaster preparedness, agricultural planning, and cli"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09870",
        "label": 1.0,
        "abstract": "deep graph learning has gained grand popularity over the past years due to its versa-\ntility and success in representing graph data across a wide range of domains. how-\never, the pervasive issue of imbalanced graph data distributions, where certain parts\nexhibit disproportionally abundant data while others remain sparse, undermines the\nefficacy of conventional graph learning algorithms, leading to biased outcomes. to\naddress this challenge, imbalanced graph learning (igl) has garnered substantial\nattention, enabling more balanced data distributions and better task performance.\ndespite the proliferation of igl algorithms, the absence of consistent experimental\nprotocols and fair performance comparisons pose a significant barrier to compre-\nhending advancements in this field. to bridge this gap, we introduce igl-bench , a\nfoundational comprehensive benchmark for imbalanced graph learning, embarking\non16diverse graph datasets and 24distinct igl algorithms with uniform data\nprocessing and splitting strategies. specifically, igl-bench systematically inves-\ntigates state-of-the-art igl algorithms in terms of effectiveness ,robustness , and\nefficiency on node-level and graph-level tasks, with the scope of class-imbalance\nandtopology-imbalance . extensive experiments demonstrate the potential benefits\nof igl algorithms on various imbalanced conditions, offering insights and opportu-\nnities in the igl field. further, we have developed an open-sourced and unified\npackage to facilitate reproducible evaluation and inspire further innovative research,\nwhich is available at https://github.com/ringbdstack/igl-bench .\n1",
        "context": "o facilitate reproducible evaluation and inspire further innovative research,\nwhich is available at  https://github.com/ringbdstack/igl-bench  .\n1 introduction\ngraphs are widely acknowledged as powerful for representing networks such as socia"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09877",
        "label": 0.0,
        "abstract": ". traditional federated learning (fl) methods have limited\nsupport for clients with varying computational and communication abil-\nities, leading to inefficiencies and potential inaccuracies in model train-\ning. this limitation hinders the widespread adoption of fl in diverse\nand resource-constrained environments, such as those with client devices\nranging from powerful servers to mobile devices. to address this need,\nthis paper introduces federated learning with flexible architectures\n(fedfa), an fl training algorithm that allows clients to train models of\ndifferent widths and depths. each client can select a network architec-\nture suitable for its resources, with shallower and thinner networks re-\nquiring fewer computing resources for training. unlike prior work in this\narea, fedfa incorporates the layer grafting technique to align clients\u2019 lo-\ncal architectures with the largest network architecture in the fl system\nduring model aggregation. layer grafting ensures that all client contribu-\ntions are uniformly integrated into the global model, thereby minimizing\nthe risk of any individual client\u2019s data skewing the model\u2019s parameters\ndisproportionately and introducing security benefits. moreover, fedfa\nintroduces the scalable aggregation method to manage scale variations in\nweights among different network architectures. experimentally, fedfa\noutperforms previous width and depth flexible aggregation strategies.\nspecifically, fedfa\u2019s testing accuracy matches (1.00 times) or is up to\n1.16 times higher globally for iid settings, 0.98 to 1.13 times locally,\nand 0.95 times to 1.20 times higher globally for non-iid settings com-\npared to earlier strategies. furthermore, fedfa demonstrates increased\nrobustness against performance degradation in backdoor attack scenar-\nios compared to earlier strategies. earlier strategies exhibit more drops\nin testing accuracy under attacks\u2014for iid data by 1.01 to 2.11 times\nglobally, and for non-iid data by 0.89 to 3.31 times locally, and 1.11 to\n1.74 times globally, compared to fedfa.\nkeywords: federated learning \u00b7heterogeneous local network archi-\ntectures \u00b7backdoor attack\n1",
        "context": "\u0000[0000\u22120003\u22120785\u22129291]\ncarnegie mellon university, pittsburgh pa 15213, usa\ncjoewong@andrew.cmu.edu\n https://www.cmu.edu/ \nabstract. traditional federated learning (fl) methods have limited\nsupport for clients with varying"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09979",
        "label": 0.0,
        "abstract": "large language models (llms) excel in natural language\ntasks but face limitations due to static training datasets, result-\ning in outdated or contextually shallow responses. retrieval-\naugmented generation (rag) addresses this by integrating\nreal-time external knowledge, enhancing model accuracy and\ncredibility, especially for knowledge-intensive tasks. how-\never, rag-enhanced llms struggle with long contexts, caus-\ning them to \u201dchoke\u201d on information overload, compromising\nresponse quality. recent rag applications use hierarchical\ndata structures for storing documents, organized at various\nlevels of summarization and information density. in this con-\ntext, we introduce hiro (hierarchical information retrieval\noptimization), a novel querying approach for rag appli-\ncations using hierarchical structures for storing documents.\nhiro employs dfs-based recursive similarity score calcula-\ntion and branch pruning to minimize the context returned to\nthe llm without informational loss. hiro outperforms ex-\nisting querying mechanisms on the narrativeqa dataset by\nan absolute performance gain of 10.85%.\ncode \u2014 https://github.com/krishgoel/hiro",
        "context": "ng querying mechanisms on the narrativeqa dataset by\nan absolute performance gain of 10.85%.\ncode \u2014  https://github.com/krishgoel/hiro \nintroduction\nthe advent of large language models (llms) has brought\nabout a significant transformat"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09988",
        "label": 1.0,
        "abstract": ". the state of an object reflects its current status or condition\nand is important for a robot\u2019s task planning and manipulation. how-\never, detecting an object\u2019s state and generating a state-sensitive plan\nfor robots is challenging. recently, pre-trained large language models\n(llms) and vision-language models (vlms) have shown impressive\ncapabilities in generating plans. however, to the best of our knowledge,\nthereishardlyanyinvestigationonwhetherllmsorvlmscanalsogen-\nerate object state-sensitive plans. to study this, we introduce an object\nstate-sensitive agent (ossa), a task-planning agent empowered by pre-\ntrained neural networks. we propose two methods for ossa: (i) a modu-\nlarmodelconsistingofapre-trainedvisionprocessingmodule(densecap-\ntioning model, dcm) and a natural language processing model (llm),\nand (ii) a monolithic model consisting only of a vlm. to quantitatively\nevaluate the performances of the two methods, we use tabletop scenarios\nwhere the task is to clear the table. we contribute a multimodal bench-\nmark dataset that takes object states into consideration. our results\nshow that both methods can be used for object state-sensitive tasks, but\nthe monolithic approach outperforms the modular approach. the code\nfor ossa is available at https://github.com/xiao-wen-sun/ossa\nkeywords: objectstateidentification \u00b7artificialintelligence \u00b7robotics\n\u00b7language models \u00b7multimodality\n1",
        "context": "ks, but\nthe monolithic approach outperforms the modular approach. the code\nfor ossa is available at  https://github.com/xiao-wen-sun/ossa \nkeywords: objectstateidentification \u00b7artificialintelligence \u00b7robotics\n\u00b7language models \u00b7multimodali"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10040",
        "label": 1.0,
        "abstract": "this paper describes the inference system of\nfzi-wim at the semeval-2024 task 2: safe\nbiomedical natural language inference for\nclinical trials. our system utilizes the chain\nof thought (cot) paradigm to tackle this com-\nplex reasoning problem and further improves\nthe cot performance with self-consistency. in-\nstead of greedy decoding, we sample multiple\nreasoning chains with the same prompt and\nmake the final verification with majority voting.\nthe self-consistent cot system achieves a base-\nline f1 score of 0.80 (1st), faithfulness score of\n0.90 (3rd), and consistency score of 0.73 (12th).\nwe release the code and data publicly1.\n1",
        "context": "chain of thought (cot) has been proposed to\nelicit the reasoning capabilities of llms (wei et al.,\n1 https://github.com/jens5588/fzi-wim-nli4ct2022).  based on the cot, further concepts like tree\nof thought (yao et al., 2023a), react (yao et al.,\n202"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09948",
        "label": 1.0,
        "abstract": "large language models (llms) often lack culture-specific knowledge of daily life,\nespecially across diverse regions and non-english languages. existing benchmarks\nfor evaluating llms\u2019 cultural sensitivities are limited to a single language or col-\nlected from online sources such as wikipedia, which do not reflect the mundane\neveryday lifestyles of diverse regions. that is, information about the food people\neat for their birthday celebrations, spices they typically use, musical instruments\nyoungsters play, or the sports they practice in school is common cultural knowledge\nbut uncommon in easily collected online sources, especially for underrepresented\ncultures. to address this issue, we introduce ble nd, a hand-crafted benchmark\ndesigned to evaluate llms\u2019 everyday knowledge across diverse cultures and lan-\nguages. ble nd comprises 52.6k question-answer pairs from 16 countries/regions,\nin 13 different languages, including low-resource ones such as amharic, assamese,\nazerbaijani, hausa, and sundanese. we construct the benchmark to include two\nformats of questions: short-answer and multiple-choice. we show that llms\nperform better for cultures that are highly represented online, with a maximum\n57.34% difference in gpt-4, the best-performing model, in the short-answer format.\nfor cultures represented by mid-to-high-resource languages, llms perform better\nin their local languages, but for cultures represented by low-resource languages,\nllms perform better in english than the local languages. we make our dataset\npublicly available at: https://github.com/nlee0212/blend .\n1",
        "context": "llms perform better in english than the local languages. we make our dataset\npublicly available at:  https://github.com/nlee0212/blend  .\n1 introduction\ndespite the worldwide usage of large language models (llms), capturing cultural ev"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09979",
        "label": 1.0,
        "abstract": "large language models (llms) excel in natural language\ntasks but face limitations due to static training datasets, result-\ning in outdated or contextually shallow responses. retrieval-\naugmented generation (rag) addresses this by integrating\nreal-time external knowledge, enhancing model accuracy and\ncredibility, especially for knowledge-intensive tasks. how-\never, rag-enhanced llms struggle with long contexts, caus-\ning them to \u201dchoke\u201d on information overload, compromising\nresponse quality. recent rag applications use hierarchical\ndata structures for storing documents, organized at various\nlevels of summarization and information density. in this con-\ntext, we introduce hiro (hierarchical information retrieval\noptimization), a novel querying approach for rag appli-\ncations using hierarchical structures for storing documents.\nhiro employs dfs-based recursive similarity score calcula-\ntion and branch pruning to minimize the context returned to\nthe llm without informational loss. hiro outperforms ex-\nisting querying mechanisms on the narrativeqa dataset by\nan absolute performance gain of 10.85%.\ncode \u2014 https://github.com/krishgoel/hiro",
        "context": "ng querying mechanisms on the narrativeqa dataset by\nan absolute performance gain of 10.85%.\ncode \u2014  https://github.com/krishgoel/hiro \nintroduction\nthe advent of large language models (llms) has brought\nabout a significant transformat"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10040",
        "label": 1.0,
        "abstract": "this paper describes the inference system of\nfzi-wim at the semeval-2024 task 2: safe\nbiomedical natural language inference for\nclinical trials. our system utilizes the chain\nof thought (cot) paradigm to tackle this com-\nplex reasoning problem and further improves\nthe cot performance with self-consistency. in-\nstead of greedy decoding, we sample multiple\nreasoning chains with the same prompt and\nmake the final verification with majority voting.\nthe self-consistent cot system achieves a base-\nline f1 score of 0.80 (1st), faithfulness score of\n0.90 (3rd), and consistency score of 0.73 (12th).\nwe release the code and data publicly1.\n1",
        "context": "chain of thought (cot) has been proposed to\nelicit the reasoning capabilities of llms (wei et al.,\n1 https://github.com/jens5588/fzi-wim-nli4ct2022).  based on the cot, further concepts like tree\nof thought (yao et al., 2023a), react (yao et al.,\n202"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10099",
        "label": 1.0,
        "abstract": "large language models (llms) have demon-\nstrated remarkable capabilities across various\ntasks but still face challenges such as hallu-\ncinations. one potential reason for halluci-\nnations is the lack of relevant knowledge or\ncontext. thus, a promising solution to miti-\ngate this issue involves instructing llms to\nrespond with \"i do not know\" when a question\nfalls outside their knowledge domain or the\nprovided context. however, in this work, we\nobserved that llms struggle to admit their lack\nof knowledge, primarily due to existing instruc-\ntion datasets designed to encourage specific\nanswers. to improve large language models\u2019\ncapability to recognize the boundaries of their\nknowledge, we propose a novel approach called\nuncertainty-sensitive tuning. this method in-\nvolves two-stage training designed for uncer-\ntainty recognition and prompt-sensitive activa-\ntion. in the first stage, we guide the llm to\nreject unknown questions. in the second stage,\nwe recover the decreased performance in qa\ntasks by incorporating designed causal instruc-\ntions. by leveraging this method, we aim to en-\nhance the model\u2019s ability to identify areas of un-\ncertainty. the experimental results demonstrate\nthat our proposed uncertainty-sensitive tuning\nmethod significantly improves the performance\nof the llama2-chat-7b model. specifically, it\nachieves a substantial 34.7% improvement in\nhandling questions involving knowledge gaps\ncompared to the original model. moreover,\nour approach outperforms gpt-4, exhibiting\na 9.4% increase in overall performance. we\nopen-source the model and code on github1.\n1",
        "context": "e are still some issues, such as halluci-\nnations (huang et al., 2023; ji et al., 2023; ye et al.,\n1 https://github.com/jiaqili404/trustworthyrag \nfigure 1: the response of different models facing two\ntypes of questions. known question: the quest"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10118",
        "label": 0.0,
        "abstract": "southeast asia (sea) is a region rich in lin-\nguistic diversity and cultural variety, with over\n1,300 indigenous languages and a population\nof 671 million people. however, prevailing ai\nmodels suffer from a significant lack of repre-\nsentation of texts, images, and audio datasets\nfrom sea, compromising the quality of ai\nmodels for sea languages. evaluating models\nfor sea languages is challenging due to the\nscarcity of high-quality datasets, compounded\nby the dominance of english training data, rais-\ning concerns about potential cultural misrep-\nresentation. to address these challenges, we\nintroduce seacrowd, a collaborative initia-\ntive that consolidates a comprehensive resource\nhub1that fills the resource gap by providing\nstandardized corpora2in nearly 1,000 sea lan-\nguages across three modalities. through our\nseacrowd benchmarks, we assess the qual-\nity of ai models on 36 indigenous languages\nacross 13 tasks, offering valuable insights into\nthe current ai landscape in sea. furthermore,\nwe propose strategies to facilitate greater ai ad-\nvancements, maximizing potential utility and\nresource equity for the future of ai in sea.\n1https://seacrowd.github.io/seacrowd-catalogue/\n2https://github.com/seacrowd/seacrowd-datahub/1",
        "context": "ufficiently documented, or varied in quality and\nformatting, thereby making access and usage chal-\n3 https://commoncrawl.github.io/cc-crawl-statistics/plots/languagesarxiv:2406.10118v1   [cs.cl]  14 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10130",
        "label": 1.0,
        "abstract": "Abstract or Introduction section not found on the first page.",
        "context": "r harms that could\nbe offensive to individuals in certain social groups.\n2our code are available at  https://github.com/thenamek/bias-neurons.git  .\n1arxiv:2406.10130v1  [cs.cl]  14 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10172",
        "label": 1.0,
        "abstract": "answer sentence selection (as2) is a criti-\ncal task for designing effective retrieval-based\nquestion answering (qa) systems. most ad-\nvancements in as2 focus on english due to\nthe scarcity of annotated datasets for other\nlanguages. this lack of resources prevents\nthe training of effective as2 models in dif-\nferent languages, creating a performance gap\nbetween qa systems in english and other lo-\ncales. in this paper, we introduce new high-\nquality datasets for as2 in five european lan-\nguages (french, german, italian, portuguese,\nand spanish), obtained through supervised au-\ntomatic machine translation (amt) of ex-\nisting english as2 datasets such as asnq,\nwikiqa, and trec-qa using a large lan-\nguage model (llm). we evaluated our ap-\nproach and the quality of the translated datasets\nthrough multiple experiments with different\ntransformer architectures. the results indicate\nthat our datasets are pivotal in producing robust\nand powerful multilingual as2 models, signifi-\ncantly contributing to closing the performance\ngap between english and other languages.\n1",
        "context": "sks\nand cross-lingual applications. mbert (devlin\net al., 2019), an extension of the original bert\n1 https://huggingface.co/datasets/ \nmatteogabburo/masnq\n2https://huggingface.co/datasets/\nmatteogabburo/mwikiqa\n3https://huggingface.co"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10173",
        "label": 1.0,
        "abstract": "enhancing language models\u2019 (lms) ability to\nunderstand purchase intentions in e-commerce\nscenarios is crucial for their effective assis-\ntance in various downstream tasks. how-\never, previous approaches that distill inten-\ntions from lms often fail to generate mean-\ningful and human-centric intentions applica-\nble in real-world e-commerce contexts. this\nraises concerns about the true comprehension\nand utilization of purchase intentions by lms.\nin this paper, we present intention qa, a\ndouble-task multiple-choice question answer-\ning benchmark to evaluate lms\u2019 comprehen-\nsion of purchase intentions in e-commerce.\nspecifically, lms are tasked to infer inten-\ntions based on purchased products and uti-\nlize them to predict additional purchases. in-\ntention qa consists of 4,360 carefully cu-\nrated problems across three difficulty levels,\nconstructed using an automated pipeline to\nensure scalability on large e-commerce plat-\nforms. human evaluations demonstrate the\nhigh quality and low false-negative rate of our\nbenchmark. extensive experiments across 19\nlanguage models show that they still strug-\ngle with certain scenarios, such as understand-\ning products and intentions accurately, jointly\nreasoning with products and intentions, and\nmore, in which they fall far behind human\nperformances. our code and data are pub-\nlicly available at https://github.com/hkust-\nknowcomp/intentionqa.\n1",
        "context": "re, in which they fall far behind human\nperformances. our code and data are pub-\nlicly available at  https://github.com/hkust- \nknowcomp/intentionqa.\n1 introduction\npurchase intentions are mental states where agents\nor humans c"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10203",
        "label": 1.0,
        "abstract": "the relationship between the quality of a string\nand its probability p(y)under a language\nmodel has been influential in the development\nof techniques to build good text generation\nsystems. for example, several decoding\nalgorithms have been motivated to manipulate\np(y)to produce higher-quality text. in this\nwork, we examine the probability\u2013quality\nrelationship in language models explicitly\naligned to human preferences, e.g., through\nreinforcement learning through human\nfeedback (rlhf). we find that, given a\ngeneral language model and its aligned version,\nfor corpora sampled from an aligned language\nmodel, there exists a trade-off between the\naverage reward and average log-likelihood of\nthe strings under the general language model.\nwe provide a formal treatment of this issue and\ndemonstrate how a choice of sampling adaptor\nallows for a selection of how much likelihood\nwe exchange for the reward.\nhttps://github.com/tanyjnaaman/\nprobability-quality-paradox\n1",
        "context": "hoice of sampling adaptor\nallows for a selection of how much likelihood\nwe exchange for the reward.\n https://github.com/tanyjnaaman/ \nprobability-quality-paradox\n1 introduction\nthe relationship between the probability of a string\nand"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10203",
        "label": 1.0,
        "abstract": "the relationship between the quality of a string\nand its probability p(y)under a language\nmodel has been influential in the development\nof techniques to build good text generation\nsystems. for example, several decoding\nalgorithms have been motivated to manipulate\np(y)to produce higher-quality text. in this\nwork, we examine the probability\u2013quality\nrelationship in language models explicitly\naligned to human preferences, e.g., through\nreinforcement learning through human\nfeedback (rlhf). we find that, given a\ngeneral language model and its aligned version,\nfor corpora sampled from an aligned language\nmodel, there exists a trade-off between the\naverage reward and average log-likelihood of\nthe strings under the general language model.\nwe provide a formal treatment of this issue and\ndemonstrate how a choice of sampling adaptor\nallows for a selection of how much likelihood\nwe exchange for the reward.\nhttps://github.com/tanyjnaaman/\nprobability-quality-paradox\n1",
        "context": "hoice of sampling adaptor\nallows for a selection of how much likelihood\nwe exchange for the reward.\n https://github.com/tanyjnaaman/ \nprobability-quality-paradox\n1 introduction\nthe relationship between the probability of a string\nand"
    }
]