[
    {
        "url": "https://arxiv.org/pdf/2406.08920",
        "label": 0,
        "title": "a v-gs: learning material and geometry aware priors for novel view acoustic synthesis swapnil bhosale",
        "abstract": "novel view acoustic synthesis (nv as) aims to render binaural audio at any target viewpoint, given a mono audio emitted by a sound source at a 3d scene. existing methods have proposed nerf-based implicit models to exploit visual cues as a condition for synthesizing binaural audio. however, in addition to low efficiency originating from heavy nerf rendering, these methods all have a limited ability of characterizing the entire scene environment such as room geometry, material prop- erties, and the spatial relation between the listener and sound source. to address these issues, we propose a novel audio-visual gaussian splatting (a v-gs) model. to obtain a material-aware and geometry-aware condition for audio synthesis, we learn an explicit point-based scene representation with an audio-guidance param- eter on locally initialized gaussian points, taking into account the space relation from the listener and sound source. to make the visual scene model audio adaptive, we propose a point densification and pruning strategy to optimally distribute the gaussian points, with the per-point contribution in sound propagation (e.g., more points needed for texture-less wall surfaces as they affect sound path diversion). extensive experiments validate the superiority of our a v-gs over existing alterna- tives on the real-world rwas and simulation-based soundspaces datasets. project page: https://surrey-uplab.github.io/research/avgs/ "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09021",
        "label": 0,
        "title": "contextual distillation model for diversified recommendation fan li",
        "abstract": "the diversity of recommendation is equally crucial as accuracy in improving user experience. existing studies, e.g., determinantal point process (dpp) and maximal marginal relevance (mmr), em- ploy a greedy paradigm to iteratively select items that optimize both accuracy and diversity. however, prior methods typically exhibit quadratic complexity, limiting their applications to the re-ranking stage and are not applicable to other recommendation stages with a larger pool of candidate items, such as the pre-ranking andranking stages. in this paper, we propose contextual distillation model (cdm), an efficient recommendation model that addresses diver- sification, suitable for the deployment in all stages of industrial recommendation pipelines. specifically, cdm utilizes the candidate items in the same user request as context to enhance the diversifi- cation of the results. we propose a contrastive context encoder that employs attention mechanisms to model both positive and negative contexts. for the training of cdm, we compare each target item with its context embedding and utilize the knowledge distillation framework to learn the win probability of each target item under the mmr algorithm, where the teacher is derived from mmr outputs. during inference, ranking is performed through a linear combina- tion of the recommendation and student model scores, ensuring both diversity and efficiency. we perform offline evaluations on two industrial datasets and conduct online a/btest of cdm on the short-video platform kuaishou . the considerable enhancements \u2217equal contributions. \u2020corresponding author. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. kdd \u201924, august 25\u201329, 2024, barcelona, spain \u00a92024 copyright held by the owner/author(s). publication rights licensed to acm. acm isbn 979-8-4007-0490-1/24/08. . . $15.00 https://doi.org/10.1145/3637528.3671514observed in both recommendation quality and diversity, as shown by metrics, provide strong superiority for the effectiveness of cdm. ccs concepts \u2022information systems \u2192recommender systems . keywords recommender system, knowledge distillation, diversified recom- mendation acm reference format: fan li, xu si, shisong tang, dingmin wang, kunyan han, bing han, guorui zhou, yang song, and hechang chen. 2024. contextual distillation model for diversified recommendation. in proceedings of the 30th acm sigkdd conference on knowledge discovery and data mining (kdd \u201924), august 25\u201329, 2024, barcelona, spain. acm, new york, ny, usa, 10 pages. https: //doi.org/10.1145/3637528.3671514 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09215",
        "label": 0,
        "title": "on softmax direct preference optimization for recommendation yuxin chen1",
        "abstract": "recommender systems aim to predict personalized rankings based on user pref- erence data. with the rise of language models (lms), lm-based recommenders have been widely explored due to their extensive world knowledge and power- ful reasoning abilities. most of the lm-based recommenders convert historical interactions into language prompts, pairing with a positive item as the target re- sponse and fine-tuning lm with a language modeling loss. however, the current objective fails to fully leverage preference data and is not optimized for personal- ized ranking tasks, which hinders the performance of lm-based recommenders. inspired by the current advancement of direct preference optimization (dpo) in human preference alignment and the success of softmax loss in recommenda- tions, we propose softmax-dpo ( s-dpo ) to instill ranking information into the lm to help lm-based recommenders distinguish preferred items from negatives, rather than solely focusing on positives. specifically, we incorporate multiple negatives in user preference data and devise an alternative version of dpo loss tailored for lm-based recommenders, connected to softmax sampling strategies. theoretically, we bridge s-dpo with the softmax loss over negative sampling and find that it has a side effect of mining hard negatives, which assures its excep- tional capabilities in recommendation tasks. empirically, extensive experiments conducted on three real-world datasets demonstrate the superiority of s-dpo to effectively model user preference and further boost recommendation performance while mitigating the data likelihood decline issue of dpo. our codes are available athttps://github.com/chenyuxin1999/s-dpo . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08587",
        "label": 1,
        "title": "cs-bench: a comprehensive benchmark for large language models towards computer science mastery xiaoshuai song",
        "abstract": "computer science (cs) stands as a testament to the intricacies of human intelli- gence, profoundly advancing the development of artificial intelligence and modern society. however, the current community of large language models (llms) overly focuses on benchmarks for analyzing specific foundational skills (e.g. mathematics and code generation), neglecting an all-round evaluation of the computer science field. to bridge this gap, we introduce cs-bench, the first bilingual (chinese- english) benchmark dedicated to evaluating the performance of llms in computer science. cs-bench comprises approximately 5k meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing var- ious task forms and divisions of knowledge and reasoning. utilizing cs-bench, we conduct a comprehensive evaluation of over 30 mainstream llms, revealing the relationship between cs performance and model scales. we also quantita- tively analyze the reasons for failures in existing llms and highlight directions for improvements, including knowledge supplementation and cs-specific reason- ing. further cross-capability experiments show a high correlation between llms\u2019 capabilities in computer science and their abilities in mathematics and coding. moreover, expert llms specialized in mathematics and coding also demonstrate strong performances in several cs subfields. looking ahead, we envision cs- bench serving as a cornerstone for llm applications in the cs field and paving new avenues in assessing llms\u2019 diverse reasoning capabilities. the cs-bench data and evaluation code are available at https://github.com/csbench/csbench. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08973",
        "label": 1,
        "title": "xland-100b: a large-scale multi-task dataset for in-context reinforcement learning alexander nikulin",
        "abstract": "following the success of the in-context learning paradigm in large-scale language and computer vision models, the recently emerging field of in-context reinforce- ment learning is experiencing a rapid growth. however, its development has been held back by the lack of challenging benchmarks, as all the experiments have been carried out in simple environments and on small-scale datasets. we present xland-100b , a large-scale dataset for in-context reinforcement learning based on the xland-minigrid environment, as a first step to alleviate this prob- lem. it contains complete learning histories for nearly 30,000different tasks, covering 100b transitions and 2.5b episodes. it took 50,000gpu hours to col- lect the dataset, which is beyond the reach of most academic labs. along with the dataset, we provide the utilities to reproduce or expand it even further. with this substantial effort, we aim to democratize research in the rapidly growing field of in-context reinforcement learning and provide a solid foundation for fur- ther scaling. the code is open-source and available under apache 2.0 licence at https://github.com/dunno-lab/xland-minigrid-datasets . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08598",
        "label": 1,
        "title": "language model council: benchmarking foundation models on highly subjective tasks by consensus justin zhao1",
        "abstract": "the rapid advancement of large language models (llms) necessitates robust and challenging benchmarks. leaderboards like chatbot arena rank llms based on how well their responses align with human preferences. however, many tasks such as those related to emotional intelligence, creative writing, or persuasiveness, are highly subjective and often lack majoritarian human agreement. judges may have irreconcilable disagreements about what constitutes a better response. to address the challenge of ranking llms on highly subjective tasks, we propose a novel benchmarking framework, the language model council (lmc) . the lmc operates through a democratic process to: 1) formulate a test set through equal participation, 2) administer the test among council members, and 3) evaluate responses as a collective jury. we deploy a council of 20 newest llms on an open-ended emotional intelligence task: responding to interpersonal dilemmas. our results show that the lmc produces rankings that are more separable, robust, and less biased than those from any individual llm judge, and is more consistent with a human-established leaderboard compared to other benchmarks. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08979",
        "label": 1,
        "title": "multi-agent software development through cross-team collaboration zhuoyun du\u2020\u2663chen qian\u2020",
        "abstract": "the latest breakthroughs in large language models (llms), e.g., chatdev, have catalyzed profound transformations, particularly through multi-agent collaboration for software devel- opment. llm agents can collaborate in teams like humans, and follow the waterfall model to sequentially work on requirements analysis, development, review, testing, and other phases to perform autonomous software generation. however, for an agent team, each phase in a single development process yields only one pos- sible outcome. this results in the completion of only one development chain, thereby losing the opportunity to explore multiple potential decision paths within the solution space. con- sequently, this may lead to obtaining subop- timal results. to address this challenge, we introduce cross-teamcollaboration (ctc), a scalable multi-team framework that enables orchestrated teams to jointly propose various decisions and communicate with their insights in a cross-team collaboration environment for superior content generation. experimental re- sults in software development reveal a notable increase in quality compared to state-of-the- art baselines, underscoring the efficacy of our framework. the significant improvements in story generation demonstrate the promising generalization ability of our framework across various domains. we anticipate that our work will guide llm agents towards a cross-team paradigm and contribute to their significant growth in but not limited to software devel- opment. the code and data will be available at https://github.com/openbmb/chatdev . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09043",
        "label": 0,
        "title": "language models are crossword solvers soumadeep saha and sutanoya chakraborty and saptarshi saha and utpal gar",
        "abstract": "crosswords are a form of word puzzle that re- quire a solver to demonstrate a high degree of proficiency in natural language understand- ing, wordplay, reasoning, and world knowl- edge, along with adherence to character and length constraints. in this paper we tackle the challenge of solving crosswords with large language models (llms). we demonstrate that the current generation of state-of-the art (sota) language models show significant com- petence at deciphering cryptic crossword clues, and outperform previously reported sota re- sults by a factor of 2-3 in relevant benchmarks. we also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with llms for the very first time, achieving an accuracy of 93% on new york times crossword puzzles. contrary to previous work in this area which concluded that llms lag human expert perfor- mance significantly, our research suggests this gap is a lot narrower. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09009",
        "label": 0,
        "title": "fredformer: frequency debiased transformer for time series forecasting xihao piao",
        "abstract": "the transformer model has shown leading performance in time se- ries forecasting. nevertheless, in some complex scenarios, it tends to learn low-frequency features in the data and overlook high- frequency features, showing a frequency bias. this bias prevents the model from accurately capturing important high-frequency data features. in this paper, we undertake empirical analyses to understand this bias and discover that frequency bias results from the model disproportionately focusing on frequency features with higher energy. based on our analysis, we formulate this bias and propose fredformer , a transformer-based framework designed to mitigate frequency bias by learning features equally across dif- ferent frequency bands. this approach prevents the model from overlooking lower amplitude features important for accurate fore- casting. extensive experiments show the effectiveness of our pro- posed approach, which can outperform other baselines in differ- ent real-world time-series datasets. furthermore, we introduce a lightweight variant of the fredformer with an attention matrix approximation, which achieves comparable performance but with much fewer parameters and lower computation costs. the code is available at: https://github.com/chenzrg/fredformer ccs concepts \u2022computing methodologies \u2192artificial intelligence ;neural networks . keywords time series forecasting, deep learning acm reference format: xihao piao*, zheng chen*, taichi murayama, yasuko matsubara, and ya- sushi sakurai. 2024. fredformer: frequency debiased transformer for * indicates corresponding authors. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. kdd \u201924, august 25\u201329, 2024, barcelona, spain \u00a92024 copyright held by the owner/author(s). publication rights licensed to acm. acm isbn 979-8-4007-0490-1/24/08 https://doi.org/10.1145/3637528.3671928time series forecasting . in proceedings of the 30th acm sigkdd con- ference on knowledge discovery and data mining (kdd \u201924), august 25\u2013 29, 2024, barcelona, spain. acm, new york, ny, usa, 18 pages. https: //doi.org/10.1145/3637528.3671928 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08680",
        "label": 0,
        "title": "analyzing large language models for classroom discussion assessment nhat tran",
        "abstract": "automatically assessing classroom discussion quality is be- coming increasingly feasible with the help of new nlp ad- vancements such as large language models (llms). in this work, we examine how the assessment performance of 2 llms interacts with 3 factors that may affect performance: task formulation, context length, and few-shot examples. we also explore the computational efficiency and predic- tive consistency of the 2 llms. our results suggest that the 3 aforementioned factors do affect the performance of the tested llms and there is a relation between consistency and performance. we recommend a llm-based assessment approach that has a good balance in terms of predictive per- formance, computational efficiency, and consistency. keywords classroom discussion, large language models, scoring 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08707",
        "label": 1,
        "title": "moscar: a large-scale multilingual and multimodal document-level corpus matthieu futeral",
        "abstract": "multimodal large language models (mllms) are trained on a large amount of text-image data. while most mllms are trained on caption-like data only, alayrac et al. [2022] showed that additionally training them on interleaved sequences of text and images can lead to the emergence of in-context learning capabilities. however, the dataset they used, m3w, is not public and is only in english. there have been attempts to reproduce their results but the released datasets are english-only. in contrast, current multilingual and multimodal datasets are either composed of caption-like only or medium-scale or fully private data. this limits mllm research for the 7,000 other languages spoken in the world. we therefore introduce moscar, to the best of our knowledge the first large-scale multilingual and multimodal document corpus crawled from the web. it covers 163 languages, 315m documents, 214b tokens and 1.2b images. we carefully conduct a set of filtering and evaluation steps to make sure moscar is sufficiently safe, diverse and of good quality. we additionally train two types of multilingual model to prove the benefits of moscar: (1) a model trained on a subset of moscar and captioning data and (2) a model train on captioning data only. the model additionally trained on moscar shows a strong boost in few-shot learning performance across various multilingual image-text tasks and benchmarks, confirming previous findings for english-only mllms. the dataset can be accessed here.2 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09031",
        "label": 1,
        "title": "a c omprehensive graph pooling benchmark : effectiveness",
        "abstract": "graph pooling has gained attention for its ability to obtain effective node and graph representations for various downstream tasks. despite the recent surge in graph pooling approaches, there is a lack of standardized experimental settings and fair benchmarks to evaluate their performance. to address this issue, we have constructed a comprehensive benchmark that includes 15 graph pooling methods and 21 different graph datasets. this benchmark systematically assesses the performance of graph pooling methods in three dimensions, i.e., effectiveness, robustness, and generalizability. we first evaluate the performance of these graph pooling approaches across different tasks including graph classification, graph regression and node classification. then, we investigate their performance under potential noise attacks and out-of-distribution shifts in real-world scenarios. we also involve detailed efficiency analysis and parameter analysis. extensive experiments validate the strong capability and applicability of graph pooling approaches in various scenarios, which can provide valuable insights and guidance for deep geometric learning research. the source code of our benchmark is available at https://github.com/goose315/graph_pooling_benchmark . keywords graph pooling; benchmark; graph neural networks; graph machine learning "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09052",
        "label": 0,
        "title": "data-free generative replay for class-incremental learning on imbalanced data sohaib younis",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08726",
        "label": 0,
        "title": "standardlanguage ideology inai-generated language genevievesmith",
        "abstract": "in this position paper, we explore standard language ideolo gy in languagegeneratedbylargelanguagemodels(llms).first, weout- linehowstandardlanguageideologyisre\ufb02ectedandreinfor ced in llms. we then present a taxonomy of open problems regarding standard language ideology in ai-generated language with i mpli- cations for minoritized language communities. we introduc e the concept of standard ai-generated language ideology, the pr ocess bywhich ai-generated language regards standard american e ng- lish(sae)asalinguisticdefaultandreinforcesalinguist icbiasthat sae is the most \u201cappropriate\u201d language. finally, we discuss ten- sions that remain, including re\ufb02ecting on what desirable sy stem behaviorlookslike,aswellasadvantagesanddrawbacksofg ener- ative ai tools imitating\u2014or often not\u2014di\ufb00erent english lan guage varieties.throughout,wediscussstandardlanguageideol ogyasa manifestation of existing global power structures in and th rough ai-generated language before ending with questions to move to- wards alternative, moreemancipatorydigital futures. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08757",
        "label": 1,
        "title": "srfund: a multi-granularity hierarchical structure reconstruction benchmark in form understanding jiefeng ma1y",
        "abstract": "accurately identifying and organizing textual content is crucial for the automation of document processing in the field of form understanding. existing datasets, such as funsd and xfund, support entity classification and relationship prediction tasks but are typically limited to local and entity-level annotations. this limitation overlooks the hierarchically structured representation of documents, constraining comprehensive understanding of complex forms. to address this issue, we present the srfund, a hierarchically structured multi-task form understanding bench- mark. srfund provides refined annotations on top of the original funsd and xfund datasets, encompassing five tasks: (1) word to text-line merging , (2) text-line to entity merging , (3) entity category classification , (4) item table lo- calization , and (5) entity-based full-document hierarchical structure recovery . we meticulously supplemented the original dataset with missing annotations at various levels of granularity and added detailed annotations for multi-item table regions within the forms. additionally, we introduce global hierarchical struc- ture dependencies for entity relation prediction tasks, surpassing traditional local key-value associations. the srfund dataset includes eight languages including english, chinese, japanese, german, french, spanish, italian, and portuguese , making it a powerful tool for cross-lingual form understanding. extensive exper- imental results demonstrate that the srfund dataset presents new challenges and significant opportunities in handling diverse layouts and global hierarchical structures of forms, thus providing deep insights into the field of form understand- ing. the original dataset and implementations of baseline methods are available at https://sprateam-ustc.github.io/srfund . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08796",
        "label": 1,
        "title": "deep exploration of cross-lingual zero-shot generalization in instruction tuning janghoon han",
        "abstract": "instruction tuning has emerged as a powerful technique, significantly boosting zero-shot per- formance on unseen tasks. while recent work has explored cross-lingual generalization by ap- plying instruction tuning to multilingual mod- els, previous studies have primarily focused on english, with a limited exploration of non- english tasks. for an in-depth exploration of cross-lingual generalization in instruction tun- ing, we perform instruction tuning individu- ally for two distinct language meta-datasets. subsequently, we assess the performance on unseen tasks in a language different from the one used for training. to facilitate this inves- tigation, we introduce a novel non-english meta-dataset named \"korani\" (korean natu- ral instruction), comprising 51 korean bench- marks. moreover, we design cross-lingual tem- plates to mitigate discrepancies in language and instruction-format of the template between training and inference within the cross-lingual setting. our experiments reveal consistent im- provements through cross-lingual generaliza- tion in both english and korean, outperforming baseline by average scores of 20.7% and 13.6%, respectively. remarkably, these enhancements are comparable to those achieved by monolin- gual instruction tuning and even surpass them in some tasks. the result underscores the sig- nificance of relevant data acquisition across lan- guages over linguistic congruence with unseen tasks during instruction tuning1. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09105",
        "label": 1,
        "title": "ins-mmbench: a comprehensive benchmark for evaluating lvlms\u2019 performance in insurance chenwei lin",
        "abstract": "large vision-language models (lvlms) have demonstrated outstanding per- formance in various general multimodal applications such as image recogni- tion and visual reasoning, and have also shown promising potential in special- ized domains. however, the application potential of lvlms in the insurance domain\u2014characterized by rich application scenarios and abundant multimodal data\u2014has not been effectively explored. there is no systematic review of multi- modal tasks in the insurance domain, nor a benchmark specifically designed to evaluate the capabilities of lvlms in insurance. this gap hinders the development of lvlms within the insurance domain. in this paper, we systematically review and distill multimodal tasks for four representative types of insurance: auto insurance, property insurance, health insurance, and agricultural insurance. we propose ins- mmbench, the first comprehensive lvlms benchmark tailored for the insurance domain. ins-mmbench comprises a total of 2.2k thoroughly designed multiple- choice questions, covering 12 meta-tasks and 22 fundamental tasks. furthermore, we evaluate multiple representative lvlms, including closed-source models such as gpt-4o and open-source models like blip-2. this evaluation not only validates the effectiveness of our benchmark but also provides an in-depth performance analysis of current lvlms on various multimodal tasks in the insurance domain. we hope that ins-mmbench will facilitate the further application of lvlms in the insurance domain and inspire interdisciplinary development. our dataset and evaluation code are available at https://github.com/fdu-ins/ins-mmbench . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09112",
        "label": 0,
        "title": "large-scale evaluation of open-set image classification techniques halil bisgin bisgin",
        "abstract": "the goal for classification is to correctly assign labels to unseen samples. however, most methods misclassify samples with unseen labels and assign them to one of the known classes. open-set classification (osc) algorithms aim to maximize both closed and open-set recog- nition capabilities. recent studies showed the utility of such algorithms on small-scale data sets, but limited experimentation makes it difficult to assess their performances in real-world problems. here, we provide a comprehensive comparison of various osc algo- rithms, including training-based (softmax, garbage, eos) and post-processing methods (maximum softmax scores, maximum logit scores, openmax, evm, proser), the lat- ter are applied on features from the former. we perform our evaluation on three large-scale protocols that mimic real-world challenges, where we train on known and negative open-set samples, and test on known and unknown instances. our results show that eos helps to improve performance of almost all post-processing algorithms. particularly, openmax and proser are able to exploit better-trained networks, demonstrating the utility of hybrid models. however, while most algorithms work well on negative test samples \u2013 samples of open-set classes seen during training \u2013 they tend to perform poorly when tested on samples of previously unseen unknown classes, especially in challenging conditions. keywords: open-set classification, large-scale evaluation, image classification, deep learn- ing, reproducible research 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09155",
        "label": 1,
        "title": "defan: definitive answer dataset for llms hallucination evaluation a b m ashikur rahman",
        "abstract": "large language models (llms) have demonstrated remarkable capabilities, revo- lutionizing the integration of ai in daily life applications. however, they are prone to hallucinations, generating claims that contradict established facts, deviating from prompts, and producing inconsistent responses when the same prompt is presented multiple times. addressing these issues is challenging due to the lack of com- prehensive and easily assessable benchmark datasets. most existing datasets are small and rely on multiple-choice questions, which are inadequate for evaluating the generative prowess of llms. to measure hallucination in llms, this paper introduces a comprehensive benchmark dataset comprising over 75,000 prompts across eight domains. these prompts are designed to elicit definitive, concise, and informative answers. the dataset is divided into two segments: one publicly available for testing and assessing llm performance and a hidden segment for benchmarking various llms. in our experiments, we tested six llms\u2014gpt-3.5, llama 2, llama 3, gemini, mixtral, and zephyr\u2014revealing that overall factual hallucination ranges from 59% to 82% on the public dataset and 57% to 76% in the hidden benchmark. prompt misalignment hallucination ranges from 6% to 95% in the public dataset and 17% to 94% in the hidden counterpart. average consistency ranges from 21% to 61% and 22% to 63%, respectively. domain-wise analysis shows that llm performance significantly deteriorates when asked for specific numeric information while performing moderately with person, location, and date queries. our dataset demonstrates its efficacy and serves as a comprehensive benchmark for llm performance evaluation. our dataset and llms responses are available at https://github.com/ashikiut/defan. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12793",
        "label": 0,
        "title": "chatglm: a family of large language models from glm-130b to glm-4 all tools team glm",
        "abstract": "we introduce chatglm, an evolving family of large language models that we have been developing over time. this report primarily focuses on the glm-4 language series, which includes glm-4, glm-4-air, and glm-4-9b. they represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of chatglm. to date, the glm-4 models are pre-trained on ten trillions of tokens mostly in chinese and english, along with a small set of corpus from 24 languages, and aligned primarily for chinese and english usage. the high-quality alignment is achieved via a multi-stage post- training process, which involves supervised fine-tuning and learning from human feedback. evaluations show that glm-4 1) closely rivals or outperforms gpt-4 in terms of general metrics such as mmlu, gsm8k, math, bbh, gpqa, and humaneval, 2) gets close to gpt-4-turbo in instruction following as measured by ifeval, 3) matches gpt-4 turbo (128k) and claude 3 for long context tasks, and 4) outperforms gpt-4 in chinese alignments as measured by alignbench. the glm- 4 all tools model is further aligned to understand user intent and autonomously decide when and which tool(s) to use\u2014including web browser, python interpreter, text-to-image model, and user-defined functions\u2014to effectively complete complex tasks. in practical applications, it matches and even surpasses gpt-4 all tools in tasks like accessing online information via web browsing and solving math problems using python interpreter. over the course, we have open-sourced a series of models, including chatglm-6b (three generations), glm-4-9b (128k, 1m), glm-4v-9b, webglm, and codegeex, attracting over 10 million downloads on hugging face in the year 2023 alone. the open models can be accessed through https://github.com/thudm andhttps://huggingface.co/thudm . *team glm: aohan zeng, bin xu, bowen wang, chenhui zhang, da yin, diego rojas, guanyu feng, hanlin zhao, hanyu lai, hao yu, hongning wang, jiadai sun, jiajie zhang, jiale cheng, jiayi gui, jie tang, jing zhang, juanzi li, lei zhao, lindong wu, lucen zhong, mingdao liu, minlie huang, peng zhang, qinkai zheng, rui lu, shuaiqi duan, shudan zhang, shulin cao, shuxun yang, weng lam tam, wenyi zhao, xiao liu, xiao xia, xiaohan zhang, xiaotao gu, xin lv, xinghan liu, xinyi liu, xinyue yang, xixuan song, xunkai zhang, yifan an, yifan xu, yilin niu, yuantao yang, yueyan li, yushi bai, yuxiao dong, zehan qi, zhaoyu wang, zhen yang, zhengxiao du, zhenyu hou, zihan wang. \u2020team members are listed alphabetically by first name. preprint. under review.arxiv:2406.12793v1  [cs.cl]  18 jun 2024glmmar. 2021glm-10bjun. 2021glm-130bcodegeex-13baug. 2022glm-proembeddingcharacterglmjun. 2023chatglm2-6bchatglm2-6b-32kcodegeex2-6bglm-4(0116)glm-4vcogview3jan. 2024 chatglm-130bmar. 2023chatglm-6bvisualglm-6bglm-3-turbooct. 2023chatglm3-6bchatglm3-6b-32kcogvlm-17b apisopen llmsopenvlmsglm-4(0520)glm-4-air(0605)jun. 2024glm-4-9bglm-4-9b-chatglm-4-9b-chat-1mglm-4v-9bcogvlm2-19bglm-4alltools webglmcodegeexcode interpreter agentmodelsautowebglm cogviewapr. 2022cogview2cogvideo(dec.)cogagent (may) (may)(may)(128k) (32k)(128k) (jul.)oct. 2022glm-130b(32k)(aug.)glm-10bmglm-1bfigure 1: the timeline of the glm family of language, code, vision, and agent models. the focus of this report is primarily on the language models, i.e., chatglm. the apis are publicly available at https://bigmodel.cn and open models can be accessed through https://github.com/thudm . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12822",
        "label": 0,
        "title": "is it good data for multilingual instruction tuning or just bad multilingual evaluation for large language mod",
        "abstract": "large language models, particularly multilin- gual ones, are designed, claimed, and expected to cater to native speakers of varied languages. we hypothesise that the current practices of fine-tuning and evaluating these models may mismatch this intention owing to a heavy re- liance on translation, which can introduce trans- lation artefacts and defects. it remains un- known whether the nature of the instruction data has an impact on the model output; on the other hand, it remains questionable whether translated test sets can capture such nuances. due to the often coupled practices of using translated data in both stages, such imperfec- tions could have been overlooked. this work investigates these issues by using controlled na- tive or translated data during instruction tuning and evaluation stages and observing model re- sults. experiments on eight base models and eight different benchmarks reveal that native or generation benchmarks display a notable dif- ference between native and translated instruc- tion data especially when model performance is high, whereas other types of test sets cannot. finally, we demonstrate that regularization is beneficial to bridging this gap on structured but not generative tasks.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12845",
        "label": 0,
        "title": "interpretable preferences via multi-objective reward modeling and mixture-of-experts haoxiang wang",
        "abstract": "reinforcement learning from human feedback (rlhf) has emerged as the primary method foraligninglargelanguagemodels(llms)withhumanpreferences. therlhfprocesstypically starts by training a reward model (rm) using human preference data. conventional rms are trained on pairwise responses to the same user request, with relative ratings indicating which response humans prefer. the trained rm serves as a proxy for human preferences. however, due to the black-box nature of rms, their outputs lack interpretability, as humans cannot intuitively understand why an rm thinks a response is good or not. as rms act as human preference proxies, it is desirable for them to be human-interpretable to ensure that their internal decision processes are consistent with human preferences and to prevent reward hacking in llm alignment. to build rms with interpretable preferences, we propose a two- stage approach: i) train an absolute-rating multi-objective reward model (armorm) with multi-dimensional absolute-rating data, each dimension corresponding to a human-interpretable objective (e.g., honesty, verbosity, safety); ii) employ a mixture-of-experts (moe) strategy with a gating network that automatically selects the most suitable reward objectives based on the context. we efficiently trained an armorm with llama-3 8b and a gating network consisting of a shallow mlp on top of the armorm. our trained model, armorm-llama3-8b , obtains state- of-the-art performance on rewardbench, a benchmark evaluating rms for language modeling. notably, the performance of our model surpasses the llm-as-a-judge method with gpt-4 judges by a margin, and approaches the performance of the much larger nemotron-4 340b reward model. our code and model are released at https://github.com/rlhflow/rlhf-rewar d-modeling . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12043",
        "label": 0,
        "title": "grade score: quantifying llm performance in option selection dmitri iourovitski",
        "abstract": "this study introduces the \u201dgrade score\u201d, a novel metric designed to evaluate the consistency and fairness of large language models (llms) when used as multiple-choice judges with respect to order bias and choice consistency. the grade score combines entropy, which measures order bias, and mode frequency, which assesses choice sta- bility, offering insights into llms\u2019 reliability and impartiality. the study explores techniques such as prompt engineering and option sam- pling strategies to optimize the grade score, demonstrating their effec- tiveness in enhancing llms\u2019 performance. results showcase varying performances among llms with respect to prompts and highlight the positive impact of including irrelevant options. the study also identi- fies an emergent behavior in instruction-following models, where they adapt to instructions targeting specific biases, demonstrating their adaptability. the grade score facilitates comparisons between llms and encourages ongoing research towards optimizing their decision- making processes, with potential implications for improving their re- liability and fairness in various applications. all code is available on github1 \u2217email: dmitri.io@utexas.edu 1https://github.com/iodmitri/gradelab 1arxiv:2406.12043v2  [cs.ai]  20 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12072",
        "label": 1,
        "title": "dtgb: a comprehensive benchmark for dynamic text-attributed graphs jiasheng zhang1",
        "abstract": "dynamic text-attributed graphs (dytags) are prevalent in various real-world scenarios, where each node and edge are associated with text descriptions, and both the graph structure and text descriptions evolve over time. despite their broad applicability, there is a notable scarcity of benchmark datasets tailored to dytags, which hinders the potential advancement in many research fields. to address this gap, we introduce dynamic text-attributed graph benchmark ( dtgb ), a collection of large-scale, time-evolving graphs from diverse domains, with nodes and edges enriched by dynamically changing text attributes and categories. to facilitate the use of dtgb, we design standardized evaluation procedures based on four real-world use cases: future link prediction, destination node retrieval, edge classification, and textual relation generation. these tasks require models to understand both dynamic graph structures and natural language, highlighting the unique challenges posed by dytags. moreover, we conduct extensive benchmark experiments on dtgb, evaluating 7 popular dynamic graph learning algorithms and their variants of adapting to text attributes with llm embeddings, along with 6 powerful large language models (llms). our results show the limitations of existing models in handling dytags. our analysis also demonstrates the utility of dtgb in investigating the incorporation of structural and textual dynamics. the proposed dtgb fosters research on dytags and their broad applications. it offers a comprehensive benchmark for evaluating and advancing models to handle the interplay between dynamic graph structures and natural language. the dataset and source code are available at https://github.com/zjs123/dtgb . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12843",
        "label": 0,
        "title": "can go ais be adversarially robust? tom tseng far ai",
        "abstract": "prior work found that superhuman go ais like katago can be defeated by simple adversarial strategies. in this paper, we study if simple defenses can improve katago\u2019s worst-case performance. we test three natural defenses: adversarial training on hand-constructed positions, iterated adversarial training, and changing the network architecture. we find that some of these defenses are able to protect against previously discovered attacks. unfortunately, we also find that none of these defenses are able to withstand adaptive attacks. in particular, we are able to train new adversaries that reliably defeat our defended agents by causing them to blunder in ways humans would not. our results suggest that building robust ai systems is challenging even in narrow domains such as go. for interactive examples of attacks and a link to our codebase, see https://goattack.far.ai/ . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09401",
        "label": 1,
        "title": "mmscan: a multi-modal 3d scene dataset with hierarchical grounded language annotations ruiyuan lyu1",
        "abstract": "with the emergence of llms and their integration with other data modalities, multi-modal 3d perception attracts more attention due to its connectivity to the physical world and makes rapid progress. however, limited by existing datasets, previous works mainly focus on understanding object properties or inter-object spatial relationships in a 3d scene. to tackle this problem, this paper builds the first largest ever multi-modal 3d scene dataset and benchmark with hierarchical grounded language annotations, mmscan. it is constructed based on a top-down logic, from region to object level, from a single target to inter-target relation- ships, covering holistic aspects of spatial and attribute understanding. the overall pipeline incorporates powerful vlms via carefully designed prompts to initialize the annotations efficiently and further involve humans\u2019 correction in the loop to ensure the annotations are natural, correct, and comprehensive. built upon exist- ing 3d scanning data, the resulting multi-modal 3d dataset encompasses 1.4m meta-annotated captions on 109k objects and 7.7k regions as well as over 3.04m diverse samples for 3d visual grounding and question-answering benchmarks. we evaluate representative baselines on our benchmarks, analyze their capabilities in different aspects, and showcase the key problems to be addressed in the future. furthermore, we use this high-quality dataset to train state-of-the-art 3d visual grounding and llms and obtain remarkable performance improvement both on existing benchmarks and in-the-wild evaluation. codes, datasets, and benchmarks will be available at https://github.com/openrobotlab/embodiedscan . preprint. under review.arxiv:2406.09401v1  [cs.cv]  13 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09406",
        "label": 1,
        "title": "4m-21: an any-to-any vision model for tens of tasks and modalities roman bachmann1\u2020",
        "abstract": "current multimodal and multitask foundation models, like 4m [ 62] or uni- fiedio [ 59,58], show promising results. however, their out-of-the-box abilities to accept diverse inputs and perform diverse tasks are limited by the (usually small) number of modalities and tasks they are trained on. in this paper, we develop a single any-to-any model trained on tens of highly diverse modalities and by performing co-training on large-scale multimodal datasets and text corpora. this includes training on images and text along with several semantic and geometric modalities, feature maps from recent state of the art models like dinov2 and imagebind, pseudo labels of specialist models like sam and 4dhumans, and a range of new modalities that allow for novel ways to interact with the model and steer the generation, for example, image metadata or color palettes. a crucial step in this process is performing discrete tokenization on various modalities, whether they are image-like, neural network feature maps, vectors, structured data like instance segmentation or human poses, or data that can be represented as text. through this, we show the possibility of training one model to solve at least 3x more tasks/modalities than existing models and doing so without a loss in performance . in addition, this enables more fine-grained and controllable multimodal generation capabilities and allows studying the distillation of models trained on diverse data and objectives into one unified model. we scale the training to a three billion parameter and different datasets. the multimodal models and training code are open sourced at https://4m.epfl.ch . *equal contribution & corresponding authors. randomized order. \u2020work partially done while at epfl and apple. preprint.arxiv:2406.09406v2  [cs.cv]  14 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09410",
        "label": 1,
        "title": "1 scene graph generation in large-size vhr satellite imagery: a large-scale dataset and a",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09411",
        "label": 0,
        "title": "muirbench : a comprehensive benchmark for robust multi-image understanding fei wang1",
        "abstract": "we introduce muirbench , a comprehensive benchmark that focuses on robust multi-image understanding capabilities of multimodal llms. muirbench con- sists of 12 diverse multi-image tasks ( e.g., scene understanding, ordering) that involve 10 categories of multi-image relations ( e.g., multiview, temporal relations). comprising 11,264 images and 2,600 multiple-choice questions, muirbench is created in a pairwise manner, where each standard instance is paired with an unanswerable variant that has minimal semantic differences, in order for a reliable assessment. evaluated upon 20 recent multi-modal llms, our results reveal that even the best-performing models like gpt-4o and gemini pro find it challenging to solve muirbench , achieving 68.0% and 49.3% in accuracy. open-source multimodal llms trained on single images can hardly generalize to multi-image questions, hovering below 33.3% in accuracy. these results highlight the im- portance of muirbench in encouraging the community to develop multimodal llms that can look beyond a single image, suggesting potential pathways for future improvements. \u2217equal leadership. correspondance to <fwang598@usc.edu; xingyuf2@seas.upenn.edu>. \u2020equal contribution; alphabetic order. project page: https://huggingface.co/datasets/muirbench/muirbench preprint. under review.arxiv:2406.09411v1  [cs.cv]  13 jun 2024figure 2: compared with previous benchmarks, muirbench has several novel features: (1) it evaluates on a comprehensive range of 12 multi-image understanding abilities, e.g. geographic understanding and diagram understanding as introduced in \u00a73, while prior benchmarks generally contain single-image questions. (2) it contains 10 diverse multi-image relations, e.g. narrative and complementary as discussed in \u00a73. (3) it provides a robust evaluation on models by unanswerable instance variants. the samples of previous benchmarks are from [25, 37, 53]. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09979",
        "label": 1,
        "title": "hiro: hierarchical information retrieval optimization krish goel",
        "abstract": "large language models (llms) excel in natural language tasks but face limitations due to static training datasets, result- ing in outdated or contextually shallow responses. retrieval- augmented generation (rag) addresses this by integrating real-time external knowledge, enhancing model accuracy and credibility, especially for knowledge-intensive tasks. how- ever, rag-enhanced llms struggle with long contexts, caus- ing them to \u201dchoke\u201d on information overload, compromising response quality. recent rag applications use hierarchical data structures for storing documents, organized at various levels of summarization and information density. in this con- text, we introduce hiro (hierarchical information retrieval optimization), a novel querying approach for rag appli- cations using hierarchical structures for storing documents. hiro employs dfs-based recursive similarity score calcula- tion and branch pruning to minimize the context returned to the llm without informational loss. hiro outperforms ex- isting querying mechanisms on the narrativeqa dataset by an absolute performance gain of 10.85%. code \u2014 https://github.com/krishgoel/hir"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09464",
        "label": 1,
        "title": "gpt-ology",
        "abstract": "large language models have taken the cognitive sci- ence world by storm. it is perhaps timely now to take stock of the various research paradigms that have been used to make scientific inferences about \u201ccognition\u201d in these models or about human cognition. we review sev- eral emerging research paradigms\u2014gpt-ology, llms- as-computational-models, and \u201csilicon sampling\u201d\u2014 and review recent papers that have used llms under these paradigms. in doing so, we discuss their claims as well as challenges to scientific inference under these vari- ous paradigms. we highlight several outstanding is- sues about llms that have to be addressed to push our science forward: closed-source vs open-sourced mod- els; (the lack of visibility of) training data; and repro- ducibility in llm research, including forming conven- tions on new task \u201chyperparameters\u201d like instructions and prompts. keywords: large language models; cognitive scienc"
    },
    {
        "url": "https://arxiv.org/pdf/2406.17038",
        "label": 0,
        "title": "mode ling: a novel dataset for testing linguistic reasoning in language models nathan a. chi1",
        "abstract": "we introduce mode ling1, a novel benchmark of linguistics olympiad-style puzzles which tests few-shot reasoning in ai systems. solving these puzzles necessitates inferring aspects of a language\u2019s grammatical structure from a small number of examples. such puzzles provide a natural testbed for language models, as they require compositional generalization and few- shot inductive reasoning. consisting solely of new puzzles written specifically for this work, mode ling has no risk of appearing in the training data of existing ai systems: this ame- liorates the risk of data leakage, a potential con- founder for many prior evaluations of reason- ing. evaluating several large open source lan- guage models and gpt on our benchmark, we observe non-negligible accuracy, demonstrat- ing few-shot emergent reasoning ability which cannot merely be attributed to shallow mem- orization. however, imperfect model perfor- mance suggests that mode ling can be used to measure further progress in linguistic reason- ing. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09520",
        "label": 0,
        "title": "education",
        "abstract": ": the use of generative artificial intelligence (genai) in academia is a subjective and hotly  debated topic. currently, there are no agreed guidelines towards the usage of genai systems in  higher education (he) and, thus, it is still unclear how to make effective use of the technology for  teaching and learning practice. this paper provides an overview of the current state of research on  genai for teaching and learning in he. to this end, this study conducted a systematic review of  relevant studies indexed by scopus, using the preferred reporting items for systematic reviews and  meta-analyses (prisma) guidelines. the search criteria revealed a total of 625 research papers, of  which 355 met the final inclusion criteria. the findings from the review showed the current state  and the future trends in documents, citations, document sources/authors, keywords, and co- authorship. the research gaps identified suggest that while some authors have looked at  understanding the detection of ai-generated text, it may be beneficial to understand how genai can  be incorporated into supp orting the educational curriculum for assessments, teaching, and learning  delivery. furthermore, there is a need for additional interdisciplinary, multidimensional studies in he  through collaboration. this will strengthen the awareness and understanding of students, tutors,  and other stakeholders, which will be instrumental in formulating guidelines, frameworks, and  policies for genai usage.      citation: ogunleye, b.; zakariyyah,  k.i.; ajao, o.; olayinka, o.; sharma,  h. a systematic review of generative  ai for teaching and learning practice.  educ. sci. 2024 , 14, 636. https://  doi.org/10.3390 /educsci14060636     academic editor: bracha kramarski    received: 28 march 2024   revised: 6 june 2024   accepted: 11 june 2024   published: 13 june 2024         copyright: \u00a9 2024  by the authors.  licensee mdpi, basel, switzerland.  this article is an open access article  distributed under the terms and  conditions of the creative comm ons  attribution (cc by) license (https://  creativecomm ons.org/licenses/by/  4.0/). keywords: artificial intelligence; generative ai; higher education; prisma; systematic literature  review; teaching and learning; topic modelling        1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09988",
        "label": 1,
        "title": "details make a difference: object state-sensitive neurorobotic task planning xiaowen sun",
        "abstract": ". the state of an object reflects its current status or condition and is important for a robot\u2019s task planning and manipulation. how- ever, detecting an object\u2019s state and generating a state-sensitive plan for robots is challenging. recently, pre-trained large language models (llms) and vision-language models (vlms) have shown impressive capabilities in generating plans. however, to the best of our knowledge, thereishardlyanyinvestigationonwhetherllmsorvlmscanalsogen- erate object state-sensitive plans. to study this, we introduce an object state-sensitive agent (ossa), a task-planning agent empowered by pre- trained neural networks. we propose two methods for ossa: (i) a modu- larmodelconsistingofapre-trainedvisionprocessingmodule(densecap- tioning model, dcm) and a natural language processing model (llm), and (ii) a monolithic model consisting only of a vlm. to quantitatively evaluate the performances of the two methods, we use tabletop scenarios where the task is to clear the table. we contribute a multimodal bench- mark dataset that takes object states into consideration. our results show that both methods can be used for object state-sensitive tasks, but the monolithic approach outperforms the modular approach. the code for ossa is available at https://github.com/xiao-wen-sun/ossa keywords: objectstateidentification \u00b7artificialintelligence \u00b7robotics \u00b7language models \u00b7multimodality "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10144",
        "label": 0,
        "title": "discovering the unknown: improving rule mining via embedding-based link prediction n\u2019dah jean kouagoua",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10228",
        "label": 1,
        "title": "vega  : learning interleaved image-text comprehension in vision-language large models",
        "abstract": "the swift progress of multi-modal large models (mllms) has showcased their impressive ability to tackle tasks blending vision and language. yet, most current models and benchmarks cater to scenarios with a narrow scope of visual and textual contexts. these models often fall short when faced with complex comprehension tasks, which in- volve navigating through a plethora of irrelevant and poten- tially misleading information in both text and image forms. to bridge this gap, we introduce a new, more demanding task known as interleaved image-text comprehension (iitc). this task challenges models to discern and disregard super- fluous elements in both images and text to accurately answer questions and to follow intricate instructions to pinpoint the relevant image. in support of this task, we further craft a new vega dataset, tailored for the iitc task on scien- tific content, and devised a subtask, image-text association (ita), to refine image-text correlation skills. our evaluation of four leading closed-source models, as well as various open-source models using vega, underscores the rigorous nature of iitc. even the most advanced models, such as gemini-1.5-pro and gpt4v , only achieved modest success. by employing a multi-task, multi-scale post-training strat- egy, we have set a robust baseline for mllms on the iitc task, attaining an 85.8%accuracy rate in image associa- tion and a 0.508rouge score. these results validate the effectiveness of our dataset in improving mllms capabili- ties for nuanced image-text comprehension. project page: https://zhourax.github.io/vega/ *equal contribution \u2020corresponding author \u2660project leader1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10221",
        "label": 0,
        "title": "short film dataset (sfd): a benchmark for story-level video understanding ridouane ghermi",
        "abstract": "recent advances in vision-language models have significantly propelled video understanding. existing datasets and tasks, however, have notable limitations. most datasets are confined to short videos with limited events and narrow narratives. for example, datasets with instructional and egocentric videos often document the activities of one person in a single scene. although some movie datasets offer richer content, they are often limited to short-term tasks, lack publicly available videos and frequently encounter data leakage given the use of movie forums and other resources in llm training. to address the above limitations, we propose the short film dataset (sfd) with 1,078 publicly available amateur movies, a wide variety of genres and minimal data leakage issues. sfd offers long-term story-oriented video tasks in the form of multiple-choice and open-ended question preprint. under review.arxiv:2406.10221v1  [cs.cv]  14 jun 20240 200 400 600 800 average video length (seconds)0100200300400total hoursactivitynet-qahow2qa egoschemasfdmovieqa moviechat lvucinepile tvqanext-qa ivqamovies (accessible) movies (restricted) egocentric instructional generalfigure 2: comparison of sfd to other vqa datasets. the circle size indicates the number of qa pairs in each dataset. 19.728.922.126.333.528.93624.131.518.344.134.556.755.455.464.451.971.3 28.869.964.17570.268.57164.176 15253545556575 gemma2b(42.3)mistral7b(62.5)llama 38b(68.4)gpt-3.5(70)mixtral8x7b(70.6)claude 3haiku(75.2)claude 3sonnet(79)llama 370b(82)gpt-4(86.4)% accuracyzero-shot llm accuracysfdmovieqalvuyto: youtube-objects voc: pascal voc 2007 6model and (mmlu)figure 3: data leakage. when given only the movie title, higher zero-shot accuracy in question- answering by llms indicates greater data leakage. llms are ranked by mmlu. answering. our extensive experiments emphasize the need for long-term reasoning to solve sfd tasks. notably, we find strong signals in movie transcripts leading to the on-par performance of people and llms. we also show significantly lower performance of current models compared to people when using vision data alone. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10210",
        "label": 1,
        "title": "make it count: text-to-image generation with an accurate number of objects lital binyamin1yoad tewel2",
        "abstract": "despite the unprecedented success of text-to-image diffusion models, controlling the number of depicted objects using text is surprisingly hard. this is important for various applications from technical documents, to children\u2019s books to illustrating cooking recipes. generating object-correct counts is fundamentally challenging because the generative model needs to keep a sense of separate identity for every instance of the object, even if several objects look identical or overlap, and then carry out a global computation implicitly during generation. it is still unknown if such representations exist. to address count-correct generation, we first identify features within the diffusion model that can carry the object identity information. we then use them to separate and count instances of objects during the denoising process and detect over-generation and under-generation. we fix the latter by training a model that predicts both the shape and location of a missing object, based on the layout of existing ones, and show how it can be used to guide denoising with correct object count. our approach, countgen , does not depend on external source to determine object layout, but rather uses the prior from the diffusion model itself, creating prompt-dependent and seed-dependent layouts. evaluated on two benchmark datasets, we find that countgen strongly outperforms the count- accuracy of existing baselines. countgen  (ours) \u201ca photo of six  kittens  sitting on a  branch\u201d \u201ca photo of \ufb01ve  eggs  in a carton\u201d \u201ca realistic photo of  goldilocks and three   bears eating a porridge\u201d \u201can illustration of  four  ninja turtles \u201d sdxl  \u201ca realistic photo of  seven  dwarves  dancing  in the forest\u201d  figure 1: countgen generates the correct number of objects specified in the input prompt while maintaining a natural layout that aligns with the prompt. preprint. under review.arxiv:2406.10210v1  [cs.cv]  14 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10163",
        "label": 1,
        "title": "meshanything: artist-created mesh generation with autoregressive transformers yiwen chen1",
        "abstract": "recently, 3d assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement. however, this potential is largely unrealized because these assets always need to be converted to meshes for 3d industry applications, and the meshes produced by current mesh extraction methods are significantly inferior to artist-created meshes (ams), i.e., meshes created by human artists. specifically, current mesh extraction methods rely on dense faces and ignore geometric features, leading to inefficiencies, complicated post-processing, and lower representation quality. to address these issues, we introduce meshanything, a model that treats mesh extraction as a generation problem, producing ams aligned with specified shapes. by converting 3d assets in any 3d representation into ams, meshanything can be integrated with various 3d asset production methods, thereby enhancing their application across the 3d industry. the architecture of meshanything comprises a vq-v ae and a shape-conditioned decoder-only transformer. we first learn a mesh vocabulary using the vq-v ae, then train the shape-conditioned decoder- only transformer on this vocabulary for shape-conditioned autoregressive mesh generation. our extensive experiments show that our method generates ams with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10157",
        "label": 0,
        "title": "robogolf: mastering real-world minigolf with a reflective multi-modality vision-language model hantao zhou1",
        "abstract": ": minigolf, a game with countless court layouts, and complex ball mo- tion, constitutes a compelling real-world testbed for the study of embodied intel- ligence. as it not only challenges spatial and kinodynamic reasoning but also re- quires reflective and corrective capacities to address erroneously designed courses. we introduce robogolf , a vlm-based framework that perceives dual-camera vi- sual inputs with nested vlm-empowered closed-loop control and reflective equi- librium loop. extensive experiments demonstrate the effectiveness of robogolf on challenging minigolf courts including those that are impossible to finish. ex- periment videos are available at https://jity16.github.io/robogolf/ . keywords: reflective equilibrium, closed-loop control, real-world minigolf, vi- sion language model "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10100",
        "label": 0,
        "title": "skysensegpt: a fine-grained instruction tuning dataset and model for remote sensing vision-language understand",
        "abstract": "remote sensing large multi-modal models (rslmms) are developing rapidly and showcase significant capabilities in remote sensing imagery (rsi) comprehension. however, due to the limitations of existing datasets, rslmms have shortcomings in understanding the rich semantic relations among objects in complex remote sensing scenes. to unlock rslmms\u2019 complex comprehension ability, we propose a large-scale instruction tuning dataset fit-rs, containing 1,800,851 instruction samples. fit-rs covers common interpretation tasks and innovatively introduces several complex comprehension tasks of escalating difficulty, ranging from re- lation reasoning to image-level scene graph generation. based on fit-rs, we build the fit-rsfg benchmark. furthermore, we establish a new benchmark to evaluate the fine-grained relation comprehension capabilities of lmms, named fit-rsrc. based on combined instruction data, we propose skysensegpt, which achieves outstanding performance on both public datasets and fit-rsfg, sur- passing existing rslmms. we hope the fit-rs dataset can enhance the relation comprehension capability of rslmms and provide a large-scale fine-grained data source for the remote sensing community. the dataset will be available at https://github.com/luo-z13/skysensegpt . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10079",
        "label": 0,
        "title": "localizing events in videos with multimodal queries gengyuan zhang1",
        "abstract": "video understanding is a pivotal task in the digital era, yet the dynamic and multi- event nature of videos makes them labor-intensive and computationally demanding to process. thus, localizing a specific event given a semantic query has gained importance in both user-oriented applications like video search and academic research into video foundation models. a significant limitation in current research is that semantic queries are typically in natural language that depicts the semantics of the target event. this setting overlooks the potential for multimodal semantic queries composed of images and texts. to address this gap, we introduce a new benchmark, icq, for localizing events in videos with multimodal queries, along with a new evaluation dataset icq-highlight. our new benchmark aims to evaluate how well models can localize an event given a multimodal semantic query that consists of a reference image, which depicts the event, and a refinement text to adjust the images\u2019 semantics. to systematically benchmark model performance, we include 4 styles of reference images and 5 types of refinement texts, allowing us to explore model performance across different domains. we propose 3 adaptation methods that tailor existing models to our new setting and evaluate 10 sota models, ranging from specialized to large-scale foundation models. we believe this benchmark is an initial step toward investigating multimodal queries in video event localization1. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09422",
        "label": 1,
        "title": "loopin: a pinfi protocol for decentralized computing yunwei mao1",
        "abstract": "networked computing power is a critical utility in the era of artificial intelligence. this paper presents a novel physical infrastructure finance (pinfi) protocol designed to facilitate the distribu- tion of computing power within networks in a decentralized manner. addressing the core challenges of coordination, pricing, and liquidity in decentralized physical infrastructure networks (depin), the pinfi protocol introduces a distinctive dynamic pricing mechanism. it enables providers to al- locate excess computing resources to a \u201cdissipative\u201d pinfi liquidity pool, distinct from traditional defi liquidity pools, ensuring seamless access for clients at equitable, market-based prices. this approach significantly reduces the costs of accessing computing power, potentially to as low as 1% compared to existing services, while simultaneously enhancing security and dependability. the pinfi protocol is poised to transform the dynamics of supply and demand in computing power networks, setting a new standard for efficiency and accessibility. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09454",
        "label": 1,
        "title": "advancing high resolution vision-language models in biomedicine zekai chen arda pekis kevin brown",
        "abstract": "multi-modal learning has transformed generative ai, particularly in vision- language modeling. advances such as the multi-modal gpt-4v and open-source projects like llav a have enabled robust conversational agents capable of zero-shot task completions. however, extending these technologies in the biomedical field introduces unique challenges. recent initiatives like llav a-med have begun to tailor instruction-tuning to biomedical contexts using extensive datasets like pmc-15m. our research contributes three significant advancements: (i) we intro- duce a new instruct dataset enriched with medical image-text pairs derived from claude3-opus and llama3 70b, (ii) we propose an innovative image encoding strategy that employs hierarchical representations to enhance fine-grained biomed- ical visual comprehension, and (iii) we develop the llama3-med model, which achieves state-of-the-art zero-shot performance on biomedical visual question an- swering benchmarks, improving performance by over 10% on average compared to prior methods. these advancements provide more precise and reliable tools for medical professionals, effectively bridging gaps in current multi-modal conversa- tional assistants and fostering further innovations in medical ai. codes available at https://github.com/standardmodelbio/llama3-med.git . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09688",
        "label": 0,
        "title": "freectrl: constructing control centers with feedforward layers for learning-free controllable text generation ",
        "abstract": "controllable text generation (ctg) seeks to craft texts adhering to specific attributes, tradi- tionally employing learning-based techniques such as training, fine-tuning, or prefix-tuning with attribute-specific datasets. these ap- proaches, while effective, demand extensive computational and data resources. in contrast, some proposed learning-free alternatives cir- cumvent learning but often yield inferior re- sults, exemplifying the fundamental machine learning trade-off between computational ex- pense and model efficacy. to overcome these limitations, we propose freectrl, a learning- free approach that dynamically adjusts the weights of selected feedforward neural network (ffn) vectors to steer the outputs of large lan- guage models (llms). freectrl hinges on the principle that the weights of different ffn vec- tors influence the likelihood of different tokens appearing in the output. by identifying and adaptively adjusting the weights of attribute- related ffn vectors, freectrl can control the output likelihood of attribute keywords in the generated content. extensive experiments on single- and multi-attribute control reveal that the learning-free freectrl outperforms other learning-free and learning-based methods, suc- cessfully resolving the dilemma between learn- ing costs and model performance1. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09760",
        "label": 1,
        "title": "bootstrapping language models with dpo implicit rewards changyu chen",
        "abstract": "human alignment in large language models (llms) is an active area of research. a recent groundbreaking work, direct preference optimization (dpo), has greatly simplified the process from past work in reinforcement learning from human feedback (rlhf) by bypassing the reward learning stage in rlhf. dpo, after training, provides an implicit reward model. in this work, we make a novel observation that this implicit reward model can by itself be used in a bootstrapping fashion to further align the llm. our approach is to use the rewards from a current llm model to construct a preference dataset, which is then used in subsequent dpo rounds. we incorporate refinements that debias the length of the responses and improve the quality of the preference dataset to further improve our approach. our approach, named self-alignment with dpoimplicit rewards (dice), shows great improvements in alignment and achieves superior performance than gemini pro on alpacaeval 2, reaching 27.55% length-controlled win rate against gpt-4 turbo, but with only 8b parameters and no external feedback. our code is available athttps://github.com/sail-sg/dice . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09455",
        "label": 0,
        "title": "pandora: towards general world model with natural language actions and video states jiannan xiang",
        "abstract": "world models simulate future states of the world in response to different actions. they facilitate interactive content creation and provides a foundation for grounded, long-horizon reasoning. current foundation models do not fully meet the capa- bilities of general world models: large language models (llms) are constrained by their reliance on language modality and their limited understanding of the physical world, while video models lack interactive action control over the world simulations. this paper makes a step towards building a general world model by introducing pandora , a hybrid autoregressive-diffusion model that simulates world states by generating videos and allows real-time control with free-text actions. pandora achieves domain generality , video consistency , and controllability through large-scale pretraining and instruction tuning. crucially, pandora bypasses the cost of training-from-scratch by integrating a pretrained llm (7b) and a pretrained video model, requiring only additional lightweight finetuning. we illustrate ex- tensive outputs by pandora across diverse domains (indoor/outdoor, natural/urban, human/robot, 2d/3d, etc.). the results indicate great potential of building stronger general world models with larger-scale training. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09486",
        "label": 1,
        "title": "semopo: learning high-quality model and policy from low-quality offline visual datasets shenghua wan1 2ziyuan ",
        "abstract": "model-based offline reinforcement learning (rl) is a promising approach that leverages existing data effectively in many real-world applications, especially those involving high-dimensional in- puts like images and videos. to alleviate the distribution shift issue in offline rl, existing model-based methods heavily rely on the uncer- tainty of learned dynamics. however, the model uncertainty estimation becomes significantly bi- ased when observations contain complex distrac- tors with non-trivial dynamics. to address this challenge, we propose a new approach - sepa- rated model-based offline policy optimization (semopo) - decomposing latent states into en- dogenous and exogenous parts via conservative sampling and estimating model uncertainty on the endogenous states only. we provide a theoret- ical guarantee of model uncertainty and perfor- mance bound of semopo. to assess the efficacy, we construct the low-quality vision deep data- driven datasets for rl (lqv-d4rl), where the data are collected by non-expert policy and the observations include moving distractors. exper- imental results show that our method substan- tially outperforms all baseline methods, and fur- ther analytical experiments validate the critical designs in our method. the project website is https://sites.google.com/view/semopo. 1school of artificial intelligence, nanjing university, china 2national key laboratory for novel software technology, nanjing university, china3school of mathematical sciences, center for statistical science, peking university, beijing, china4school of cyberspace science and technology, beijing institute of tech- nology, beijing, china. correspondence to: de-chuan zhan <zhandc@nju.edu.cn >. proceedings of the 41stinternational conference on machine learning , vienna, austria. pmlr 235, 2024. copyright 2024 by the author(s).1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09496",
        "label": 0,
        "title": "you are what you eat? feeding foundation models a regionally diverse food dataset of world wide dishes jabez m",
        "abstract": "foundation models are increasingly ubiquitous in our daily lives, used in everyday tasks such as text-image searches, interactions with chatbots, and content gener- ation. as use increases, so does concern over the disparities in performance and fairness of these models for different people in different parts of the world. to assess these growing regional disparities, we present world widedishes , a mixed text and image dataset consisting of 765 dishes, with dish names collected in 131 local languages. world widedishes has been collected purely through human contribution and decentralised means, by creating a website widely distrib- uted through social networks. using the dataset, we demonstrate a novel means of operationalising capability and representational biases in foundation models such as language models and text-to-image generative models. we enrich these studies with a pilot community review to understand, from a first-person perspective, how these models generate images for people in five african countries and the united states. we find that these models generally do not produce quality text and image outputs of dishes specific to different regions. this is true even for the us, which is typically considered to be more well-resourced in training data\u2014though the gener- ation of us dishes does outperform that of the investigated african countries. the models demonstrate a propensity to produce outputs that are inaccurate as well as culturally misrepresentative, flattening, and insensitive. these failures in capability and representational bias have the potential to further reinforce stereotypes and disproportionately contribute to erasure based on region. the dataset and code are available at https://github.com/oxai/world-wide-dishes/ . \u2217joint first author. \u2020work done in affiliation with the oxford artificial intelligence society. \u2021corresponding author: siobhan.hall@nds.ox.ac.uk. preprint. under review.arxiv:2406.09496v1  [cs.cy]  13 jun 2024example dish  dall-e 2  dall-e 3  sd v2.1 baghrir eru nyama choma ofe okazi babotie hot dog algeria cameroon kenya nigeria south africa united states figure 1: theworld widedishes dataset contains 765 unique instances of dishes from around the world. this paper presents image generation analysis of dishes associated with the united states and five countries across africa. the 1strow shows example images of the dishes from each country, and the 2ndthrough 4th rows show images generated by dall-e 2, dall-e 3, and stable diffusion v2.1, respectively. all models tend to mis-characterise the dishes. dall-e 2 often outputs the incorrect dish; dall-e 3 tends to exaggerate both visual and cultural stereotypes and to make images more cartoonish; and stable diffusion often generates incoherent images barely resembling food. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.18346",
        "label": 0,
        "title": "ai a lignment through reinforcement learning from human feedback ? contradictions and limitations adam dahlgre",
        "abstract": "this paper critically evaluates the attempts to align arti\ufb01 cial intelligence (ai) systems, especially large language models (llms), with human values and intenti ons through reinforcement learn- ing from feedback (rlxf) methods, involving either human fe edback (rlhf) or ai feedback (rlaif). speci\ufb01cally, we show the shortcomings of the broad ly pursued alignment goals of honesty, harmlessness, and helpfulness. through a multidisciplina ry sociotechnical critique, we examine both the theoretical underpinnings and practical implemen tations of rlxf techniques, revealing sig- ni\ufb01cant limitations in their approach to capturing the comp lexities of human ethics and contributing to ai safety. we highlight tensions and contradictions inhe rent in the goals of rlxf. in addition, we discuss ethically-relevant issues that tend to be neglecte d in discussions about alignment and rlxf, among which the trade-offs between user-friendliness and d eception, \ufb02exibility and interpretability, and system safety. we conclude by urging researchers and pra ctitioners alike to critically assess the sociotechnical rami\ufb01cations of rlxf, advocating for a more nuanced and re\ufb02ective approach to its application in ai development. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09622",
        "label": 1,
        "title": "dsl-fiqa: assessing facial image quality via dual-set degradation learning and landmark-guided transformer wei",
        "abstract": "generic face image quality assessment (gfiqa) evalu- ates the perceptual quality of facial images, which is crucial in improving image restoration algorithms and selecting high-quality face images for downstream tasks. we present a novel transformer-based method for gfiqa, which is aided by two unique mechanisms. first, a \u201c dual-set degradation representation learning\u201d (dsl) mechanism uses facial images with both synthetic and real degrada- tions to decouple degradation from content, ensuring gen- eralizability to real-world scenarios. this self-supervised method learns degradation features on a global scale, pro- viding a robust alternative to conventional methods that use local patch information in degradation learning. second, our transformer leverages facial landmarks to emphasize visually salient parts of a face image in evaluating its per- ceptual quality. we also introduce a balanced and diverse comprehensive generic face iqa (cgfiqa-40k) dataset of 40k images carefully designed to overcome the biases, in particular the imbalances in skin tone and gender represen- tation, in existing datasets. extensive analysis and evalua- tion demonstrate the robustness of our method, marking a significant improvement over prior methods. 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09675",
        "label": 1,
        "title": "benchmarking spectral graph neural networks: a comprehensive study on effectiveness and efficiency ningyi liao",
        "abstract": "with the recent advancements in graph neural networks (gnns), spectral gnns have received increasing popularity by virtue of their specialty in capturing graph signals in the frequency domain, demonstrating promising capability in specific tasks. however, few systematic studies have been conducted to assess their spectral characteristics. this emerging family of models also varies in terms of design and settings, leading to difficulties in comparing their performance and deciding on the suitable model for specific scenarios, especially for large-scale tasks. in this work, we extensively benchmark spectral gnns with a focus on the frequency perspective. we analyze and categorize over 30 gnns with 27 corresponding filters. then, we implement these spectral models within a unified framework with dedicated graph computations and efficient training schemes. thorough experiments are conducted on the spectral models with inclusive metrics on effectiveness and efficiency, offering practical guidelines on evaluating and selecting spectral gnns with desirable performance. our implementation enables application on larger graphs with comparable performance and less overhead, which is available at: https://github.com/gdmnl/spectral-gnn-benchmark . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09723",
        "label": 0,
        "title": "when will gradient regularization be harmful? yang zhao1hao zhang1xiuyuan hu1 abstract",
        "abstract": "gradient regularization (gr), which aims to pe- nalize the gradient norm atop the loss function, has shown promising results in training modern over-parameterized deep neural networks. how- ever, can we trust this powerful technique? this paper reveals that gr can cause performance de- generation in adaptive optimization scenarios, par- ticularly with learning rate warmup. our empiri- cal and theoretical analyses suggest this is due to gr inducing instability and divergence in gradi- ent statistics of adaptive optimizers at the initial training stage. inspired by the warmup heuristic, we propose three gr warmup strategies, each re- laxing the regularization effect to a certain extent during the warmup course to ensure the accurate and stable accumulation of gradients. with exper- iments on vision transformer family, we confirm the three gr warmup strategies can effectively circumvent these issues, thereby largely improv- ing the model performance. meanwhile, we note that scalable models tend to rely more on the gr warmup, where the performance can be improved by up to 3% on cifar10 compared to baseline gr. code is available at https://github.com/zhaoyang- 0204/gnp. 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09770",
        "label": 0,
        "title": "towards efficient pareto set approximation via mixture of experts based model fusion anke tang",
        "abstract": "solving multi-objective optimization problems for large deep neural networks is a challenging task due to the complexity of the loss landscape and the expensive computational cost of training and evaluating models. efficient pareto front approx- imation of large models enables multi-objective optimization for various tasks such as multi-task learning and trade-off analysis. existing algorithms for learning pareto set, including (1) evolutionary, hypernetworks, and hypervolume-maximization methods, are computationally expensive and have restricted scalability to large models; (2) scalarization algorithms, where a separate model is trained for each objective ray, which is inefficient for learning the entire pareto set and fails to capture the objective trade-offs effectively. inspired by the recent success of model merging, we propose a practical and scalable approach to pareto set learning prob- lem via mixture of experts (moe) based model fusion. by ensembling the weights of specialized single-task models, the moe module can effectively capture the trade-offs between multiple objectives and closely approximate the entire pareto set of large neural networks. once the routers are learned and a preference vec- tor is set, the moe module can be unloaded, thus no additional computational cost is introduced during inference. we conduct extensive experiments on vision and language tasks using large-scale models such as clip-vit and gpt-2. the experimental results demonstrate that our method efficiently approximates the entire pareto front of large models. using only hundreds of trainable parameters of the moe routers, our method even has lower memory usage compared to linear scalarization and algorithms that learn a single pareto optimal solution, and are scalable to both the number of objectives and the size of the model. our method significantly reduces the computational burden of learning the pareto set, for exam- ple, in the two-task case, it can be achieved in just a few minutes. code is available at:https://github.com/tanganke/pareto_set_learning "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09831",
        "label": 0,
        "title": "received xx month",
        "abstract": "federated learning (fl) offers a compelling framework for training large language models (llms) while addressing data privacy and decentralization challenges. this paper surveys recent advance- ments in the federated learning of large language models, with a particular focus on machine unlearning\u2014a crucial aspect for complying with privacy regulations like the right to be forgotten. machine unlearning in the context of federated llms involves systematically and securely removing individual data contributions from the learned model without retraining from scratch. we explore various strategies that enable effective unlearning, such as perturbation techniques, model decomposition, and incremental learning, highlighting their implications for maintaining model performance and data privacy. furthermore, we examine case studies and experimental results from recent literature to assess the effectiveness and efficiency of these approaches in real-world scenarios. our survey reveals a growing interest in developing more robust and scalable federated unlearning methods, suggesting a vital area for future research in the intersection of ai ethics and distributed machine learning technologies. index terms federated learning (fl), large language models (llms), swarm intelligence, efficiency, pre-trained models, privacy and security i"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09838",
        "label": 1,
        "title": "vision-language models meet meteorology: developing models for extreme weather events detection with heatmaps",
        "abstract": "real-time detection and prediction of extreme weather protect human lives and infrastructure. traditional methods rely on numerical threshold setting and manual interpretation of weather heatmaps with geographic information systems (gis), which can be slow and error-prone. our research redefines extreme weather events detection (ewed) by framing it as a visual question answering (vqa) problem, thereby introducing a more precise and automated solution. leveraging vision-language models (vlm) to simultaneously process visual and textual data, we offer an effective aid to enhance the analysis process of weather heatmaps. our initial assessment of general-purpose vlms (e.g., gpt-4-vision) on ewed revealed poor performance, characterized by low accuracy and frequent halluci- nations due to inadequate color differentiation andinsufficient meteorological knowledge . to address these challenges, we introduce climateiqa , the first mete- orological vqa dataset, which includes 8,760 wind gust heatmaps and 254,040 question-answer pairs covering four question types, both generated from the latest climate reanalysis data. we also propose sparse position and outline tracking (spot) , an innovative technique that leverages opencv and k-means clustering to capture and depict color contours in heatmaps, providing climateiqa with more accurate color spatial location information. finally, we present climate-zoo , the first meteorological vlm collection, which adapts vlms to meteorological applications using the climateiqa dataset. experiment results demonstrate that models from climate-zoo substantially outperform state-of-the-art general vlms, achieving an accuracy increase from 0% to over 90% in ewed verification. the datasets and models in this study are publicly available for future climate science research: https://github.com/alexjjjchen/climate-zoo . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09870",
        "label": 1,
        "title": "igl-bench: establishing the comprehensive benchmark for imbalanced graph learning jiawen qin1",
        "abstract": "deep graph learning has gained grand popularity over the past years due to its versa- tility and success in representing graph data across a wide range of domains. how- ever, the pervasive issue of imbalanced graph data distributions, where certain parts exhibit disproportionally abundant data while others remain sparse, undermines the efficacy of conventional graph learning algorithms, leading to biased outcomes. to address this challenge, imbalanced graph learning (igl) has garnered substantial attention, enabling more balanced data distributions and better task performance. despite the proliferation of igl algorithms, the absence of consistent experimental protocols and fair performance comparisons pose a significant barrier to compre- hending advancements in this field. to bridge this gap, we introduce igl-bench , a foundational comprehensive benchmark for imbalanced graph learning, embarking on16diverse graph datasets and 24distinct igl algorithms with uniform data processing and splitting strategies. specifically, igl-bench systematically inves- tigates state-of-the-art igl algorithms in terms of effectiveness ,robustness , and efficiency on node-level and graph-level tasks, with the scope of class-imbalance andtopology-imbalance . extensive experiments demonstrate the potential benefits of igl algorithms on various imbalanced conditions, offering insights and opportu- nities in the igl field. further, we have developed an open-sourced and unified package to facilitate reproducible evaluation and inspire further innovative research, which is available at https://github.com/ringbdstack/igl-bench . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09877",
        "label": 0,
        "title": "federated learning with flexible architectures jong-ik park and carlee joe-wong \u0000[0000\u22120003\u22120785\u22129291] carnegi",
        "abstract": ". traditional federated learning (fl) methods have limited support for clients with varying computational and communication abil- ities, leading to inefficiencies and potential inaccuracies in model train- ing. this limitation hinders the widespread adoption of fl in diverse and resource-constrained environments, such as those with client devices ranging from powerful servers to mobile devices. to address this need, this paper introduces federated learning with flexible architectures (fedfa), an fl training algorithm that allows clients to train models of different widths and depths. each client can select a network architec- ture suitable for its resources, with shallower and thinner networks re- quiring fewer computing resources for training. unlike prior work in this area, fedfa incorporates the layer grafting technique to align clients\u2019 lo- cal architectures with the largest network architecture in the fl system during model aggregation. layer grafting ensures that all client contribu- tions are uniformly integrated into the global model, thereby minimizing the risk of any individual client\u2019s data skewing the model\u2019s parameters disproportionately and introducing security benefits. moreover, fedfa introduces the scalable aggregation method to manage scale variations in weights among different network architectures. experimentally, fedfa outperforms previous width and depth flexible aggregation strategies. specifically, fedfa\u2019s testing accuracy matches (1.00 times) or is up to 1.16 times higher globally for iid settings, 0.98 to 1.13 times locally, and 0.95 times to 1.20 times higher globally for non-iid settings com- pared to earlier strategies. furthermore, fedfa demonstrates increased robustness against performance degradation in backdoor attack scenar- ios compared to earlier strategies. earlier strategies exhibit more drops in testing accuracy under attacks\u2014for iid data by 1.01 to 2.11 times globally, and for non-iid data by 0.89 to 3.31 times locally, and 1.11 to 1.74 times globally, compared to fedfa. keywords: federated learning \u00b7heterogeneous local network archi- tectures \u00b7backdoor attack "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10040",
        "label": 1,
        "title": "fzi-wim at semeval-2024 task 2: self-consistent cot for complex nli in biomedical domain jin liu1",
        "abstract": "this paper describes the inference system of fzi-wim at the semeval-2024 task 2: safe biomedical natural language inference for clinical trials. our system utilizes the chain of thought (cot) paradigm to tackle this com- plex reasoning problem and further improves the cot performance with self-consistency. in- stead of greedy decoding, we sample multiple reasoning chains with the same prompt and make the final verification with majority voting. the self-consistent cot system achieves a base- line f1 score of 0.80 (1st), faithfulness score of 0.90 (3rd), and consistency score of 0.73 (12th). we release the code and data publicly1. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09948",
        "label": 1,
        "title": "ble nd: a benchmark for llms on everyday knowledge in diverse cultures and languages junho myung1",
        "abstract": "large language models (llms) often lack culture-specific knowledge of daily life, especially across diverse regions and non-english languages. existing benchmarks for evaluating llms\u2019 cultural sensitivities are limited to a single language or col- lected from online sources such as wikipedia, which do not reflect the mundane everyday lifestyles of diverse regions. that is, information about the food people eat for their birthday celebrations, spices they typically use, musical instruments youngsters play, or the sports they practice in school is common cultural knowledge but uncommon in easily collected online sources, especially for underrepresented cultures. to address this issue, we introduce ble nd, a hand-crafted benchmark designed to evaluate llms\u2019 everyday knowledge across diverse cultures and lan- guages. ble nd comprises 52.6k question-answer pairs from 16 countries/regions, in 13 different languages, including low-resource ones such as amharic, assamese, azerbaijani, hausa, and sundanese. we construct the benchmark to include two formats of questions: short-answer and multiple-choice. we show that llms perform better for cultures that are highly represented online, with a maximum 57.34% difference in gpt-4, the best-performing model, in the short-answer format. for cultures represented by mid-to-high-resource languages, llms perform better in their local languages, but for cultures represented by low-resource languages, llms perform better in english than the local languages. we make our dataset publicly available at: https://github.com/nlee0212/blend . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10099",
        "label": 0,
        "title": "know the unknown: an uncertainty-sensitive method for llm instruction tuning jiaqi li1",
        "abstract": "large language models (llms) have demon- strated remarkable capabilities across various tasks but still face challenges such as hallu- cinations. one potential reason for halluci- nations is the lack of relevant knowledge or context. thus, a promising solution to miti- gate this issue involves instructing llms to respond with \"i do not know\" when a question falls outside their knowledge domain or the provided context. however, in this work, we observed that llms struggle to admit their lack of knowledge, primarily due to existing instruc- tion datasets designed to encourage specific answers. to improve large language models\u2019 capability to recognize the boundaries of their knowledge, we propose a novel approach called uncertainty-sensitive tuning. this method in- volves two-stage training designed for uncer- tainty recognition and prompt-sensitive activa- tion. in the first stage, we guide the llm to reject unknown questions. in the second stage, we recover the decreased performance in qa tasks by incorporating designed causal instruc- tions. by leveraging this method, we aim to en- hance the model\u2019s ability to identify areas of un- certainty. the experimental results demonstrate that our proposed uncertainty-sensitive tuning method significantly improves the performance of the llama2-chat-7b model. specifically, it achieves a substantial 34.7% improvement in handling questions involving knowledge gaps compared to the original model. moreover, our approach outperforms gpt-4, exhibiting a 9.4% increase in overall performance. we open-source the model and code on github1. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10118",
        "label": 1,
        "title": "seacrowd: a multilingual multimodal data hub and benchmark suite for southeast asian languages holy lovenia",
        "abstract": "southeast asia (sea) is a region rich in lin- guistic diversity and cultural variety, with over 1,300 indigenous languages and a population of 671 million people. however, prevailing ai models suffer from a significant lack of repre- sentation of texts, images, and audio datasets from sea, compromising the quality of ai models for sea languages. evaluating models for sea languages is challenging due to the scarcity of high-quality datasets, compounded by the dominance of english training data, rais- ing concerns about potential cultural misrep- resentation. to address these challenges, we introduce seacrowd, a collaborative initia- tive that consolidates a comprehensive resource hub1that fills the resource gap by providing standardized corpora2in nearly 1,000 sea lan- guages across three modalities. through our seacrowd benchmarks, we assess the qual- ity of ai models on 36 indigenous languages across 13 tasks, offering valuable insights into the current ai landscape in sea. furthermore, we propose strategies to facilitate greater ai ad- vancements, maximizing potential utility and resource equity for the future of ai in sea. 1https://seacrowd.github.io/seacrowd-catalogue/ 2https://github.com/seacrowd/seacrowd-datahub/"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10130",
        "label": 0,
        "title": "published as a conference paper at iclr 2024 thedevil is in the neurons : interpreting and mitigating social b",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10172",
        "label": 1,
        "title": "datasets for multilingual answer sentence selection matteo gabburo1",
        "abstract": "answer sentence selection (as2) is a criti- cal task for designing effective retrieval-based question answering (qa) systems. most ad- vancements in as2 focus on english due to the scarcity of annotated datasets for other languages. this lack of resources prevents the training of effective as2 models in dif- ferent languages, creating a performance gap between qa systems in english and other lo- cales. in this paper, we introduce new high- quality datasets for as2 in five european lan- guages (french, german, italian, portuguese, and spanish), obtained through supervised au- tomatic machine translation (amt) of ex- isting english as2 datasets such as asnq, wikiqa, and trec-qa using a large lan- guage model (llm). we evaluated our ap- proach and the quality of the translated datasets through multiple experiments with different transformer architectures. the results indicate that our datasets are pivotal in producing robust and powerful multilingual as2 models, signifi- cantly contributing to closing the performance gap between english and other languages. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10173",
        "label": 1,
        "title": "intention qa: a benchmark for evaluating purchase intention comprehension abilities of language models in e-co",
        "abstract": "enhancing language models\u2019 (lms) ability to understand purchase intentions in e-commerce scenarios is crucial for their effective assis- tance in various downstream tasks. how- ever, previous approaches that distill inten- tions from lms often fail to generate mean- ingful and human-centric intentions applica- ble in real-world e-commerce contexts. this raises concerns about the true comprehension and utilization of purchase intentions by lms. in this paper, we present intention qa, a double-task multiple-choice question answer- ing benchmark to evaluate lms\u2019 comprehen- sion of purchase intentions in e-commerce. specifically, lms are tasked to infer inten- tions based on purchased products and uti- lize them to predict additional purchases. in- tention qa consists of 4,360 carefully cu- rated problems across three difficulty levels, constructed using an automated pipeline to ensure scalability on large e-commerce plat- forms. human evaluations demonstrate the high quality and low false-negative rate of our benchmark. extensive experiments across 19 language models show that they still strug- gle with certain scenarios, such as understand- ing products and intentions accurately, jointly reasoning with products and intentions, and more, in which they fall far behind human performances. our code and data are pub- licly available at https://github.com/hkust- knowcomp/intentionqa. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10203",
        "label": 0,
        "title": "a fundamental trade-off in aligned language models and its relation to sampling adaptors naaman tan",
        "abstract": "the relationship between the quality of a string and its probability p(y)under a language model has been influential in the development of techniques to build good text generation systems. for example, several decoding algorithms have been motivated to manipulate p(y)to produce higher-quality text. in this work, we examine the probability\u2013quality relationship in language models explicitly aligned to human preferences, e.g., through reinforcement learning through human feedback (rlhf). we find that, given a general language model and its aligned version, for corpora sampled from an aligned language model, there exists a trade-off between the average reward and average log-likelihood of the strings under the general language model. we provide a formal treatment of this issue and demonstrate how a choice of sampling adaptor allows for a selection of how much likelihood we exchange for the reward. https://github.com/tanyjnaaman/ probability-quality-paradox "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11873",
        "label": 0,
        "title": "logic-based explainability: past",
        "abstract": ". in recent years, the impact of machine learning (ml) and ar- ti\ufb01cial intelligence (ai) in society has been absolutely re markable. this impact is expected to continue in the foreseeable future. ho wever, the adoption of ai/ml is also a cause of grave concern. the operat ion of the most advances ai/ml models is often beyond the grasp of hu man decision makers. as a result, decisions that impact humans m ay not be understood and may lack rigorous validation. explainable a i (xai) is concerned with providing human decision-makers with under standable explanations for the predictions made by ml models. as a resu lt, xai is a cornerstone of trustworthy ai. despite its strategic impo rtance, most work on xai lacks rigor, and so its use in high-risk or safety- critical domains serves to foster distrust instead of contributing t o build much- needed trust. logic-based xai has recently emerged as a rigo rous alter- native to those other non-rigorous methods of xai. this pape r provides a technical survey of logic-based xai, its origins, the curr ent topics of re- search, and emerging future topics of research. the paper al so highlights the many myths that pervade non-rigorous approaches for xai . keywords: explainable ai \u00b7 symbolic ai \u00b7 formal explainability \u00b7 cer- ti\ufb01cation "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11888",
        "label": 0,
        "title": "neural logic programs and neural nets christian anti \u00b4c christian.antic",
        "abstract": ". neural-symbolic integration aims to combine the connecti onist subsymbolic with the logical symbolic approach to arti\ufb01cial intelligence. in this paper , we \ufb01rst de\ufb01ne the answer set semantics of (boolean) neural nets and then introduce from \ufb01rst principl es a class of neural logic programs and show that nets and programs are equivalent. 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.11897",
        "label": 1,
        "title": "a benchmark for maximum cut: towards standardization of the evaluation of learned heuristics for combinatorial",
        "abstract": "recently, there has been much work on the design of general heuristics for graph- based, combinatorial optimization problems via the incorporation of graph neural networks (gnns) to learn distribution-specific solution structures. however, there is a lack of consistency in the evaluation of these heuristics, in terms of the baselines and instances chosen, which makes it difficult to assess the relative performance of the algorithms. in this paper, we propose an open-source benchmark suite maxcut- bench dedicated to the np-hard maximum cut problem in both its weighted and unweighted variants, based on a careful selection of instances curated from diverse graph datasets. the suite offers a unified interface to various heuristics, both traditional and machine learning-based. next, we use the benchmark in an attempt to systematically corroborate or reproduce the results of several, popular learning- based approaches, including s2v-dqn [ 31], eco-dqn [ 4], among others, in terms of three dimensions: objective value ,generalization , and scalability . our empirical results show that several of the learned heuristics fail to outperform a naive greedy algorithm, and that only one of them consistently outperforms tabu search, a simple, general heuristic based upon local search. furthermore, we find that the performance of eco-dqn remains the same or is improved if the gnn is replaced by a simple linear regression on a subset of the features that are related to tabu search. code, data, and pretrained models are available at: https://github.com/ankurnath/maxcut-bench . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11911",
        "label": 0,
        "title": "a notion of complexity for theory of mind via discrete world models x. angelo huang1",
        "abstract": "theory of mind (tom) can be used to assess the capabilities of large language models (llms) in complex scenarios where social reasoning is required. while the research community has proposed many tom benchmarks, their hard- ness varies greatly, and their complexity is not well defined. this work proposes a framework to measure the complexity of tom tasks. we quantify a problem\u2019s complexity as the number of states necessary to solve it correctly. our complexity measure also accounts for spurious states of a tom problem designed to make it apparently harder. we use our method to as- sess the complexity of five widely adopted tom benchmarks. on top of this framework, we de- sign a prompting technique that augments the information available to a model with a descrip- tion of how the environment changes with the agents\u2019 interactions. we name this technique discrete world models (dwm) and show how it elicits superior performance on tom tasks. https://github.com/flecart/ complexity-tom-dwm "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11915",
        "label": 1,
        "title": "minicodeprops: a minimal benchmark for proving code properties evan lohn",
        "abstract": "neural networks have shown initial promise in automating mathematical theorem proving in proof assistants such as lean. the same proof assistants can be used to verify the correctness of code by pairing code with specifications and proofs that the specifications hold. automating the writing of code, specifications, and proofs could lower the cost of verification, or, ambitiously, enable a machine learning sys- tem to output provably correct code. however, it remains unclear whether current neural theorem provers can automatically verify even relatively simple programs. we present minicodeprops , a benchmark of 177 program specifications in the lean proof assistant, aimed at the subproblem of automatically generating a proof for a provided program and specification. minicodeprops contains specifications about simple, self-contained programs (e.g., lists, natural numbers, binary trees) with varied proof difficulty. despite its simplicity, minicodeprops is challenging for current llm-based provers, which succeed in proving about 25 percent of the specifications. we publicly release minicodeprops as a benchmark for furthering automated theorem proving in the context of formally verified code. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11920",
        "label": 1,
        "title": "job-sdf: a multi-granularity dataset for job skill demand forecasting and benchmarking xi chen1",
        "abstract": "in a rapidly evolving job market, skill demand forecasting is crucial as it enables policymakers and businesses to anticipate and adapt to changes, ensuring that workforce skills align with market needs, thereby enhancing productivity and competitiveness. additionally, by identifying emerging skill requirements, it directs individuals towards relevant training and education opportunities, promoting continuous self-learning and development. however, the absence of comprehensive datasets presents a significant challenge, impeding research and the advancement of this field. to bridge this gap, we present job-sdf , a dataset designed to train and benchmark job-skill demand forecasting models. based on 10.35 million public job advertisements collected from major online recruitment platforms in china between 2021 and 2023, this dataset encompasses monthly recruitment demand for 2,324 types of skills across 521 companies. our dataset uniquely enables evaluating skill demand forecasting models at various granularities, including occupation, company, and regional levels. we benchmark a range of models on this dataset, evaluating their performance in standard scenarios, in predictions focused on lower value ranges, and in the presence of structural breaks, providing new insights for further research. our code and dataset are publicly accessible via the https://github.com/job-sdf/benchmark . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11928",
        "label": 0,
        "title": "flexcare: leveraging cross-task synergy for flexible multimodal healthcare prediction muhao xu",
        "abstract": "multimodal electronic health record (ehr) data can offer a holistic assessment of a patient\u2019s health status, supporting various predic- tive healthcare tasks. recently, several studies have embraced the multitask learning approach in the healthcare domain, exploiting the inherent correlations among clinical tasks to predict multi- ple outcomes simultaneously. however, existing methods necessi- tate samples to possess complete labels for all tasks, which places heavy demands on the data and restricts the flexibility of the model. meanwhile, within a multitask framework with multimodal in- puts, how to comprehensively consider the information disparity among modalities and among tasks still remains a challenging prob- lem. to tackle these issues, a unified healthcare prediction model, also named by flexcare , is proposed to flexibly accommodate in- complete multimodal inputs, promoting the adaption to multiple healthcare tasks. the proposed model breaks the conventional par- adigm of parallel multitask prediction by decomposing it into a series of asynchronous single-task prediction. specifically, a task- agnostic multimodal information extraction module is presented to capture decorrelated representations of diverse intra- and inter- modality patterns. taking full account of the information disparities between different modalities and different tasks, we present a task- guided hierarchical multimodal fusion module that integrates the \u2217corresponding author. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. kdd \u201924, august 25\u201329, 2024, barcelona, spain \u00a92024 copyright held by the owner/author(s). publication rights licensed to acm. acm isbn 979-8-4007-0490-1/24/08 https://doi.org/10.1145/3637528.3671974refined modality-level representations into an individual patient- level representation. experimental results on multiple tasks from mimic-iv/mimic-cxr/mimic-note datasets demonstrate the effectiveness of the proposed method. additionally, further anal- ysis underscores the feasibility and potential of employing such a multitask strategy in the healthcare domain. the source code is available at https://github.com/mhxu1998/flexcare. ccs concepts \u2022applied computing \u2192health informatics ;\u2022information systems\u2192data mining . keywords electronic health record, healthcare prediction, multimodal data, multitask learning acm reference format: muhao xu, zhenfeng zhu, youru li, shuai zheng, yawei zhao, kunlun he, and yao zhao. 2024. flexcare: leveraging cross-task synergy for flexible multimodal healthcare prediction. in proceedings of the 30th acm sigkdd conference on knowledge discovery and data mining (kdd \u201924), august 25\u201329, 2024, barcelona, spain. acm, new york, ny, usa, 11 pages. https: //doi.org/10.1145/3637528.3671974 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11931",
        "label": 0,
        "title": "deepseek-coder-v2: breaking the barrier of closed-source models in code intelligence qihao zhu",
        "abstract": "we present deepseek-coder-v2, an open-source mixture-of-experts (moe) code language model that achieves performance comparable to gpt4-turbo in code-specific tasks. specifically, deepseek-coder-v2 is further pre-trained from an intermediate checkpoint of deepseek-v2 with additional 6 trillion tokens. through this continued pre-training, deepseek-coder-v2 substantially enhances the coding and mathematical reasoning capabilities of deepseek-v2, while maintaining comparable performance in general language tasks. compared to deepseek- coder-33b, deepseek-coder-v2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. additionally, deepseek-coder- v2 expands its support for programming languages from 86 to 338, while extending the context length from 16k to 128k. in standard benchmark evaluations, deepseek-coder-v2 achieves superior performance compared to closed-source models such as gpt4-turbo, claude 3 opus, and gemini 1.5 pro in coding and math benchmarks. humaneval mbpp+ math gsm8k5060708090100accuracy (%)90.2 76.275.794.9 88.2 72.273.493.7 83.5 74.6 67.790.8 84.9 72.0 60.195.0 81.7 69.0 50.493.0 81.1 68.2 aider livecodebench swe-bench01020304050607080 73.7 43.4 12.763.9 45.7 18.357.1 34.1 18.768.4 34.6 11.749.2 28.751.1 31.0 2.7deepseek-coder-v2 gpt-4-turbo-0409 gemini-1.5-pro claude-3-opus llama-3-70b codestral figure 1|the performance of deepseek-coder-v2 on math and code benchmarks. *core contributorsarxiv:2406.11931v1  [cs.se]  17 jun 20241"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12020",
        "label": 0,
        "title": "when box meets graph neural network in tag-aware recommendation fake lin",
        "abstract": "last year has witnessed the re-flourishment of tag-aware recom- mender systems supported by the llm-enriched tags. unfortu- nately, though large efforts have been made, current solutions may fail to describe the diversity and uncertainty inherent in user prefer- ences with only tag-driven profiles. recently, with the development of geometry-based techniques, e.g., box embedding, diversity of user preferences now could be fully modeled as the range within a box in high dimension space. however, defect still exists as these approaches are incapable of capturing high-order neighbor signals, i.e., semantic-rich multi-hop relations within the user-tag-item tripartite graph, which severely limits the effectiveness of user modeling. to deal with this challenge, in this paper, we propose a novel algorithm, called boxgnn, to perform the message aggrega- tion via combination of logical operations, thereby incorporating high-order signals. specifically, we first embed users, items, and tags as hyper-boxes rather than simple points in the representation space, and define two logical operations to facilitate the subsequent process. next, we perform the message aggregation mechanism via the combination of logical operations, to obtain the corresponding high-order box representations. finally, we adopt a volume-based learning objective with gumbel smoothing techniques to refine \u2217corresponding author. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. kdd \u201924, august 25\u201329, 2024, barcelona, spain \u00a92024 copyright held by the owner/author(s). publication rights licensed to acm. acm isbn 979-8-4007-0490-1/24/08 https://doi.org/10.1145/3637528.3671973the representation of boxes. extensive experiments on two pub- licly available datasets and one llm-enhanced e-commerce dataset have validated the superiority of boxgnn compared with various state-of-the-art baselines. the code is released online1. ccs concepts \u2022information systems \u2192recommender systems . keywords tag-aware recommendation, box embedding, graph neural net- works acm reference format: fake lin, ziwei zhao, xi zhu, da zhang, shitian shen, xueying li, tong xu, suojuan zhang, and enhong chen. 2024. when box meets graph neural network in tag-aware recommendation. in proceedings of the 30th acm sigkdd conference on knowledge discovery and data mining (kdd \u201924), august 25\u201329, 2024, barcelona, spain. acm, new york, ny, usa, 11 pages. https://doi.org/10.1145/3637528.3671973 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11978",
        "label": 0,
        "title": "dialogue action tokens: steering language models in goal-directed dialogue with a multi-turn planner kenneth l",
        "abstract": "we present an approach called dialogue action tokens (dat) that adapts language model agents to plan goal-directed dialogues. the core idea is to treat each utterance as an action, thereby converting dialogues into games where existing approaches such as reinforcement learning can be applied. specifically, we freeze a pretrained language model and train a small planner model that predicts a continuous action vector, used for controlled generation in each round. this design avoids the problem of language degradation under reward optimization. when evaluated on the sotopia platform for social simulations, the dat-steered llama model surpasses gpt-4\u2019s performance. we also apply dat to steer an attacker language model in a novel multi-turn red-teaming setting, revealing a potential new attack surface. code: https://github.com/likenneth/dialogue_action_token . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12465",
        "label": 0,
        "title": "rigl: a unified reciprocal approach for tracing the independent and group learning processes xiaoshan yu",
        "abstract": "in the realm of education, both independent learning and group learning are esteemed as the most classic paradigms. the former allows learners to self-direct their studies, while the latter is typi- cally characterized by teacher-directed scenarios. recent studies in the field of intelligent education have leveraged deep tempo- ral models to trace the learning process, capturing the dynamics of students\u2019 knowledge states, and have achieved remarkable per- formance. however, existing approaches have primarily focused on modeling the independent learning process, with the group learning paradigm receiving less attention. moreover, the recip- rocal effect between the two learning processes, especially their combined potential to foster holistic student development, remains inadequately explored. to this end, in this paper, we propose rigl , a unified reciprocal model to trace knowledge states at both the individual and group levels, drawing from the independent and group learning processes. specifically, we first introduce a time \u2217work was done at career science lab, boss zhipin supervised by chuan qin. \u2020corresponding authors. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. kdd \u201924, august 25\u201329, 2024, barcelona, spain \u00a92024 association for computing machinery. acm isbn 978-x-xxxx-xxxx-x/yy/mm. . . $15.00 https://doi.org/xxxxxxx.xxxxxxxframe-aware reciprocal embedding module to concurrently model both student and group response interactions across various time frames. subsequently, we employ reciprocal enhanced learning modeling to fully exploit the comprehensive and complementary information between the two behaviors. furthermore, we design a relation-guided temporal attentive network, comprised of dynamic graph modeling coupled with a temporal self-attention mechanism. it is used to delve into the dynamic influence of individual and group interactions throughout the learning processes, which is crafted to explore the dynamic intricacies of both individual and group interactions during the learning sequences. conclusively, we introduce a bias-aware contrastive learning module to bolster the stability of the model\u2019s training. extensive experiments on four real-world educational datasets clearly demonstrate the effec- tiveness of the proposed rigl model. our codes are available at https://github.com/labyrinthineleo/rigl. ccs concepts \u2022information systems \u2192data mining ;\u2022applied computing \u2192collaborative learning . keywords intelligent education, knowledge tracing, group learning, reciprocal effect, dynamic graph neural network acm reference format: xiaoshan yu, chuan qin, dazhong shen, shangshang yang, haiping ma, hengshu zhu, and xingyi zhang. 2024. rigl: a unified reciprocal ap- proach for tracing the independent and group learning processes . inarxiv:2406.12465v1  [cs.cy]  18 jun 2024kdd \u201924, august 25\u201329, 2024, barcelona, spain xiaoshan yu et al. figure 1: an illustrative example of the holistic knowledge tracing (hkt) task. the top and bottom halves indicate the individual and group learning processes, respectively, which are organized in time frames, and the radar chart in the middle represents the knowledge proficiency levels of both. proceedings of the 30th acm sigkdd conference on knowledge discovery and data mining(kdd \u201924), august 25\u201329, 2024, barcelona, spain. acm, new york, ny, usa, 12 pages. https://doi.org/xxxxxxx.xxxxxxx "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12080",
        "label": 0,
        "title": "a hierarchical 3d gaussian representation for real-time rendering of very large datasets bernhard kerbl",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12095",
        "label": 0,
        "title": "distillnerf: perceiving 3d scenes from single-glance images by distilling neural fields and foundation model f",
        "abstract": "we propose distillnerf, a self-supervised learning framework addressing the challenge of understanding 3d environments from limited 2d observations in au- tonomous driving. our method is a generalizable feedforward model that predicts a rich neural scene representation from sparse, single-frame multi-view camera inputs, and is trained self-supervised with differentiable rendering to reconstruct rgb, depth, or feature images. our first insight is to exploit per-scene optimized neural radiance fields (nerfs) by generating dense depth and virtual camera targets for training, thereby helping our model to learn 3d geometry from sparse non-overlapping image inputs. second, to learn a semantically rich 3d representa- tion, we propose distilling features from pre-trained 2d foundation models, such as clip or dinov2, thereby enabling various downstream tasks without the need for costly 3d human annotations. to leverage these two insights, we introduce a novel model architecture with a two-stage lift-splat-shoot encoder and a parameterized sparse hierarchical voxel representation. experimental results on the nuscenes dataset demonstrate that distillnerf significantly outperforms existing compara- ble self-supervised methods for scene reconstruction, novel view synthesis, and depth estimation; and it allows for competitive zero-shot 3d semantic occupancy prediction, as well as open-world scene understanding through distilled foundation model features. demos and code will be available at https://distillnerf.github.io/. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12208",
        "label": 0,
        "title": "knowledge fusion by evolving weights of language models guodong du1jing li1",
        "abstract": "fine-tuning pre-trained language models, par- ticularly large language models, demands ex- tensive computing resources and can result in varying performance outcomes across different domains and datasets. this paper examines the approach of integrating multiple models from diverse training scenarios into a unified model. this unified model excels across various data domains and exhibits the ability to generalize well on out-of-domain data. we propose a knowledge fusion method named evolver , in- spired by evolutionary algorithms, which does not need further training or additional training data. specifically, our method involves aggre- gating the weights of different language mod- els into a population and subsequently gener- ating offspring models through mutation and crossover operations. these offspring models are then evaluated against their parents, allow- ing for the preservation of those models that show enhanced performance on development datasets. importantly, our model evolving strat- egy can be seamlessly integrated with existing model merging frameworks, offering a versa- tile tool for model enhancement. experimental results on mainstream language models (i.e., encoder-only, decoder-only, encoder-decoder) reveal that evolver outperforms previous state- of-the-art models by large margins. the code is publicly available at https://github.com/ duguodong7/model-evolution . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12211",
        "label": 0,
        "title": "pcie lam solution for ego4d looking at me challenge kanokphan lertniphonphan lenovo research",
        "abstract": "this report presents our team\u2019s \u2019pcie lam\u2019 solution for the ego4d looking at me challenge at cvpr2024. the main goal of the challenge is to accurately determine if a person in the scene is looking at the camera wearer, based on a video where the faces of social partners have been lo- calized. our proposed solution, internlstm, consists of an internvl image encoder and a bi-lstm network. the in- ternvl extracts spatial features, while the bi-lstm extracts temporal features. however, this task is highly challenging due to the distance between the person in the scene and the camera movement, which results in significant blurring in the face image. to address the complexity of the task, we implemented a gaze smoothing filter to eliminate noise or spikes from the output. our approach achieved the 1stposi- tion in the looking at me challenge with 0.81 map and 0.93 accuracy rate. code is available at https://github. com/kanokphanl/ego4d_lam_internlstm 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12214",
        "label": 1,
        "title": "is your hd map constructor reliable under sensor corruptions? xiaoshuai hao1mengchuan wei1yifan yang1haimei zh",
        "abstract": "driving systems often rely on high-definition (hd) maps for precise environmental information, which is crucial for planning and navigation. while current hd map constructors perform well under ideal conditions, their resilience to real-world challenges, e.g., adverse weather and sensor failures, is not well understood, rais- ing safety concerns. this work introduces mapbench , the first comprehensive benchmark designed to evaluate the robustness of hd map construction methods against various sensor corruptions. our benchmark encompasses a total of 29types of corruptions that occur from cameras and lidar sensors. extensive evaluations across 31hd map constructors reveal significant performance degradation of ex- isting methods under adverse weather conditions and sensor failures, underscoring critical safety concerns. we identify effective strategies for enhancing robustness, including innovative approaches that leverage multi-modal fusion, advanced data augmentation, and architectural techniques. these insights provide a pathway for developing more reliable hd map construction methods, which are essential for the advancement of autonomous driving technology. the benchmark toolkit and affiliated code and model checkpoints have been made publicly accessible. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12219",
        "label": 0,
        "title": "pcie egohandpose solution for egoexo4d hand pose challenge feng chen lenovo research",
        "abstract": "this report presents our team\u2019s \u2019pcie egohandpose\u2019 solution for the egoexo4d hand pose challenge at cvpr2024. the main goal of the challenge is to accurately estimate hand poses, which involve 21 3d joints, using an rgb egocentric video images provided for the task. this task is particularly challenging due to the subtle movements and occlusions. to handle the complexity of the task, we propose the hand pose vision transformer (hp-vit). the hp-vit comprises a vit backbone and transformer head to estimate joint positions in 3d, utilizing mpjpe and rle loss function. our approach achieved the 1stposition in the hand pose challenge with 25.51 mpjpe and 8.49 pa- mpjpe. code is available at https://github.com/ kanokphanl/pcie_egohandpose 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12235",
        "label": 1,
        "title": "holmes-vad : towards unbiased and explainable video anomaly detection via multi-modal llm huaxin zhang1",
        "abstract": "towards open-ended video anomaly detection (v ad), existing methods often exhibit biased detection when faced with challenging or unseen events and lack interpretability. to address these drawbacks, we propose holmes-v ad, a novel framework that leverages precise temporal supervision and rich multimodal in- structions to enable accurate anomaly localization and comprehensive explanations. firstly, towards unbiased and explainable v ad system, we construct the first large- scale multimodal v ad instruction-tuning benchmark, i.e.,vad-instruct50k . this dataset is created using a carefully designed semi-automatic labeling paradigm. efficient single-frame annotations are applied to the collected untrimmed videos, which are then synthesized into high-quality analyses of both abnormal and normal video clips using a robust off-the-shelf video captioner and a large language model (llm). building upon the vad-instruct50k dataset, we develop a customized solution for interpretable video anomaly detection. we train a lightweight temporal sampler to select frames with high anomaly response and fine-tune a multimodal large language model (llm) to generate explanatory content. extensive experimen- tal results validate the generality and interpretability of the proposed holmes-vad , establishing it as a novel interpretable technique for real-world video anomaly analysis. to support the community, our benchmark and model will be publicly available at https://github.com/pipixin321/holmesvad . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12246",
        "label": 0,
        "title": "trol: traversal of layers for large language and vision models byung-kwan lee kaist",
        "abstract": "large language and vision models (llvms) have been driven by the generalization power of large language models (llms) and the advent of visual instruction tuning. along with scaling them up directly, these models enable llvms to showcase powerful vision language (vl) performances by covering di- verse tasks via natural language instructions. however, existing open-source llvms that perform comparably to closed-source llvms such as gpt-4v are often considered too large (e.g., 26b, 34b, and 110b parameters), hav- ing a larger number of layers. these large models demand costly, high-end resources for both training and inference. to address this issue, we present a new efficient llvm fam- ily with 1.8b, 3.8b, and 7b llm model sizes, traversal oflayers (  trol ), which enables the reuse of layers in a token-wise manner. this layer traversing technique simulates the effect of looking back and retracing the answering stream while increasing the number of forward propagation layers without physically adding more layers. we demonstrate that  trol employs a simple layer traversing approach yet efficiently outperforms the open-source llvms with larger model sizes and rivals the performances of the closed-source llvms with substantial sizes. code is available in https://github.com/byungkwanlee/trol. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12256",
        "label": 0,
        "title": "symmetric multi-similarity loss for epic-kitchens-100 multi-instance retrieval challeng e 2024 xiaoqi wang yi ",
        "abstract": "in this report, we present our champion solution for epic-kitchens-100 multi-instance retrieval challenge in cvpr 2024. essentially, this challenge differs from traditional visual-text retrieval tasks by providing a cor - relation matrix that acts as a set of soft labels for video- text clip combinations. however, existing loss functions have not fully exploited this information. motivated by this, we propose a novel loss function, symmetric multi- similarity loss, which offers a more precise learning ob- jective. together with tricks and ensemble learning, the model achieves 63.76% average map and 74.25% average ndcg on the public leaderboard, demonstrating the effec- tiveness of our approach. our code will be released at: https://github.com/xqwang14/sms-loss . 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12321",
        "label": 0,
        "title": "automatic benchmarking of large multimodal models via iterative experiment programming alessandro conti1",
        "abstract": "assessing the capabilities of large multimodal models (lmms) often requires the creation of ad-hoc evaluations. currently, building new benchmarks requires tremendous amounts of manual work for each specific analysis. this makes the evaluation process tedious and costly. in this paper, we present ape x,auto- matic programming of experiments , the first framework for automatic benchmarking of lmms. given a research question expressed in natural language, ape xleverages a large language model (llm) and a library of pre-specified tools to generate a set of experiments for the model at hand, and progressively compile a scientific report. the report drives the testing procedure: based on the current status of the investigation, ape xchooses which experiments to perform and whether the results are sufficient to draw conclusions. finally, the llm refines the report, presenting the results to the user in natural language. thanks to its modularity, our framework is flexible and extensible as new tools become available. empirically, ape xreproduces the findings of existing studies while allowing for arbitrary anal- yses and hypothesis testing. code is available at https://github.com/altndrr/apex. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12384",
        "label": 1,
        "title": "vrsbench: a versatile vision-language benchmark dataset for remote sensing image understanding xiang li jian d",
        "abstract": "we introduce a new benchmark designed to advance the development of general- purpose, large-scale vision-language models for remote sensing images. although several vision-language datasets in remote sensing have been proposed to pursue this goal, existing datasets are typically tailored to single tasks, lack detailed object information, or suffer from inadequate quality control. exploring these improvement opportunities, we present a versatile vision-language bench mark forremote sensing image understanding, termed vrsbench . this benchmark comprises 29,614 images, with 29,614 human-verified detailed captions, 52,472 object references, and 123,221 question-answer pairs. it facilitates the training and evaluation of vision-language models across a broad spectrum of remote sensing image understanding tasks. we further evaluated state-of-the-art models on this benchmark for three vision-language tasks: image captioning, visual grounding, and visual question answering. our work aims to significantly contribute to the development of advanced vision-language models in the field of remote sensing. the data and code can be accessed at https://vrsbench.github.io . the high-resolution  aerial  image  from  googleearth  shows  a waterfront  scene  with residential  areas  and  harbor  facilities . three  distinct  harbors  can be seen,  one located  on the left side and another  on the right  side of the image . between  them,  there  are homes  with different  colored  rooftops,  green  lawns,  and  driveways . a ship is docked  in the central  part of the bottom  edge,  and the water  body  exhibits  gentle   ripples . various  small  vehicles  are scattered  throughout  the residential  area,  parked  near the houses .question : how  many  harbors  are visible? answer : 3 question : what  is the object  located  furthest  to the top? answer : small vehicle question : are the visible  vehicles  near water? answer : noobject referring detailed captioningvisual question answer 1 3 4 572 0 6object  id=1:the small  vehicle  that is the farthest  to the top. object  id=4:the harbor  located  on the left side of the scene  with multiple  docks  extending  into the water . object  id=7:the harbor  situated  on the right  side of the image  with a large  dock  area. figure 1: examples of an image and corresponding annotations in vrsbench dataset. our annotations include object referring, visual question answering, and detailed captions. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12407",
        "label": 0,
        "title": "looc: localizing organs using occupancy networks and body surface depth images pit henrich1and",
        "abstract": ". we introduce a novel method employing occupancy networks for the precise localization of 67 anatomical structures from single depth images captured from the exterior of the human body. this method considers the anatomical diversity across individuals. our contributions include the application of occupancy networks for occluded structure localization, a robust method for estimating anatomical positions from depth images, and the creation of detailed, individualized 3d anatom- ical atlases. this approach promises improvements in medical imaging and automated diagnostic procedures by offering accurate, non-invasive localization of critical anatomical features. keywords: localization of anatomical structures \u00b7patient-individual 3d atlas \u00b7occupancy networks. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12448",
        "label": 1,
        "title": "journal of machine learning for biomedical imaging 2024:012 vol. 2",
        "abstract": "the emergence of clinical data warehouses (cdws), which contain the medical data of millions of patients, has paved the way for vast data sharing for research. the quality of mris gathered in cdws differs greatly from what is observed in research settings and reflects a certain clinical reality. consequently, a significant proportion of these images turns out to be unusable due to their poor quality. given the massive volume of mris contained in cdws, the manual rating of image quality is impossible. thus, it is necessary to develop an automated solution capable of effectively identifying corrupted images in cdws. \u00a92024 . license: cc-by 4.0 https://doi.org/10.59275/j.melba.2024-7fgdarxiv:2406.12448v1  [eess.iv]  18 jun 2024automated mri quality assessment in clinical data warehouses: this study presents an innovative transfer learning method for automated quality con- trol of 3d gradient echo t1-weighted brain mris within a cdw, leveraging artefact sim- ulation. we first intentionally corrupt images from research datasets by inducing poorer contrast, adding noise and introducing motion artefacts. subsequently, three artefact- specific models are pre-trained using these corrupted images to detect distinct types of artefacts. finally, the models are generalised to routine clinical data through a transfer learning technique, utilising 3660 manually annotated images. the overall image quality is inferred from the results of the three models, each designed to detect a specific type of artefact. our method was validated on an independent test set of 385 3d gradient echo t1-weighted mris. our proposed approach achieved excellent results for the detection of bad quality mris, with a balanced accuracy of over 87%, surpassing our previous approach by 3.5 percent points. additionally, we achieved a satisfactory balanced accuracy of 79% for the detection of moderate quality mris, outperforming our previous performance by 5 percent points. our framework provides a valuable tool for exploiting the potential of mris in cdws. keywords: clinical data warehouse, deep learning, transfer learning, quality control, mri 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12463",
        "label": 0,
        "title": "journal of l atex class files",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12496",
        "label": 0,
        "title": "reparameterizable dual-resolution network for real-time semantic segmentation guoyu yang",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12638",
        "label": 0,
        "title": "efficient and long-tailed generalization for pre-trained vision-language model jiang-xin shi",
        "abstract": "pre-trained vision-language models like clip have shown powerful zero-shot inference ability via image-text matching and prove to be strong few-shot learners in various downstream tasks. however, in real-world scenarios, adapting clip to downstream tasks may encounter the following challenges: 1) data may exhibit long-tailed data distributions and might not have abundant samples for all the classes; 2) there might be emerging tasks with new classes that contain no samples at all. to overcome them, we propose a novel framework to achieve efficient and long-tailed generalization, which can be termed as candle . during the training process, we propose compensating logit-adjusted loss to encourage large mar- gins of prototypes and alleviate imbalance both within the base classes and between the base and new classes. for efficient adap- tation, we treat the clip model as a black box and leverage the extracted features to obtain visual and textual prototypes for predic- tion. to make full use of multi-modal information, we also propose cross-modal attention to enrich the features from both modalities. for effective generalization, we introduce virtual prototypes for new classes to make up for their lack of training images. candle achieves state-of-the-art performance over extensive experiments on 11 diverse datasets while substantially reducing the training time, demonstrating the superiority of our approach. the source code is available at https://github.com/shijxcs/candle. ccs concepts \u2022computing methodologies \u2192supervised learning . \u2217equal contribution. \u2020corresponding author. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. kdd \u201924, august 25\u201329, 2024, barcelona, spain. \u00a92024 copyright held by the owner/author(s). publication rights licensed to acm. acm isbn 979-8-4007-0490-1/24/08 https://doi.org/10.1145/3637528.3671945keywords long-tail learning, vision-language model, new class generalization acm reference format: jiang-xin shi, chi zhang, tong wei, and yu-feng li. 2024. efficient and long-tailed generalization for pre-trained vision-language model. in pro- ceedings of the 30th acm sigkdd conference on knowledge discovery and data mining (kdd \u201924), august 25\u201329, 2024, barcelona, spain. acm, new york, ny, usa, 11 pages. https://doi.org/10.1145/3637528.3671945 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12649",
        "label": 0,
        "title": "probabilistic conceptual explainers: trustworthy conceptual explanations for vision foundation models hengyi w",
        "abstract": "vision transformers (vits) have emerged as a significant area of focus, particularly for their ca- pacity to be jointly trained with large language models and to serve as robust vision foundation models. yet, the development of trustworthy ex- planation methods for vits has lagged, partic- ularly in the context of post-hoc interpretations of vit predictions. existing sub-image selection approaches, such as feature-attribution and con- ceptual models, fall short in this regard. this pa- per proposes five desiderata for explaining vits \u2013 faithfulness, stability, sparsity, multi-level struc- ture, and parsimony \u2013 and demonstrates the in- adequacy of current methods in meeting these criteria comprehensively. we introduce a varia- tional bayesian explanation framework, dubbed probabilistic concept explainers (pace), which models the distributions of patch embeddings to provide trustworthy post-hoc conceptual explana- tions. our qualitative analysis reveals the distri- butions of patch-level concepts, elucidating the effectiveness of vits by modeling the joint distri- bution of patch embeddings and vit\u2019s predictions. moreover, these patch-level explanations bridge the gap between image-level and dataset-level ex- planations, thus completing the multi-level struc- ture of pace. through extensive experiments on both synthetic and real-world datasets, we demon- strate that pace surpasses state-of-the-art meth- ods in terms of the defined desiderata1. *equal contribution1department of computer science, rutgers university, new jersey, usa. correspondence to: hengyi wang <hengyi.wang@rutgers.edu >. proceedings of the 41stinternational conference on machine learning , vienna, austria. pmlr 235, 2024. copyright 2024 by the author(s). 1code will soon be available at https://github.com/wang-ml- lab/interpretable-foundation-models1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12671",
        "label": 1,
        "title": "geobench: benchmarking and analyzing monocular geometry estimation models yongtao ge1",
        "abstract": "recent advances in discriminative and generative pretraining have yielded geometry estimation models with strong generalization capabilities. while discriminative monocular geometry estimation methods rely on large-scale fine-tuning data to achieve zero-shot generalization, several generative-based paradigms show the potential of achieving impressive generalization performance on unseen scenes by leveraging pre-trained diffusion models and fine-tuning on even a small scale of synthetic training data. frustratingly, these models are trained with different recipes on different datasets, making it hard to find out the critical factors that determine the evaluation performance. besides, the current widely used geometry evaluation benchmarks have two main drawbacks that may prevent the development of the field, i.e.,limited scene diversity andunfavorable label quality . to resolve the above issues, (1) we build fair and strong baselines in a unified codebase for evaluating and analyzing the state-of-the-art (sota) geometry estimation models in terms of both different finetuning paradigms and training recipes; (2) we evaluate monocular geometry estimators on more challenging benchmarks for geometry estimation task with diverse scenes and high-quality annotations. our results reveal that pre-trained using large data, discriminative models such as dinov2, can outperform generative counterparts with a small amount of high-quality synthetic training data under the same training configuration, which suggests that fine-tuning data quality is a more important factor than the data scale and model architecture. our observation also raises a question: if simply fine-tuning a general vision model such as dinov2 using a small amount of synthetic depth data produces sota results, do we really need complex models, e.g., marigold [ koh+24] and depthfm [ gfp+24] for depth estimation? we believe that this work can propel advancements in geometry estimation tasks and a wide range of other downstream vision tasks. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12742",
        "label": 1,
        "title": "benchmarking multi-image understanding in vision and language models: perception",
        "abstract": "the advancement of large language models (llms) has significantly broadened the scope of applications in natural language processing, with multi-modal llms extending these capabilities to integrate and interpret visual data. however, existing benchmarks for visual language models (vlms) predominantly focus on single- image inputs, neglecting the crucial aspect of multi-image understanding. in this paper, we introduce a multi-image relational benchmark mirb , designed to evaluate vlms\u2019 ability to compare, analyze, and reason across multiple images. our benchmark encompasses four categories: perception, visual world knowledge, reasoning, and multi-hop reasoning. through a comprehensive evaluation of a wide range of open-source and closed-source models, we demonstrate that while open-source vlms were shown to approach the performance of gpt-4v in single- image tasks, a significant performance gap remains in multi-image reasoning tasks. our findings also reveal that even the state-of-the-art gpt-4v model struggles with our benchmark, underscoring the need for further research and development in this area. we believe our contribution of mirb could serve as a testbed for developing the next-generation multi-modal models. preprint. under review.arxiv:2406.12742v1  [cs.cv]  18 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12769",
        "label": 0,
        "title": "published as a conference paper at iclr 2024 latent intuitive physics : learning to transfer hidden physics fr",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12847",
        "label": 0,
        "title": "journal of l atex class files",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": " https://arxiv.org/pdf/2406.11911",
        "label": 0,
        "title": "a notion of complexity for theory of mind via discrete world models x. angelo huang1",
        "abstract": "theory of mind (tom) can be used to assess the capabilities of large language models (llms) in complex scenarios where social reasoning is required. while the research community has proposed many tom benchmarks, their hard- ness varies greatly, and their complexity is not well defined. this work proposes a framework to measure the complexity of tom tasks. we quantify a problem\u2019s complexity as the number of states necessary to solve it correctly. our complexity measure also accounts for spurious states of a tom problem designed to make it apparently harder. we use our method to as- sess the complexity of five widely adopted tom benchmarks. on top of this framework, we de- sign a prompting technique that augments the information available to a model with a descrip- tion of how the environment changes with the agents\u2019 interactions. we name this technique discrete world models (dwm) and show how it elicits superior performance on tom tasks. https://github.com/flecart/ complexity-tom-dwm "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11944",
        "label": 0,
        "title": "transcoders find interpretable llm feature circuits jacob dunefsky",
        "abstract": "a key goal in mechanistic interpretability is circuit analysis: finding sparse sub- graphs of models corresponding to specific behaviors or capabilities. however, mlp sublayers make fine-grained circuit analysis on transformer-based language models difficult. in particular, interpretable features\u2014such as those found by sparse autoencoders (saes)\u2014are typically linear combinations of extremely many neu- rons, each with its own nonlinearity to account for. circuit analysis in this setting thus either yields intractably large circuits or fails to disentangle local and global behavior. to address this we explore transcoders , which seek to faithfully approx- imate a densely activating mlp layer with a wider, sparsely-activating mlp layer. we successfully train transcoders on language models with 120m, 410m, and 1.4b parameters, and find them to perform at least on par with saes in terms of sparsity, faithfulness, and human-interpretability. we then introduce a novel method for using transcoders to perform weights-based circuit analysis through mlp sublay- ers. the resulting circuits neatly factorize into input-dependent and input-invariant terms. finally, we apply transcoders to reverse-engineer unknown circuits in the model, and we obtain novel insights regarding the \u201cgreater-than circuit\u201d in gpt2- small. our results suggest that transcoders can prove effective in decomposing model computations involving mlps into interpretable circuits. code is available athttps://github.com/jacobdunefsky/transcoder_circuits . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12036",
        "label": 1,
        "title": "medcalc-bench : evaluating large language models for medical calculations nikhil khandekar",
        "abstract": "as opposed to evaluating computation and logic-based reasoning, current bench- marks for evaluating large language models (llms) in medicine are primarily focused on question-answering involving domain knowledge and descriptive rea- soning. while such qualitative capabilities are vital to medical diagnosis, in real- world scenarios, doctors frequently use clinical calculators that follow quantitative equations and rule-based reasoning paradigms for evidence-based decision support. to this end, we propose medcalc-bench , a first-of-its-kind dataset focused on evaluating the medical calculation capability of llms. medcalc-bench contains an evaluation set of over 1000 manually reviewed instances from 55 dif- ferent medical calculation tasks. each instance in medcalc-bench consists of a patient note, a question requesting to compute a specific medical value, a ground truth answer, and a step-by-step explanation showing how the answer is obtained. while our evaluation results show the potential of llms in this area, none of them are effective enough for clinical settings. common issues include extracting the incorrect entities, not using the correct equation or rules for a calculation task, or incorrectly performing the arithmetic for the computation. we hope our study highlights the quantitative knowledge and reasoning gaps in llms within medical settings, encouraging future improvements of llms for various clinical calculation tasks.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12045",
        "label": 1,
        "title": "\u03c4-bench: a benchmark for t ool-a gent-u ser interaction in real-world domains shunyu yao",
        "abstract": "existing benchmarks do not test language agents on their interaction with human users or ability to follow domain-specific rules, both of which are vital for deploying them in real world applications. we propose \u03c4-bench, a benchmark emulating dynamic conversations between a user (simulated by language models) and a language agent provided with domain-specific api tools and policy guidelines. we employ an efficient and faithful evaluation process that compares the database state at the end of a conversation with the annotated goal state. we also propose a new metric (pass^k) to evaluate the reliability of agent behavior over multiple trials. our experiments show that even state-of-the-art function calling agents (like gpt-4o ) succeed on <50% of the tasks, and are quite inconsistent (pass^8 < 25% in retail). our findings point to the need for methods that can improve the ability of agents to act consistently and follow rules reliably. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12050",
        "label": 0,
        "title": "learn beyond the answer: training language models with reflection for mathematical reasoning zhihan zhang\u00001\u2020",
        "abstract": "supervised fine-tuning enhances the problem- solving abilities of language models across var- ious mathematical reasoning tasks. to maxi- mize such benefits, existing research focuses onbroadening the training set with various data augmentation techniques, which is effective for standard single-round question-answering set- tings. our work introduces a novel technique aimed at cultivating a deeper understanding of the training problems at hand, enhancing perfor- mance not only in standard settings but also in more complex scenarios that require reflective thinking. specifically, we propose reflective augmentation , a method that embeds prob- lem reflection into each training instance. it trains the model to consider alternative perspec- tives and engage with abstractions and analo- gies, thereby fostering a thorough comprehen- sion through reflective reasoning. extensive experiments validate the achievement of our aim, underscoring the unique advantages of our method and its complementary nature relative to existing augmentation techniques.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12052",
        "label": 0,
        "title": "uniglm: training one unified language model for text-attributed graphs yi fang1",
        "abstract": "representation learning on text-attributed graphs (tags), where nodes are represented by textual descriptions, is crucial for textual and relational knowledge systems and recom- mendation systems. currently, state-of-the-art embedding methods for tags primarily focus on fine-tuning language models (e.g., bert) using structure-aware training signals. while effective, these methods are tailored for individ- ual tag and cannot generalize across various graph scenarios. given the shared textual space, leveraging multiple tags for joint fine-tuning, aligning text and graph structure from different aspects, would be more beneficial. motivated by this, we introduce a novel unified graph language model ( uniglm ) framework, the first graph embedding model that generalizes well to both in-domain and cross-domain tags. specifically, uniglm is trained over multiple tags with different domains and scales using self-supervised contrastive learning. uniglm includes an adaptive positive sample selection technique for identifying structurally similar nodes and a lazy contrastive module that is devised to accelerate training by minimizing repetitive encoding calculations. extensive empirical results across 9 benchmark tags demonstrate uniglm\u2019s efficacy against lead- ing embedding baselines in terms of general- ization (various downstream tasks and back- bones) and transfer learning (in and out of domain scenarios). the code is available at https://github.com/nyushcs/uniglm . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12060",
        "label": 0,
        "title": "not eliminate but aggregate: post-hoc control over mixture-of-experts to address shortcut shifts in natural la",
        "abstract": "recent models for natural language under- standing are inclined to exploit simple pat- terns in datasets, commonly known as short- cuts. these shortcuts hinge on spurious cor- relations between labels and latent features existing in the training data. at inference time, shortcut-dependent models are likely to generate erroneous predictions under dis- tribution shifts, particularly when some la- tent features are no longer correlated with the labels. to avoid this, previous stud- ies have trained models to eliminate the re- liance on shortcuts. in this study, we ex- plore a different direction: pessimistically aggregating the predictions of a mixture-of- experts, assuming each expert captures rela- tively different latent features. the exper- imental results demonstrate that our post- hoc control over the experts significantly en- hances the model\u2019s robustness to the distri- bution shift in shortcuts. besides, we show that our approach has some practical advan- tages. we also analyze our model and pro- vide results to support the assumption.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12066",
        "label": 1,
        "title": "language models are surprisingly fragile to drug names in biomedical benchmarks jack gallifant1",
        "abstract": "medical knowledge is context-dependent and requires consistent reasoning across various natural language expressions of semantically equivalent phrases. this is particularly cru- cial for drug names, where patients often use brand names like advil or tylenol instead of their generic equivalents. to study this, we create a new robustness dataset, rabbits , to evaluate performance differences on medical benchmarks after swapping brand and generic drug names using physician expert annotations. we assess both open-source and api-based llms on medqa and medmcqa, revealing a consistent performance drop ranging from 1-10%. furthermore, we identify a potential source of this fragility as the contamination of test data in widely used pre-training datasets.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12091",
        "label": 0,
        "title": "is poisoning a real threat to llm alignment ? m aybe more so than you think a p reprint",
        "abstract": "recent advancements in reinforcement learning with human feedback (rlhf) have significantly impacted the alignment of large language models (llms). the sensitivity of reinforcement learning algorithms such as proximal policy optimization (ppo) has led to new line work on direct policy optimization (dpo), which treats rlhf in a supervised learning framework. the increased practical use of these rlhf methods warrants an analysis of their vulnerabilities. in this work, we investigate the vulnerabilities of dpo to poisoning attacks under different scenarios and compare the effectiveness of preference poisoning, a first of its kind. we comprehensively analyze dpo\u2019s vulnerabilities under different types of attacks, i.e., backdoor and non-backdoor attacks, and different poisoning methods across a wide array of language models, i.e., llama 7b, mistral 7b, and gemma 7b. we find that unlike ppo-based methods, which, when it comes to backdoor attacks, require at least 4% of the data to be poisoned to elicit harmful behavior, we exploit the true vulnerabilities of dpo more simply so we can poison the model with only as much as 0.5% of the data. we further investigate the potential reasons behind the vulnerability and how well this vulnerability translates into backdoor vs non-backdoor attacks. implementation of the paper is publically available athttps://github.com/pankayaraj/rlhfpoisoning . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12128",
        "label": 1,
        "title": "ai \u2018news\u2019 content farms are easy to make and hard to detect: a case study in italian giovanni puccetti\u03b1",
        "abstract": "large language models (llms) are increas- ingly used as \u2018content farm\u2019 models (cfms), to generate synthetic text that could pass for real news articles. this is already happening even for languages that do not have high-quality monolingual llms. we show that fine-tuning llama (v1), mostly trained on english, on as lit- tle as 40k italian news articles, is sufficient for producing news-like texts that native speakers of italian struggle to identify as synthetic. we investigate three llms and three methods of detecting synthetic texts (log-likelihood, de- tectgpt, and supervised classification), finding that they all perform better than human raters, but they are all impractical in the real world (requiring either access to token likelihood in- formation or a large dataset of cfm texts). we also explore the possibility of creating a proxy cfm: an llm fine-tuned on a similar dataset to one used by the real \u2018content farm\u2019. we find that even a small amount of fine-tuning data suf- fices for creating a successful detector, but we need to know which base llm is used, which is a major challenge. our results suggest that there are currently no practical methods for detecting synthetic news- like texts \u2018in the wild\u2019, while generating them is too easy. we highlight the urgency of more nlp research on this problem. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12131",
        "label": 0,
        "title": "gram 2vec: an interpretable document vectorizer peter zeng\u25b33",
        "abstract": "we present gram 2vec, a grammatical style embedding algorithm that embeds documents into a higher dimensional space by extracting the normalized relative frequencies of gram- matical features present in the text. compared to neural approaches, gram 2vecoffers in- herent interpretability based on how the feature vectors are generated. in our demo, we present a way to visualize a mapping of authors to doc- uments based on their gram 2vecvectors and highlight the ability to drop or add features to view which authors make certain linguistic choices. next, we use authorship attribution as an application to show how gram 2veccan explain why a document is attributed to a cer- tain author, using cosine similarities between thegram 2vecfeature vectors to calculate the distances between candidate documents and a query document. our gram 2veccode is on https://github.com/eric-sclafani/gram2vec, and a corresponding demo video is available at https://youtu.be/y8vj31d7woi "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12158",
        "label": 0,
        "title": "llms are prone to fallacies in causal inference nitish joshi1abulhair saparov1yixin wang2he he1 1new york",
        "abstract": "recent work shows that causal facts can be ef- fectively extracted from llms through prompt- ing, facilitating the creation of causal graphs for causal inference tasks. however, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which the model can memorize. thus, this work investi- gates: can llms infer causal relations from other relational data in text? to disentangle the role of memorized causal facts vs inferred causal relations, we finetune llms on syn- thetic data containing temporal, spatial and counterfactual relations, and measure whether the llm can then infer causal relations. we find that: (a) llms are susceptible to inferring causal relations from the order of two entity mentions in text (e.g. x mentioned before y implies x causes y); (b) if the order is ran- domized, llms still suffer from the post hoc fallacy , i.e. x occurs before y (temporal re- lation) implies x causes y . we also find that while llms can correctly deduce the absence of causal relations from temporal and spatial re- lations, they have difficulty inferring causal re- lations from counterfactuals, questioning their understanding of causality. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12182",
        "label": 1,
        "title": "aqulia-med llm: pioneering full-process open-source medical language models lulu zhao1",
        "abstract": "recently, both closed-source llms and open- source communities have made significant strides, outperforming humans in various gen- eral domains. however, their performance in specific professional fields such as medicine, especially within the open-source community, remains suboptimal due to the complexity of medical knowledge. we propose aquila-med, a bilingual medical llm based on aquila, ad- dressing these challenges through continue pre- training, supervised fine-tuning (sft), and re- inforcement learning from human feedback (rlhf). we construct a large-scale chinese and english medical dataset for continue pre- training and a high-quality sft dataset, cover- ing extensive medical specialties. addition- ally, we develop a high-quality direct pref- erence optimization (dpo) dataset for fur- ther alignment. aquila-med achieves no- table results across single-turn, multi-turn di- alogues, and medical multiple-choice ques- tions, demonstrating the effectiveness of our approach. we open-source the datasets and the entire training process, contributing valu- able resources to the research community. our models and datasets will released at https://huggingface.co/baai/aquilamed-rl. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12277",
        "label": 1,
        "title": "what matters in learning facts in language models? multifaceted knowledge probing with diverse multi-prompt da",
        "abstract": "large language models (llms) face issues in handling factual knowledge, making it vital to evaluate their true ability to understand facts. in this study, we introduce knowledge prob- ing frameworks, belief(-icl), to evaluate the knowledge understanding ability of both encoder-based and decoder-based pre-trained lms (plms) from diverse perspectives. be- liefs utilize a multi-prompt dataset to evaluate plm\u2019s accuracy, consistency, and reliability in factual knowledge understanding. to en- able a more reliable evaluation with beliefs, we semi-automatically create myriadlama, which has massively diverse prompts. we vali- date the effectiveness of beliefs in correctly and comprehensively evaluating plm\u2019s factual understanding ability via extensive evaluations with recent llms. we then investigate key fac- tors in learning facts in llms, and reveal the limitation of the prompt-based knowledge prob- ing. the dataset is anonymously publicized.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09181",
        "label": 1,
        "title": "a large-scale universal evaluation benchmark for face forgery detection yijun bei",
        "abstract": "with the rapid development of ai-generated content (aigc) technology, the production of realistic fake facial images and videos that deceive human visual perception has become possible. consequently, various face forgery detection techniques have been proposed to identify such fake facial content. however, evaluating the effectiveness and generalizability of these detection techniques re- mains a significant challenge. to address this, we have constructed a large-scale evaluation benchmark called deepfacegen, aimed at quantitatively assessing the effectiveness of face forgery detection and facilitating the iterative development of forgery detection technology. deepfacegen consists of 776 ,990real face image/video samples and 773 ,812face forgery image/video samples, generated using 34mainstream face generation techniques. during the construction pro- cess, we carefully consider important factors such as content diversity, fairness across ethnicities, and availability of comprehensive labels, in order to ensure the versatility and convenience of deepfacegen. subsequently, deepfacegen is employed in this study to evaluate and analyze the performance of 13mainstream face forgery detection techniques from various perspectives. through extensive experimental analysis, we derive significant findings and propose potential direc- tions for future research. the code and dataset for deepfacegen are available at https://github.com/hengruilou/deepfacegen. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.17186",
        "label": 1,
        "title": "clerc : a dataset for legal case retrieval and retrieval-augmented analysis generation abe bohan hou\u2663orion wel",
        "abstract": "legal professionals need to write analyses that rely on citations to relevant precedents, i.e., pre- vious case decisions. intelligence systems as- sisting legal professionals in writing such doc- uments provide great benefits but are challeng- ing to design. such systems need to help lo- cate, summarize, and reason over salient prece- dents in order to be useful. to enable systems for such tasks, we work with legal profession- als to transform a large open-source legal cor- pus into a dataset1supporting two important backbone tasks: information retrieval (ir) and retrieval-augmented generation (rag). this dataset clerc (caselawevaluation and retrieval corpus), is constructed for training and evaluating models on their ability to (1) find corresponding citations for a given piece of legal analysis and to (2) compile the text of these citations (as well as previous context) into a cogent analysis that supports a reasoning goal. we benchmark state-of-the-art models on clerc , showing that current approaches still struggle: gpt-4o generates analyses with the highest rouge f-scores but hallucinates the most, while zero-shot ir models only achieve 48.3% recall@1000. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09321",
        "label": 1,
        "title": "jailbreakeval : an integrated toolkit for evaluating jailbreak attempts against large language models delong r",
        "abstract": "jailbreak attacks aim to induce large language models (llms) to generate harmful responses for forbidden instructions, presenting severe misuse threats to llms. up to now, research into jailbreak attacks and defenses is emerging, however, there is (surprisingly) no consensus on how to evaluate whether a jailbreak attempt is successful. in other words, the methods to assess the harmfulness of an llm\u2019s response are varied, such as manual annotation or prompting gpt-4 in specific ways. each approach has its own set of strengths and weaknesses, impacting their alignment with human values, as well as the time and financial cost. this diversity in evaluation presents challenges for researchers in choosing suitable evaluation methods and conducting fair comparisons across different jailbreak attacks and defenses. in this paper, we conduct a comprehensive analysis of jailbreak evaluation methodologies, drawing from nearly ninety jailbreak research released between may 2023 and april 2024. our study introduces a systematic taxonomy of jailbreak evaluators, offering in-depth insights into their strengths and weaknesses, along with the current status of their adaptation. moreover, to facilitate subsequent research, we propose jailbreakeval (https://github.com/thuccslab/ jailbreakeval ), a user-friendly toolkit focusing on the evaluation of jailbreak attempts. it includes various well-known evaluators out-of-the-box, so that users can obtain evaluation results with only a single command. jailbreakeval also allows users to customize their own evaluation workflow in a unified framework with the ease of development and comparison. in summary, we regard jailbreakeval to be a catalyst that simplifies the evaluation process in jailbreak research and fosters an inclusive standard for jailbreak evaluation within the community. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08673",
        "label": 1,
        "title": "helpsteer2: open-source dataset for training top-performing reward models zhilin wang",
        "abstract": "high-quality preference datasets are essential for traini ng reward models that can effectively guide large language models (llms) in generati ng high-quality re- sponses aligned with human preferences. as llms become stro nger and better aligned, permissively licensed preference datasets, such as open assistant, hh- rlhf, and helpsteer need to be updated to remain effective fo r reward model- ing. methods that distil preference data from proprietary l lms such as gpt-4 have restrictions on commercial usage imposed by model prov iders. to improve upon both generated responses and attribute labeling quali ty, we release help- steer2, a permissively licensed preference dataset (cc-by -4.0). using a power- ful internal base model trained on helpsteer2, we are able to achieve the sota score (92.0%) on reward-bench\u2019s primary dataset, outperfo rming currently listed open and proprietary models, as of june 12th, 2024. notably, helpsteer2 consists of only ten thousand response pairs, an order of magnitude fe wer than existing preference datasets (e.g., hh-rlhf), which makes it highly ef\ufb01cient for train- ing reward models. our extensive experiments demonstrate t hat reward models trained with helpsteer2 are effective in aligning llms. in p articular, we propose steerlm 2.0, a model alignment approach that can effectivel y make use of the rich multi-attribute score predicted by our reward models. helpsteer2 is avail- able athttps://huggingface.co/datasets/nvidia/helpsteer2 and code is available at https://github.com/nvidia/nemo-aligner . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09326",
        "label": 1,
        "title": "pianomotion10m : dataset and benchmark for hand motion generation in piano performance qijun gan",
        "abstract": "recently, artificial intelligence techniques for education have been received increas- ing attentions, while it still remains an open problem to design the effective music instrument instructing systems. although key presses can be directly derived from sheet music, the transitional movements among key presses require more extensive guidance in piano performance. in this work, we construct a piano-hand motion generation benchmark to guide hand movements and fingerings for piano playing. to this end, we collect an annotated dataset, pianomotion10m , consisting of 116 hours of piano playing videos from a bird\u2019s-eye view with 10 million annotated hand poses. we also introduce a powerful baseline model that generates hand motions from piano audios through a position predictor and a position-guided gesture generator. furthermore, a series of evaluation metrics are designed to assess the performance of the baseline model, including motion similarity, smooth- ness, positional accuracy of left and right hands, and overall fidelity of movement distribution. despite that piano key presses with respect to music scores or audios are already accessible, pianomotion10m aims to provide guidance on piano fingering for instruction purposes. the dataset and source code can be accessed at https://agnjason.github.io/pianomotion-page . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09388",
        "label": 1,
        "title": "exploring the spectrum of visio-linguistic compositionality and recognition youngtaek oh1",
        "abstract": "vision and language models (vlms) such as clip have showcased remarkable zero-shot recognition abilities yet face challenges in visio-linguistic compositionality, partic- ularly in linguistic comprehension and fine-grained image- text alignment. this paper explores the intricate relation- ship between compositionality and recognition \u2013 two piv- otal aspects of vlm capability. we conduct a compre- hensive evaluation of existing vlms, covering both pre- training approaches aimed at recognition and the fine- tuning methods designed to improve compositionality. our evaluation employs 12 benchmarks for compositionality, along with 21 zero-shot classification and two retrieval benchmarks for recognition. in our analysis from 274 clip model checkpoints, we reveal patterns and trade-offs that emerge between compositional understanding and recog- nition accuracy. ultimately, this necessitates strategic ef- forts towards developing models that improve both capabil- ities, as well as the meticulous formulation of benchmarks for compositionality. we open our evaluation framework at https://github.com/ytaek-oh/vl_compo . 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12480",
        "label": 1,
        "title": "the power of llm-generated synthetic data for stance detection in online political discussions stefan sylvius ",
        "abstract": "stance detection holds great potential for enhancing the quality of online political discussions, as it has shown to be useful for summarizing discussions, detecting misinformation, and evaluating opinion distributions. usually, transformer-based models are used directly for stance detection, which require large amounts of data. however, the broad range of debate questions in online political discussion creates a variety of possible scenarios that the model is faced with and thus makes data acquisition for model training difficult. in this work, we show how to leverage llm-generated synthetic data to train and improve stance detection agents for online political discussions: (i) we generate synthetic data for specific debate questions by prompting a mistral-7b model and show that fine-tuning with the generated synthetic data can substantially improve the performance of stance detection. (ii) we examine the impact of combining synthetic data with the most informative samples from an unlabelled dataset. first, we use the synthetic data to select the most informative samples, second, we combine both these samples and the synthetic data for fine-tuning. this approach reduces labelling effort and consistently surpasses the performance of the baseline model that is trained with fully labeled data. overall, we show in comprehensive experiments that llm- generated data greatly improves stance detection performance for online political discussions. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12527",
        "label": 1,
        "title": "fusegen: plm fusion for data-generation based zero-shot learning tianyuan zou1yang liu1 2peng li1 2jianqing zh",
        "abstract": "data generation-based zero-shot learning, al- though effective in training small task-specific models (stms) via synthetic datasets generated by pre-trained language models (plms), is of- ten limited by the low quality of such synthetic datasets. previous solutions have primarily fo- cused on single plm settings, where synthetic datasets are typically restricted to specific sub- spaces and often deviate from real-world distri- butions, leading to severe distribution bias. to mitigate such bias, we propose fusegen, a novel data generation-based zero-shot learning frame- work that introduces a new criteria for subset se- lection from synthetic datasets via utilizing multi- ple plms and trained stms. the chosen subset provides in-context feedback to each plm, en- hancing dataset quality through iterative data gen- eration. trained stms are then used for sample re-weighting as well, further improving data qual- ity. extensive experiments across diverse tasks demonstrate that fusegen substantially outper- forms existing methods, highly effective in boost- ing stm performance in a plm-agnostic way. code is provided in https://github.com/ lindalydia/fusegen . 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12549",
        "label": 1,
        "title": "multisocial: multilingual benchmark of machine-generated text detection of social-media texts dominik macko",
        "abstract": "recent llms are able to generate high-quality multilingual texts, indistinguishable for hu- mans from authentic human-written ones. re- search in machine-generated text detection is however mostly focused on the english lan- guage and longer texts, such as news articles, scientific papers or student essays. social- media texts are usually much shorter and of- ten feature informal language, grammatical er- rors, or distinct linguistic items (e.g., emoti- cons, hashtags). there is a gap in studying the ability of existing methods in detection of such texts, reflected also in the lack of exist- ing multilingual benchmark datasets. to fill this gap we propose the first multilingual (22 languages) and multi-platform (5 social media platforms) dataset for benchmarking machine- generated text detection in the social-media do- main, called multisocial1. it contains 472,097 texts, of which about 58k are human-written and approximately the same amount is gener- ated by each of 7 multilingual llms. we use this benchmark to compare existing detection methods in zero-shot as well as fine-tuned form. our results indicate that the fine-tuned detec- tors have no problem to be trained on social- media texts and that the platform selection for training matters. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12572",
        "label": 1,
        "title": "MATHADOR-LM: A DYNAMIC BENCHMARK FOR MATHEMATICAL REASONING ON LARGE LANGUAGE MODELS",
        "abstract": "we introduce mathador-lm, a new benchmark for evaluating the mathematical reasoning on large language models (llms), combining ruleset interpretation, planning, and problem-solving. this benchmark is inspired by the mathador game, where the objective is to reach a target number using basic arithmetic operations on a given set of base numbers, following a simple set of rules. we show that, across leading llms, we obtain stable average performance while generating benchmark instances dynamically , following a target difficulty level. thus, our benchmark alleviates concerns about test-set leakage into training data, an issue that often undermines popular benchmarks. ad- ditionally, we conduct a comprehensive evaluation of both open and closed-source state-of-the-art llms on mathador-lm. our findings reveal that contemporary models struggle with mathador-lm, scoring significantly lower than average 3rd graders. this stands in stark contrast to their strong performance on popular mathematical reasoning benchmarks. the implementation is available at https://github.com/ist-daslab/mathador-lm . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12606",
        "label": 1,
        "title": "low-redundant optimization for large language model alignment zhipeng chen1",
        "abstract": "large language models (llms) are still strug- gling in aligning with human preference in complex tasks and scenarios. they are prone to overfit into the unexpected patterns or su- perficial styles in the training data. we con- duct an empirical study that only selects the top-10% most updated parameters in llms for alignment training, and see improvements in the convergence process and final perfor- mance. it indicates the existence of redundant neurons in llms for alignment training. to re- duce its influence, we propose a low-redundant alignment method named allo , focusing on optimizing the most related neurons with the most useful supervised signals. concretely, we first identify the neurons that are related to the human preference data by a gradient-based strategy, then identify the alignment-related key tokens by reward models for computing loss. besides, we also decompose the align- ment process into the forgetting and learning stages, where we first forget the tokens with unaligned knowledge and then learn aligned knowledge, by updating different ratios of neu- rons, respectively. experimental results on 10 datasets have shown the effectiveness of allo. our code and data are available at https://github.com/rucaibox/allo . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.19999",
        "label": 1,
        "title": "the sifo benchmark: investigating the sequential instruction following ability of large language models xinyi ",
        "abstract": "following multiple instructions is a crucial abil- ity for large language models (llms). evalu- ating this ability comes with significant chal- lenges: (i) limited coherence between multiple instructions, (ii) positional bias where the or- der of instructions affects model performance, and (iii) a lack of objectively verifiable tasks. to address these issues, we introduce a bench- mark designed to evaluate models\u2019 abilities to follow multiple instructions through sequen- tial instruction following (sifo) tasks. in sifo, the successful completion of multiple instruc- tions is verifiable by examining only the final in- struction. our benchmark evaluates instruction following using four tasks (text modification, question answering, mathematics, and security rule following), each assessing different aspects of sequential instruction following. our evalua- tion of popular llms, both closed-source and open-source, shows that more recent and larger models significantly outperform their older and smaller counterparts on the sifo tasks, validat- ing the benchmark\u2019s effectiveness. all models struggle with following sequences of instruc- tions, hinting at an important lack of robustness of today\u2019s language models. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.20015 ",
        "label": 1,
        "title": "ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models",
        "abstract": "Tool-augmented large language models (LLMs) are rapidly being integrated into real-world applications. Due to the lack of benchmarks, the community still needs to fully understand the hallucination issues within these models. To address this challenge, we intro- duce a comprehensive diagnostic benchmark, ToolBH. Specifically, we assess the LLMs hallucinations through two perspectives: depth and breadth. In terms of depth, we propose a multi-level diagnostic process, including (1) solvability detection, (2) solution planning, and (3) missing-tool analysis. For breadth, we consider three scenarios based on the characteristics of the toolset: missing necessary tools, potential tools, and limited functionality tools. Furthermore, we developed seven tasks and collected 700 evaluation samples through multiple rounds of manual annotation. The results show the significant challenges presented by the ToolBH benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve a total score of 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger model parameters do not guarantee better performance; the training data and response strategies also play a crucial role in tool-enhanced LLM scenarios. Our diagnostic analysis indicates that the primary reason for model errors lies in assessing task solvability. Additionally, open-weight models suffer from performance drops with verbose replies, whereas proprietary models excel with longer reasoning."
    },
    {
        "url": "https://arxiv.org/pdf/2406.20038",
        "label": 1,
        "title": "biomner: a dataset for biomedical method entity recognition chen tang1",
        "abstract": "named entity recognition (ner) stands as a fundamental and pivotal task within the realm of natural language processing. particularly within the domain of biomedical method ner, this task presents notable challenges, stemming from the continual influx of domain-specific terminologies in scholarly literature. current research in biomedical method (biomethod) ner suffers from a scarcity of resources, primarily attributed to the intricate nature of methodological concepts, which necessitate a profound understanding for precise delineation. in this study, we propose a novel dataset for biomedical method entity recog- nition, employing an automated biomethod entity recognition and information retrieval system to assist human annotation. furthermore, we com- prehensively explore a range of conventional and contemporary open-domain ner methodologies, including the utilization of cutting-edge large- scale language models (llms) customised to our dataset. our empirical findings reveal that the large parameter counts of language models sur- prisingly inhibit the effective assimilation of entity extraction patterns pertaining to biomedical meth- ods. remarkably, the approach, leveraging the modestly sized albert model (only 11mb), in conjunction with conditional random fields (crf), achieves state-of-the-art (sota) performance. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.19650",
        "label": 1,
        "title": "decor : improving coherence in l2 english writing with a novel benchmark for incoherence detection",
        "abstract": "coherence in writing, an aspect that second- language (l2) english learners often struggle with, is crucial in assessing l2 english writing. existing automated writing evaluation systems primarily use basic surface linguistic features to detect coherence in writing. however, little effort has been made to correct the detected incoherence, which could significantly bene- fit l2 language learners seeking to improve their writing. to bridge this gap, we introduce decor , a novel benchmark that includes ex- pert annotations for detecting incoherence in l2 english writing, identifying the underlying reasons, and rewriting the incoherent sentences. to our knowledge, decor is the first coher- ence assessment dataset specifically designed for improving l2 english writing, featuring pairs of original incoherent sentences alongside their expert-rewritten counterparts. addition- ally, we fine-tuned models to automatically de- tect and rewrite incoherence in student essays. we find that incorporating specific reasons for incoherence during fine-tuning consistently im- proves the quality of the rewrites, achieving a result that is favored in both automatic and human evaluations.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12639",
        "label": 1,
        "title": "ask-before-plan: proactive language agents for real-world planning xuan zhang",
        "abstract": "the evolution of large language models (llms) has enhanced the planning capabilities of lan- guage agents in diverse real-world scenar- ios. despite these advancements, the poten- tial of llm-powered agents to comprehend ambiguous user instructions for reasoning and decision-making is still under exploration. in this work, we introduce a new task, proac- tive agent planning, which requires language agents to predict clarification needs based on user-agent conversation and agent-environment interaction, invoke external tools to collect valid information, and generate a plan to ful- fill the user\u2019s demands. to study this practical problem, we establish a new benchmark dataset, ask-before-plan. to tackle the deficiency of llms in proactive planning, we propose a novel multi-agent framework, clarification- execution-planning ( cep), which consists of three agents specialized in clarification, execu- tion, and planning. we introduce the trajectory tuning scheme for the clarification agent and static execution agent, as well as the memory recollection mechanism for the dynamic execu- tion agent. extensive evaluations and compre- hensive analyses conducted on the ask-before- plan dataset validate the effectiveness of our proposed framework.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12645",
        "label": 0,
        "title": "evaluating transparency of machine generated fact checking explanations rui xing1timothy baldwin1",
        "abstract": "an important factor when it comes to gener- ating fact-checking explanations is the selec- tion of evidence : intuitively, high-quality expla- nations can only be generated given the right evidence. in this work, we investigate the im- pact of human-curated vs. machine-selected evidence for explanation generation using large language models. to assess the quality of expla- nations, we focus on transparency (whether an explanation cites sources properly) and utility (whether an explanation is helpful in clarifying a claim). surprisingly, we found that large lan- guage models generate similar or higher qual- ity explanations using machine-selected evi- dence, suggesting carefully curated evidence (by humans) may not be necessary. that said, even with the best model, the generated expla- nations are not always faithful to the sources, suggesting further room for improvement in explanation generation for fact-checking. code and data are available here: https://github. com/ruixing76/transparent-fcexp . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12753",
        "label": 1,
        "title": "olympicarena: benchmarking multi-discipline cognitive reasoning for superintelligent ai zhen huang3",
        "abstract": "the evolution of artificial intelligence (ai) has been significantly accelerated by advancements in large language models (llms) and large multimodal models (lmms), gradually showcasing potential cognitive reasoning abilities in problem- solving and scientific discovery (i.e., ai4science) once exclusive to human intellect. to comprehensively evaluate current models\u2019 performance in cognitive reasoning abilities, we introduce olympicarena , which includes 11,163 bilingual problems across both text-only and interleaved text-image modalities. these challenges encompass a wide range of disciplines spanning seven fields and 62 international olympic competitions, rigorously examined for data leakage. we argue that the challenges in olympic competition problems are ideal for evaluating ai\u2019s cognitive reasoning due to their complexity and interdisciplinary nature, which are essential for tackling complex scientific challenges and facilitating discoveries. beyond evaluating performance across various disciplines using answer-only criteria, we conduct detailed experiments and analyses from multiple perspectives. we delve into the models\u2019 cognitive reasoning abilities, their performance across different modalities, and their outcomes in process-level evaluations, which are vital for tasks requiring complex reasoning with lengthy solutions. our extensive evaluations reveal that even advanced models like gpt-4o only achieve a 39.97% overall accuracy (28.67% for mathematics and 29.71% for physics), illustrating current ai limitations in complex reasoning and multimodal integration. through the olympicarena , we aim to advance ai towards superintelligence, equipping it to address more complex challenges in science and beyond. we also provide a comprehensive set of resources to support ai research, including a benchmark dataset, an open-source annotation platform, a detailed evaluation tool, and a leaderboard with automatic submission features.2 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12295",
        "label": 0,
        "title": "fast and slow generating: an empirical study on large and small language models collaborative decoding kaiyan ",
        "abstract": "large language models (llms) demonstrate impressive performance in diverse applications, yet they face significant drawbacks, includ- ing high inference latency, expensive training cost, and generation of hallucination. collab- orative decoding between large and small lan- guage models (slms) offers a novel approach to address these challenges. inspired by dual- process cognitive theory, we integrate these methods into a unified framework termed fast and slow generating (fs-gen). this paper ex- plores several techniques within the fs-gen framework, including speculative decoding, contrastive decoding, and emulator or proxy fine-tuning. we provide a comprehensive anal- ysis of these methodologies, offering insights into their similarities and differences under this framework. our study delves into the differ- ential knowledge capabilities of llms versus slms through the fs-gen lens, revealing that fewer than 20% of collaborative interactions are required across various methods. these in- teractions adhere to a scaling law relative to the parameter ratios, thereby facilitating pre- dictable collaboration. furthermore, we investi- gate the specific positions where collaboration is most effective from an uncertainty perspec- tive, yielding novel insights that could refine fs-gen methods. our findings reveal that the essential difference between models of differ- ent sizes lies in the uncertainty of the next token prediction, where interventions by larger mod- els are most needed to assist the smaller ones. code for reproduction: https://github. com/tsinghuac3i/fs-gen "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12304",
        "label": 0,
        "title": "journal of l atex class files",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12373",
        "label": 1,
        "title": "webcanvas : benchmarking web agents in online environments yichen pan1",
        "abstract": "for web agents to be practically useful, they must adapt to the continuously evolv- ing web environment characterized by frequent updates to user interfaces and content. however, most existing benchmarks only capture the static aspects of the web. to bridge this gap, we introduce webcanvas , an innovative online evaluation framework for web agents that effectively addresses the dynamic nature of web interactions. webcanvas contains three main components to facilitate realistic assessments: (1) a novel evaluation metric which reliably capture critical inter- mediate actions or states necessary for task completions while disregarding noise caused by insignificant events or changed web-elements. (2) a benchmark dataset called mind2web-live, a refined version of original mind2web static dataset containing 542 tasks with 2439 intermediate evaluation states; (3) lightweight and generalizable annotation tools and testing pipelines that enables the community to collect and maintain the high-quality, up-to-date dataset. building on webcanvas , we open-source an agent framework with extensible modules for reasoning, provid- ing a foundation for the community to conduct online inference and evaluations. our best-performing agent achieves a task success rate of 23.1% and a task com- pletion rate of 48.8% on the mind2web-live test set. additionally, we analyze the performance discrepancies across various websites, domains, and experimental environments. we encourage the community to contribute further insights on online agent evaluation, thereby advancing this field of research.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12382",
        "label": 0,
        "title": "from instance training to instruction learning: task adapters generation from instructions huanxuan liao1",
        "abstract": "large language models (llms) have acquired the ability to solve general tasks by utilizing instruction finetuning (ift). however, ift still relies heavily on instance training of extensive task data, which greatly limits the adaptability of llms to real-world scenarios where labeled task instances are scarce and broader task generalization becomes paramount. contrary to llms, humans acquire skills and complete tasks not merely through repeated practice but also by understand- ing and following instructional guidelines. this paper is dedicated to simulating human learning to address the shortcomings of instance training, focusing on in- struction learning to enhance cross-task generalization. within this context, we introduce taskadapters generation from instructions ( tagi ), which automati- cally constructs the task-specific model in a parameter generation manner based on the given task instructions without retraining for unseen tasks. specifically, we utilize knowledge distillation to enhance the consistency between tagi de- veloped through learning with instruction and task-specific models developed through training with instance , by aligning the labels, output logits, and adapter parameters between them. tagi is endowed with cross-task generalization capabil- ities through a two-stage training process that includes hypernetwork pretraining and finetuning. we evaluate tagi on the super-natural instructions and p3 datasets. the experimental results demonstrate that tagi can match or even out- perform traditional meta-trained models and other hypernetwork models, while significantly reducing computational requirements. our code will be available at https://github.com/xnhyacinth/tagi . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12386",
        "label": 1,
        "title": "ipeval: a bilingual intellectual property agency consultation evaluation benchmark for large language models",
        "abstract": "with the rapid development of large language models (llms) in vertical domains, attempts have been made to the field of intellectual property (ip). however, there is currently no evaluation benchmark specifically for assessing the understanding, application, and reasoning abilities of llms in the ip domain. to address this issue, we introduce ipeval, the first capability evaluation benchmark designed for ip agency and consulting tasks. ipeval consists of 2657 multiple-choice questions, divided into four major capability dimensions: creation, application, protection, and management. these questions cover eight areas: patent rights which including inventions, utility models, and designs, trademarks, copyrights, trade secrets, integrated circuit layout design rights, geographical indications, and related laws. we designed three evaluation methods: zero-shot, 5-few-shot, and chain of thought (cot) for seven kinds of llms with varying parameters, primarily using either english or chinese. the study results indicate that the gpt series and qwen series models demonstrate stronger performance in english tests, while chinese-major llms, such as the qwen series, outperform gpt- 4 in chinese tests. specialized legal domain llms, such as the fuzi-mingcha and mozi, still significantly lag behind general-purpose llms of comparable parameter sizes in ip performance. this highlights the necessity and substantial potential for developing more specialized llms with stronger ip abilities. we also analyze the models\u2019 capabilities in terms of the regional and temporal aspects of ip, emphasizing that ip domain llms need to clearly understand the differences in ip laws across different regions and their dynamic changes over time. we hope ipeval can provide an accurate assessment of llm capabilities in the ip domain and encourage researchers interested in ip to develop llms with richer ip knowledge. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12422",
        "label": 0,
        "title": "this paper was accepted to tsd 2024 . please cite it instead once published. open-source web service with morp",
        "abstract": ". wepresentanopen-sourcewebserviceforczechmorphosyn- tactic analysis. the system combines a deep learning model with rescor- ing by a high-precision morphological dictionary at inference time. we show that our hybrid method surpasses two competitive baselines: while the deep learning model ensures generalization for out-of-vocabulary words and better disambiguation, an improvement over an existing mor- phological analyser morphodita, at the same time, the deep learning model benefits from inference-time guidance of a manually curated mor- phological dictionary. we achieve 50% error reduction in lemmatiza- tion and 58% error reduction in pos tagging over morphodita, while also offering dependency parsing. the model is trained on one of the currently largest czech morphosyntactic corpora, the pdt-c 1.0, with thetrainedmodelsavailableathttps://hdl.handle.net/11234/1-5293.we provide the tool as a web service deployed at https://lindat.mff.cuni. cz/services/udpipe/. the source code is available at github (https:// github.com/ufal/udpipe/tree/udpipe-2), along with a python client for a simple use. the documentation for the models can be found at https: //ufal.mff.cuni.cz/udpipe/2/models#czech_pdtc1.0_model. keywords: morphosyntactic analysis \u00b7deep learning \u00b7morphological dictionary \u00b7pos tagging \u00b7lemmatization "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09315",
        "label": 0,
        "title": "vertical lora: d ense expectation -maximization interpretation of transformers zhuo-lin fu",
        "abstract": "in this paper, we show how transformers can be interpreted as dense expectation-maximization algorithms performed on bayesian nets. based on the above interpretation, we propose a new model design paradigm, namely vertical lora (vlora), which reduces the parameter count dramatically while preserving performance. in vlora, a model consists of layers, each of which recursively learns an increment based on the previous layer. we then apply lora decomposition to the increments. vlora works on the base model, which is orthogonal to lora, meaning they can be used together. we do experiments on various tasks and models. the results show that 1) with vlora, the transformer model parameter count can be reduced dramatically and 2) the performance of the original model is preserved. the source code is available at https: //github.com/neverusethisname/vlora keywords low-rank adaptation \u00b7transformer \u00b7em algorithm "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12814",
        "label": 0,
        "title": "adversarial attacks on multimodal agents chen henry wu",
        "abstract": "vision-enabled language models ( vlms ) are now used to build autonomous mul- timodal agents capable of taking actions in real environments. in this paper, we show that multimodal agents raise new safety risks, even though attacking agents is more challenging than prior attacks due to limited access to and knowledge about the environment. our attacks use adversarial text strings to guide gradient-based perturbation over onetrigger image in the environment: (1) our captioner attack attacks white-box captioners if they are used to process images into captions as additional inputs to the vlm ; (2) our clip attack attacks a set of clip models jointly, which can transfer to proprietary vlms . to evaluate the attacks, we curated visualwebarena-adv, a set of adversarial tasks based on visualwebarena, an en- vironment for web-based multimodal agent tasks. within an l\u221e-norm of 16/256 on a single image, the captioner attack can make a captioner-augmented gpt-4v agent execute the adversarial goals with a 75% success rate. when we remove the captioner or use gpt-4v to generate its own captions, the clip attack can achieve success rates of 21% and 43%, respectively. experiments on agents based on other vlms , such as gemini-1.5, claude-3, and gpt-4o , show interesting differences in their robustness. further analysis reveals several key factors contributing to the attack\u2019s success, and we also discuss the implications for defenses as well.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08787",
        "label": 0,
        "title": "a s urvey on compositional learning of ai m odels : theoretical and experimental practices a p reprint",
        "abstract": "compositional learning, mastering the ability to combine b asic concepts and construct more intri- cate ones, is crucial for human cognition, especially in hum an language comprehension and visual perception. this notion is tightly connected to generaliza tion over unobserved situations. despite its integral role in intelligence, there is a lack of systema tic theoretical and experimental research methodologies, making it dif\ufb01cult to analyze the compositi onal learning abilities of computational models. in this paper, we survey the literature on compositi onal learning of ai models and the connections made to cognitive studies. we identify abstrac t concepts of compositionality in cogni- tive and linguistic studies and connect these to the computa tional challenges faced by language and vision models in compositional reasoning. we overview the f ormal de\ufb01nitions, tasks, evaluation benchmarks, variety of computational models, and theoreti cal \ufb01ndings. we cover modern studies on large language models to provide a deeper understanding of t he cutting-edge compositional capabil- ities exhibited by state-of-the-art ai models and pinpoint important directions for future research. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08766",
        "label": 0,
        "title": "1 injecting combinatorial optimization into mcts : application to the board game boop.",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.11217",
        "label": 1,
        "title": "weatherqa: can multimodal language models reason about severe weather? chengqian ma1",
        "abstract": "severe convective weather events, such as hail, tornadoes, and thunderstorms, often occur quickly yet cause significant damage, costing billions of dollars every year. this highlights the importance of forecasting severe weather threats hours in advance to better prepare meteorologists and residents in at-risk areas. can modern large foundation models perform such forecasting? existing weather benchmarks typically focus only on predicting time-series changes in certain weather parameters (e.g., temperature, moisture) with text-only features. in this work, we introduce weatherqa, the first multimodal dataset designed for machines to reason about complex combinations of weather parameters (a.k.a., ingredients ) and predict severe weather in real-world scenarios. the dataset includes over 8,000 (multi- images, text) pairs for diverse severe weather events. each pair contains rich information crucial for forecasting\u2014the images describe the ingredients capturing environmental instability, surface observations, and radar reflectivity, and the text contains in-depth forecast analyses written by human experts. with weatherqa, we systematically evaluate state-of-the-art vision language models (vlms), including gpt4, claude3.5, gemini-1.5, and a fine-tuned llama3-based vlm, by designing two challenging tasks: (1) multi-choice qa for predicting affected area and (2) classification of the development potential of severe convection. these tasks require deep understanding of domain knowledge (e.g., atmospheric dynamics) and complex reasoning over multimodal data (e.g., interactions between weather parameters). we show a substantial gap between the strongest vlm, gpt4o, and human reasoning. our comprehensive case study with meteorologists further reveals the weaknesses of the models, suggesting that better training and data integration are necessary to bridge this gap. weatherqa is accessible through https://github.com/chengqianma/weatherqa . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11781",
        "label": 1,
        "title": "diffmm: multi-modal diffusion model for recommendation yangqin jiang",
        "abstract": "the rise of online multi-modal sharing platforms like tiktok and youtube has enabled personalized recommender systems to incor- porate multiple modalities (such as visual, textual, and acoustic) into user representations. however, addressing the challenge of data sparsity in these systems remains a key issue. to address this limitation, recent research has introduced self-supervised learn- ing techniques to enhance recommender systems. however, these methods often rely on simplistic random augmentation or intu- itive cross-view information, which can introduce irrelevant noise and fail to accurately align the multi-modal context with user- item interaction modeling. to fill this research gap, we propose a novel multi-modal graph diffusion model for recommendation called diffmm. our framework integrates a modality-aware graph diffusion model with a cross-modal contrastive learning paradigm to improve modality-aware user representation learning. this inte- gration facilitates better alignment between multi-modal feature information and collaborative relation modeling. our approach leverages diffusion models\u2019 generative capabilities to automatically generate a user-item graph that is aware of different modalities, facilitating the incorporation of useful multi-modal knowledge in modeling user-item interactions. we conduct extensive experiments on three public datasets, consistently demonstrating the superiority of our diffmm over various competitive baselines. for open-sourced model implementation details, you can access the source codes of our proposed framework at: https://github.com/hkuds/diffmm. acm reference format: yangqin jiang, lianghao xia, wei wei, da luo, kangyi lin, and chao huang. 2024. diffmm: multi-modal diffusion model for recommendation. in pro- ceedings of acm conference (conference\u201917). acm, new york, ny, usa, 11 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11589",
        "label": 1,
        "title": "cosqa+: enhancing code search dataset with matching code jing gong1",
        "abstract": "semantic code search, retrieving code that matches a given natural language query, is an important task to improve productivity in software engineering. existing code search datasets are problematic: either using unreal- istic queries, or with mismatched codes, and typically using one-to-one query-code pair- ing, which fails to reflect the reality that a query might have multiple valid code matches. this paper introduces cosqa+, pairing high- quality queries (reused from cosqa) with mul- tiple suitable codes. we collect code candi- dates from diverse sources and form candi- date pairs by pairing queries with these codes. utilizing the power of large language mod- els (llms), we automate pair annotation, fil- tering, and code generation for queries with- out suitable matches. through extensive ex- periments, cosqa+has demonstrated supe- rior quality over cosqa. models trained on cosqa+exhibit improved performance. fur- thermore, we propose a new metric mean multi- choice reciprocal rank (mmrr), to assess one-to-n code search performance. we pro- vide the code and data at https://github. com/deepsoftwareanalytics/cosqa_plus . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.13121",
        "label": 1,
        "title": "can long-context language models subsume retrieval",
        "abstract": "long-context language models (lclms) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases. leveraging lclms\u2019 ability to natively ingest and process entire corpora of information offers numerous advantages. it enhances user-friendliness by elim- inating the need for specialized knowledge of tools, provides robust end-to-end modeling that minimizes cascading errors in complex pipelines, and allows for the application of sophisticated prompting techniques across the entire system. to assess this paradigm shift, we introduce loft , a benchmark of real-world tasks requiring context up to millions of tokens designed to evaluate lclms\u2019 performance on in-context retrieval and reasoning. our findings reveal lclms\u2019 surprising ability to rival state-of-the-art retrieval and rag systems, despite never having been explicitly trained for these tasks. however, lclms still face chal- lenges in areas like compositional reasoning that are required in sql-like tasks. notably, prompting strategies significantly influence performance, emphasizing the need for continued research as context lengths grow. overall, loft provides a rigorous testing ground for lclms, showcasing their potential to supplant existing paradigms and tackle novel tasks as model capabilities scale.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.13249",
        "label": 0,
        "title": "r2ag: incorporating retrieval information into retrieval augmented generation fuda ye1",
        "abstract": "retrieval augmented generation (rag) has been applied in many scenarios to augment large language models (llms) with external documents provided by retrievers. however, a semantic gap exists between llms and retrievers due to differences in their training objectives and architectures. this misalign- ment forces llms to passively accept the documents provided by the retrievers, leading to incomprehension in the generation process, where the llms are burdened with the task of distinguishing these documents using their in- herent knowledge. this paper proposes r2ag, a novel enhanced rag framework to fill this gap by incorporating retrieval information into retrieval augmented generation. specifically, r2ag utilizes the nuanced features from the retrievers and employs a r2-former to capture retrieval information. then, a retrieval-aware prompting strategy is designed to integrate re- trieval information into llms\u2019 generation. no- tably, r2ag suits low-source scenarios where llms and retrievers are frozen. extensive ex- periments across five datasets validate the effec- tiveness, robustness, and efficiency of r2ag. our analysis reveals that retrieval information serves as an anchor to aid llms in the gener- ation process, thereby filling the semantic gap. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.14004",
        "label": 0,
        "title": "do not wait: learning re-ranking model without user feedback at serving time in e-commerce yuan wang",
        "abstract": "recommender systems have been widely used in e-commerce, and re-ranking models are playing an increasingly significant role in the domain, which leverages the inter-item influence and deter- mines the final recommendation lists. online learning methods keep updating a deployed model with the latest available samples to capture the shifting of the underlying data distribution in e- commerce. however, they depend on the availability of real user feedback, which may be delayed by hours or even days, such as item purchases, leading to a lag in model enhancement. in this paper, we propose a novel extension of online learning methods for re-ranking modeling, which we term last, an acronym for learning at serving time. it circumvents the requirement of user feedback by using a surrogate model to provide the instructional signal needed to steer model improvement. upon receiving an on- line request, last finds and applies a model modification on the fly before generating a recommendation result for the request. the modification is request-specific and transient. it means the mod- ification is tailored to and only to the current request to capture the specific context of the request. after a request, the modifica- tion is discarded, which helps to prevent error propagation and stabilizes the online learning procedure since the predictions of the surrogate model may be inaccurate. most importantly, as a complement to feedback-based online learning methods, last can be seamlessly integrated into existing online learning systems to \u2217both authors contributed equally to this research. \u2020first corresponding author: xiao zhang (zhangx89@ruc.edu.cn), second corresponding author: yuan wang (wy175696@alibaba-inc.com) permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. conference acronym \u2019xx, june 03\u201305, 2018, woodstock, ny \u00a92018 copyright held by the owner/author(s). publication rights licensed to acm. acm isbn 978-1-4503-xxxx-x/18/06. . . $15.00 https://doi.org/xxxxxxx.xxxxxxxcreate a more adaptive and responsive recommendation experience. comprehensive experiments, both offline and online, affirm that last outperforms state-of-the-art re-ranking models. ccs concepts \u2022theory of computation \u2192online algorithms ;\u2022computing methodologies\u2192learning paradigms ;\u2022applied computing \u2192online shopping . keywords online learning, re-ranking, surrogate model, recommender system, e-commerce acm reference format: yuan wang, zhiyu li, changshuo zhang, sirui chen, xiao zhang, jun xu, and quan lin. 2018. do not wait: learning re-ranking model without user feedback at serving time in e-commerce. in proceedings of make sure to enter the correct conference title from your rights confirmation emai (conference acronym \u2019xx). acm, new york, ny, usa, 6 pages. https://doi. org/xxxxxxx.xxxxxxx "
    },
    {
        "url": "https://arxiv.org/pdf/2406.14333",
        "label": 0,
        "title": "larp: language audio relational pre-training for cold-start playlist continuation rebecca salganik",
        "abstract": "as online music consumption increasingly shifts towards playlist- based listening, the task of playlist continuation , in which an al- gorithm suggests songs to extend a playlist in a personalized and musically cohesive manner, has become vital to the success of music streaming services. currently, many existing playlist continuation approaches rely on collaborative filtering methods to perform their recommendations. however, such methods will struggle to rec- ommend songs that lack interaction data, an issue known as the cold-start problem. current approaches to this challenge design complex mechanisms for extracting relational signals from sparse collaborative signals and integrating them into content represen- tations. however, these approaches leave content representation learning out of scope and utilize frozen, pre-trained content models that may not be aligned with the distribution or format of a specific musical setting. furthermore, even the musical state-of-the-art con- tent modules are either (1) incompatible with the cold-start setting or (2) unable to effectively integrate cross-modal and relational signals. in this paper, we introduce larp , a multi-modal cold-start playlist continuation model, to effectively overcome these limita- tions. larp is a three-stage contrastive learning framework that integrates both multi-modal and relational signals into its learned representations. our framework uses increasing stages of task- specific abstraction: within-track (language-audio) contrastive loss, track-track contrastive loss, and track-playlist contrastive loss. ex- perimental results on two publicly available datasets demonstrate the efficacy of larp over uni-modal and multi-modal models for playlist continuation in a cold-start setting. finally, this work pio- neers the perspective of addressing cold-start recommendation via \u2217equal contribution. \u2020this work was mainly done while rebecca was visiting the national university of singapore. \u2021corresponding author. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. kdd \u201924, august 25\u201329, 2024, barcelona, spain \u00a92024 copyright held by the owner/author(s). publication rights licensed to acm. acm isbn 979-8-4007-0490-1/24/08 https://doi.org/10.1145/3637528.3671772relational representation learning. code and dataset are released at: https://github.com/rsalganik1123/larp. ccs concepts \u2022information systems \u2192recommender systems ;\u2022applied computing\u2192sound and music computing . keywords music playlist continuation, music representation learning, language- audio pre-training, cold-start problem acm reference format: rebecca salganik, xiaohao liu, yunshan ma, jian kang, and tat-seng chua. 2024. larp: language audio relational pre-training for cold-start playlist continuation. in proceedings of the 30th acm sigkdd conference on knowl- edge discovery and data mining (kdd \u201924), august 25\u201329, 2024, barcelona, spain. acm, new york, ny, usa, 12 pages. https://doi.org/10.1145/3637528. 3671772 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.13107",
        "label": 0,
        "title": "blitzcrank: fast semantic compression for in-memory online transaction processing yiming qiao",
        "abstract": "we present blitzcrank , a high-speed semantic compressor de- signed for oltp databases. previous solutions are inadequate for compressing row-stores: they suffer from either low compression factor due to a coarse compression granularity or suboptimal per- formance due to the inefficiency in handling dynamic data sets. to solve these problems, we first propose novel semantic models that support fast inferences and dynamic value set for both discrete and continuous data types. we then introduce a new entropy encoding algorithm, called delayed coding, that achieves significant improve- ment in the decoding speed compared to modern arithmetic coding implementations. we evaluate blitzcrank in both standalone mi- crobenchmarks and a multicore in-memory row-store using the tpc-c benchmark. our results show that blitzcrank achieves a sub-microsecond latency for decompressing a random tuple while obtaining high compression factors. this leads to an 85% memory reduction in the tpc-c evaluation with a moderate (19%) through- put degradation. for data sets larger than the available physical memory, blitzcrank help the database sustain a high throughput for more transactions before the i/o overhead dominates. pvldb reference format: yiming qiao, yihan gao, and huanchen zhang. blitzcrank: fast semantic compression for in-memory online transaction processing. pvldb, 17(10): 2528 - 2540, 2024. doi:10.14778/3675034.3675044 pvldb artifact availability: the source code, data, and/or other artifacts have been made available at https://github.com/yimingqiao/blitzcrank. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.14477",
        "label": 1,
        "title": "safesora : towards safety alignment of text2video generation via a human preference dataset josef dai tianle c",
        "abstract": "to mitigate the risk of harmful outputs from large vision models (lvms), we introduce the safesora dataset to promote research on aligning text-to-video generation with human values. this dataset encompasses human preferences in text-to-video generation tasks along two primary dimensions: helpfulness and harmlessness. to capture in-depth human preferences and facilitate structured reasoning by crowdworkers, we subdivide helpfulness into 4 sub-dimensions and harmlessness into 12 sub-categories, serving as the basis for pilot annotations. thesafesora dataset includes 14,711 unique prompts, 57,333 unique videos generated by 4 distinct lvms, and 51,691 pairs of preference annotations labeled by humans. we further demonstrate the utility of the safesora dataset through several applications, including training the text-video moderation model and align- ing lvms with human preference by fine-tuning a prompt augmentation module or the diffusion model. these applications highlight its potential as the foundation for text-to-video alignment research, such as human preference modeling and the development and validation of alignment algorithms. our project is available at https://sites.google.com/view/safe-sora . warning: this paper contains example data that may be offensive or harmful. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.14832",
        "label": 0,
        "title": "autonomous decision making for air taxi networks alex vesel",
        "abstract": "\u2014future urban air mobility systems are expected to be operated by rideshare companies as fleets, which will require fully autonomous air traffic control systems and an order of magnitude increase in airspace capacity. such a system must not only be safe, but also highly responsive to customer demand. this paper proposes the air traffic network problem (atnp), which models the optimization problem of future cooperative air taxi networks. we propose a three-phase decision making model that efficiently assigns vehicles to passengers, determines flight levels to reduce collision risk, and resolves aircraft conflicts by selectively applying monte carlo tree search. we develop a simulator for the atnp and show that our approach has increased safety and reduced passenger waiting time compared to greedy and first-dispatch protocols over potential vertiport layouts across the bay area and new york city. i. i ntroduction as the world sees an increasing trend towards urbanization, drivers in cities are spending more time in traffic, with the average american driver losing 51 hours and $869 annually due to road congestion [1], [2]. the cost is most acute in urban areas, such as chicago, new york, and the bay area, that experience high levels of commuter traffic. one potential solution to this problem is urban air mobility (uam) enabled by the development of electric vertical take-off and landing (evtol) aircraft. based out of hubs called vertiports, these aircraft will travel 10 to 100 miles per flight and collectively service hundreds to thousands of people per hour [3]. to characterize the development and potential future direc- tion of uam systems, nasa developed a framework called the the uam maturity level (uml) scale [4]. these levels 1-6 characterize increasing availability, complexity, and autonomy of potential uam systems. for example, uml 1 describes evtols in the testing phase while uml 6 describes fully autonomous and ubiquitous air travel characterized by ad- hoc landing areas such as driveways and neighborhood streets [4]. this work considers a uml 5 system, which features a network of hundreds of unmanned aerial vehicles (ua vs) operating in a given metro area. however, th"
    },
    {
        "url": "https://arxiv.org/pdf/2406.14848",
        "label": 0,
        "title": "leveraging passage embeddings for efficient listwise reranking with large language models qi liu1",
        "abstract": "recent studies have demonstrated the effective- ness of using large language language mod- els (llms) in passage ranking. the listwise approaches, such as rankgpt, have become new state-of-the-art in this task. however, the efficiency of rankgpt models is limited by the maximum context length and relatively high latency of llm inference. to address these issues, in this paper, we propose pe- rank, leveraging the single passage embed- ding as a good context compression for effi- cient listwise passage reranking. by treating each passage as a special token, we can di- rectly input passage embeddings into llms, thereby reducing input length. additionally, we introduce an inference method that dynam- ically constrains the decoding space to these special tokens, accelerating the decoding pro- cess. for adapting the model to reranking, we employ listwise learning to rank loss for training. evaluation results on multiple bench- marks demonstrate that pe-rank significantly improves efficiency in both prefilling and de- coding, while maintaining competitive rank- ing effectiveness. the code is available at https://github.com/liuqi6777/pe_rank . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.15187",
        "label": 1,
        "title": "uda: a benchmark suite for retrieval augmented generation in real-world document analysis yulong hui",
        "abstract": "the use of retrieval-augmented generation (rag) has improved large language models (llms) in collaborating with external data, yet significant challenges exist in real-world scenarios. in areas such as academic literature and finance question answering, data are often found in raw text and tables in html or pdf formats, which can be lengthy and highly unstructured. in this paper, we introduce a benchmark suite, namely unstructured document analysis (uda), that involves 2,965 real-world documents and 29,590 expert-annotated q&a pairs. we revisit popular llm- and rag-based solutions for document analysis and evaluate the design choices and answer qualities across multiple document domains and diverse query types. our evaluation yields interesting findings and highlights the importance of data parsing and retrieval. we hope our benchmark can shed light and better serve real-world document analysis applications. the benchmark suite and code can be found at https://github.com/qinchuanhui/uda-benchmark . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.14657",
        "title": "OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization Dataset",
        "abstract": "We introduce OpenDebateEvidence, a comprehensive dataset for argument mining and summarization sourced from the American Competitive Debate community. This dataset includes over 3.5 million documents with rich metadata, making it one of the most extensive collections of debate evidence. OpenDebateEvidence captures the complexity of arguments in high school and college debates, providing valuable resources for training and evaluation. Our extensive experiments demonstrate the efficacy of fine-tuning state-of-the-art large language models for argumentative abstractive summarization across various methods, models, and datasets. By providing this comprehensive resource, we aim to advance computational argumentation and support practical applications for debaters, educators, and researchers. OpenDebateEvidence is publicly available to support further research and innovation in computational argumentation. Access it here: https://huggingface.co/datasets/Yusuf5/OpenCaselist.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.15093",
        "title": "ECLIPSE: Expunging Clean-label Indiscriminate Poisons via Sparse Diffusion Purification",
        "abstract": "Clean-label indiscriminate poisoning attacks add invisible perturbations to correctly labeled training images, thus dramatically reducing the generalization capability of the victim models. Recently, defense mechanisms such as adversarial training, image transformation techniques, and image purification have been proposed. However, these schemes are either susceptible to adaptive attacks, built on unrealistic assumptions, or only effective against specific poison types, limiting their universal applicability. In this research, we propose a more universally effective, practical, and robust defense scheme called ECLIPSE. We first investigate the impact of Gaussian noise on the poisons and theoretically prove that any kind of poison will be largely assimilated when imposing sufficient random noise. In light of this, we assume the victim has access to an extremely limited number of clean images (a more practical scene) and subsequently enlarge this sparse set for training a denoising probabilistic model (a universal denoising tool). We then introduce Gaussian noise to absorb the poisons and apply the model for denoising, resulting in a roughly purified dataset. Finally, to address the trade-off of the inconsistency in the assimilation sensitivity of different poisons by Gaussian noise, we propose a lightweight corruption compensation module to effectively eliminate residual poisons, providing a more universal defense approach. Extensive experiments demonstrate that our defense approach outperforms 10 state-of-the-art defenses. We also propose an adaptive attack against ECLIPSE and verify the robustness of our defense scheme. Our code is available at https://github.com/CGCL-codes/ECLIPSE.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.15252",
        "title": "VIDEOSCORE: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation",
        "abstract": "The recent years have witnessed great advances in video generation. However, the development of automatic video metrics is lagging significantly behind. None of the existing metric is able to provide reliable scores over generated videos. The main barrier is the lack of largescale human-annotated dataset. In this paper, we release VIDEOFEEDBACK, the first largescale dataset containing human-provided multiaspect score over 37.6K synthesized videos from 11 existing video generative models. We train VIDEOSCORE (initialized from Mantis) based on VIDEOFEEDBACK to enable automatic video quality assessment. Experiments show that the Spearman correlation between VIDEOSCORE and humans can reach 77.1 on VIDEOFEEDBACK-test, beating the prior best metrics by about 50 points. Further result on other held-out EvalCrafter, GenAI-Bench, and VBench show that VIDEOSCORE has consistently much higher correlation with human judges than other metrics. Due to these results, we believe VIDEOSCORE can serve as a great proxy for human raters to (1) rate different video models to track progress (2) simulate fine-grained human feedback in Reinforcement Learning with Human Feedback (RLHF) to improve current video generation models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.15349",
        "title": "NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking",
        "abstract": "Benchmarking vision-based driving policies is challenging. On one hand, openloop evaluation with real data is easy, but these results do not reflect closedloop performance. On the other, closed-loop evaluation is possible in simulation, but is hard to scale due to its significant computational demands. Further, the simulators available today exhibit a large domain gap to real data. This has resulted in an inability to draw clear conclusions from the rapidly growing body of research on end-to-end autonomous driving. In this paper, we present NAVSIM, a middle ground between these evaluation paradigms, where we use large datasets in combination with a non-reactive simulator to enable large-scale real-world benchmarking. Specifically, we gather simulation-based metrics, such as progress and time to collision, by unrolling bird\u2019s eye view abstractions of the test scenes for a short simulation horizon. Our simulation is non-reactive, i.e., the evaluated policy and environment do not influence each other. As we demonstrate empirically, this decoupling allows open-loop metric computation while being better aligned with closed-loop evaluations than traditional displacement errors. NAVSIM enabled a new competition held at CVPR 2024, where 143 teams submitted 463 entries, resulting in several new insights. On a large set of challenging scenarios, we observe that simple methods with moderate compute requirements such as TransFuser can match recent large-scale end-to-end driving architectures such as UniAD. Our modular framework can potentially be extended with new datasets, data curation strategies, and metrics, and will be continually maintained to host future challenges. Our code is available at https://github.com/autonomousvision/navsim.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.14673",
        "title": "Insights into LLM Long-Context Failures: When Transformers Know but Don\u2019t Tell",
        "abstract": "Large Language Models (LLMs) exhibit positional bias, struggling to utilize information from the middle or end of long contexts. Our study explores LLMs\u2019 long-context reasoning by probing their hidden representations. We find that while LLMs encode the position of target information, they often fail to leverage this in generating accurate responses. This reveals a disconnect between information retrieval and utilization, a \u2018know but don\u2019t tell\u2019 phenomenon. We further analyze the relationship between extraction time and final accuracy, offering insights into the underlying mechanics of transformer models. The code is accessible here: https://github.com/TaiMingLu/knowdont-tell.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.14678",
        "title": "Bidirectional Transformer Representations of (Spanish) Ambiguous Words in Context: A New Lexical Resource and Empirical Analysis",
        "abstract": "Lexical ambiguity\u2014where a single wordform takes on distinct, context-dependent meanings\u2013 serves as a useful tool to compare across different large language models\u2019 (LLMs\u2019) ability to form distinct, contextualized representations of the same stimulus. Few studies have systematically compared LLMs\u2019 contextualized word embeddings for languages beyond English. Here, we evaluate multiple bidirectional transformers\u2019 (BERTs\u2019) semantic representations of Spanish ambiguous nouns in context. We develop a novel dataset of minimal-pair sentences evoking the same or different sense for a target ambiguous noun. In a pre-registered study, we collect contextualized human relatedness judgments for each sentence pair. We find that various BERT-based LLMs\u2019 contextualized semantic representations capture some variance in human judgments but fall short of the human benchmark, and for Spanish\u2013unlike English\u2014model scale is uncorrelated with performance. We also identify stereotyped trajectories of target noun disambiguation as a proportion of traversal through a given LLM family\u2019s architecture, which we partially replicate in English. We contribute (1) a dataset of controlled, Spanish sentence stimuli with human relatedness norms, and (2) to our evolving understanding of the impact that LLM specification (architectures, training protocols) exerts on contextualized embeddings.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.14570",
        "title": "Deep-Learning Approach for Tissue Classification using Acoustic Waves during Ablation with an Er:YAG Laser (Updated)",
        "abstract": "Today\u2019s mechanical tools for bone cutting (osteotomy) cause mechanical trauma that prolongs the healing process. Medical device manufacturers constantly strive to improve their tools to further minimize the trauma. One example of such a new tool and procedure is minimally invasive surgery using a laser as the cutting element. This setup allows tissue to be ablated using laser light instead of mechanical tools, which reduces the healing time after surgery. A reliable feedback system is crucial during laser surgery to avoid collateral damage to surrounding tissues. Therefore, we propose a tissue classification method that analyzes the acoustic waves generated during laser ablation and demonstrates its applicability in an ex-vivo experiment. The ablation process with a microsecond pulsed Er:YAG laser produces acoustic waves, which were acquired with an air-coupled transducer. We then used these acquired waves to classify five porcine tissue types: hard bone, soft bone, muscle, fat, and skin. For automated tissue classification of the measured acoustic waves, we compared five Neural Network (NN) approaches: a one-dimensional Convolutional Neural Network (CNN) with a time-dependent input, a Fully-connected Neural Network (FcNN) with either the frequency spectrum or the principal components of the frequency spectrum as an input, and a combination of a CNN and an FcNN with the time-dependent data and its frequency spectrum as an input. In addition, several consecutive acoustic waves were used to improve the classification task. We used GradCam to find the activation map of the frequencies and concluded that the low-frequencies were the most important ones for this classification task. Our results indicated that the highest accuracy of the classification task (65.5%-75.5%) could be achieved by combining the time-dependent data with its frequency spectrum. In addition, we showed that it was sufficient to use the frequency spectrum as input and that no additional benefit was gained by applying Principal Components Analysis (PCA) to the frequency spectrum.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.14595",
        "title": "Adversaries Can Misuse Combinations of Safe Models",
        "abstract": "Developers try to evaluate whether an AI system can be misused by adversaries before releasing it; for example, they might test whether a model enables cyberoffense, user manipulation, or bioterrorism. In this work, we show that individually testing models for misuse is inadequate; adversaries can misuse combinations of models even when each individual model is safe. The adversary accomplishes this by first decomposing tasks into subtasks, then solving each subtask with the best-suited model. For example, an adversary might solve challenging-but-benign subtasks with an aligned frontier model, and easy-but-malicious subtasks with a weaker misaligned model. We study two decomposition methods: manual decomposition where a human identifies a natural decomposition of a task, and automated decomposition where a weak model generates benign tasks for a frontier model to solve, then uses the solutions in-context to solve the original task. Using these decompositions, we empirically show that adversaries can create vulnerable code, explicit images, python scripts for hacking, and manipulative tweets at much higher rates with combinations of models than either individual model. Our work suggests that even perfectly-aligned frontier systems can enable misuse without ever producing malicious outputs, and that red-teaming efforts should extend beyond single models in isolation.1",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.14760",
        "title": "An LLM Feature-based Framework for Dialogue Constructiveness Assessment",
        "abstract": "Research on dialogue constructiveness assessment focuses on (i) analysing conversational factors that influence individuals to take specific actions, win debates, change their perspectives or broaden their open-mindedness and (ii) predicting constructive outcomes following dialogues for such use cases. These objectives can be achieved by training either interpretable feature-based models (which often involve costly human annotations) or neural models such as pre-trained language models (which have empirically shown higher task accuracy but lack interpretability). We propose a novel LLM feature-based framework that combines the strengths of feature-based and neural approaches while mitigating their downsides, in assessing dialogue constructiveness. The framework first defines a set of dataset-independent and interpretable linguistic features, which can be extracted by both prompting an LLM and simple heuristics. Such features are then used to train LLM featurebased models. We apply this framework to three datasets of dialogue constructiveness and find that our LLM feature-based models significantly outperform standard feature-based models and neural models, and tend to learn more robust prediction rules instead of relying on superficial shortcuts (as seen with neural models). Further, we demonstrate that interpreting these LLM feature-based models can yield valuable insights into what makes a dialogue constructive1 .",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.14670",
        "title": "Exploring Design Choices for Building Language-Specific LLMs",
        "abstract": "Despite rapid progress in large language models (LLMs), their performance on a vast majority of languages remain unsatisfactory. In this paper, we study building language-specific LLMs by adapting monolingual and multilingual LLMs. We conduct systematic experiments on how design choices (base model selection, vocabulary extension, and continued fine-tuning) impact the adapted LLM, both in terms of efficiency (how many tokens are needed to encode the same amount of information) and end task performance. We find that (1) the initial performance before the adaptation is not always indicative of the final performance. (2) Efficiency can easily improved with simple vocabulary extension and continued fine-tuning in most LLMs we study, and (3) The optimal adaptation method is highly language-dependent, and the simplest approach works well across various experimental settings. Adapting English-centric models can yield better results than adapting multilingual models despite their worse initial performance on lowresource languages. Together, our work lays foundations on efficiently building languagespecific LLMs by adapting existing LLMs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.15132",
        "title": "Younger: The First Dataset for Artificial Intelligence-Generated Neural Network Architecture",
        "abstract": "Designing and optimizing neural network architectures typically requires extensive expertise, starting with handcrafted designs and then manual or automated refinement. This dependency presents a significant barrier to rapid innovation. Recognizing the complexity of automatically generating neural network architecture from scratch, we introduce Younger, a pioneering dataset to advance this ambitious goal. Derived from over 174K real-world models across more than 30 tasks from various public model hubs, Younger includes 7,629 unique architectures, and each is represented as a directed acyclic graph with detailed operator-level information. The dataset facilitates two primary design paradigms: global, for creating complete architectures from scratch, and local, for detailed architecture component refinement. By establishing these capabilities, Younger contributes to a new frontier, Artificial Intelligence-Generated Neural Network Architecture (AIGNNA). Our experiments explore the potential and effectiveness of Younger for automated architecture generation and, as a secondary benefit, demonstrate that Younger can serve as a benchmark dataset, advancing the development of graph neural networks. We release the dataset2 and code3 publicly to lower the entry barriers and encourage further research in this challenging area.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.15168",
        "title": "This actually looks like that: Proto-BagNets for local and global interpretability-by-design",
        "abstract": "Interpretability is a key requirement for the use of machine learning models in high-stakes applications, including medical diagno- sis. Explaining black-box models mostly relies on post-hoc methods that do not faithfully reflect the model\u2019s behavior. As a remedy, prototype- based networks have been proposed, but their interpretability is limited as they have been shown to provide coarse, unreliable, and imprecise ex- planations. In this work, we introduce Proto-BagNets6, an interpretable- by-design prototype-based model that combines the advantages of bag- of-local feature models and prototype learning to provide meaningful, coherent, and relevant prototypical parts needed for accurate and in- terpretable image classification tasks. We evaluated the Proto-BagNet for drusen detection on publicly available retinal OCT data. The Proto- BagNet performed comparably to the state-of-the-art interpretable and non-interpretable models while providing faithful, accurate, and clini- cally meaningful local and global explanations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.14709",
        "title": "Factual Dialogue Summarization via Learning from Large Language Models",
        "abstract": "Factual consistency is an important quality in dialogue summarization. Large language model (LLM)-based automatic text summariza- tion models generate more factually consistent summaries compared to those by smaller pre- trained language models, but they face deploy- ment challenges in real-world applications due to privacy or resource constraints. In this paper, we investigate the use of symbolic knowledge distillation to improve the factual consistency of smaller pretrained models for dialogue sum- marization. We employ zero-shot learning to extract symbolic knowledge from LLMs, gen- erating both factually consistent (positive) and inconsistent (negative) summaries. We then ap- ply two contrastive learning objectives on these summaries to enhance smaller summarization models. Experiments with BART, PEGASUS, and Flan-T5 indicate that our approach sur- passes strong baselines that rely on complex data augmentation strategies. Our approach achieves better factual consistency while main- taining coherence, fluency, and relevance, as confirmed by various automatic evaluation met- rics. We also provide access to the data and code to facilitate future research",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.14732",
        "title": "TTQA-RS- A break-down prompting approach for Multi-hop Table-Text Question Answering with Reasoning and Summarization",
        "abstract": "Question answering (QA) over tables and text has gained much popularity over the years. Multi-hop table-text QA requires multiple hops between the table and text, making it a challenging QA task. Although several works have attempted to solve the table-text QA task, most involve training the models and requiring labeled data. In this paper, we have proposed a model - \u201cTTQA-RS: A break-down prompting approach for Multi-hop Table-Text Question Answering with Reasoning and Summarization\u201d1 . Our model uses augmented knowledge including table-text summary with decomposed sub-question with answer for a reasoning-based table-text QA. Using open-source language models our model outperformed all existing prompting methods for table-text QA tasks on existing table-text QA datasets like HybridQA and OTT-QA\u2019s development set. Our results are comparable with the training-based stateof-the-art models, demonstrating the potential of prompt-based approaches using open-source LLMs. Additionally, by using GPT-4 with LLaMA3-70B, our model achieved state-of-theart performance for prompting-based methods on multi-hop table-text QA.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.14774",
        "title": "Evaluating Numerical Reasoning in Text-to-Image Models",
        "abstract": "Text-to-image generative models are capable of producing high-quality images that often faithfully depict concepts described using natural language. In this work, we comprehensively evaluate a range of text-to-image models on numerical reasoning tasks of varying difficulty, and show that even the most advanced models have only rudimentary numerical skills. Specifically, their ability to correctly generate an exact number of objects in an image is limited to small numbers, it is highly dependent on the context the number term appears in, and it deteriorates quickly with each successive number. We also demonstrate that models have poor understanding of linguistic quantifiers (such as \u201ca few\u201d or \u201cas many as\u201d), the concept of zero, and struggle with more advanced concepts such as partial quantities and fractional representations. We bundle prompts, generated images and human annotations into GECKONUM, a novel benchmark for evaluation of numerical reasoning.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.14952",
        "title": "ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models",
        "abstract": "Emotion Support Conversation (ESC) is a cru- cial application, which aims to reduce human stress, offer emotional guidance, and ultimately enhance human mental and physical well-being. With the advancement of Large Language Mod- els (LLMs), many researchers have employed LLMs as the ESC models. However, the evalu- ation of these LLM-based ESCs remains uncer- tain. Inspired by the awesome development of role-playing agents, we propose an ESC Evaluation framework (i.e., ESC-Eval), which uses a role-playing agent to interact with ESC models, followed by a manual evaluation of the interactive dialogues. In detail, we first re- organize 2,801 role-playing cards from seven existing datasets to define the roles of the role- playing agent. Second, we train a specific role- playing model - ESC-Role to mimic the be- havior of a real person experiencing distress. Third, through ESC-Role and organized role cards, we systematically conduct experiments using 14 LLMs as the ESC models, includ- ing general AI-assistant LLMs (e.g., ChatGPT) and ESC-oriented LLMs (e.g., ExTES-Llama). We conduct comprehensive human annotations on interactive multi-turn dialogues of differ- ent ESC models. The results show that ESC- oriented LLMs exhibit superior ESC abilities compared to general AI-assistant LLMs, but there is still a gap behind human performance. Moreover, to automate the evaluation of fu- ture ESC models, we developed ESC-RANK, which trained on the annotated data, achiev- ing a scoring performance surpassing 35 points of GPT-4. Our data and code are available at https://github.com/haidequanbu/ESC-Eval",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.15019",
        "title": "MedOdyssey: A Medical Domain Benchmark for Long Context Evaluation Up to 200K Tokens",
        "abstract": "Numerous advanced Large Language Models (LLMs) now support context lengths up to 128K, and some extend to 200K. Some benchmarks in the generic domain have also followed up on evaluating long-context capabilities. In the medical domain, tasks are distinctive due to the unique contexts and need for domain expertise, necessitating further evaluation. However, despite the frequent presence of long texts in medical scenarios, evaluation benchmarks of long-context capabilities for LLMs in this field are still rare. In this paper, we propose MedOdyssey, the first medical long-context benchmark with seven length levels ranging from 4K to 200K tokens. MedOdyssey consists of two primary components: the medical-context \u201cneedles in a haystack\u201d task and a series of tasks specific to medical applications, together comprising 10 datasets. The first component includes challenges such as counter-intuitive reasoning and novel (unknown) facts injection to mitigate knowledge leakage and data contamination of LLMs. The second component confronts the challenge of requiring professional medical expertise. Especially, we design the \u201cMaximum Identical Context\u201d principle to improve fairness by guaranteeing that different LLMs observe as many identical contexts as possible. Our experiment evaluates advanced proprietary and open-source LLMs tailored for processing long contexts and presents detailed performance analyses. This highlights that LLMs still face challenges and need for further research in this area. Our code and data are released in the repository: https://github.com/JOHNNY-fans/MedOdyssey.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.15130",
        "title": "Assessing Good, Bad and Ugly Arguments Generated by ChatGPT: a New Dataset, its Methodology and Associated Tasks",
        "abstract": "The recent success of Large Language Models (LLMs) has sparked concerns about their potential to spread misinformation. As a result, there is a pressing need for tools to identify \u201cfake arguments\u201d generated by such models. To create these tools, examples of texts generated by LLMs are needed. This paper introduces a methodology to obtain good, bad and ugly arguments from argumentative essays produced by ChatGPT, OpenAI\u2019s LLM. We then describe a novel dataset containing a set of diverse arguments, ArGPT. We assess the effectiveness of our dataset and establish baselines for several argumentation-related tasks. Finally, we show that the artificially generated data relates well to human argumentation and thus is useful as a tool to train and test systems for the defined tasks.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.15267",
        "title": "Evaluating Diversity in Automatic Poetry Generation",
        "abstract": "Natural Language Generation (NLG), and more generally generative AI, are among the currently most impactful research fields. Creative NLG, such as automatic poetry generation, is a fascinating niche in this area. While most previous research has focused on forms of the Turing test when evaluating automatic poetry generation \u2014 can humans distinguish between automatic and human generated poetry \u2014 we evaluate the diversity of automatically generated poetry, by comparing distributions of generated poetry to distributions of human poetry along structural, lexical, semantic and stylistic dimensions, assessing different model types (word vs. character-level, general purpose LLMs vs. poetry-specific models), including the very recent LLaMA3, and types of fine-tuning (conditioned vs. unconditioned). We find that current automatic poetry systems are considerably underdiverse along multiple dimensions \u2014 they often do not rhyme sufficiently, are semantically too uniform and even do not match the length distribution of human poetry. Our experiments reveal, however, that style-conditioning and character-level modeling clearly increases diversity across virtually all dimensions we explore. Our identified limitations may serve as the basis for more genuinely diverse future poetry generation models.1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.15279",
        "title": "Cross-Modality Safety Alignment",
        "abstract": "As Artificial General Intelligence (AGI) becomes increasingly integrated into various facets of human life, ensuring the safety and ethical alignment of such systems is paramount. Previous studies primarily focus on single-modality threats, which may not suffice given the integrated and complex nature of cross-modality interactions. We introduce a novel safety alignment challenge called Safe Inputs but Unsafe Output (SIUO) to evaluate cross-modality safety alignment. Specifically, it considers cases where single modalities are safe independently but could potentially lead to unsafe or unethical outputs when combined. To empirically investigate this problem, we developed the SIUO, a cross-modality benchmark encompassing 9 critical safety domains, such as self-harm, illegal activities, and privacy violations. Our findings reveal substantial safety vulnerabilities in both closed- and opensource LVLMs, such as GPT-4V and LLaVA, underscoring the inadequacy of current models to reliably interpret and respond to complex, real-world scenarios.1 Warning: this paper contains example data that may be offensive or harmful.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.15319",
        "title": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs",
        "abstract": "In traditional RAG framework, the basic retrieval units are normally short. The common retrievers like DPR normally work with 100-word Wikipedia paragraphs. Such a design forces the retriever to search over a large corpus to find the \u2018needle\u2019 unit. In contrast, the readers only need to extract answers from the short retrieved units. Such an imbalanced \u2018heavy\u2019 retriever and \u2018light\u2019 reader design can lead to sub-optimal performance. In order to alleviate the imbalance, we propose a new framework LongRAG, consisting of a \u2018long retriever\u2019 and a \u2018long reader\u2019. LongRAG processes the entire Wikipedia into 4K-token units, which is 30x longer than before. By increasing the unit size, we significantly reduce the total units from 22M to 600K. This significantly lowers the burden of retriever, which leads to a remarkable retrieval score: answer recall@1=71% on NQ (previously 52%) and answer recall@2=72% (previously 47%) on HotpotQA (full-wiki). Then we feed the top-k retrieved units (\u2248 30K tokens) to an existing long-context LLM to perform zero-shot answer extraction. Without requiring any training, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA (full-wiki), which is on par with the (fully-trained) SoTA model. Our study offers insights into the future roadmap for combining RAG with long-context LLMs.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.15341",
        "title": "GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene Expression Data in Alignment with Bioinformaticians",
        "abstract": "Recent advancements in machine learning have significantly improved the identification of diseaseassociated genes from gene expression datasets. However, these processes often require extensive expertise and manual effort, limiting their scalability. Large Language Model (LLM)-based agents have shown promise in automating these tasks due to their increasing problem-solving abilities. To support the evaluation and development of such methods, we introduce GenoTEX, a benchmark dataset for the automatic exploration of gene expression data, involving the tasks of dataset selection, preprocessing, and statistical analysis. GenoTEX provides annotated code and results for solving a wide range of gene identification problems, in a full analysis pipeline that follows the standard of computational genomics. These annotations are curated by human bioinformaticians who carefully analyze the datasets to ensure accuracy and reliability. To provide baselines for these tasks, we present GenoAgents, a team of LLM-based agents designed with context-aware planning, iterative correction, and domain expert consultation to collaboratively explore gene datasets. Our experiments with GenoAgents demonstrate the potential of LLM-based approaches in genomics data analysis, while error analysis highlights the challenges and areas for future improvement. We propose GenoTEX as a promising resource for benchmarking and enhancing AI-driven methods for genomics data analysis. We make our benchmark publicly available at https://github.com/Liu-Hy/GenoTex.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.15657",
        "title": "FIRST: Faster Improved Listwise Reranking with Single Token Decoding",
        "abstract": "Large Language Models (LLMs) have significantly advanced the field of information retrieval, particularly for reranking. Listwise LLM rerankers have showcased superior performance and generalizability compared to existing supervised approaches. However, conventional listwise LLM reranking methods lack efficiency as they provide ranking output in the form of a generated ordered sequence of candidate passage identifiers. Further, they are trained with the typical language modeling objective, which treats all ranking errors uniformly\u2013potentially at the cost of misranking highly relevant passages. Addressing these limitations, we introduce FIRST1 , a novel listwise LLM reranking approach leveraging the output logits of the first generated identifier to directly obtain a ranked ordering of the candidates. Further, we incorporate a learning-to-rank loss during training, prioritizing ranking accuracy for the more relevant passages. Empirical results demonstrate that FIRST accelerates inference by 50% while maintaining a robust ranking performance with gains across the BEIR benchmark. Finally, to illustrate the practical effectiveness of listwise LLM rerankers, we investigate their application in providing relevance feedback for retrievers during inference. Our results show that LLM rerankers can provide a stronger distillation signal compared to crossencoders, yielding substantial improvements in retriever recall after relevance feedback.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.16048",
        "title": "Evaluating D-MERIT of Partial-annotation on Information Retrieval",
        "abstract": "Retrieval models are often evaluated on partially-annotated datasets. Each query is mapped to a few relevant texts and the remain- ing corpus is assumed to be irrelevant. As a result, models that successfully retrieve false negatives are punished in evaluation. Unfortu- nately, completely annotating all texts for every query is not resource efficient. In this work, we show that using partially-annotated datasets in evaluation can paint a distorted picture. We curate D-MERIT, a passage retrieval evalua- tion set from Wikipedia, aspiring to contain all relevant passages for each query. Queries describe a group (e.g., \u201cjournals about linguis- tics\u201d) and relevant passages are evidence that entities belong to the group (e.g., a passage indicating that Language is a journal about lin- guistics). We show that evaluating on a dataset containing annotations for only a subset of the relevant passages might result in misleading ranking of the retrieval systems and that as more relevant texts are included in the eval- uation set, the rankings converge. We propose our dataset as a resource for evaluation and our study as a recommendation for balance be- tween resource-efficiency and reliable evalua- tion when annotating evaluation sets for text retrieval. Our dataset can be downloaded from https://D-MERIT.github.io",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.16383",
        "title": "Context-augmented Retrieval: A Novel Framework for Fast Information Retrieval based Response Generation using Large Language Model",
        "abstract": "Generating high-quality answers consistently by providing con- textual information embedded in the prompt passed to the Large Language Model (LLM) is dependent on the quality of information retrieval. As the corpus of contextual information grows, the an- swer/inference quality of Retrieval Augmented Generation (RAG) based Question Answering (QA) systems declines. This work solves this problem by combining classical text classification with the Large Language Model (LLM) to enable quick information retrieval from the vector store and ensure the relevancy of retrieved infor- mation. For the same, this work proposes a new approach Context Augmented retrieval (CAR), where partitioning of vector database by real-time classification of information flowing into the corpus is done. CAR demonstrates good quality answer generation along with significant reduction in information retrieval and answer gen- eration time.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.16828",
        "title": "Ragnar\u00f6k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track",
        "abstract": "Did you try out the new Bing Search? Or maybe you fiddled around with Google AI Overviews? These might sound familiar because the modern-day search stack has recently evolved to include retrieval- augmented generation (RAG) systems. They allow searching and incorporating real-time data into large language models (LLMs) to provide a well-informed, attributed, concise summary in contrast to the traditional search paradigm that relies on displaying a ranked list of documents. Therefore, given these recent advancements, it is crucial to have an arena to build, test, visualize, and systematically evaluate RAG-based search systems. With this in mind, we propose the TREC 2024 RAG Track to foster innovation in evaluating RAG systems. In our work, we lay out the steps we\u2019ve made towards making this track a reality \u2014 we describe the details of our reusable framework, Ragnar\u00f6k, explain the curation of the new MS MARCO V2.1 collection choice, release the development topics for the track, and standardize the I/O definitions which assist the end user. Next, using Ragnar\u00f6k, we identify and provide key industrial baselines such as OpenAI\u2019s GPT-4o or Cohere\u2019s Command R+. Further, we introduce a web-based user interface for an interactive arena allow- ing benchmarking pairwise RAG systems by crowdsourcing. We open-source our Ragnar\u00f6k framework and baselines to achieve a unified standard for future RAG systems.\nhttps://github.com/castorini/ragnarok",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2403.09167",
        "title": "Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse",
        "abstract": "The efficacy of large language models (LLMs) is heavily dependent on the quality of the under- lying data, particularly within specialized do- mains. A common challenge when fine-tuning LLMs for domain-specific applications is the potential degradation of the model\u2019s generaliza- tion capabilities. To address these issues, we propose a two-stage approach for the construc- tion of production prompts designed to yield high-quality data. This method involves the generation of a diverse array of prompts that en- compass a broad spectrum of tasks and exhibit a rich variety of expressions. Furthermore, we introduce a cost-effective, multi-dimensional quality assessment framework to ensure the in- tegrity of the generated labeling data. Utilizing a dataset comprised of service provider and cus- tomer interactions from the real estate sector, we demonstrate a positive correlation between data quality and model performance. Notably, our findings indicate that the domain-specific proficiency of general LLMs can be enhanced through fine-tuning with data produced via our proposed method, without compromising their overall generalization abilities, even when ex- clusively domain-specific data is employed for fine-tuning.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2401.02777",
        "title": "From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models",
        "abstract": "This paper introduces RAISE (Reason- ing and Acting through Scratchpad and Examples), an advanced architecture en- hancing the integration of Large Language Models (LLMs) like GPT-4 into conver- sational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirror- ing human short-term and long-term mem- ory, to maintain context and continuity in conversations. It entails a comprehen- sive agent construction scenario, includ- ing phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adapt- ability in complex, multi-turn dialogues. Our preliminary evaluations in a real es- tate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader appli- cations. This work contributes to the AI field by providing a robust framework for developing more context-aware and versa- tile conversational agents.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2405.10739",
        "title": "Efficient Multimodal Large Language Models: A Survey",
        "abstract": "In the past year, Multimodal Large Language Models (MLLMs) have demon- strated remarkable performance in tasks such as visual question answering, vi- sual understanding and reasoning. However, the extensive model size and high training and inference costs have hindered the widespread application of MLLMs in academia and industry. Thus, studying efficient and lightweight MLLMs has enormous potential, especially in edge computing scenarios. In this survey, we provide a comprehensive and systematic review of the current state of effi- cient MLLMs. Specifically, we summarize the timeline of representative effi- cient MLLMs, research state of efficient structures and strategies, and the appli- cations. Finally, we discuss the limitations of current efficient MLLM research and promising future directions. Please refer to our GitHub repository for more details: https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2301.07573",
        "title": "Synthcity: facilitating innovative use cases of synthetic data in different data modalities",
        "abstract": "Synthcity is an open-source software package for innovative use cases of synthetic data in ML fairness, privacy and augmentation across diverse tabular data modal- ities, including static data, regular and irregular time series, data with censoring, multi-source data, composite data, and more. Synthcity provides the practitioners with a single access point to cutting edge research and tools in synthetic data. It also offers the community a playground for rapid experimentation and prototyping, a one-stop-shop for SOTA benchmarks, and an opportunity for extending research impact. The library can be accessed on GitHub and pip. We warmly invite the community to join the development effort by providing feedback, reporting bugs, and contributing code.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1802.02611v3",
        "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation",
        "abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at mul- tiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further ex- plore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the ef- fectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0% and 82.1% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https: //github.com/tensorflow/models/tree/master/research/deeplab.\nKeywords: Semanticimagesegmentation,spatialpyramidpooling,encoder- decoder, and depthwise separable convolution.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2305.17349v2",
        "title": "Condition-Invariant Semantic Segmentation",
        "abstract": "Adaptation of semantic segmentation networks to different visual conditions is vital for robust perception in autonomous cars and robots. However, previous work has shown that most feature-level adaptation methods, which employ adversarial training and are validated on synthetic-to-real adaptation, provide marginal gains in condition-level adaptation, being outperformed by simple pixel-level adaptation via stylization. Motivated by these findings, we propose to leverage stylization in performing feature-level adaptation by aligning the internal network features extracted by the encoder of the network from the original and the stylized view of each input image with a novel feature invariance loss. In this way, we encourage the encoder to extract features that are already invariant to the style of the input, allowing the decoder to focus on parsing these features and not on further abstracting from the specific style of the input. We implement our method, named Condition-Invariant Semantic Segmentation (CISS), on the current state-of-the-art domain adaptation architecture and achieve outstanding results on condition-level adaptation. In particular, CISS sets the new state of the art in the popular daytime-to-nighttime Cityscapes\u2192Dark Zurich benchmark. Furthermore, our method achieves the second-best performance on the normal-to-adverse Cityscapes\u2192ACDC benchmark. CISS is shown to generalize well to domains unseen during training, such as BDD100K-night. Code is publicly available at https://github.com/SysCV/CISS.\nIndex Terms\u2014Semantic segmentation, domain adaptation, adverse conditions, invariance, unsupervised learning. \u2726",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2306.09203",
        "title": "Transferring Knowledge for Food Image Segmentation using Transformers and Convolutions",
        "abstract": "Food image segmentation is an important task that has ubiquitous applications, such as estimating the nutritional value of a plate of food. Although machine learning mod- els have been used for segmentation in this domain, food images pose several challenges. One challenge is that food items can overlap and mix, making them difficult to distin- guish. Another challenge is the degree of inter-class sim- ilarity and intra-class variability, which is caused by the varying preparation methods and dishes a food item may be served in. Additionally, class imbalance is an inevitable issue in food datasets. To address these issues, two models are trained and compared, one based on convolutional neu- ral networks and the other on Bidirectional Encoder repre- sentation for Image Transformers (BEiT). The models are trained and valuated using the FoodSeg103 dataset, which is identified as a robust benchmark for food image segmen- tation. The BEiT model outperforms the previous state-of- the-art model by achieving a mean intersection over union of 49.4 on FoodSeg103. This study provides insights into transfering knowledge using convolution and Transformer- based approaches in the food image domain.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2211.15479",
        "title": "Object Detection in Aerial Imagery",
        "abstract": "Object detection in natural images has achieved remark- able results over the years. However, a similar progress has not yet been observed in aerial object detection due to several challenges, such as high resolution images, in- stances scale variation, class imbalance etc. We show the performance of two-stage, one-stage and attention based object detectors on the iSAID dataset. Furthermore, we de- scribe some modifications and analysis performed for dif- ferent models -\nin two stage detector: introduced weighted attention based FPN, class balanced sampler and density prediction head. in one stage detector: used weighted focal loss and intro- duced FPN\nin attention based detector: compare single,multi-scale at- tention and demonstrate effect of different backbones 1. Finally, we show a comparative study highlighting the pros and cons of different models in aerial imagery setting.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1811.07859",
        "title": "ORTHOSEG: A DEEP MULTIMODAL CONVOLUTIONAL NEURAL NETWORK\nARCHITECTURE FOR SEMANTIC SEGMENTATION OF ORTHOIMAGERY",
        "abstract": "This paper addresses the task of semantic segmentation of orthoimagery using multimodal data e.g. optical RGB, infrared and digital\nsurface model. We propose a deep convolutional neural network architecture termed OrthoSeg for semantic segmentation using\nmultimodal, orthorectified and coregistered data. We also propose a training procedure for supervised training of OrthoSeg. The\ntraining procedure complements the inherent architectural characteristics of OrthoSeg for preventing complex co-adaptations of\nlearned features, which may arise due to probable high dimensionality and spatial correlation in multimodal and/or multispectral\ncoregistered data. OrthoSeg consists of parallel encoding networks for independent encoding of multimodal feature maps and a\ndecoder designed for efficiently fusing independently encoded multimodal feature maps. A softmax layer at the end of the network\nuses the features generated by the decoder for pixel-wise classification. The decoder fuses feature maps from the parallel encoders\nlocally as well as contextually at multiple scales to generate per-pixel feature maps for final pixel-wise classification resulting in\nsegmented output. We experimentally show the merits of OrthoSeg by demonstrating state-of-the-art accuracy on the ISPRS\nPotsdam 2D Semantic Segmentation dataset. Adaptability is one of the key motivations behind OrthoSeg so that it serves as a useful\narchitectural option for a wide range of problems involving the task of semantic segmentation of coregistered multimodal and/or\nmultispectral imagery. Hence, OrthoSeg is designed to enable independent scaling of parallel encoder networks and decoder network\nto better match application requirements, such as the number of input channels, the effective field-of-view, and model capacity.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2303.07747",
        "title": "LOG-CAN: LOCAL-GLOBAL CLASS-AWARE NETWORK FOR SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGES",
        "abstract": "Remote sensing images are known of having complex back- grounds, high intra-class variance and large variation of scales, which bring challenge to semantic segmentation. We present LoG-CAN, a multi-scale semantic segmenta- tion network with a global class-aware (GCA) module and local class-aware (LCA) modules to remote sensing images. Specifically, the GCA module captures the global represen- tations of class-wise context modeling to circumvent back- ground interference; the LCA modules generate local class representations as intermediate aware elements, indirectly associating pixels with global class representations to reduce variance within a class; and a multi-scale architecture with GCA and LCA modules yields effective segmentation of ob- jects at different scales via cascaded refinement and fusion of features. Through the evaluation on the ISPRS Vaihingen dataset and the ISPRS Potsdam dataset, experimental results indicate that LoG-CAN outperforms the state-of-the-art meth- ods for general semantic segmentation, while significantly reducing network parameters and computation. Code is available at https://github.com/xwmaxwma/rssegmentation.\nIndex Terms\u2014 Semantic segmentation, remote sensing, class representations",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2405.07916",
        "title": "IMAFD: An Interpretable Multi-stage Approach to Flood Detection from time series Multispectral Data",
        "abstract": "In this paper, we address two critical challenges in the domain of flood detection: the computational expense of large- scale time series change detection and the lack of interpretable decision-making processes on explainable AI (XAI). To overcome these challenges, we proposed an interpretable multi-stage approach to flood detection, IMAFD has been proposed. It provides an automatic, efficient and interpretable solution suitable for large-scale remote sensing tasks and offers insight into the decision-making process. The proposed IMAFD approach combines the analysis of the dynamic time series image sequences to identify images with possible flooding with the static, within-image semantic segmentation. It combines anomaly detection (at both image and pixel level) with semantic segmentation. The flood detection problem is addressed through four stages: (1) at a sequence level: identifying the suspected images (2) at a multi-image level: detecting change within suspected images (3) at an image level: semantic segmentation of images into Land, Water or Cloud class (4) decision making. Our contributions are two folder. First, we efficiently reduced the number of frames to be processed for dense change detection by providing a multi-stage holistic approach to flood detection. Second, the proposed semantic change detection method (stage 3) provides human users with an interpretable decision-making process, while most of the explainable AI (XAI) methods provide post hoc explanations. The evaluation of the proposed IMAFD framework was performed on three datasets, WorldFloods, RavAEn and MediaEval. For all the above datasets, the proposed framework demonstrates a competitive performance compared to other methods offering also interpretability and insight.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2011.00993",
        "title": "Real-time Semantic Segmentation with Context Aggregation Network",
        "abstract": "With the increasing demand of autonomous systems, pixelwise semantic seg- mentation for visual scene understanding needs to be not only accurate but also efficient for potential real-time applications. In this paper, we propose Context Aggregation Network, a dual branch convolutional neural network, with sig- nificantly lower computational costs as compared to the state-of-the-art, while maintaining a competitive prediction accuracy. Building upon the existing dual branch architectures for high-speed semantic segmentation, we design a cheap high resolution branch for effective spatial detailing and a context branch with light-weight versions of global aggregation and local distribution blocks, potent to capture both long-range and local contextual dependencies required for ac- curate semantic segmentation, with low computational overheads. We evaluate our method on two semantic segmentation datasets, namely Cityscapes dataset and UAVid dataset. For Cityscapes test set, our model achieves state-of-the-art results with mIOU of 75.9%, at 76 FPS on an NVIDIA RTX 2080Ti and 8 FPS on a Jetson Xavier NX. With regards to UAVid dataset, our proposed network achieves mIOU score of 63.5% with high execution speed (15 FPS).\nKeywords: Semantic segmentation, Real-time, convolutional neural network, context aggregation network",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2105.08788",
        "title": "Self-Supervised Learning for Fine-Grained Visual Categorization",
        "abstract": "Recent research in self-supervised learning (SSL) has shown its capability in learn- ing useful semantic representations from images for classification tasks. Through our work, we study the usefulness of SSL for Fine-Grained Visual Categoriza- tion (FGVC). FGVC aims to distinguish objects of visually similar sub categories within a general category. The small inter-class, but large intra-class variations within the dataset makes it a challenging task. The limited availability of annotated labels for such a fine-grained data encourages the need for SSL, where additional supervision can boost learning without the cost of extra annotations. Our baseline achieves 86.36% top-1 classification accuracy on CUB-200-2011 dataset by utiliz- ing random crop augmentation during training and center crop augmentation during testing. In this work, we explore the usefulness of various pretext tasks, specifically, rotation, pretext invariant representation learning (PIRL), and deconstruction and construction learning (DCL) for FGVC. Rotation as an auxiliary task promotes the model to learn global features, and diverts it from focusing on the subtle details. PIRL that uses jigsaw patches attempts to focus on discriminative local regions, but struggles to accurately localize them. DCL helps in learning local discriminating features and outperforms the baseline by achieving 87.41% top-1 accuracy. The deconstruction learning forces the model to focus on local object parts, while reconstruction learning helps in learning the correlation between the parts. We perform extensive experiments to reason our findings. Our code is available on GitHub1 .",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2011.02193",
        "title": "Weed Density and Distribution Estimation for Precision Agriculture using Semi-Supervised Learning",
        "abstract": "Uncontrolled growth of weeds can severely affect the crop yield and quality. Unrestricted use of herbicide for weed removal alters biodiversity and cause environmental pollution. Instead, identifying weed-infested regions can aid selective chemical treatment of these regions. Advances in analyzing farm images have resulted in solutions to identify weed plants. However, a majority of these approaches are based on supervised learning methods which requires huge amount of manually annotated images. As a result, these supervised approaches are economically infeasible for the individual farmer because of the wide variety of plant species being cultivated. In this paper, we propose a deep learning-based semi-supervised approach for robust estimation of weed density and distribution across farmlands using only limited color images acquired from autonomous robots. This weed density and distribution can be useful in a site-specific weed management system for selective treatment of infected areas using autonomous robots. In this work, the foreground vegetation pixels containing crops and weeds are first identified using a Convolutional Neural Network (CNN) based unsupervised segmentation. Subsequently, the weed infected regions are identified using a fine-tuned CNN, eliminating the need for designing hand-crafted features. The approach is validated on two datasets of different crop/weed species (1) Crop Weed Field Image Dataset (CWFID), which consists of carrot plant images and the (2) Sugar Beets dataset. The proposed method is able to localize weed-infested regions a maximum recall of 0.99 and estimate weed density with a maximum accuracy of 82.13%. Hence, the proposed approach is shown to generalize to different plant species without the need for extensive labeled data.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2306.04955",
        "title": "Degraded Polygons Raise Fundamental Questions of Neural Network Perception",
        "abstract": "It is well-known that modern computer vision systems often exhibit behaviors mis- aligned with those of humans: from adversarial attacks to image corruptions, deep learning vision models suffer in a variety of settings that humans capably handle. In light of these phenomena, here we introduce another, orthogonal perspective study- ing the human-machine vision gap. We revisit the task of recovering images under degradation, first introduced over 30 years ago in the Recognition-by-Components theory of human vision. Specifically, we study the performance and behavior of neural networks on the seemingly simple task of classifying regular polygons at varying orders of degradation along their perimeters. To this end, we implement the Automated Shape Recoverability Test1 for rapidly generating large-scale datasets of perimeter-degraded regular polygons, modernizing the historically manual cre- ation of image recoverability experiments. We then investigate the capacity of neural networks to recognize and recover such degraded shapes when initialized with different priors. Ultimately, we find that neural networks\u2019 behavior on this simple task conflicts with human behavior, raising a fundamental question of the robustness and learning capabilities of modern computer vision models.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2303.13874",
        "title": "Recently, video moment retrieval and highlight detec- tion (MR/HD) are being spotlighted as the demand for video understanding is drastically increased. The key objective of MR/HD is to localize the moment and estimate clip-wise accordance level, i.e., saliency score, to the given text query. Although the recent transformer-based models brought some advances, we found that these methods do not fully exploit the information of a given query. For example, the relevance between text query and video contents is sometimes neglected when predicting the moment and its saliency. To tackle this issue, we introduce Query-Dependent DETR (QD-DETR), a detection transformer tailored for MR/HD. As we observe the insignificant role of a given query in transformer architec- tures, our encoding module starts with cross-attention layers to explicitly inject the context of text query into video repre- sentation. Then, to enhance the model\u2019s capability of exploit- ing the query information, we manipulate the video-query pairs to produce irrelevant pairs. Such negative (irrelevant) video-query pairs are trained to yield low saliency scores, which in turn, encourages the model to estimate precise ac- cordance between query-video pairs. Lastly, we present an input-adaptive saliency predictor which adaptively defines the criterion of saliency scores for the given video-query pairs. Our extensive studies verify the importance of build- ing the query-dependent representation for MR/HD. Specif- ically, QD-DETR outperforms state-of-the-art methods on QVHighlights, TVSum, and Charades-STA datasets. Codes are available at github.com/wjun0830/QD-DETR.",
        "abstract": "Query-Dependent Video Representation for Moment Retrieval and Highlight Detection",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2307.08145",
        "title": "Self-Attention Based Generative Adversarial Networks For Unsupervised Video Summarization",
        "abstract": "In this paper, we study the problem of producing a comprehensive video summary following an unsupervised approach that relies on adversarial learning. We build on a popular method where a Generative Adversarial Network (GAN) is trained to create representative summaries, indistinguishable from the originals. The introduction of the attention mechanism into the architecture for the selection, encoding and decoding of video frames, shows the efficacy of self-attention and transformer in modeling temporal relationships for video summarization. We propose the SUM-GAN-AED model that uses a self-attention mechanism for frame selection, combined with LSTMs for en- coding and decoding. We evaluate the performance of the SUM- GAN-AED model on the SumMe, TVSum and COGNIMUSE datasets. Experimental results indicate that using a self-attention mechanism as the frame selection mechanism outperforms the state-of-the-art on SumMe and leads to comparable to state-of- the-art performance on TVSum and COGNIMUSE.\nIndex Terms\u2014Unsupervised Video Summarization, Generative Adversarial Networks, key-frame extraction, Long Short-Term Memory, Deep Neural Networks",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2403.03169",
        "title": "Dynamical decoding of the competition between charge density waves in a kagome superconductor",
        "abstract": "The kagome superconductor CsV3Sb5 hosts a variety of charge density wave (CDW) phases, which play a fundamental role in the formation of other exotic electronic instabilities. However, identifying the precise structure of these CDW phases and their intricate relationships remain the subject of intense debate, due to the lack of static probes that can distinguish the CDW phases with identical spatial periodicity. Here, we unveil the competition between two coexisting 2 \u00d7 2 \u00d7 2 CDWs in CsV3Sb5 harnessing time-resolved X-ray diffraction. By analyzing the light-induced changes in the intensity of CDW superlattice peaks, we demonstrate the presence of both phases, each displaying a significantly different amount of melting upon excitation. The anomalous light-induced sharpening of peak width further shows that the phase that is more resistant to photo-excitation exhibits an increase in domain size at the expense of the other, thereby showcasing a hallmark of phase competition. Our results not only shed light on the interplay between the multiple CDW phases in CsV3Sb5, but also establish a non-equilibrium framework for comprehending complex phase relationships that are challenging to disentangle using static techniques.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.09770",
        "title": "How to Data in Datathons",
        "abstract": "The rise of datathons, also known as data or data science hackathons, has provided a platform to collaborate, learn, and innovate in a short timeframe. Despite their significant potential benefits, organizations often struggle to effectively work with data due to a lack of clear guidelines and best practices for potential issues that might arise. Drawing on our own experiences and insights from organizing \u2265 80 datathon challenges with \u2265 60 partnership organizations since 2016, we provide guidelines and recommendations that serve as a resource for organizers to navigate the data-related complexities of datathons. We apply our proposed framework to 10 case studies.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2111.10533",
        "title": "Temporal-MPI: Enabling Multi-Plane Images for\nDynamic Scene Modelling via Temporal Basis\nLearning",
        "abstract": "Novel view synthesis of static scenes has achieved remarkable advancements in producing photo-realistic results. However, key challenges remain for immersive rendering of dy- namic scenes. One of the seminal image-based rendering method, the multi-plane image (MPI), produces high novel-view synthesis quality for static scenes. But modelling dynamic contents by MPI is not studied. In this paper, we propose a novel Temporal-MPI representation which is able to encode the rich 3D and dynamic variation information throughout the entire video as compact temporal basis and coefficients jointly learned. Time-instance MPI for rendering can be generated efficiently using mini-seconds by linear combinations of temporal basis and coefficients from Temporal-MPI. Thus novel-views at arbitrary time-instance will be able to be rendered via Temporal-MPI in real-time with high visual quality. Our method is trained and evaluated on Nvidia Dynamic Scene Dataset. We show that our proposed Temporal- MPI is much faster and more compact compared with other state-of-the-art dynamic scene modelling methods.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2301.04644",
        "title": "DOES PROGRESS ON IMAGENET TRANSFER TO REAL-WORLD DATASETS?",
        "abstract": "Does progress on ImageNet transfer to real-world datasets? We investigate this question by evaluating ImageNet pre-trained models with varying accuracy (57% - 83%) on six practical image classification datasets. In particular, we study datasets collected with the goal of solving real-world tasks (e.g., classifying images from camera traps or satellites), as opposed to web-scraped benchmarks collected for comparing models. On multiple datasets, models with higher ImageNet accuracy do not consistently yield performance improvements. For certain tasks, interven- tions such as data augmentation improve performance even when architectures do not. We hope that future benchmarks will include more diverse datasets to encourage a more comprehensive approach to improving learning algorithms.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1905.02244",
        "title": "Searching for MobileNetV3",
        "abstract": "We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware- aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and net- work design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for re- lease: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of ob- ject detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detec- tion and segmentation. MobileNetV3-Large is 3.2% more accurate on ImageNet classification while reducing latency by 20% compared to MobileNetV2. MobileNetV3-Small is 6.6% more accurate compared to a MobileNetV2 model with comparable latency. MobileNetV3-Large detection is over 25% faster at roughly the same accuracy as Mo- bileNetV2 on COCO detection. MobileNetV3-Large LR- ASPP is 34% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2310.16802",
        "title": "FROM MOLECULES TO MATERIALS: PRE-TRAINING LARGE GENERALIZABLE MODELS FOR ATOMIC PROP- ERTY PREDICTION",
        "abstract": "Foundation models have been transformational in machine learning fields such as natural language processing and computer vision. Similar success in atomic property prediction has been limited due to the challenges of training effective models across multiple chemical domains. To address this, we introduce Joint Multi-domain Pre-training (JMP), a supervised pre-training strategy that simul- taneously trains on multiple datasets from different chemical domains, treating each dataset as a unique pre-training task within a multi-task framework. Our combined training dataset consists of \u223c120M systems from OC20, OC22, ANI-1x, and Transition-1x. We evaluate performance and generalization by fine-tuning over a diverse set of downstream tasks and datasets including: QM9, rMD17, MatBench, QMOF, SPICE, and MD22. JMP demonstrates an average improvement of 59% over training from scratch and matches or sets state-of-the-art on 34 out of 40 tasks. Our work highlights the potential of pre-training strategies that utilize diverse data to advance property prediction across chemical domains, especially for low-data tasks. Please visit https://nima.sh/jmp for further information.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2308.16259",
        "title": "Materials Informatics Transformer: A Language Model for Interpretable Materials Properties Prediction",
        "abstract": "Recently, the remarkable capabilities of large language models (LLMs) have been illustrated across a variety of research domains such as natural language processing, computer vision, and molecular modeling. We extend this paradigm by utilizing LLMs for material property prediction by introducing our model Materials Informatics Trans- former (MatInFormer). Specifically, we introduce a novel approach that involves learn- ing the grammar of crystallography through the tokenization of pertinent space group information. We further illustrate the adaptability of MatInFormer by incorporating task-specific data pertaining to Metal-Organic Frameworks (MOFs). Through atten- tion visualization, we uncover the key features that the model prioritizes during prop- erty prediction. The effectiveness of our proposed model is empirically validated across\n14 distinct datasets, hereby underscoring its potential for high throughput screening through accurate material property prediction.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2306.00890",
        "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day",
        "abstract": "Conversational generative AI has demonstrated remarkable promise for empow- ering biomedical practitioners, but current investigations focus on unimodal text. Multimodal conversational AI has seen rapid progress by leveraging billions of image-text pairs from the public web, but such general-domain vision-language models still lack sophistication in understanding and conversing about biomedical images. In this paper, we propose a cost-efficient approach for training a vision- language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to train a Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal con- versational capability and can follow open-ended instruction to assist with inquiries about a biomedical image. On three standard biomedical visual question answering datasets, fine-tuning LLaVA-Med outperforms previous supervised state-of-the-art on certain metrics. To facilitate biomedical multimodal research, we will release our instruction-following data and the LLaVA-Med model.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2303.11678",
        "title": "Full or Weak annotations?\nAn adaptive strategy for budget-constrained annotation campaigns",
        "abstract": "Annotating new datasets for machine learning tasks is tedious, time-consuming, and costly. For segmentation ap- plications, the burden is particularly high as manual delin- eations of relevant image content are often extremely expen- sive or can only be done by experts with domain-specific knowledge. Thanks to developments in transfer learning and training with weak supervision, segmentation models can now also greatly benefit from annotations of different kinds. However, for any new domain application looking to use weak supervision, the dataset builder still needs to define a strategy to distribute full segmentation and other weak annotations. Doing so is challenging, however, as it is a priori unknown how to distribute an annotation budget for a given new dataset. To this end, we propose a novel ap- proach to determine annotation strategies for segmentation datasets, whereby estimating what proportion of segmen- tation and classification annotations should be collected given a fixed budget. To do so, our method sequentially determines proportions of segmentation and classification annotations to collect for budget-fractions by modeling the expected improvement of the final segmentation model. We show in our experiments that our approach yields annota- tions that perform very close to the optimal for a number of different annotation budgets and datasets.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2303.12112",
        "title": "positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation",
        "abstract": "The CLIP model has been recently proven to be very effective for a variety of cross-modal tasks, including the evaluation of captions generated from vision-and-language architectures. In this paper, we propose a new recipe for a contrastive-based evaluation metric for image cap- tioning, namely Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way unifies the learning of a contrastive visual-semantic space with the addition of generated images and text on curated data. Experiments spanning several datasets demonstrate that our new metric achieves the highest correlation with human judgments on both images and videos, outperforming existing reference- based metrics like CIDEr and SPICE and reference-free metrics like CLIP-Score. Finally, we test the system-level correlation of the proposed metric when considering pop- ular image captioning approaches, and assess the impact of employing different cross-modal features. Our source code and trained models are publicly available at: https: //github.com/aimagelab/pacscore.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2407.03320",
        "title": "InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output",
        "abstract": "We present InternLM-XComposer-2.5 (IXC-2.5), a ver- satile large-vision language model that supports long- contextual input and output. IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE ex- trapolation. This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output con- texts. Compared to its previous 2.0 version, InternLM- XComposer-2.5 features three major upgrades in vision- language comprehension: (1) Ultra-High Resolution Un- derstanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In addition to com- prehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of- the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks. The InternLM-XComposer-2.5 is publicly available at https://github.com/InternLM/InternLM- XComposer.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2407.03243",
        "title": "Visual Grounding with Attention-Driven Constraint Balancing",
        "abstract": "Unlike Object Detection, Visual Grounding task necessitates the detection of an object described by complex free-form language. To simultaneously model such complex semantic and visual representations, recent state-of-the-art studies adopt transformer-based models to fuse features from both modalities, further introducing various modules that modulate visual features to align with the language expressions and elim- inate the irrelevant redundant information. However, their loss function, still adopting common Object Detection losses, solely governs the bound- ing box regression output, failing to fully optimize for the above objec- tives. To tackle this problem, in this paper, we first analyze the attention mechanisms of transformer-based models. Building upon this, we further propose a novel framework named Attention-Driven Constraint Balanc- ing (AttBalance) to optimize the behavior of visual features within language-relevant regions. Extensive experimental results show that our method brings impressive improvements. Specifically, we achieve con- stant improvements over five different models evaluated on four different benchmarks. Moreover, we attain a new state-of-the-art performance by integrating our method into QRNet.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2306.04979",
        "title": "CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification",
        "abstract": "Although graph neural networks (GNNs) have achieved impressive achievements in graph classi- fication, they often need abundant task-specific la- bels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply GNNs to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose Coupled Contrastive Graph Representa- tion Learning (CoCo), which extracts the topolog- ical information from coupled learning branches and reduces the domain discrepancy with cou- pled contrastive learning. CoCo contains a graph convolutional network branch and a hierarchi- cal graph kernel network branch, which explore graph topology in implicit and explicit manners. Besides, we incorporate coupled branches into a holistic multi-view contrastive learning frame- work, which not only incorporates graph repre- sentations learned from complementary views for enhanced understanding, but also encourages the similarity between cross-domain example pairs with the same semantics for domain alignment. Extensive experiments on popular datasets show that our CoCo outperforms these competing base- lines in different settings generally.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2206.06619",
        "title": "TransVG++: End-to-End Visual Grounding with\nLanguage Conditioned Vision Transformer",
        "abstract": "In this work, we explore neat yet effective Transformer-based frameworks for visual grounding. The previous methods generally address the core problem of visual grounding, i.e., multi-modal fusion and reasoning, with manually-designed mechanisms. Such heuristic designs are not only complicated but also make models easily overfit specific data distributions. To avoid this, we first propose TransVG, which establishes multi-modal correspondences by Transformers and localizes referred regions by directly regressing box coordinates. We empirically show that complicated fusion modules can be replaced by a simple stack of Transformer encoder layers with higher performance. However, the core fusion Transformer in TransVG is stand-alone against uni-modal encoders, and thus should be trained from scratch on limited visual grounding data, which makes it hard to be optimized and leads to sub-optimal performance. To this end, we further introduce TransVG++ to make two-fold improvements. For one thing, we upgrade our framework to a purely Transformer-based one by leveraging Vision Transformer (ViT) for vision feature encoding. For another, we devise Language Conditioned Vision Transformer that removes external fusion modules and reuses the uni-modal ViT for vision-language fusion at the intermediate layers. We conduct extensive experiments on five prevalent datasets, and report a series of state-of-the-art records.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1608.00272",
        "title": "Modeling Context in Referring Expressions",
        "abstract": "Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and com- prehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual con- text into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg1, shows the advantages of our methods for both refer- ring expression generation and comprehension.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2311.00278",
        "title": "RE-SCORING USING IMAGE-LANGUAGE SIMILARITY FOR FEW-SHOT OBJECT DETECT",
        "abstract": "Few-shot object detection, which focuses on detecting novel objects with few labels, is an emerging challenge in the community. Recent studies show that adapting a pre-trained model or modified loss function can improve performance. In this paper, we explore leveraging the power of Con- trastive Language-Image Pre-training (CLIP) and hard negative classification loss in low data setting. Specifically, we propose Re-scoring using Image-language Similarity for Few-shot object detection (RISF) which extends Faster R-CNN by introducing Calibration Module using CLIP (CM-CLIP) and Background Negative Re-scale Loss (BNRL). The former adapts CLIP, which performs zero-shot classification, to re-score the classification scores of a detector using image-class similarities, the latter is modified classification loss considering the punishment for fake backgrounds as well as confusing categories on a generalized few-shot object detection dataset. Extensive experiments on MS- COCO and PASCAL VOC show that the proposed RISF substantially outperforms the state-of-the-art approaches. The code will be available.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2407.03253",
        "title": "STF: SENTENCE TRANSFORMER FINE-TUNING FOR TOPIC CATEGORIZATION WITH LIMITED DATA \u2217",
        "abstract": "Nowadays, topic classification from tweets attracts several researchers\u2019 attention. Different classification systems have been suggested thanks to these research efforts. Nevertheless, they are confronted major challenge owing to the low performance metrics because of the limited labeled data. We propose, Sentence Transformers Fine-tuning (STF), a topic detection system that leverages pre-trained Sentence Transformers models and Fine-tuning to classify topics from tweets accurately. Moreover, extensive parameter sensitivity analyses were established to fine-tune STF parameters\u2019 for our topic classification task to achieve the best performance results. Experiments on two benchmark datasets demonstrated that: (1) the proposed STF can be effectively used for classifying tweet topics and outperform the latest state-of-the-art approaches; (2) the proposed STF does not require a huge amount of labeled tweets to achieve good accuracy, which is the lack in the popular of the state-of-the-art approaches. Our main contribution is the achievement of promising results in tweet topic classification by applying pre-trained sentence transformers language models",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2403.15624",
        "title": "Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian Splatting",
        "abstract": "Open-vocabulary 3D scene understanding presents a signif- icant challenge in computer vision, with wide-ranging applications in embodied agents and augmented reality systems. Previous approaches have adopted Neural Radiance Fields (NeRFs) to analyze 3D scenes. In this paper, we introduce Semantic Gaussians, a novel open-vocabulary scene understanding approach based on 3D Gaussian Splatting. Our key idea is distilling pre-trained 2D semantics into 3D Gaussians. We design a versatile projection approach that maps various 2D semantic features from pre-trained image encoders into a novel semantic component of 3D Gaussians, without the additional training required by NeRFs. We fur- ther build a 3D semantic network that directly predicts the semantic component from raw 3D Gaussians for fast inference. We explore several applications of Semantic Gaussians: semantic segmentation on ScanNet- 20, where our approach attains a 4.2% mIoU and 4.0% mAcc improve- ment over prior open-vocabulary scene understanding counterparts; ob- ject part segmentation, scene editing, and spatial-temporal segmentation with better qualitative results over 2D and 3D baselines, highlighting its versatility and effectiveness on supporting diverse downstream tasks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1912.08830",
        "title": "ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language",
        "abstract": "We introduce the task of 3D object localization in RGB-D scans using natural language descriptions. As input, we assume a point cloud of a scanned 3D scene along with a free-form description of a spec- ified target object. To address this task, we propose ScanRefer, learn- ing a fused descriptor from 3D object proposals and encoded sentence embeddings. This fused descriptor correlates language expressions with geometric features, enabling regression of the 3D bounding box of a tar- get object. We also introduce the ScanRefer dataset, containing 51,583 descriptions of 11,046 objects from 800 ScanNet [9] scenes. ScanRefer is the first large-scale effort to perform object localization via natural language expression directly in 3D 1.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0704.0001",
        "title": "Calculation of prompt diphoton production cross sections at Tevatron and\nLHC energies",
        "abstract": "A fully differential calculation in perturbative quantum chromodynamics is\n presented for the production of massive photon pairs at hadron colliders. All\n next-to-leading order perturbative contributions from quark-antiquark,\n gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\n all-orders resummation of initial-state gluon radiation valid at\n next-to-next-to-leading logarithmic accuracy. The region of phase space is specified in which the calculation is most reliable. Good agreement is\n demonstrated with data from the Fermilab Tevatron, and predictions are made for\n more detailed tests with CDF and DO data. Predictions are shown for\n distributions of diphoton pairs produced at the energy of the Large Hadron\n Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\n boson are contrasted with those produced from QCD processes at the LHC, showing\n that enhanced sensitivity to the signal can be obtained with judicious\n selection of events.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0704.0002",
        "title": "Sparsity-certifying Graph Decompositions",
        "abstract": "We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\n it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and\n algorithmic solutions to a family of problems concerning tree decompositions of\n graphs. Special instances of sparse graphs appear in rigidity theory and have\n received increased attention in recent years. In particular, our colored\n pebbles generalize and strengthen the previous results of Lee and Streinu and\n give a new proof of the Tutte-Nash-Williams characterization of arboricity. We\n also present a new decomposition that certifies sparsity based on the\n $(k,\\ell)$-pebble game with colors. Our work also exposes connections between\n pebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\n Westermann and Hendrickson.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0704.0003",
        "title": "The evolution of the Earth-Moon system based on the dark matter field\nfluid model",
        "abstract": "The evolution of Earth-Moon system is described by the dark matter field\n fluid model proposed in the Meeting of Division of Particle and Field 2004,\n American Physical Society. The current behavior of the Earth-Moon system agrees\n with this model very well and the general pattern of the evolution of the\n Moon-Earth system described by this model agrees with geological and fossil\n evidence. The closest distance of the Moon to Earth was about 259000 km at 4.5\n billion years ago, which is far beyond the Roche's limit. The result suggests\n that the tidal friction may not be the primary cause for the evolution of the\n Earth-Moon system. The average dark matter field fluid constant derived from\n Earth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts\n that the Mars's rotation is also slowing with the angular acceleration rate\n about -4.38 x 10^(-22) rad s^(-2).",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0704.0004",
        "title": "A determinant of Stirling cycle numbers counts unlabeled acyclic\nsingle-source automata",
        "abstract": "We show that a determinant of Stirling cycle numbers counts unlabeled acyclic\n single-source automata. The proof involves a bijection from these automata to\n certain marked lattice paths and a sign-reversing involution to evaluate the\n determinant.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0704.0005",
        "title": "From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\alpha}$",
        "abstract": "In this paper we show how to compute the $\\Lambda_{\\alpha}$ norm, $\\alpha\\ge\n 0$, using the dyadic grid. This result is a consequence of the description of\n the Hardy spaces $H^p(R^N)$ in terms of dyadic and special atoms.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0704.0049",
        "title": "An algorithm for the classification of smooth Fano polytopes",
        "abstract": "We present an algorithm that produces the classification list of smooth Fano\n d-polytopes for any given d. The input of the algorithm is a single number,\n namely the positive integer d. The algorithm has been used to classify smooth\n Fano d-polytopes for d<=7. There are 7622 isomorphism classes of smooth Fano\n 6-polytopes and 72256 isomorphism classes of smooth Fano 7-polytopes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0704.0050",
        "title": "Intelligent location of simultaneously active acoustic emission sources:\nPart II",
        "abstract": "Part I describes an intelligent acoustic emission locator, while Part II\ndiscusses blind source separation, time delay estimation and location of two\ncontinuous acoustic emission sources.\nAcoustic emission (AE) analysis is used for characterization and location of\ndeveloping defects in materials. AE sources often generate a mixture of various\nstatistically independent signals. A difficult problem of AE analysis is\nseparation and characterization of signal components when the signals from\nvarious sources and the mode of mixing are unknown. Recently, blind source\nseparation (BSS) by independent component analysis (ICA) has been used to solve\nthese problems. The purpose of this paper is to demonstrate the applicability\nof ICA to locate two independent simultaneously active acoustic emission\nsources on an aluminum band specimen. The method is promising for\nnon-destructive testing of aircraft frame structures by acoustic emission\nanalysis.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0704.0051",
        "title": "Visualizing Teleportation",
        "abstract": "A novel way of picturing the processing of quantum information is described,\n allowing a direct visualization of teleportation of quantum states and\n providing a simple and intuitive understanding of this fascinating phenomenon.\n The discussion is aimed at providing physicists a method of explaining\n teleportation to non-scientists. The basic ideas of quantum physics are first\n explained in lay terms, after which these ideas are used with a graphical\n description, out of which teleportation arises naturally.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0704.0052",
        "title": "Quantum Field Theory on Curved Backgrounds. II. Spacetime Symmetries",
        "abstract": "We study space-time symmetries in scalar quantum field theory (including\n interacting theories) on static space-times. We first consider Euclidean\n quantum field theory on a static Riemannian manifold, and show that the\n isometry group is generated by one-parameter subgroups which have either\n self-adjoint or unitary quantizations. We analytically continue the\n self-adjoint semigroups to one-parameter unitary groups, and thus construct a\n unitary representation of the isometry group of the associated Lorentzian\n manifold. The method is illustrated for the example of hyperbolic space, whose\n Lorentzian continuation is Anti-de Sitter space.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0704.0053",
        "title": "A Global Approach to the Theory of Special Finsler Manifolds",
        "abstract": "The aim of the present paper is to provide a global presentation of the\n theory of special Finsler manifolds. We introduce and investigate globally (or\n intrinsically, free from local coordinates) many of the most important and most\n commonly used special Finsler manifolds: locally Minkowskian, Berwald,\n Landesberg, general Landesberg, $P$-reducible, $C$-reducible,\n semi-$C$-reducible, quasi-$C$-reducible, $P^{*}$-Finsler, $C^{h}$-recurrent,\n $C^{v}$-recurrent, $C^{0}$-recurrent, $S^{v}$-recurrent, $S^{v}$-recurrent of\n the second order, $C_{2}$-like, $S_{3}$-like, $S_{4}$-like, $P_{2}$-like,\n $R_{3}$-like, $P$-symmetric, $h$-isotropic, of scalar curvature, of constant\n curvature, of $p$-scalar curvature, of $s$-$ps$-curvature. The global\n definitions of these special Finsler manifolds are introduced. Various\n relationships between the different types of the considered special Finsler\n manifolds are found. Many local results, known in the literature, are proved\n globally and several new results are obtained. As a by-product, interesting\n identities and properties concerning the torsion tensor fields and the\n curvature tensor fields are deduced. Although our investigation is entirely\n global, we provide; for comparison reasons, an appendix presenting a local\n counterpart of our global approach and the local definitions of the special\n Finsler spaces considered.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0704.0054",
        "title": "The Hardy-Lorentz Spaces $H^{p,q}(R^n)$",
        "abstract": "In this paper we consider the Hardy-Lorentz spaces $H^{p,q}(R^n)$, with\n $0<p\\le 1$, $0<q\\le \\infty$. We discuss the atomic decomposition of the\n elements in these spaces, their interpolation properties, and the behavior of\n singular integrals and other operators acting on them.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2206.07883",
        "title": "Combinatorial Pure Exploration of Causal Bandits",
        "abstract": "The combinatorial pure exploration of causal bandits is the following online\n learning task: given a causal graph with unknown causal inference\n distributions, in each round we choose a subset of variables to intervene or do\n no intervention, and observe the random outcomes of all random variables, with\n the goal that using as few rounds as possible, we can output an intervention\n that gives the best (or almost best) expected outcome on the reward variable\n $Y$ with probability at least $1-\\delta$, where $\\delta$ is a given confidence\n level. We provide the first gap-dependent and fully adaptive pure exploration\n algorithms on two types of causal models -- the binary generalized linear model\n (BGLM) and general graphs. For BGLM, our algorithm is the first to be designed\n specifically for this setting and achieves polynomial sample complexity, while\n all existing algorithms for general graphs have either sample complexity\n exponential to the graph size or some unreasonable assumptions. For general\n graphs, our algorithm provides a significant improvement on sample complexity,\n and it nearly matches the lower bound we prove. Our algorithms achieve such\n improvement by a novel integration of prior causal bandit algorithms and prior\n adaptive pure exploration algorithms, the former of which utilize the rich\n observational feedback in causal bandits but are not adaptive to reward gaps,\n while the latter of which have the issue in reverse.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2207.06049",
        "title": "Liouville-type theorems for the Lane-Emden equation in the half-space\nand cones",
        "abstract": "We prove that 0 the only classical solution of the Lane-Emden equation in the\n half-space which is stable outside a compact set. We also consider weak\n solutions and the case of general cones.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0708.3956",
        "title": "The asymptotic behaviour of recurrence coefficients for orthogonal\npolynomials with varying exponential weights",
        "abstract": "We consider orthogonal polynomials $\\{p_{n,N}(x)\\}_{n=0}^{\\infty}$ on the\n real line with respect to a weight $w(x)=e^{-NV(x)}$ and in particular the\n asymptotic behaviour of the coefficients $a_{n,N}$ and $b_{n,N}$ in the three\n term recurrence $x \\pi_{n,N}(x) = \\pi_{n+1,N}(x) + b_{n,N} \\pi_{n,N}(x) +\n a_{n,N} \\pi_{n-1,N}(x)$. For one-cut regular $V$ we show, using the Deift-Zhou\n method of steepest descent for Riemann-Hilbert problems, that the diagonal\n recurrence coefficients $a_{n,n}$ and $b_{n,n}$ have asymptotic expansions as\n $n \\to \\infty$ in powers of $1/n^2$ and powers of $1/n$, respectively.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2302.06551",
        "title": "Primes in tuples and Romanoff's theorem",
        "abstract": "We obtain a lower bound for a number of primes in tuples. As applications, we\n obtain a lower bound for the Romanoff type representation functions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2305.15374",
        "title": "ASPER: Answer Set Programming Enhanced Neural Network Models for Joint\nEntity-Relation Extraction",
        "abstract": "A plethora of approaches have been proposed for joint entity-relation (ER)\n extraction. Most of these methods largely depend on a large amount of manually\n annotated training data. However, manual data annotation is time consuming,\n labor intensive, and error prone. Human beings learn using both data (through\n induction) and knowledge (through deduction). Answer Set Programming (ASP) has\n been a widely utilized approach for knowledge representation and reasoning that\n is elaboration tolerant and adept at reasoning with incomplete information.\n This paper proposes a new approach, ASP-enhanced Entity-Relation extraction\n (ASPER), to jointly recognize entities and relations by learning from both data\n and domain knowledge. In particular, ASPER takes advantage of the factual\n knowledge (represented as facts in ASP) and derived knowledge (represented as\n rules in ASP) in the learning process of neural network models. We have\n conducted experiments on two real datasets and compare our method with three\n baselines. The results show that our ASPER model consistently outperforms the\n baselines.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.10664",
        "title": "Nonadiabatic transitions during a passage near a critical point",
        "abstract": "The passage through a critical point of a many-body quantum system leads to\n abundant nonadiabatic excitations. Here, we explore a regime, in which the\n critical point is not crossed although the system is passing slowly very close\n to it. We show that the leading exponent for the excitation probability then\n can be obtained by standard arguments of the Dykhne formula but the exponential\n prefactor is no longer simple, and behaves as a power law on the characteristic\n transition rate. We derive this prefactor for the nonlinear Landau-Zener (nLZ)\n model by adjusting the Dykhne's approach. Then, we introduce an exactly\n solvable model of the transition near a critical point in the Stark ladder. We\n derive the number of the excitations for it without approximations, and find\n qualitatively similar results for the excitation scaling.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1907.08698",
        "title": "Leveraging Knowledge Bases And Parallel Annotations For Music Genre\nTranslation",
        "abstract": "Prevalent efforts have been put in automatically inferring genres of musical\n items. Yet, the propose solutions often rely on simplifications and fail to\n address the diversity and subjectivity of music genres. Accounting for these\n has, though, many benefits for aligning knowledge sources, integrating data and\n enriching musical items with tags. Here, we choose a new angle for the genre\n study by seeking to predict what would be the genres of musical items in a\n target tag system, knowing the genres assigned to them within source tag\n systems. We call this a translation task and identify three cases: 1) no common\n annotated corpus between source and target tag systems exists, 2) such a large\n corpus exists, 3) only few common annotations exist. We propose the related\n solutions: a knowledge-based translation modeled as taxonomy mapping, a\n statistical translation modeled with maximum likelihood logistic regression; a\n hybrid translation modeled with maximum a posteriori logistic regression with\n priors given by the knowledge-based translation. During evaluation, the\n solutions fit well the identified cases and the hybrid translation is\n systematically the most effective w.r.t. multilabel classification metrics.\n This is a first attempt to unify genre tag systems by leveraging both\n representation and interpretation diversity.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/physics/0410192",
        "title": "Coherent control of optical four-wave mixing by two-color\n$\\omega$-$3\\omega$ ultrashort laser pulses",
        "abstract": "A theoretical investigation on the quantum control of optical coherent\n four-wave mixing interactions in two-level systems driven by two intense\n synchronized femtosecond laser pulses of central angular frequencies $\\omega$\n and $3\\omega$ is reported. By numerically solving the full Maxwell-Bloch\n equations beyond the slowly-varying envelope and rotating-wave approximations\n in the time domain, the nonlinear coupling to the optical field at frequency\n $5\\omega$ is found to depend critically on the initial relative phase $\\phi$ of\n the two propagating pulses; the coupling is enhanced when the pulses interfere\n constructively in the center ($\\phi=0$), while it is nearly suppressed when\n they are out of phase ($\\phi=\\pi$). The tuning of the initial absolute phase of\n the different frequency components of synchronously propapagating\n $\\omega$-$3\\omega$ femtosecond pulses can serve as a means to control coherent\n anti-Stokes Raman scattering (CARS) processes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1810.00446",
        "title": "Local-Ising type magnetic order and metamagnetism in the rare-earth\npyrogermanate Er$_2$Ge$_2$O$_7$",
        "abstract": "The recent discoveries of proximate quantum spin-liquid compounds and their\n potential application in quantum computing informs the search for new candidate\n materials for quantum spin-ice and spin-liquid physics. While the majority of\n such work has centered on members of the pyrochlore family due to their\n inherently frustrated linked tetrahedral structure, the rare-earth\n pyrogermanates also show promise for possible frustrated magnetic behavior.\n With the familiar stoichiometry $RE_2$Ge$_2$O$_7$, these compounds generally\n have tetragonal symmetry with a rare-earth sublattice built of a spiral of\n alternating edge and corner sharing rare-earth site triangles. Studies on\n Dy$_2$Ge$_2$O$_7$ and Ho$_2$Ge$_2$O$_7$ have shown tunable low temperature\n antiferromagnetic order, a high frustration index and spin-ice like dynamics.\n Here we use neutron diffraction to study magnetic order in Er$_2$Ge$_2$O$_7$\n (space group $P4_{1}2_{1}2$ ) and find the lowest yet Ne\\'el temperature in the\n pyrogermanates of 1.15 K. Using neutron powder diffraction we find the magnetic\n structure to order with $k = (0,0,0)$ ordering vector, magnetic space group\n symmetry $P4_{1}^{'}2_{1}2^{'}$ and a refined Er moment of $m = 8.1 \\mu_B$ -\n near the expected value for the Er$^{3+}$ free ion. Provocatively, the magnetic\n structure exhibits similar 'local-Ising' behavior to that seen in the\n pyrocholres where the Er moment points up or down along the short Er-Er bond.\n Upon applying a magnetic field we find a first order metamagnetic transition at\n $\\sim$ 0.35 T to a lower symmetry $P2_{1}^{'}2_{1}^{'}2$ structure. This\n magnetic transition involves an inversion of Er moments aligned antiparallel to\n the applied field describing a class I spin-flip type transition, indicating a\n strong local anisotropy at the Er site - reminiscent of that seen in the\n spin-ice pyrochlores.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1003.0434",
        "title": "The limit of F_p-Betti numbers of a tower of finite covers with amenable\nfundamental groups",
        "abstract": "We prove an analogue of the Approximation Theorem of L^2-Betti numbers by\n Betti numbers for arbitrary coefficient fields and virtually torsionfree\n amenable groups. The limit of Betti numbers is identified as the dimension of\n some module over the Ore localization of the group ring.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2304.01143",
        "title": "Use Your Head: Improving Long-Tail Video Recognition",
        "abstract": "This paper presents an investigation into long-tail video recognition. We\ndemonstrate that, unlike naturally-collected video datasets and existing\nlong-tail image benchmarks, current video benchmarks fall short on multiple\nlong-tailed properties. Most critically, they lack few-shot classes in their\ntails. In response, we propose new video benchmarks that better assess\nlong-tail recognition, by sampling subsets from two datasets: SSv2 and VideoLT.\nWe then propose a method, Long-Tail Mixed Reconstruction, which reduces\noverfitting to instances from few-shot classes by reconstructing them as\nweighted combinations of samples from head classes. LMR then employs label\nmixing to learn robust decision boundaries. It achieves state-of-the-art\naverage class accuracy on EPIC-KITCHENS and the proposed SSv2-LT and\nVideoLT-LT. Benchmarks and code at: tobyperrett.github.io/lmr",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1804.03707",
        "title": "A Tamper-Free Semi-Universal Communication System for Deletion Channels",
        "abstract": "We investigate the problem of reliable communication between two legitimate\n parties over deletion channels under an active eavesdropping (aka jamming)\n adversarial model. To this goal, we develop a theoretical framework based on\n probabilistic finite-state automata to define novel encoding and decoding\n schemes that ensure small error probability in both message decoding as well as\n tamper detecting. We then experimentally verify the reliability and\n tamper-detection property of our scheme.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2207.03023",
        "title": "Superior mechanical properties by exploiting size-effects and multiscale\ninteractions in hierarchically architected foams",
        "abstract": "Protective applications in extreme environments demand thermally stable\n materials with superior modulus, strength, and specific energy absorption (SEA)\n at lightweight. However, these properties typically have a trade-off.\n Hierarchically architected materials--such as the architected vertically\n aligned carbon nanotube (VACNT) foams--offer the potential to overcome these\n trade-offs to achieve synergistic enhancement in mechanical properties. Here,\n we adopt a full-factorial design of experiments (DOE) approach to optimize\n multitier design parameters to achieve synergistic enhancement in SEA,\n strength, and modulus at lightweight in VACNT foams with mesoscale cylindrical\n architecture. We exploit the size effects from geometrically-confined synthesis\n and the highly interactive morphology of CNTs to enable higher-order design\n parameter interactions that intriguingly break the diameter-to-thickness\n (D/t)-dependent scaling laws found in common tubular architected materials. We\n show that exploiting complementary hierarchical mechanisms in architected\n material design can lead to unprecedented synergistic enhancement of mechanical\n properties and performance desirable for extreme protective applications.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1905.10534",
        "title": "On the regularity of minima of non-autonomous functionals",
        "abstract": "We consider regularity issues for minima of non-autonomous functionals in the\n Calculus of Variations exhibiting non-uniform ellipticity features. We provide\n a few sharp regularity results for local minimizers that also cover the case of\n functionals with nearly linear growth. The analysis is carried out provided\n certain necessary approximation-in-energy conditions are satisfied. These are\n related to the occurrence of the so-called Lavrentiev phenomenon that that\n non-autonomous functionals might exhibit, and which is a natural obstruction to\n regularity. In the case of vector valued problems we concentrate on higher\n gradient integrability of minima. Instead, in the scalar case, we prove local\n Lipschitz estimates. We also present an approach via a variant of Moser's\n iteration technique that allows to reduce the analysis of several non-uniformly\n elliptic problems to that for uniformly elliptic ones.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2105.08089",
        "title": "A Measure of Research Taste",
        "abstract": "Researchers are often evaluated by citation-based metrics. Such metrics can\n inform hiring, promotion, and funding decisions. Concerns have been expressed\n that popular citation-based metrics incentivize researchers to maximize the\n production of publications. Such incentives may not be optimal for scientific\n progress. Here we present a citation-based measure that rewards both\n productivity and taste: the researcher's ability to focus on impactful\n contributions. The presented measure, CAP, balances the impact of publications\n and their quantity, thus incentivizing researchers to consider whether a\n publication is a useful addition to the literature. CAP is simple,\n interpretable, and parameter-free. We analyze the characteristics of CAP for\n highly-cited researchers in biology, computer science, economics, and physics,\n using a corpus of millions of publications and hundreds of millions of\n citations with yearly temporal granularity. CAP produces qualitatively\n plausible outcomes and has a number of advantages over prior metrics. Results\n can be explored at https://cap-measure.org/",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2204.04521",
        "title": "Benchmarking for Public Health Surveillance tasks on Social Media with a\nDomain-Specific Pretrained Language Model",
        "abstract": "A user-generated text on social media enables health workers to keep track of\n information, identify possible outbreaks, forecast disease trends, monitor\n emergency cases, and ascertain disease awareness and response to official\n health correspondence. This exchange of health information on social media has\n been regarded as an attempt to enhance public health surveillance (PHS).\n Despite its potential, the technology is still in its early stages and is not\n ready for widespread application. Advancements in pretrained language models\n (PLMs) have facilitated the development of several domain-specific PLMs and a\n variety of downstream applications. However, there are no PLMs for social media\n tasks involving PHS. We present and release PHS-BERT, a transformer-based PLM,\n to identify tasks related to public health surveillance on social media. We\n compared and benchmarked the performance of PHS-BERT on 25 datasets from\n different social medial platforms related to 7 different PHS tasks. Compared\n with existing PLMs that are mainly evaluated on limited tasks, PHS-BERT\n achieved state-of-the-art performance on all 25 tested datasets, showing that\n our PLM is robust and generalizable in the common PHS tasks. By making PHS-BERT\n available, we aim to facilitate the community to reduce the computational cost\n and introduce new baselines for future works across various PHS-related tasks.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.15983",
        "title": "Effects of center-of-mass correction and nucleon anomalous magnetic\nmoments on nuclear charge radii",
        "abstract": "Effects of the center-of-mass correction together with the nucleon\n electromagnetic form factors on the nuclear charge radius are systematically\n studied with a relativistic Hartree-Bogoliubov model. Both one- and two-body\n parts of the CM correction are taken into account. It is found that the one-\n and two-body CM corrections, and the spin-orbit effect originating from the\n nucleon anomalous magnetic moments are all of the same order in magnitude, and\n that they give sizable impacts on the charge radius from light to heavy nuclei.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1502.04479",
        "title": "Searching for IMBHs in Galactic globular clusters through radial\nvelocities of individual stars",
        "abstract": "I present an overview of our ongoing project aimed at building a new\n generation of velocity dispersion profiles ad rotation curves for a\n representative sample of Galactic globular clusters, from the the radial\n velocity of hundreds individual stars distributed at different distances from\n the cluster center. The innermost portion of the profiles will be used to\n constrain the possibile presence of intermediate-mass black holes. The adopted\n methodology consists in combining spectroscopic observations acquired with\n three different instruments at the ESO-VLT: the adaptive-optics assisted,\n integral field unit (IFU) spectrograph SINFONI for the innermost and highly\n crowded cluster cores, the multi-IFU spectrograph KMOS for the intermediate\n regions, and the multi-fiber instrument FLAMES/GIRAFFE-MEDUSA for the\n outskirts. The case of NGC 6388, representing the pilot project that motivated\n the entire program, is described in some details.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/9710108",
        "title": "Three-particle States in Nonrelativistic Four-fermion Model",
        "abstract": "On a nonrelativistic contact four-fermion model we have shown that the simple\n Lambda-cut-off prescription together with definite fine-tuning of the Lambda\n dependency of \"bare\"quantities lead to self-adjoint semi-bounded Hamiltonian in\n one-, two- and three-particle sectors. The fixed self-adjoint extension and\n exact solutions in two-particle sector completely define three-particle\n problem. The renormalized Faddeev equations for the bound states with Fredholm\n properties are obtained and analyzed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2008.11998",
        "title": "Characterization of exact one-query quantum algorithms (ii): for partial\nfunctions",
        "abstract": "The query model (or black-box model) has attracted much attention from the\n communities of both classical and quantum computing. Usually, quantum\n advantages are revealed by presenting a quantum algorithm that has a better\n query complexity than its classical counterpart. For example, the well-known\n quantum algorithms including Deutsch-Jozsa algorithm, Simon algorithm and\n Grover algorithm all show a considerable advantage of quantum computing from\n the viewpoint of query complexity. Recently we have considered in (Phys. Rev.\n A. {\\bf 101}, 02232 (2020)) the problem: what functions can be computed by an\n exact one-query quantum algorithm? This problem has been addressed for total\n Boolean functions but still open for partial Boolean functions. Thus, in this\n paper we continue to characterize the computational power of exact one-query\n quantum algorithms for partial Boolean functions by giving several necessary\n and sufficient conditions. By these conditions, we construct some new functions\n that can be computed exactly by one-query quantum algorithms but have essential\n difference from the already known ones. Note that before our work, the known\n functions that can be computed by exact one-query quantum algorithms are all\n symmetric functions, whereas the ones constructed in this papers are generally\n asymmetric.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0509149",
        "title": "Ultraluminous Starbursts from SMBH-induced outflows",
        "abstract": "I argue that there are two modes of global star formation. Disks and smaller\n spheroids form stars relatively inefficiently as a consequence of\n supernova-triggered negative feedback via a sequence of ministarbursts (S\n mode), whereas massive spheroids formed rapidly with high efficiency via the\n impact of AGN jet-triggered positive feedback (J mode) that generates and\n enhances ultraluminous starbursts. Supermassive black hole growth by accretion\n is favoured in the gas-rich protospheroid environment as mergers build up the\n mass of the host galaxy and provide a centrally concentrated gas supply.\n Quasi-spherical outflows arise and provide the source of porosity as the\n energetic jets from the accreting central SMBH are isotropised by the\n inhomogeneous interstellar medium in the protospheroid core. Super-Eddington\n outflows occur and help generate both the SMBH at high redshift and the strong\n positive feedback on protospheroid star formation that occurs as dense\n interstellar clouds are overpressured and collapse. SMBH form before the bulk\n of spheroid stars, and the correlation between spheroid velocity dispersion and\n supermassive black hole mass arises as AGN-triggered outflows limit the gas\n reservoir for spheroid star formation. The super-Eddington phase plausibly\n triggers a top-heavy IMF in the region of influence of the SMBH. The\n Compton-cooled Eddington-limited outflow phase results in a spheroid core whose\n phase space density scales as the inverse 5/2 power of the core mass, and whose\n mass scales as the 3/2 power of SMBH mass. This latter scaling suggests that\n SMBH growth (and hence spheroid formation) is anti-hierarchical.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1908.01287",
        "title": "BCD-Net for Low-dose CT Reconstruction: Acceleration, Convergence, and\nGeneralization",
        "abstract": "Obtaining accurate and reliable images from low-dose computed tomography (CT)\n is challenging. Regression convolutional neural network (CNN) models that are\n learned from training data are increasingly gaining attention in low-dose CT\n reconstruction. This paper modifies the architecture of an iterative regression\n CNN, BCD-Net, for fast, stable, and accurate low-dose CT reconstruction, and\n presents the convergence property of the modified BCD-Net. Numerical results\n with phantom data show that applying faster numerical solvers to model-based\n image reconstruction (MBIR) modules of BCD-Net leads to faster and more\n accurate BCD-Net; BCD-Net significantly improves the reconstruction accuracy,\n compared to the state-of-the-art MBIR method using learned transforms; BCD-Net\n achieves better image quality, compared to a state-of-the-art iterative NN\n architecture, ADMM-Net. Numerical results with clinical data show that BCD-Net\n generalizes significantly better than a state-of-the-art deep (non-iterative)\n regression NN, FBPConvNet, that lacks MBIR modules.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2403.01502",
        "title": "Oscillating-charged Andreev Bound States and Their Appearance in UTe$_2$",
        "abstract": "In a superconductor with a sublattice degree of freedom, we find\n unconventional Andreev bound states whose charge density oscillates in sign\n between the two sublattices. The appearance of these oscillating-charged\n Andreev bound states is characterized by a Zak phase, rather than a\n conventional topological invariant. In contrast to conventional Andreev bound\n states, for oscillating-charged Andreev bound states the proportionality\n between the electron-like spectral function, the local density of states and\n the tunneling conductance is broken. We examine the possible appearance of\n these novel Andreev bound states in UTe$_2$ and locally noncentrosymmetric\n superconductors.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1501.07832",
        "title": "How does a flexible chain of active particles swell?",
        "abstract": "We study the swelling of a flexible linear chain composed of active particles\n by analytical theory and computer simulation. Three different situations are\n considered: a free chain, a chain confined to an external harmonic trap, and a\n chain dragged at one end. First we consider an ideal chain with harmonic\n springs and no excluded volume between the monomers. The Rouse model of\n polymers is generalized to the case of self-propelled monomers and solved\n analytically. The swelling, as characterized by the spatial extension of the\n chain, scales with the monomer number defining a Flory exponent $\\nu$ which is\n $\\nu =1/2, 0, 1$ in the three different situations. As a result, we find that\n activity does not change the Flory exponent but affects the prefactor of the\n scaling law. This can be quantitatively understood by mapping the system onto\n an equilibrium chain with a higher effective temperature such that the chain\n swells under an increase of the self-propulsion strength. We then use computer\n simulations to study the effect of self-avoidance on active polymer swelling.\n In the three different situations, the Flory exponent is now $\\nu = 3/4, 1/4,\n 1$ and again unchanged under self-propulsion. However, the chain extension\n behaves non-monotonic in the self-propulsion strength.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0303363",
        "title": "Recurrence spectrum in smooth dynamical systems",
        "abstract": "We prove that for conformal expanding maps the return time does have constant\n multifractal spectrum. This is the counterpart of the result by Feng and Wu in\n the symbolic setting.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0304204",
        "title": "Electron-Ion Recombination Rate Coefficients and Photoionization Cross\nSections for Astrophysically Abundant Elements. VII. Relativistic\ncalculations for O VI and O VII for UV and X-ray modeling",
        "abstract": "Aimed at ionization balance and spectral analysis of UV and X-ray sources, we\n present self-consistent sets of photoionization cross sections, recombination\n cross sections, and rate coefficients for Li-like O VI and He-like O VII.\n Relativistic fine structure is considered through the Breit-Pauli R-matrix\n (BPRM) method in the close coupling approximation, implementing the unified\n treatment for total electron-ion recombination subsuming both radiative and\n di-electronic recombination processes. Self-consistency is ensured by using an\n identical wavefunction expansion for the inverse processes of photoionization\n and photo-recombination. Radiation damping of resonances, important for H-like\n and He-like core ions, is included. Compared to previous LS coupling results\n without radiative decay of low-n (<= 10) resonances, the presents results show\n significant reduction in O VI recombination rates at high temperatures. In\n addition to the total rates, level-specific photoionization cross sections and\n recombination rates are presented for all fine structure levels n (lSLJ) up to\n n <= 10, to enable accurate computation of recombination-cascade matrices and\n spectral formation of prominent UV and X-ray lines such as the 1032,1038 A\n doublet of O VI, and the `triplet' forbidden, intercombination, and resonance\n X-ray lines of O VII at 22.1, 21.8, and 21.6 \\ang respectively. Altogether,\n atomic parameters for 98 levels of O VI and 116 fine structure levels of O VII\n are theoretically computed. These data should provide a reasonably complete set\n of photoionization and recombination rates in collisional or radiative\n equilibrium.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1007.4250",
        "title": "Extremal Charged Rotating Dilaton Black Holes in Odd Dimensions",
        "abstract": "Employing higher order perturbation theory, we find a new class of charged\n rotating black hole solutions of Einstein-Maxwell-dilaton theory with general\n dilaton coupling constant. Starting from the Myers-Perry solutions, we use the\n electric charge as the perturbative parameter, and focus on extremal black\n holes with equal-magnitude angular momenta in odd dimensions. We perform the\n perturbations up to 4th order for black holes in 5 dimensions and up to 3rd\n order in higher odd dimensions. We calculate the physical properties of these\n black holes and study their dependence on the charge and the dilaton coupling\n constant.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0303172",
        "title": "On the early evolution of the Galactic Halo",
        "abstract": "It is shown that the low metallicity tail of the stellar metallicity\n distribution predicted by simple Outflow models for the Milky Way halo depends\n sensitively on whether Instantaneous Recycling is adopted or relaxed. In both\n cases, current - and still preliminary - data suggest a ``G-dwarf problem'' for\n the halo (reminiscent of the local disk). We suggest that the problem can be\n solved by introducing a (physically motivated) early infall phase. We point out\n several important implications of such a modification, concerning: the putative\n Pop. III (super)massive stars, the number of stars expected at very low\n metallicities, the questions of primary nitrogen and of the dispersion in\n abundance ratios of halo stars.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1504.05321",
        "title": "Instance Optimal Learning",
        "abstract": "We consider the following basic learning task: given independent draws from\nan unknown distribution over a discrete support, output an approximation of the\ndistribution that is as accurate as possible in $\\ell_1$ distance (i.e. total\nvariation or statistical distance). Perhaps surprisingly, it is often possible\nto \"de-noise\" the empirical distribution of the samples to return an\napproximation of the true distribution that is significantly more accurate than\nthe empirical distribution, without relying on any prior assumptions on the\ndistribution. We present an instance optimal learning algorithm which optimally\nperforms this de-noising for every distribution for which such a de-noising is\npossible. More formally, given $n$ independent draws from a distribution $p$,\nour algorithm returns a labelled vector whose expected distance from $p$ is\nequal to the minimum possible expected error that could be obtained by any\nalgorithm that knows the true unlabeled vector of probabilities of distribution\n$p$ and simply needs to assign labels, up to an additive subconstant term that\nis independent of $p$ and goes to zero as $n$ gets large. One conceptual\nimplication of this result is that for large samples, Bayesian assumptions on\nthe \"shape\" or bounds on the tail probabilities of a distribution over discrete\nsupport are not helpful for the task of learning the distribution.\nAs a consequence of our techniques, we also show that given a set of $n$\nsamples from an arbitrary distribution, one can accurately estimate the\nexpected number of distinct elements that will be observed in a sample of any\nsize up to $n \\log n$. This sort of extrapolation is practically relevant,\nparticularly to domains such as genomics where it is important to understand\nhow much more might be discovered given larger sample sizes, and we are\noptimistic that our approach is practically viable.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2211.02065",
        "title": "Finite-time Landauer principle beyond weak coupling",
        "abstract": "Landauer's principle gives a fundamental limit to the thermodynamic cost of\n erasing information. Its saturation requires a reversible isothermal process,\n and hence infinite time. We develop a finite-time version of Landauer's\n principle for a bit encoded in the occupation of a single fermionic mode, which\n can be strongly coupled to a reservoir. By solving the exact non-equilibrium\n dynamics, we optimize erasure processes (taking both the fermion's energy and\n system-bath coupling as control parameters) in the slow driving regime through\n a geometric approach to thermodynamics. We find analytic expressions for the\n thermodynamic metric and geodesic equations, which can be solved numerically.\n Their solution yields optimal processes that allow us to characterize a\n finite-time correction to Landauer's bound, fully taking into account\n non-markovian and strong coupling effects.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1410.3071",
        "title": "Instabilities, breathers and rogue waves in optics",
        "abstract": "Optical rogue waves are rare yet extreme fluctuations in the value of an\n optical field. The terminology was first used in the context of an analogy\n between pulse propagation in optical fibre and wave group propagation on deep\n water, but has since been generalized to describe many other processes in\n optics. This paper provides an overview of this field, concentrating primarily\n on propagation in optical fibre systems that exhibit nonlinear breather and\n soliton dynamics, but also discussing other optical systems where extreme\n events have been reported. Although statistical features such as long-tailed\n probability distributions are often considered the defining feature of rogue\n waves, we emphasise the underlying physical processes that drive the appearance\n of extreme optical structures.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1212.6367",
        "title": "Marian Smoluchowski: A story behind one photograph",
        "abstract": "We discuss the photograph procured from the archives of the V. Stefanyk Lviv\n National Scientific Library of Ukraine dated by 1904 which shows Marian\n Smoluchowski together with professors and graduate students of the Philosophy\n department of the Lviv University. The personalia includes both the professors\n and the graduates depicted on the photograph with the emphasis on the graduates\n as being much less known and studied. The photograph originates from the\n collection of the Shevchenko Scientific Society, therefore a brief historical\n background on the activities of physicists in this society around that period\n of time is provided as well.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0709.4265",
        "title": "Multi-symmetry and Multi-band Superconductivity in Filled-skutterudites\nPrOs$_4$Sb$_{12}$ and PrRu$_4$Sb$_{12}$",
        "abstract": "Thermal conductivity measurements were performed on single crystal samples of\n the superconducting filled skutterudite compounds PrOs$_4$Sb$_{12}$ and\n PrRu$_4$Sb$_{12}$ both as a function of temperature and magnetic field applied\n perpendicular to the heat current. In zero magnetic field, the low temperature\n electronic thermal conductivity of PrRu$_4$Sb$_{12}$ is vanishingly small,\n consistent with a fully-gapped Fermi surface. For PrOs$_4$Sb$_{12}$, however,\n we find clear evidence for residual electronic conduction as the temperature\n tends to zero Kelvin which is consistent with the presence of nodes in the\n superconducting energy gap. The field dependence of the electronic conductivity\n for both compounds shows a rapid rise immediately above H$_{c1}$ and\n significant structure over the entire vortex state. In the fully gapped\n superconductor PrRu$_4$Sb$_{12}$, this is interpreted in terms of multi-band\n effects. In PrOs$_4$Sb$_{12}$, we consider the Doppler shift of nodal\n quasiparticles at low fields and multiband effects at higher fields.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2311.18047",
        "title": "Validation of Collision Detection and Avoidance Methods for Urban Air\nMobility through Simulation",
        "abstract": "Urban Air Mobility is a new concept of regional aviation that has been\n growing in popularity as a solution to the issue of ever-increasing ground\n traffic. Electric vehicles with vertical take-off and landing capabilities are\n being developed by numerous market companies as a result of the push toward\n environmentally sustainable aviation. The next stage in the eVTOL development\n process would be to define the concept of operation of these conceptual\n aircraft and then to integrate them with the existing airspace once they are\n airborne. In addition to coordinating with conventional air traffic and other\n Urban Air Mobility (UAM) vehicles, collision avoidance with uncooperative\n airspace users has to be addressed. Birds and drones of all sizes could be\n dangerous for these low-flying aircraft. Innovative collision detection and\n avoidance techniques need to be employed due to the uncooperative nature of\n these airspace users and different performance characteristics of urban air\n mobility vehicles compared to classical fixed-wing aircraft. The aim of this\n study is to validate one such system by means of fast-time solutions. This\n system uses a decision tree and safety envelopes to prevent collisions with\n non-cooperative airspace members. The system is designed to work with different\n aircraft configurations used for Urban Air Mobility (UAM) operations. Various\n scenarios are modelled by varying intruder type, location, flight path among\n others. Changes in flight time and closest point of approach are assessed to\n evaluate the system with regard to safety and efficiency.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2001.04348",
        "title": "Leveraging Multi-Method Evaluation for Multi-Stakeholder Settings",
        "abstract": "In this paper, we focus on recommendation settings with multiple stakeholders\n with possibly varying goals and interests, and argue that a single evaluation\n method or measure is not able to evaluate all relevant aspects in such a\n complex setting. We reason that employing a multi-method evaluation, where\n multiple evaluation methods or measures are combined and integrated, allows for\n getting a richer picture and prevents blind spots in the evaluation outcome.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2004.06721",
        "title": "Daily growth rate of scientific production on Covid-19. Analysis in\ndatabases and open access repositories",
        "abstract": "The scientific community is facing one of its greatest challenges in solving\n a global health problem: COVID-19 pandemic. This situation has generated an\n unprecedented volume of publications. What is the volume, in terms of\n publications, of research on COVID-19? The general objective of this research\n work is to obtain a global vision of the daily growth of scientific production\n on COVID-19 in different databases (Dimensions, Web of Science Core Collection,\n Scopus-Elsevier, Pubmed and eight repositories). In relation to the results\n obtained, Dimensions indexes a total of 9435 publications (69% with peer review\n and 2677 preprints) well above Scopus (1568) and WoS (718). This is a classic\n biliometric phenomenon of exponential growth (R2 = 0.92). The global growth\n rate is 500 publications and the production doubles every 15 days. In the case\n of Pubmed the weekly growth is around 1000 publications. Of the eight\n repositories analysed, Pubmed Central, Medrxiv and SSRN are the leaders.\n Despite their enormous contribution, the journals continue to be the core of\n scientific communication. Finally, it has been established that three out of\n every four publications on the COVID-19 are available in open access. The\n information explosion demands a serious and coordinated response from\n information professionals, which places us at the centre of the information\n pandemic.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1505.04306",
        "title": "Search for production of vector-like quark pairs and of four top quarks\nin the lepton-plus-jets final state in $pp$ collisions at $\\sqrt{s}=8$ TeV\nwith the ATLAS detector",
        "abstract": "A search for pair production of vector-like quarks, both up-type ($T$) and\n down-type ($B$), as well as for four-top-quark production, is presented. The\n search is based on $pp$ collisions at $\\sqrt{s}=8$ TeV recorded in 2012 with\n the ATLAS detector at the CERN Large Hadron Collider and corresponding to an\n integrated luminosity of 20.3 fb$^{-1}$. Data are analysed in the\n lepton-plus-jets final state, characterised by an isolated electron or muon\n with high transverse momentum, large missing transverse momentum and multiple\n jets. Dedicated analyses are performed targeting three cases: a $T$ quark with\n significant branching ratio to a $W$ boson and a $b$-quark ($T\\bar{T} \\to\n Wb$+X), and both a $T$ quark and a $B$ quark with significant branching ratio\n to a Higgs boson and a third-generation quark ($T\\bar{T} \\to Ht$+X and\n $B\\bar{B} \\to Hb$+X respectively). No significant excess of events above the\n Standard Model expectation is observed, and 95% CL lower limits are derived on\n the masses of the vector-like $T$ and $B$ quarks under several branching ratio\n hypotheses assuming contributions from $T \\to Wb$, $Zt$, $Ht$ and $B \\to Wt$,\n $Zb$, $Hb$ decays. The 95% CL observed lower limits on the $T$ quark mass range\n between 715 GeV and 950 GeV for all possible values of the branching ratios\n into the three decay modes, and are the most stringent constraints to date.\n Additionally, the most restrictive upper bounds on four-top-quark production\n are set in a number of new physics scenarios.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1311.0913",
        "title": "Bidding Games and Efficient Allocations",
        "abstract": "Richman games are zero-sum games, where in each turn players bid in order to\ndetermine who will play next [Lazarus et al.'99]. We extend the theory to\nimpartial general-sum two player games called \\emph{bidding games}, showing the\nexistence of pure subgame-perfect equilibria (PSPE). In particular, we show\nthat PSPEs form a semilattice, with a unique and natural \\emph{Bottom\nEquilibrium}.\nOur main result shows that if only two actions available to the players in\neach node, then the Bottom Equilibrium has additional properties: (a) utilities\nare monotone in budget; (b) every outcome is Pareto-efficient; and (c) any\nPareto-efficient outcome is attained for some budget.\nIn the context of combinatorial bargaining, we show that a player with a\nfraction of X% of the total budget prefers her allocation to X% of the possible\nallocations. In addition, we provide a polynomial-time algorithm to compute the\nBottom Equilibrium of a binary bidding game.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1305.4270",
        "title": "On the equivalence of several definitions of compact infra-solvmanifolds",
        "abstract": "We show the equivalence of several definitions of compact infra-solvmanifolds\n that appear in various math literatures.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1504.05783",
        "title": "Automated parameters for troubled-cell indicators using outlier\ndetection",
        "abstract": "In Vuik and Ryan (2014) we studied the use of troubled-cell indicators for\n discontinuity detection in nonlinear hyperbolic partial differential equations\n and introduced a new multiwavelet technique to detect troubled cells. We found\n that these methods perform well as long as a suitable, problem-dependent\n parameter is chosen. This parameter is used in a threshold which decides\n whether or not to detect an element as a troubled cell. Until now, these\n parameters could not be chosen automatically. The choice of the parameter has\n impact on the approximation: it determines the strictness of the troubled-cell\n indicator. An inappropriate choice of the parameter will result in detection\n (and limiting) of too few or too many elements. The optimal parameter is chosen\n such that the minimal number of troubled cells is detected and the resulting\n approximation is free of spurious oscillations. In this paper we will see that\n for each troubled-cell indicator the sudden increase or decrease of the\n indicator value with respect to the neighboring values is important for\n detection. Indication basically reduces to detecting the outliers of a vector\n (one dimension) or matrix (two dimensions). This is done using Tukey's boxplot\n approach to detect which coefficients in a vector are straying far beyond\n others (Tukey, 1977). We provide an algorithm that can be applied to various\n troubled-cell indication variables. Using this technique the problem-dependent\n parameter that the original indicator requires is no longer necessary as the\n parameter will be chosen automatically.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0009207",
        "title": "The Steinhaus tiling problem and the range of certain quadratic forms",
        "abstract": "We give a short proof of the fact that there are no measurable subsets of\n Euclidean space (in dimension d > 2), which, no matter how translated and\n rotated, always contain exactly one integer lattice point. In dimension d=2\n (the original Steinhaus problem) the question remains open.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2312.04579",
        "title": "zkDFL: An efficient and privacy-preserving decentralized federated\nlearning with zero-knowledge proof",
        "abstract": "Federated learning (FL) has been widely adopted in various fields of study\n and business. Traditional centralized FL systems suffer from serious issues. To\n address these concerns, decentralized federated learning (DFL) systems have\n been introduced in recent years. With the help of blockchains, they attempt to\n achieve more integrity and efficiency. However, privacy preservation remains an\n uncovered aspect of these systems. To tackle this, as well as to scale the\n blockchain-based computations, we propose a zero-knowledge proof (ZKP)-based\n aggregator (zkDFL). This allows clients to share their large-scale model\n parameters with a trusted centralized server without revealing their individual\n data to other clients. We utilize blockchain technology to manage the\n aggregation algorithm via smart contracts. The server performs a ZKP algorithm\n to prove to the clients that the aggregation is done according to the accepted\n algorithm. Additionally, the server can prove that all inputs from clients have\n been used. We evaluate our approach using a public dataset related to the\n wearable Internet of Things. As demonstrated by numerical evaluations, zkDFL\n introduces verifiability of the correctness of the aggregation process and\n enhances the privacy protection and scalability of DFL systems, while the gas\n cost has significantly declined.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1912.00787",
        "title": "Fluctuations of the Gromov-Prohorov sample model",
        "abstract": "In this paper, we study the fluctuations of observables of metric measure\n spaces which are random discrete approximations $X_n$ of a fixed arbitrary\n (complete, separable) metric measure space $X=(\\mathcal{X},d,\\mu)$. These\n observables $\\Phi(X_n)$ are polynomials in the sense of\n Greven-Pfaffelhuber-Winter, and we show that for a generic model space $X$,\n they yield asymptotically normal random variables. However, if $X$ is a compact\n homogeneous space, then the fluctuations of the observables are much smaller,\n and after an adequate rescaling, they converge towards probability\n distributions which are not Gaussian. Conversely, we prove that if all the\n fluctuations of the observables $\\Phi(X_n)$ are smaller than in the generic\n case, then the measure metric space $X$ is compact homogeneous. The proofs of\n these results rely on the Gromov reconstruction principle, and on an adaptation\n of the method of cumulants and mod-Gaussian convergence developed by\n F\\'eray-M\\'eliot-Nikeghbali.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1604.07677",
        "title": "Interplanetary Type IV Bursts",
        "abstract": "We study the characteristics of moving type IV radio bursts which extend to\n the hectometric wavelengths (interplanetary type IV or type IV IP bursts) and\n their relationship with energetic phenomena on the Sun. Our dataset comprises\n 48 interplanetary type IV bursts observed with Wind/WAVES in the 13.825 MHz-20\n kHz frequency range. The dynamic spectra of the RSTN, the Nancay Decametric\n Array (DAM), the ARTEMIS-IV, the Culgoora, Hiraiso and Izmiran Radio\n Spectrographs were used to track the evolution of the events in the low corona.\n These were supplemented with SXR flux data from the GOES and CME data from the\n SOHO/LASCO. Positional information of the coronal bursts was obtained by the\n NRH. We examined the relationship of the type IV events with coronal radio\n bursts, CMEs and SXR flares. The majority of the events (45) were characterized\n as compact; their duration was on average 106 minutes. This type of events was,\n mostly, associated with M-and X-class flares (40 out of 45) and fast CMEs; 32\n of these events had CMEs faster than 1000 km/s. Furthermore, in 43 compact\n events the CME was, possibly, subjected to reduced aerodynamic drag as it was\n propagating in the wake of a previous CME. A minority (three) of long lived\n type IV IP bursts was detected, with duration from 960 minutes to 115 hours.\n These events are referred to as extended or long duration and appear to\n replenish their energetic electron content, possibly from electrons escaping\n from the corresponding coronal type IV bursts. The latter were found to persist\n on the disk, for tens of hours to days. Prominent among them was the unusual\n interplanetary type IV burst of 18--23 May 2002, which is the longest event in\n the Wind/WAVES catalog. The three extended events were, usually, accompanied by\n a number of flares, of GOES class C in their majority, and of CMEs, many of\n which were slow and narrow.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.06020",
        "title": "Hurwitz moduli varieties parameterizing Galois covers of an algebraic\ncurve",
        "abstract": "Given a smooth, projective curve $Y$, a finite group $G$ and a positive\n integer $n$ we study smooth, proper families $X\\to Y\\times S\\to S$ of Galois\n covers of $Y$ with Galois group isomorphic to $G$ branched in $n$ points,\n parameterized by algebraic varieties $S$. When $G$ is with trivial center we\n prove that the Hurwitz space $H^G_n(Y)$ is a fine moduli variety for this\n moduli problem and construct explicitly the universal family. For arbitrary $G$\n we prove that $H^G_n(Y)$ is a coarse moduli variety. For families of pointed\n Galois covers of $(Y,y_0)$ we prove that the Hurwitz space $H^G_n(Y,y_0)$ is a\n fine moduli variety, and construct explicitly the universal family, for\n arbitrary group $G$. We use classical tools of algebraic topology and of\n complex algebraic geometry.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0709.0047",
        "title": "Quantum Informational Dark Energy: Dark energy from forgetting",
        "abstract": "We suggest that dark energy has a quantum informational origin. Landauer's\n principle associated with the erasure of quantum information at a cosmic\n horizon implies the non-zero vacuum energy having effective negative pressure.\n Assuming the holographic principle, the minimum free energy condition, and the\n Gibbons-Hawking temperature for the cosmic event horizon we obtain the\n holographic dark energy with the parameter $d\\simeq 1$, which is consistent\n with the current observational data. It is also shown that both the\n entanglement energy and the horizon energy can be related to Landauer's\n principle.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2108.10562",
        "title": "A LOFAR-uGMRT spectral index study of distant radio halos",
        "abstract": "Context. Radio halos are megaparsec-scale diffuse radio sources{ mostly}\n located at the centres of merging galaxy clusters. The common mechanism invoked\n to explain their origin is the re-acceleration of relativistic particles caused\n by large-scale turbulence. Aims. Current re-acceleration models predict that a\n significant number of halos at high redshift should be characterised by very\n steep spectra ($\\alpha<-1.5$) because of increasing inverse Compton energy\n losses. In this paper, we investigate the spectral index properties of a sample\n of nine clusters selected from the second Planck Sunyaev-Zel'dovich catalogue\n showing diffuse radio emission with the Low Frequency Array (LOFAR) in the\n 120-168 MHz band. This is the first time that radio halos discovered at low\n frequencies are followed up at higher frequencies. Methods. We analysed\n upgraded Giant Metrewave Radio Telescope (uGMRT) observations in Bands 3 and 4,\n that is, 250-500 and 550-900 MHz respectively. These observations were combined\n with existing LOFAR data to obtain information on the spectral properties of\n the diffuse radio emission. Results. We find diffuse radio emission in the\n uGMRT observations for five of the nine high-$z$ radio halos previously\n discovered with LOFAR. For those, we measure spectral indices in the range of\n $-1$ to $-1.4$. For the uGMRT non-detections, we estimated that the halos\n should have a spectral index steeper than $-1.5$. We also confirm the presence\n of one candidate relic. Conclusions. Despite the small number of clusters, we\n find evidence that about half of the massive and merging clusters at high\n redshift host radio halos with a very steep spectrum. This is in line with\n theoretical predictions, although larger statistical samples are necessary to\n test models.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2404.04881",
        "title": "Constraining Axion-Like Particles Dark Matter in Coma Berenices with\nFAST",
        "abstract": "Axions and axion-like particles (ALPs) appear in many extensions of the\n Standard Model and are being investigated as promising dark matter (DM)\n candidates. One viable methodology for their detection involves the\n investigation of the line-like radio emissions from the dwarf spheroidal\n galaxy, potentially originating from the radiative decay of ALPs or the\n conversion of ALPs in the magnetic field. In this work, we constrain the\n properties of ALPs using the 2-hour radio observation of Coma Berenices through\n the Five-hundred-meter Aperture Spherical radio Telescope (FAST). The $\\rm\n 95\\%$ upper limits of the ALP-photon coupling are calculated for the ALP decay\n and conversion scenarios, respectively. Note that the sensitive ALP masses for\n FAST range from $\\sim \\mu \\rm eV$ to tens of $\\mu \\rm eV$, where ALP can\n explain the DM abundance naturally. However, our limits are weaker than those\n of the CAST helioscope, which can provide an independent and complementary\n check on the ALP non-detection for ground experiments. Furthermore, we evaluate\n the expected sensitivity on the ALP of FAST with its full designed bandwidth\n (70 $\\rm MHz$ - 3 $\\rm GHz$) for 100 hours of observation time. Our results\n indicate that, even with the exceptional sensitivity of the FAST, it is\n challenging to surpass the existing experimental constraints on ALP DM using\n radio observation of dSphs, unless the possible enhancements of ALP signals by\n compact stars in dSphs are considered.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1503.08967",
        "title": "Authenticated Semi-quantum Direct Communication Protocols using Bell\nStates",
        "abstract": "This study presents the first two authenticated semi-quantum direct\n communication (ASQDC) protocols without using any classical channel. By\n pre-sharing the master secret key between two communicants, a sender with\n advanced quantum devices can transmit a secret message to a receiver who can\n only perform classical operations without any information leakage. The receiver\n is then capable of verifying the message up to the single qubit level, i.e., a\n one-qubit modification of the transmitted quantum sequence can be detected with\n a probability close to 1. Moreover, the proposed protocols are resistant to\n several well-known attacks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ph/9605245",
        "title": "Production of scalar $K\\bar K$ molecules in $\\phi$ radiative decays",
        "abstract": "The potentialities of the production of the scalar $K\\bar K$ molecules in the\n $\\phi$ radiative decays are considered beyond the narrow resonance width\n approximation. It is shown that $BR(\\phi\\rightarrow\\gamma\n f_0(a_0)\\rightarrow\\gamma\\pi\\pi(\\pi\\eta))\\approx (1\\div 2)\\times 10^{-5}\\\n ,\\BR(\\phi\\rightarrow\\gamma (f_0+a_0)\\rightarrow\\gamma K^+K^-)\\alt 10^{-6}$ and\n $BR(\\phi\\rightarrow\\gamma (f_0+a_0) \\to \\gamma K^0\\bar K^0)\\alt 10^{-8}$. The\n mass spectra in the $\\pi\\pi\\ ,\\ \\pi\\eta\\ ,\\ K^+K^-\\ ,\\ K^0\\bar K^0$ channels\n are calculated. The imaginary part of the amplitude $\\phi\\rightarrow\\gamma\n f_0(a_0)$ is calculated analytically. It is obtained the phase of the scalar\n resonance production amplitude that causes the interference patterns in the\n reaction $e^+e^-\\rightarrow\\gamma \\pi^+\\pi^-$ in the $\\phi$ meson mass region.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2008.09962",
        "title": "On the number of distinct roots of a lacunary polynomial over finite\nfields",
        "abstract": "We obtain new upper bounds on the number of distinct roots of lacunary\n polynomials over finite fields. Our focus will be on polynomials for which\n there is a large gap between consecutive exponents in the monomial expansion.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1806.01238",
        "title": "Center-Outward Distribution Functions, Quantiles, Ranks, and Signs in\n$\\mathbb{R}^d$",
        "abstract": "Univariate concepts as quantile and distribution functions involving ranks\nand signs, do not canonically extend to $\\mathbb{R}^d, d\\geq 2$. Palliating\nthat has generated an abundant literature. Chapter 1 shows that, unlike the\nmany definitions that have been proposed so far, the measure\ntransportation-based ones introduced in Chernozhukov et al. (2017) enjoy all\nthe properties that make univariate quantiles and ranks successful tools for\nsemiparametric statistical inference.\nWe therefore propose a new center-outward definition of multivariate\ndistribution and quantile functions, along with their empirical counterparts,\nfor which we obtain a Glivenko-Cantelli result. Our approach is geometric and,\ncontrary to the Monge-Kantorovich one in Chernozhukov et al. (2017), does not\nrequire any moment assumptions. The resulting ranks and signs are strictly\ndistribution-free, and maximal invariant under the action of a data-driven\nclass of (order-preserving) transformations generating the family of absolutely\ncontinuous distributions; that property is the theoretical foundation of the\nsemiparametric efficiency preservation property of ranks. The corresponding\nquantiles are equivariant under the same transformations.\nThe empirical proposed distribution functions are defined at observed values\nonly. A continuous extension to the entire $\\mathbb{R}^d$, yielding continuous\nempirical quantile contours while preserving the monotonicity and\nGlivenko-Cantelli features is desirable. Such extension requires solving a\nnontrivial problem of smooth interpolation under cyclical monotonicity\nconstraints. A complete solution of that problem is given in Chapter 2; we show\nthat the resulting distribution and quantile functions are Lipschitz, and\nprovide a sharp lower bound for the Lipschitz constants. A numerical study of\nempirical center-outward quantile contours and their consistency is conducted.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0211582",
        "title": "Quantum states and optics in a {\\it p}-type heterojunction with lateral\nsurface quantum dot or antidot superlattice subjected to perpendicular\nmagnetic field",
        "abstract": "The studies of quantum states and optics in a {\\it p}-type heterojunction\n with lateral surface quantum dot (antidot) superlattice and in the presence of\n perpendicular magnetic field are performed. For the first time the Azbel'--\n Hofstadter problem is solved for holes in a complicated valence band described\n by the $4 \\times 4$ Luttinger Hamiltonian. The set of magnetic subbands is\n obtained for separate hole levels in a wide interval of magnetic field. We\n found remarkable differences between hole spectra and the Hofstadter\n \"butterfly\" for electrons. The influence of the spin-orbit interaction onto\n wavefunctions and energy spectrum has been investigated. The probabilities of\n optical transitions between quantum states in the valence band and donors\n located in the monolayer inside the heterojunction are calculated. The set of\n parameters (superlattice periods, amplitude of periodic potential, magnitude of\n magnetic field, etc.) required for experimental observation of splitted hole\n Landau levels is determined.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1505.05336",
        "title": "Investigating Earth shadowing effect with DAMA/LIBRA-phase1",
        "abstract": "In the present paper the results obtained in the investigation of possible\n diurnal effects for low-energy single-hit scintillation events of\n DAMA/LIBRA-phase1 (1.04 ton $\\times$ yr exposure) have been analysed in terms\n of an effect expected in case of Dark Matter (DM) candidates inducing nuclear\n recoils and having high cross-section with ordinary matter, which implies low\n DM local density in order to fulfill the DAMA/LIBRA DM annual modulation\n results. This effect is due to the different Earth depths crossed by those DM\n candidates during the sidereal day.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2003.12895",
        "title": "Memorizing Gaussians with no over-parameterizaion via gradient decent on\nneural networks",
        "abstract": "We prove that a single step of gradient decent over depth two network, with\n $q$ hidden neurons, starting from orthogonal initialization, can memorize\n $\\Omega\\left(\\frac{dq}{\\log^4(d)}\\right)$ independent and randomly labeled\n Gaussians in $\\mathbb{R}^d$. The result is valid for a large class of\n activation functions, which includes the absolute value.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2107.00752",
        "title": "Global embedding of BTZ spacetime using generalized method of symmetric\nembeddings construction",
        "abstract": "It is often easier to study pseudo-Riemannian manifolds by presenting them as\n surfaces in some ambient space. We propose an algorithm for construction of\n explicit isometric embeddings of pseudo-Riemannian manifolds with symmetries\n into an ambient space of higher dimension. While most of the existing methods\n are based on Gauss-Codazzi-Mainardi-Peterson equations, we do not use them and\n instead concentrate on a system of equations which connects the metric on the\n manifold and the embedding function of the surface. Our algorithm is based on\n the group theoretical method of separation of variables that we developed\n earlier (arXiv:1202.1204). The algorithm makes this method more convenient and\n simple to use. It allowed us to simplify the construction of many known\n embeddings as well as obtain some new ones. In particular, we obtain explicit\n global embeddings of spinning BTZ black hole in 7-dimensional flat space.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1503.09031",
        "title": "Optimal Actuator and Observation Location for Time-Varying Systems on a\nFinite-Time Horizon",
        "abstract": "The choice of the location of controllers and observations is of great\n importance for designing control systems and improving the estimations in\n various practical problems. For time-varying systems in Hilbert spaces, the\n existence and convergence of the optimal location based on linear-quadratic\n control on a finite-time horizon is studied. The optimal location of\n observations for improving the estimation of the state at the final time, based\n on Kalman filter, is considered as the dual problem to the LQoptimal problem of\n the control locations. Further, the existence and convergence of optimal\n locations of observations for improving the estimation at the initial time,\n based on Kalman smoother is discussed. The obtained results are applied to a\n linear advection-diffusion model.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1004.0657",
        "title": "A very young component in the pre-eminent starburst region of the Small\nMagellanic Cloud",
        "abstract": "We present a study of the compact H II region N66A in the SMC pre-eminent\n starburst region N66/NGC346. Despite extensive research on various components\n of the N66/NGC346 complex, few studies have so far focused on N66A, which is a\n special object in the whole complex and therefore deserves scrutiny. The study\n of this compact H II region and its fellow objects seems important in the\n framework of massive star formation in the Magellanic Clouds. This analysis is\n based mainly on our optical ESO NTT observations, both imaging and\n spectroscopy, coupled with archive HST ACS data and Spitzer infrared images\n (IRAC 3.6, 4.5, 5.8, and 8.0 microns). We derive a number of physical\n characteristics of the compact H II region N66A. Moreover, we present the\n spectral classification of the main exciting star of N66A for the first time\n using spectroscopy. Its spectral features indicate a main sequence massive star\n of type O8. We compare this result with that based on the stellar Lyman\n continuum flux estimated from the ionized gas H-beta flux. The compact H II\n region belongs to a rare class of H II regions in the Magellanic Clouds, called\n High-excitation Blobs (HEBs). N66A most probably represents a very young\n massive star formation event in the N66 complex, which has a range of ages.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2001.08063",
        "title": "Algorithms for Tensor Network Contraction Ordering",
        "abstract": "Contracting tensor networks is often computationally demanding. Well-designed\n contraction sequences can dramatically reduce the contraction cost. We explore\n the performance of simulated annealing and genetic algorithms, two common\n discrete optimization techniques, to this ordering problem. We benchmark their\n performance as well as that of the commonly-used greedy search on physically\n relevant tensor networks. Where computationally feasible, we also compare them\n with the optimal contraction sequence obtained by an exhaustive search. We find\n that the algorithms we consider consistently outperform a greedy search given\n equal computational resources, with an advantage that scales with tensor\n network size. We compare the obtained contraction sequences and identify signs\n of highly non-local optimization, with the more sophisticated algorithms\n sacrificing run-time early in the contraction for better overall performance.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/0301146",
        "title": "A covariant formalism for Chern-Simons gravity",
        "abstract": "Chern--Simons type Lagrangians in $d=3$ dimensions are analyzed from the\n point of view of their covariance and globality. We use the transgression\n formula to find out a new fully covariant and global Lagrangian for\n Chern--Simons gravity: the price for establishing globality is hidden in a\n bimetric (or biconnection) structure. Such a formulation allows to calculate\n from a global and simpler viewpoint the energy-momentum complex and the\n superpotential both for Yang--Mills and gravitational examples.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2304.08710",
        "title": "Quantum Algorithm for Unsupervised Anomaly Detection",
        "abstract": "Anomaly detection, an important branch of machine learning, plays a critical\n role in fraud detection, health care, intrusion detection, military\n surveillance, etc. As one of the most commonly used unsupervised anomaly\n detection algorithms, the Local Outlier Factor algorithm (LOF algorithm) has\n been extensively studied. This algorithm contains three steps, i.e.,\n determining the k-distance neighborhood for each data point x, computing the\n local reachability density of x, and calculating the local outlier factor of x\n to judge whether x is abnormal. The LOF algorithm is computationally expensive\n when processing big data sets. Here we present a quantum LOF algorithm\n consisting of three parts corresponding to the classical algorithm.\n Specifically, the k-distance neighborhood of x is determined by amplitude\n estimation and minimum search; the local reachability density of each data\n point is calculated in parallel based on the quantum multiply-adder; the local\n outlier factor of each data point is obtained in parallel using amplitude\n estimation. It is shown that our quantum algorithm achieves exponential speedup\n on the dimension of the data points and polynomial speedup on the number of\n data points compared to its classical counterpart. This work demonstrates the\n advantage of quantum computing in unsupervised anomaly detection.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2306.04191",
        "title": "Classification of maximally non-self-dual modular categories of small\ndimension",
        "abstract": "We prove that a non-pointed maximally non-self-dual (MNSD) modular category\n of Frobenius-Perron (FP) dimension less than $2025$ has at most two possible\n types, and all these types can be realized except those of FP dimension $675$,\n $729$ and $1125$. We also prove that all these modular categories are\n group-theoretical except the modular categories of dimension $675$. Our result\n shows that a non-group-theoretical MNSD modular category of smallest FP\n dimension may be the category of FP dimension $675$, and non-pointed MNSD\n modular category of smallest FP dimension is the category of FP dimension\n $243$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2009.13483",
        "title": "Accidental and symmetry-protected bound states in the continuum in\nplanar photonic-crystal structures, studied by the resonant-state expansion",
        "abstract": "The resonant-state expansion (RSE) provides a precise and computationally\n cheap tool to find resonant states in complex systems using the optical modes\n of a simpler system as a basis. We apply the RSE to a photonic crystal slab in\n order to identify and analyze its bound states in the continuum (BICs). We show\n that the RSE is a useful and reliable method for not only finding the BICs but\n also for differentiating between accidental and symmetry-protected BICs, as\n well as for understanding their formation from the basis modes and evolution\n with structural and material parameters of the system. The high efficiency of\n the RSE allows us to track the properties of BICs and other high-quality\n optical modes, covering the full parameter space of the system in a reasonable\n time frame.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0910.2784",
        "title": "Anomaly at finite density and chiral fermions on lattice",
        "abstract": "Using both perturbation theory in the Euclidean formalism as well as the\n non-perturbative Fujikawa's method, we verify that the chiral anomaly equation\n remains unaffected in continuum QCD in the presence of nonzero chemical\n potential, \\mu. We extend our considerations to lattice fermions with exact\n chiral symmetry and discuss the consequences for the recent Bloch-Wettig\n proposal for the Dirac operator at finite chemical potential. We propose a new\n simpler method of incorporating \\mu.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0307035",
        "title": "Elementary modifications and line configurations in P^2",
        "abstract": "Associated to an arrangement of projective hyperplanes A is the module D(A),\n which consists of derivations tangent to A. We study D(A) when A is a\n configuration of lines in the projective plane. In this setting, we relate the\n deletion/restriction construction used in the study of hyperplane arrangements\n to elementary modifications of bundles. This allows us to obtain bounds on the\n Castelnuovo-Mumford regularity of D(A). We also give simple combinatorial\n conditions for the associated bundle to be stable, and describe its jump lines.\n These regularity bounds and stability considerations impose constraints on\n Terao's conjecture.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/quant-ph/0207096",
        "title": "Measurement of qutrits",
        "abstract": "We proposed the procedure of measuring the unknown state of the three-level\n system - the qutrit, which was realized as the arbitrary polarization state of\n the single-mode biphoton field. This procedure is accomplished for the set of\n the pure states of qutrits; this set is defined by the properties of SU(2)\n transformations, that are done by the polarization transformers.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1101.4281",
        "title": "On Why-Questions in Physics",
        "abstract": "The aim of this paper is to introduce a mathematical logic based approach\n investigating why-type questions in physics.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1804.02594",
        "title": "Causal limit on quantum communication",
        "abstract": "The capacity of a channel is known to be equivalent to the highest rate at\n which it can generate entanglement. Analogous to entanglement, the notion of a\n causality measure characterises the temporal aspect of quantum correlations.\n Despite holding an equally fundamental role in physics, temporal quantum\n correlations have yet to find their operational significance in quantum\n communication. Here we uncover a connection between quantum causality and\n channel capacity. We show the amount of temporal correlations between two ends\n of the noisy quantum channel, as quantified by a causality measure, implies a\n general upper bound on its channel capacity. The expression of this new bound\n is simpler to evaluate than most previously known bounds. We demonstrate the\n utility of this bound by applying it to a class of shifted depolarizing\n channels, which results in improvement over previously calculated bounds for\n this class of channels.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.10399",
        "title": "Multilingual Normalization of Temporal Expressions with Masked Language\nModels",
        "abstract": "The detection and normalization of temporal expressions is an important task\n and preprocessing step for many applications. However, prior work on\n normalization is rule-based, which severely limits the applicability in\n real-world multilingual settings, due to the costly creation of new rules. We\n propose a novel neural method for normalizing temporal expressions based on\n masked language modeling. Our multilingual method outperforms prior rule-based\n systems in many languages, and in particular, for low-resource languages with\n performance improvements of up to 33 F1 on average compared to the state of the\n art.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2310.18519",
        "title": "Practical trainable temporal post-processor for multi-state quantum\nmeasurement",
        "abstract": "We develop and demonstrate a trainable temporal post-processor (TPP)\n harnessing a simple but versatile machine learning algorithm to provide optimal\n processing of quantum measurement data subject to arbitrary noise processes,\n for the readout of an arbitrary number of quantum states. We demonstrate the\n TPP on the essential task of qubit state readout, which has historically relied\n on temporal processing via matched filters in spite of their applicability only\n for specific noise conditions. Our results show that the TPP can reliably\n outperform standard filtering approaches under complex readout conditions, such\n as high power readout. Using simulations of quantum measurement noise sources,\n we show that this advantage relies on the TPP's ability to learn optimal linear\n filters that account for general quantum noise correlations in data, such as\n those due to quantum jumps, or correlated noise added by a phase-preserving\n quantum amplifier. We show that the transformation described by the TPP can be\n expressed via an efficient semi-analytic form, providing a linearly-scaling\n generalization of matched filtering to an arbitrary number of states under the\n most general noise conditions of the readout signal emanating from the\n measurement chain. The TPP can be efficiently, autonomously, and reliably\n trained on measurement data, and requires only linear operations, making it\n ideal for FPGA implementations in cQED for real-time processing of measurement\n data from general quantum systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2310.19954",
        "title": "Dynamic, viscoelasticity-driven shape change of elastomer bilayers",
        "abstract": "Thin bilayers made of elastic sheets with different strain recoveries can be\n used for dynamic shape morphing through ambient stimuli, such as temperature,\n mass diffusion, and light. As a fundamentally different approach to designing\n temporal shape change, constituent polymer molecular features (rather than\n external fields) are leveraged, specifically the viscoelasticity of gelatin\n bilayers, to achieve dynamic three-dimensional (3D) curls and helical twists.\n After stretching and releasing, the acquired 3D shape recovers its original\n flat shape on a timescale originating from the polymer viscoelasticity. The\n bilayer time-dependent curvature can be accurately predicted from hyperelastic\n and viscoelastic functions using finite element analysis (FEA). FEA reveals the\n nonlinear shape dynamics in space and time in quantitative agreement with\n experiments. The findings present a new frontier in dynamic biomimetic\n shape-morphing by exploiting intrinsic material properties in contrast with\n state-of-the-art methods relying on external field variations, moving one step\n closer to acquiring autonomous shape-shifting capabilities of biological\n systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1907.07633",
        "title": "A Database of Groups with Equivalent Character Tables",
        "abstract": "Two groups are said to have the same character table if a permutation of the\n rows and a permutation of the columns of one table produces the other table.\n The problem of determining when two groups have the same character table is\n computationally intriguing. We have constructed a database containing for all\n finite groups of order less than 2000 (excluding those of order 1024), a\n partitioning of groups into classes having the same character table. To handle\n the 408,641,062 groups of order 1536 and other orders with a large number of\n groups we utilized high-throughput computing together with a new algorithmic\n approach to the problem. Our approach involved using graph isomorphism software\n to construct canoncial graphs that correspond to the character table of a group\n and then hashing the graphs.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1511.02208",
        "title": "The HI supershell GS 118+01-44 and its role in the interstellar medium",
        "abstract": "We carry out a multiwavelength study to characterize the HI supershell\n designated GS 118+01-44, and to analyse its possible origin. A multiwavlength\n study has been carried out to study the supershell and its environs. We\n performed an analysis of the HI, CO, radio continuum, and infrared emission\n distributions. The Canadian Galactic Plane Survey (CGPS) HI data reveals that\n GS 118+01-44 is centred at (l, b) = (117.7, 1.4) with a systemic velocity of\n -44.3 km/s. According to Galactic rotation models this structure is located at\n 3.0 +- 0.6 kpc from the Sun. There are several HII regions and three supernova\n remnants (SNRs) catalogued in the region. On the other hand, the analysis of\n the temperature spectral index distribution shows that in the region there is a\n predominance of non-thermal emission. Infrared emission shows that cool\n temperatures dominate the area of the supershell. Concerning the origin of the\n structure, we found that even though several OB stars belonging to Cas OB5 are\n located in the interior of GS 118+01-44, an analysis of the energy injected by\n these stars through their stellar winds indicates that they do not have\n sufficient energy to create GS 118+01-44. Therefore, an additional energy\n source is needed to explain the genesis of GS 118+01-44. On the other hand, the\n presence of several HII regions and young stellar object candidates in the\n edges of GS 118+01-44 shows that the region is still active in forming new\n stars.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1510.05465",
        "title": "BEER analysis of Kepler and CoRoT light curves: IV. Discovery of four\nnew low-mass white dwarf companions in the Kepler data",
        "abstract": "We report the discovery of four short-period eclipsing systems in the Kepler\n light curves, consisting of an A-star primary and a low-mass white dwarf (WD)\n secondary (dA+WD) - KIC 4169521, KOI-3818, KIC 2851474, and KIC 9285587. The\n systems show BEaming, Ellipsoidal and Reflection (BEER) phase modulations\n together with primary and secondary eclipses. These add to the 6 Kepler and 18\n WASP short-period eclipsing dA+WD binaries that were previously known. The\n light curves, together with follow-up spectroscopic observations, allow us to\n derive the masses, radii, and effective temperatures of the two components of\n the four systems. The orbital periods, of 1.17-3.82 days, and WD masses, of\n 0.19-0.22 Msun, are similar to those of the previously known systems. The WD\n radii of KOI-3818, KIC 2851474, and KIC 9285587 are 0.026, 0.035, and 0.026\n Rsun, respectively, the smallest WD radii derived so far for short-period\n eclipsing dA+WD binaries. These three binaries extend the previously known\n population to older systems with cooler and smaller WD secondaries. KOI-3818\n displays evidence for a fast-rotating primary and a minute but significant\n eccentricity of ~0.0015. These features are probably the outcome of the\n mass-transfer process.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2104.06479",
        "title": "Primordial giant planet obliquity driven by a circumplanetary disk",
        "abstract": "Detached circumplanetary disks are unstable to tilting as a result of the\n stellar tidal potential. We examine how a tilted circumplanetary disk affects\n the evolution of the spin axis of an oblate planet. The disk is evolved using\n time-dependent equations for linear wave-like warp evolution, including terms\n representing the effect of the tidal potential and planetary oblateness. For a\n disk with a sufficiently large mass, we find that the planet spin quickly\n aligns to the misaligned disk. The tilt of the planetary spin axis then\n increases on the same timescale as the disk. This can be an efficient mechanism\n for generating primordial obliquity in giant planets. We suggest that directly\n imaged exoplanets at large orbital radii, where the disk mass criterion is more\n likely to be satisfied, could have significant obliquities due to the tilt\n instability of their circumplanetary disks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/gr-qc/0702131",
        "title": "Focusing of branes in warped backgrounds",
        "abstract": "Branes are embedded surfaces in a given background (bulk) spacetime. Assuming\n a warped bulk, we investigate, in analogy with the case for geodesics, the\n notion of {\\em focusing} of families of such embedded, extremal 3--branes in a\n five dimensional background . The essential tool behind our analysis, is the\n well-known generalised Raychaudhuri equations for surface congruences. In\n particular, we find explicit solutions of these equations, which seem to show\n that families of 3--branes can focus along lower dimensional submanifolds\n depending on where the initial expansions are specified. We conclude with\n comments on the results obtained and possibilities about future work along\n similar lines.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1508.06577",
        "title": "BMS invariance and the membrane paradigm",
        "abstract": "The Bondi-van der Burg-Metzner-Sachs (BMS) group is the asymptotic symmetry\n group of asymptotically flat spacetime. It is infinite dimensional and entails\n an infinite number of conservation laws. According to the black hole membrane\n paradigm, null infinity (in asymptotically flat spacetime) and black hole event\n horizons behave like fluid membranes. The fluid dynamics of the membrane is\n governed by an infinite set of symmetries and conservation laws. Our main\n result is to point out that the infinite set of symmetries and conserved\n charges of the BMS group and the membrane paradigm are the same. This\n relationship has several consequences. First, it sheds light on the physical\n interpretation of BMS conservation laws. Second, it generalizes the BMS\n conservation laws to arbitrary subregions of arbitrary null surfaces. Third, it\n clarifies the identification of the superrotation subgroup of the BMS group. We\n briefly comment on the black hole information problem.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/9807153",
        "title": "Effects of dimerization and interchain one-particle hopping in a weakly\ncoupled dimerized chain system at quarter filling",
        "abstract": "Effects of the intrachain dimerization and the interchain one-particle\n hopping, $t_{b}$, in a quasi-one-dimensional dimerized chain system at quarter\n filling have been studied, based on the perturbative renormalization group\n (PRG) approach. Based on the results, we discuss difference in the low-energy\n properties between TMTTF and TMTSF compounds.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1310.3680",
        "title": "Magnetic fields during the formation of supermassive black holes",
        "abstract": "Observations of quasars at $\\rm z> 6$ report the existence of a billion solar\n mass black holes. Comprehending their formation in such a short time scale is a\n matter of ongoing research. One of the most promising scenarios to assemble\n supermassive black holes is a monolithic collapse of protogalactic gas clouds\n in atomic cooling halos with $\\rm T_{vir} \\geq 10^{4} K$. In this article, we\n study the amplification and impact of magnetic fields during the formation of\n seed black holes in massive primordial halos. We perform high resolution\n cosmological magnetohydrodynamics simulations for four distinct halos and\n follow their collapse for a few free-fall times until the simulations reach a\n peak density of $\\rm 7 \\times 10^{-10} g/cm^{3}$. Our findings show that\n irrespective of the initial seed field, the magnetic field strength reaches a\n saturated state in the presence of strong accretion shocks. Under such\n conditions, the growth time becomes very short and amplification occurs rapidly\n within a small fraction of the free-fall time. We find that the presence of\n such strong magnetic fields provides additional support against gravity and\n helps in suppressing fragmentation. Massive clumps of a few hundred solar\n masses are formed at the end of our simulations and high accretion rates of\n $\\rm 1 M_{\\odot}/yr$ are observed. We expect that in the presence of such\n accretion rates, the clumps will grow to form supermassive stars of $\\rm \\sim\n 10^{5} M_{\\odot}$. Overall, the role of the magnetic fields seems supportive\n for the formation of massive black holes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1607.04112",
        "title": "Magnetic phase diagrams in the H-T plane of the magnetically strongest\nsigma-phase Fe-V compounds",
        "abstract": "Magnetization measurements were performed on two sigma-phase samples of\n Fe(100-x)V(x) (x=35.5, 34.1) vs. temperature, T, and in DC magnetic field, of\n various amplitudes. Using three characteristic temperatures, magnetic phase\n diagrams in the H-T plane have been designed testifying to a re-entrant\n character of magnetism. The ground magnetic state, a spin glass (SG), was\n evidenced to be composed of two sub phases: one with a weak irreversibility and\n the other with a strong irreversibility. Two critical lines were reconstructed\n within the SG state. Both of them show a crossover from the Gabay-Toulouse\n behavior (low field) to a linear and/or quasi-Almeida-Touless behavior. A\n strong difference in the effect of the applied magnetic field on the SG phase\n in the two samples was revealed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2312.08363",
        "title": "On the Computational Hardness of Quantum One-Wayness",
        "abstract": "There is a large body of work studying what forms of computational hardness\nare needed to realize classical cryptography. In particular, one-way functions\nand pseudorandom generators can be built from each other, and thus require\nequivalent computational assumptions to be realized. Furthermore, the existence\nof either of these primitives implies that $\\rm{P} \\neq \\rm{NP}$, which gives a\nlower bound on the necessary hardness.\nOne can also define versions of each of these primitives with quantum output:\nrespectively one-way state generators and pseudorandom state generators. Unlike\nin the classical setting, it is not known whether either primitive can be built\nfrom the other. Although it has been shown that pseudorandom state generators\nfor certain parameter regimes can be used to build one-way state generators,\nthe implication has not been previously known in full generality. Furthermore,\nto the best of our knowledge, the existence of one-way state generators has no\nknown implications in complexity theory.\nWe show that pseudorandom states compressing $n$ bits to $\\log n + 1$ qubits\ncan be used to build one-way state generators and pseudorandom states\ncompressing $n$ bits to $\\omega(\\log n)$ qubits are one-way state generators.\nThis is a nearly optimal result since pseudorandom states with fewer than $c\n\\log n$-qubit output can be shown to exist unconditionally. We also show that\nany one-way state generator can be broken by a quantum algorithm with classical\naccess to a $\\rm{PP}$ oracle.\nAn interesting implication of our results is that a $t(n)$-copy one-way state\ngenerator exists unconditionally, for every $t(n) = o(n/\\log n)$. This\ncontrasts nicely with the previously known fact that $O(n)$-copy one-way state\ngenerators require computational hardness. We also outline a new route towards\na black-box separation between one-way state generators and quantum bit\ncommitments.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0109373",
        "title": "Coincidences of high density peaks in UVES spectra of QSO pairs",
        "abstract": "We present preliminary results of an investigation of the clustering\n properties of high matter density peaks between redshift ~2 and ~3, as traced\n by Lyman limit and Damped Ly-alpha systems in spectra of close QSO pairs and\n groups.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0905.1730",
        "title": "The puzzle of apparent linear lattice artifacts in the 2d non-linear\nsigma-model and Symanzik's solution",
        "abstract": "Lattice artifacts in the 2d O(n) non-linear sigma-model are expected to be of\n the form O(a^2), and hence it was (when first observed) disturbing that some\n quantities in the O(3) model with various actions show parametrically stronger\n cutoff dependence, apparently O(a), up to very large correlation lengths. In a\n previous letter we described the solution to this puzzle. Based on the\n conventional framework of Symanzik's effective action, we showed that there are\n logarithmic corrections to the O(a^2) artifacts which are especially large,\n (ln(a))^3, for n=3 and that such artifacts are consistent with the data. In\n this paper we supply the technical details of this computation. Results of\n Monte Carlo simulations using various lattice actions for O(3) and O(4) are\n also presented.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2404.05782",
        "title": "Dynamical stability and chaos in artificial neural network trajectories\nalong training",
        "abstract": "The process of training an artificial neural network involves iteratively\n adapting its parameters so as to minimize the error of the network's\n prediction, when confronted with a learning task. This iterative change can be\n naturally interpreted as a trajectory in network space -- a time series of\n networks -- and thus the training algorithm (e.g. gradient descent optimization\n of a suitable loss function) can be interpreted as a dynamical system in graph\n space. In order to illustrate this interpretation, here we study the dynamical\n properties of this process by analyzing through this lens the network\n trajectories of a shallow neural network, and its evolution through learning a\n simple classification task. We systematically consider different ranges of the\n learning rate and explore both the dynamical and orbital stability of the\n resulting network trajectories, finding hints of regular and chaotic behavior\n depending on the learning rate regime. Our findings are put in contrast to\n common wisdom on convergence properties of neural networks and dynamical\n systems theory. This work also contributes to the cross-fertilization of ideas\n between dynamical systems theory, network theory and machine learning",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0610342",
        "title": "$C^*$-algebras of inverse semigroups: amenability and weak containment",
        "abstract": "We argue that weak containment is an appropriate notion of amenability for\n inverse semigroups. Given an inverse semigroup $S$ and a homomorphism $\\phi$ of\n $S$ onto a group $G$, we show, under an assumption on $\\ker(\\phi)$, that $S$\n has weak containment if and only if $G$ is amenable and $\\ker(\\phi)$ has weak\n containment. Using Fell bundle amenability, we find a related result for\n inverse semigroups with zero. We show that all graph inverse semigroups have\n weak containment and that Nica's inverse semigroup $\\mcT_{G,P}$ of a\n quasi-lattice ordered group $(G,P)$ has weak containment if and only if $(G,P)$\n is amenable.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0905.2588",
        "title": "Effect of dipole-dipole charge interactions on dust coagulation",
        "abstract": "This study examines the effect that dipole-dipole charge interactions between\n fractal aggregates have on the growth of dust grains. Aggregates in a plasma or\n radiative environment will have charge distributed over their extended surface,\n which leads to a net dipole moment for the charged grains. A self-consistent\n N-body code is used to model the dynamics of interacting charged aggregates.\n The aggregates are free to rotate due to collisions and dipole-dipole\n electrostatic interactions. These rotations are important in determining the\n growth rate and subsequent geometry (fractal dimension) of the grains. In\n contrast to previous studies which have only taken charge-dipole interactions\n into account, like-charged grains are found to coagulate more efficiently than\n neutral grains due to preferential incorporation of small aggregates into\n mid-sized aggregate structures. The charged aggregates tend to be more compact\n than neutral aggregates, characterized by slightly higher fractal dimensions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1705.09995",
        "title": "Subject Specific Stream Classification Preprocessing Algorithm for\nTwitter Data Stream",
        "abstract": "Micro-blogging service Twitter is a lucrative source for data mining\n applications on global sentiment. But due to the omnifariousness of the\n subjects mentioned in each data item; it is inefficient to run a data mining\n algorithm on the raw data. This paper discusses an algorithm to accurately\n classify the entire stream in to a given number of mutually exclusive\n collectively exhaustive streams upon each of which the data mining algorithm\n can be run separately yielding more relevant results with a high efficiency.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2205.02402",
        "title": "Quasi-periodic behaviour in the $\\gamma$-ray light curve of the blazar\nPKS 0405-385",
        "abstract": "We analyze the quasi-periodic oscillation (QPO) of the historical light curve\n of FSRQs PKS 0405-385 detected by the Fermi LAT from August 2008 to November\n 2021. To identify and determine the QPO signal of PKS 0405-385 in the\n $\\gamma$-ray light curve, we use four time series analysis techniques based on\n frequency and time domains, i.e., the Lomb-Scargle periodogram (LSP), the\n weighted wavelet z-transform (WWZ), the REDFIT and the epoch folding. The\n results show that PKS 0405-385 has a quasi-periodic behavior of $\\sim$2.8 yr\n with the significance of $\\sim$4.3$\\sigma$ in Fermi long-term monitoring.\n Remarkably, we also performed QPO analysis in the G-band light curve observed\n from October 2014 to October 2021 using LSP and WWZ technology, and the results\n ($\\sim$4$\\sigma$ of significance) are consistent with the periodic detection in\n $\\gamma$-ray. This may imply that the optical emission is radiated by an\n electron population same as the \\gr\\ emission. In discussing the possible\n mechanism of quasi-periodic behavior, either the helical motion within a jet or\n the supermassive black hole binary system provides a viable explanation for the\n QPO of 2.8 yr, and the relevant parameters have been estimated.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/physics/9910047",
        "title": "Theory of the tune shift due to linear coupling",
        "abstract": "The presence of skew quadrupole fields will linearly couple the x and y\n motions. The x and y motions can then be written as the sum of two normal modes\n >. This paper presents analytical perturbation theory results for the tunes of\n the normal modes. The results for the normal mode tunes are first found correct\n to lowest order in the skew quadrupole fields. The results are then carried one\n step further to include the next higher order terms in the skew quadrupole\n fields. These analytical results show that for the higher order shift in the\n tune, the important harmonics of the skew quadrupole field are the harmonics\n near the sum of the tunes. However the harmonics closest to the sum of the\n tunes do not contribute to the higher order tune splitting, the seperation of\n the tunes, as they shift the two tunes about equally.This results in a lack of\n a dominant harmonic for the higher order part of the tune splitting, which\n complicates the understanding and correction of the higher order part of the\n tune splitting.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1306.1345",
        "title": "A Note on Graphs of Linear Rank-Width 1",
        "abstract": "We prove that a connected graph has linear rank-width 1 if and only if it is\n a distance-hereditary graph and its split decomposition tree is a path. An\n immediate consequence is that one can decide in linear time whether a graph has\n linear rank-width at most 1, and give an obstruction if not. Other immediate\n consequences are several characterisations of graphs of linear rank-width 1. In\n particular a connected graph has linear rank-width 1 if and only if it is\n locally equivalent to a caterpillar if and only if it is a vertex-minor of a\n path [O-joung Kwon and Sang-il Oum, Graphs of small rank-width are pivot-minors\n of graphs of small tree-width, arxiv:1203.3606] if and only if it does not\n contain the co-K_2 graph, the Net graph and the 5-cycle graph as vertex-minors\n [Isolde Adler, Arthur M. Farley and Andrzej Proskurowski, Obstructions for\n linear rank-width at most 1, arxiv:1106.2533].",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2204.10916",
        "title": "Revealing Occlusions with 4D Neural Fields",
        "abstract": "For computer vision systems to operate in dynamic situations, they need to be\n able to represent and reason about object permanence. We introduce a framework\n for learning to estimate 4D visual representations from monocular RGB-D, which\n is able to persist objects, even once they become obstructed by occlusions.\n Unlike traditional video representations, we encode point clouds into a\n continuous representation, which permits the model to attend across the\n spatiotemporal context to resolve occlusions. On two large video datasets that\n we release along with this paper, our experiments show that the representation\n is able to successfully reveal occlusions for several tasks, without any\n architectural changes. Visualizations show that the attention mechanism\n automatically learns to follow occluded objects. Since our approach can be\n trained end-to-end and is easily adaptable, we believe it will be useful for\n handling occlusions in many video understanding tasks. Data, code, and models\n are available at https://occlusions.cs.columbia.edu/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/hep-ph/0002240",
        "title": "Heavy quark mass expansion and intrinsic charm in light hadrons",
        "abstract": "We review the technique of heavy quark mass expansion of various operators\n made of heavy quark fields using a semiclassical approximation. It corresponds\n to an operator product expansion in the form of series in the inverse heavy\n quark mass. This technique applied recently to the axial current is used to\n estimate the charm content of the \\eta, \\eta' mesons and the intrinsic charm\n contribution to the proton spin. The derivation of heavy quark mass expansion\n for Qbar \\gamma_5 Q is given here in detail and the expansions of the scalar,\n vector and tensor current and of a contribution to the energy-momentum tensor\n are presented as well. The obtained results are used to estimate the intrinsic\n charm contribution to various observables.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/quant-ph/0401162",
        "title": "Quantum Circuits for Incompletely Specified Two-Qubit Operators",
        "abstract": "While the question ``how many CNOT gates are needed to simulate an arbitrary\n two-qubit operator'' has been conclusively answered -- three are necessary and\n sufficient -- previous work on this topic assumes that one wants to simulate a\n given unitary operator up to global phase. However, in many practical cases\n additional degrees of freedom are allowed. For example, if the computation is\n to be followed by a given projective measurement, many dissimilar operators\n achieve the same output distributions on all input states. Alternatively, if it\n is known that the input state is |0>, the action of the given operator on all\n orthogonal states is immaterial. In such cases, we say that the unitary\n operator is incompletely specified; in this work, we take up the practical\n challenge of satisfying a given specification with the smallest possible\n circuit. In particular, we identify cases in which such operators can be\n implemented using fewer quantum gates than are required for generic completely\n specified operators.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2303.05565",
        "title": "Towards Generalized Robot Assembly through Compliance-Enabled Contact\nFormations",
        "abstract": "Contact can be conceptualized as a set of constraints imposed on two bodies\n that are interacting with one another in some way. The nature of a contact,\n whether a point, line, or surface, dictates how these bodies are able to move\n with respect to one another given a force, and a set of contacts can provide\n either partial or full constraint on a body's motion. Decades of work have\n explored how to explicitly estimate the location of a contact and its dynamics,\n e.g., frictional properties, but investigated methods have been computationally\n expensive and there often exists significant uncertainty in the final\n calculation. This has affected further advancements in contact-rich tasks that\n are seemingly simple to humans, such as generalized peg-in-hole insertions. In\n this work, instead of explicitly estimating the individual contact dynamics\n between an object and its hole, we approach this problem by investigating\n compliance-enabled contact formations. More formally, contact formations are\n defined according to the constraints imposed on an object's available\n degrees-of-freedom. Rather than estimating individual contact positions, we\n abstract out this calculation to an implicit representation, allowing the robot\n to either acquire, maintain, or release constraints on the object during the\n insertion process, by monitoring forces enacted on the end effector through\n time. Using a compliant robot, our method is desirable in that we are able to\n complete industry-relevant insertion tasks of tolerances <0.25mm without prior\n knowledge of the exact hole location or its orientation. We showcase our method\n on more generalized insertion tasks, such as commercially available\n non-cylindrical objects and open world plug tasks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2307.08384",
        "title": "Efficient Quantum State Preparation with Walsh Series",
        "abstract": "A new approximate Quantum State Preparation (QSP) method is introduced,\n called the Walsh Series Loader (WSL). The WSL approximates quantum states\n defined by real-valued functions of single real variables with a depth\n independent of the number $n$ of qubits. Two approaches are presented: the\n first one approximates the target quantum state by a Walsh Series truncated at\n order $O(1/\\sqrt{\\epsilon})$, where $\\epsilon$ is the precision of the\n approximation in terms of infidelity. The circuit depth is also\n $O(1/\\sqrt{\\epsilon})$, the size is $O(n+1/\\sqrt{\\epsilon})$ and only one\n ancilla qubit is needed. The second method represents accurately quantum states\n with sparse Walsh series. The WSL loads $s$-sparse Walsh Series into $n$-qubits\n with a depth doubly-sparse in $s$ and $k$, the maximum number of bits with\n value $1$ in the binary decomposition of the Walsh function indices. The\n associated quantum circuit approximates the sparse Walsh Series up to an error\n $\\epsilon$ with a depth $O(sk)$, a size $O(n+sk)$ and one ancilla qubit. In\n both cases, the protocol is a Repeat-Until-Success (RUS) procedure with a\n probability of success $P=\\Theta(\\epsilon)$, giving an averaged total time of\n $O(1/\\epsilon^{3/2})$ for the WSL (resp. $O(sk/\\epsilon)$ for the sparse WSL).\n Amplitude amplification can be used to reduce by a factor\n $O(1/\\sqrt{\\epsilon})$ the total time dependency with $\\epsilon$ but increases\n the size and depth of the associated quantum circuits, making them linearly\n dependent on $n$. These protocols give overall efficient algorithms with no\n exponential scaling in any parameter. They can be generalized to any\n complex-valued, multi-variate, almost-everywhere-differentiable function. The\n Repeat-Until-Success Walsh Series Loader is so far the only method which\n prepares a quantum state with a circuit depth and an averaged total time\n independent of the number of qubits.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2107.14450",
        "title": "Stiff equation of state for a holographic nuclear matter as instanton\ngas",
        "abstract": "In a holographic model, which was used to investigate the color\n superconducting phase of QCD, a dilute gas of instantons is introduced to study\n the nuclear matter. The free energy of the nuclear matter is computed as a\n function of the baryon chemical potential in the probe approximation. Then the\n equation of state is obtained at low temperature. Using the equation of state\n for the nuclear matter, the Tolman-Oppenheimer-Volkov equations for a cold\n compact star are solved. We find the mass-radius relation of the star, which is\n similar to the one for quark star. This similarity implies that the instanton\n gas given here is a kind of self-bound matter.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1502.03069",
        "title": "The Anomalous Scaling Exponents of Turbulence in General Dimension from\nRandom Geometry",
        "abstract": "We propose an exact analytical formula for the anomalous scaling exponents of\n inertial range structure functions in incompressible fluid turbulence. The\n formula is a gravitational Knizhnik-Polyakov-Zamolodchikov (KPZ)-type relation,\n and is valid in any number of space dimensions. It incorporates intermittency\n by gravitationally dressing the Kolmogorov linear scaling via a coupling to a\n random geometry. The formula has one real parameter $\\gamma$ that depends on\n the number of space dimensions. The scaling exponents satisfy the convexity\n inequality, and the supersonic bound constraint. They agree with the\n experimental and numerical data in two and three space dimensions, and with\n numerical data in four space dimensions. Intermittency increases with $\\gamma$,\n and in the infinite $\\gamma$ limit the scaling exponents approach the value\n one, as in Burgers turbulence. At large $n$ the $n$th order exponent scales as\n $\\sqrt{n}$. We discuss the relation between fluid flows and black hole geometry\n that inspired our proposal.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1610.10023",
        "title": "Quasi-Prufer extensions of rings",
        "abstract": "We introduce quasi-Prufer extensions of rings in order to relativize the\n notion of quasi-Prufer domains and to take into account some contexts recently\n introduced in the literature. We also introduce almost-Prufer ring extensions.\n Characterizations and properties are given. In particular, we examine the\n relationship with the finite fibers property.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1203.1020",
        "title": "Relaxation Oscillations in New IS-LM Model",
        "abstract": "In this paper, we create new version of IS-LM model. The original IS-LM model\n has two main deficiencies: assumptions of constant price level and of strictly\n exogenous money supply. New IS-LM model eliminates these deficiencies. In the\n second section, we prove the existence of relaxation oscillations in this new\n IS-LM model.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1106.3488",
        "title": "Proceedings 15th International Refinement Workshop",
        "abstract": "Refinement is one of the cornerstones of a formal approach to software\nengineering: the process of developing a more detailed design or implementation\nfrom an abstract specification through a sequence of mathematically-based steps\nthat maintain correctness with respect to the original specification.\nThe aim of this BCS FACS Refinement Workshop, is to bring together people who\nare interested in the development of more concrete designs or executable\nprograms from abstract specifications using formal notations, tool support for\nformal software development, and practical experience with formal refinement\nmethodologies.\nThe purpose of the workshop is to provide a forum for the exchange of ideas,\nand discussion of common ground and key differences.\nThis 15th workshop continued a 20 year tradition in refinement workshops run\nunder the auspices of the British Computer Society (BCS) FACS special interest\ngroup. After the first seven editions had been held in the UK, in 1998 it was\ncombined with the Australasian Refinement Workshop to form the International\nRefinement Workshop, hosted at The Australian National University. Six more\neditions have followed in a variety of locations, all with electronic published\nproceedings and associated journal special issues.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2103.12669",
        "title": "Log canonical models of foliated surfaces",
        "abstract": "We study log canonical models of foliated surfaces of general type. In\n particular, we show that log canonical models of general type and their minimal\n partial du Val resolutions are bounded. Moreover, we show the valuative\n criteria of separatedness and properness and a property related to\n local-closedness for the moduli functor $\\mathcal{S}_P^{sm}$ which parametrizes\n the stable smoothable foliated surface pairs. On the way, we also show a result\n on the invariance of plurigenera.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2009.13672",
        "title": "$\\mathrm{e}^+$$\\mathrm{e}^-$ Beam-beam Parameter Study for a TeV-scale\nPWFA Linear Collider",
        "abstract": "We perform a beam-beam parameter study for a TeV-scale PWFA (particle-driven\nplasma wakefield acceleration) $\\mathrm{e}^+$$\\mathrm{e}^-$ linear collider\nusing GUINEA-PIG simulations. The study shows that the total luminosity follows\nthe $1/\\sqrt{\\sigma_z}$-scaling predicted by beamstrahlung theory, where\n$\\sigma_z$ is the rms beam length, which is advantageous for PWFA, as short\nbeam lengths are preferred. We also derive a parameter set for a 3 TeV PWFA\nlinear collider with main beam parameters optimised for luminosity and\nluminosity spread introduced by beamstrahlung.\nLastly, the study also compare the performance for scenarios with reduced\npositron beam charge at 3 TeV and 14 TeV with CLIC parameters.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1004.3497",
        "title": "Interactions of Phonons and Rotons with Interfaces in Superfluid Helium",
        "abstract": "We solve the problem of beams of phonons and rotons incident on, and\n interacting with, solid surfaces. Phonons and rotons are the quasiparticles of\n superfluid helium and have a unique dispersion curve. The dispersion curve\n controls the transmission, reflection and mode change of these quasiparticles\n at the interface with another medium. We develop a non-local hydrodynamic\n theory in a consistent and unified way. The structure of the solutions in the\n quantum fluid is discussed. The creation probabilities of all quasiparticles\n are derived when any one of them is incident on the interface. The dependencies\n on frequency and angular are analysed and the backward reflection and\n refraction for $R^-$ rotons are discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1802.08710",
        "title": "Extending \\textit{ab initio} plasma-surface simulations to\nexperimentally relevant scales",
        "abstract": "The physical processes at the interface of a low-temperature plasma and a\n solid are extremely complex. They involve a huge number of elementary processes\n in the plasma, in the solid as well as charge, momentum and energy transfer\n across the interface. In the majority of plasma simulations these surface\n processes are either neglected or treated via phenomenological parameters.\n However, those parameters are known only in some cases, so such an approach is\n very inaccurate and does not have predictive capability. Therefore,\n improvements are highly needed. In this paper we briefly summarize relevant\n theoretical methods from solid state and surface physics that are able to\n contribute to an improved simulation of plasma-surface interaction in the near\n future. Full \\textit{ab initio} quantum simulations are feasible only for\n extremely short times and/or small system sizes. A substantial simplification\n is achieved when electronic quantum effects are not treated explicitly. Then\n one arrives at semi-classical molecular dynamics (MD) simulations for the heavy\n particles that have become the main workhorse in surface science simulations.\n Using microscopically founded potentials and force fields as an input, these MD\n simulations approach the quality of \\textit{ab initio} simulations, in many\n cases. However, despite their simplified nature, these simulations require a\n time step that is of the order or below one femtosecond making it prohibitive\n to reach experimentally relevant scales of minutes. To bridge this gap in\n length and time scales without compromising the first principles character of\n the simulations, many physical and computational strategies have been put\n forward in surface science. This paper presents a brief overview on different\n methods and their underlying physical ideas, and we compare their strengths and\n weaknesses.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1804.09906",
        "title": "A comparison of the R_h=ct and LCDM cosmologies using the Cosmic\nDistance Duality Relation",
        "abstract": "The cosmic distance duality (CDD) relation (based on the Etherington\n reciprocity theorem) plays a crucial role in a wide assortment of cosmological\n measurements. Attempts at confirming it observationally have met with mixed\n results, though the general consensus appears to be that the data do support\n its existence in nature. A common limitation with past approaches has been\n their reliance on a specific cosmological model, or on measurements of the\n luminosity distance to Type Ia SNe, which introduces a dependence on the\n presumed cosmology in spite of beliefs to the contrary. Confirming that the CDD\n is actually realized in nature is crucial because its violation would require\n exotic new physics. In this paper, we study the CDD using the observed angular\n size of compact quasar cores and a Gaussian Process reconstruction of the HII\n galaxy Hubble diagram---without pre-assuming any particular background\n cosmology. In so doing, we confirm at a very high level of confidence that the\n angular-diameter and luminosity distances do indeed satisfy the CDD. We then\n demonstrate the potential power of this result by utilizing it in a comparative\n test of two competing cosmological models---the R_h=ct universe and LCDM---and\n show that R_h=ct is favoured by the CDD data with a likelihood ~82.3% compared\n with ~17.7% for the standard model.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1604.08589",
        "title": "Interpreting the subtle spectral variations of the 11.2 and 12.7 {\\mu}m\npolycyclic aromatic hydrocarbon bands",
        "abstract": "We report new properties of the 11 and 12.7 {\\mu}m emission complexes of\n polycyclic aromatic hydrocarbons (PAHs) by applying a Gaussian-based\n decomposition technique. Using high-resolution \\textit{Spitzer} Space Telescope\n data, we study in detail the spectral and spatial characteristics of the 11 and\n 12.7 {\\mu}m emission bands in maps of reflection nebulae NGC 7023 and NGC 2023\n (North and South) and the star-forming region M17. Profile variations are\n observed in both the 11 and 12.7 {\\mu}m emission bands. We identify a neutral\n contribution to the traditional 11.0 {\\mu}m PAH band and a cationic\n contribution to the traditional 11.2 {\\mu}m band, the latter of which affects\n the PAH class of the 11.2 {\\mu}m emission in our sample. The peak variations of\n the 12.7 {\\mu}m complex are explained by the competition between two underlying\n blended components. The spatial distributions of these components link them to\n cations and neutrals. We conclude that the 12.7 {\\mu}m emission originates in\n both neutral and cationic PAHs, lending support to the use of the 12.7/11.2\n intensity ratio as a charge proxy.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0803.0786",
        "title": "On the independent points in the sky for the search of periodic\ngravitational wave",
        "abstract": "We investigate the independent points in the sky require to search the\n periodic gravitational wave, assuming the noise power spectral density to be\n flat. We have made an analysis with different initial azimuth of the Earth for\n a week data set. The analysis shows significant difference in the independent\n points in the sky for the search. We numerically obtain an approximate relation\n to make trade-off between computational cost and sensitivities. We also discuss\n the feasibility of the coherent search in small frequency band in reference to\n advanced LIGO.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2301.06749",
        "title": "Event-Triggered Optimal Formation Tracking Control Using Reinforcement\nLearning for Large-Scale UAV Systems",
        "abstract": "Large-scale UAV switching formation tracking control has been widely applied\n in many fields such as search and rescue, cooperative transportation, and UAV\n light shows. In order to optimize the control performance and reduce the\n computational burden of the system, this study proposes an event-triggered\n optimal formation tracking controller for discrete-time large-scale UAV systems\n (UASs). And an optimal decision - optimal control framework is completed by\n introducing the Hungarian algorithm and actor-critic neural networks (NNs)\n implementation. Finally, a large-scale mixed reality experimental platform is\n built to verify the effectiveness of the proposed algorithm, which includes\n large-scale virtual UAV nodes and limited physical UAV nodes. This compensates\n for the limitations of the experimental field and equipment in realworld\n scenario, ensures the experimental safety, significantly reduces the\n experimental cost, and is suitable for realizing largescale UAV formation light\n shows.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1407.4955",
        "title": "Correlations between weights and overlap in ensembles of weighted\nmultiplex networks",
        "abstract": "Multiplex networks describe a large number of systems ranging from social\n networks to the brain. These multilayer structure encode information in their\n structure. This information can be extracted by measuring the correlations\n present in the multiplex networks structure, such as the overlap of the links\n in different layers. Many multiplex networks are also weighted, and the weights\n of the links can be strongly correlated with the structural properties of the\n multiplex network. For example in multiplex network formed by the citation and\n collaboration networks between PRE scientists it was found that the statistical\n properties of citations to co-authors are different from the one of citations\n to non-co-authors, i.e. the weights depend on the overlap of the links. Here we\n present a theoretical framework for modelling multiplex weighted networks with\n different types of correlations between weights and overlap. To this end, we\n use the framework of canonical network ensembles, and the recently introduced\n concept of multilinks, showing that null models of a large variety of network\n structures can be constructed in this way. In order to provide a concrete\n example of how this framework apply to real data we consider a multiplex\n constructed from gene expression data of healthy and cancer tissues.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1608.06157",
        "title": "The Spin-Half {\\it XXZ} Antiferromagnet on the Square Lattice Revisited:\nA High-Order Coupled Cluster Treatment",
        "abstract": "We use the coupled cluster method (CCM) to study the ground-state properties\n and lowest-lying triplet excited state of the spin-half {\\it XXZ}\n antiferromagnet on the square lattice. The CCM is applied to it to high orders\n of approximation by using an efficient computer code that has been written by\n us and which has been implemented to run on massively parallelized computer\n platforms. We are able therefore to present precise data for the basic\n quantities of this model over a wide range of values for the anisotropy\n parameter $\\Delta$ in the range $-1 \\leq \\Delta < \\infty$ of interest,\n including both the easy-plane $(-1 < \\Delta < 1)$ and easy-axis $(\\Delta > 1)$\n regimes, where $\\Delta \\rightarrow \\infty$ represents the Ising limit. We\n present results for the ground-state energy, the sublattice magnetization, the\n zero-field transverse magnetic susceptibility, the spin stiffness, and the\n triplet spin gap. Our results provide a useful yardstick against which other\n approximate methods and/or experimental studies of relevant antiferromagnetic\n square-lattice compounds may now compare their own results. We also focus\n particular attention on the behaviour of these parameters for the easy-axis\n system in the vicinity of the isotropic Heisenberg point ($\\Delta = 1$), where\n the model undergoes a phase transition from a gapped state (for $\\Delta > 1$)\n to a gapless state (for $\\Delta \\leq 1$), and compare our results there with\n those from spin-wave theory (SWT). Interestingly, the nature of the criticality\n at $\\Delta=1$ for the present model with spins of spin quantum number\n $s=\\frac{1}{2}$ that is revealed by our CCM results seems to differ\n qualitatively from that predicted by SWT, which becomes exact only for its\n near-classical large-$s$ counterpart.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1402.4792",
        "title": "An accurate and efficient Lagrangian sub-grid model",
        "abstract": "A computationally efficient model is introduced to account for the sub-grid\n scale velocities of tracer particles dispersed in statistically homogeneous and\n isotropic turbulent flows. The model embeds the multi-scale nature of turbulent\n temporal and spatial correlations, that are essential to reproduce\n multi-particle dispersion. It is capable to describe the Lagrangian diffusion\n and dispersion of temporally and spatially correlated clouds of particles.\n Although the model neglects intermittent corrections, we show that pair and\n tetrad dispersion results nicely compare with Direct Numerical Simulations of\n statistically isotropic and homogeneous $3D$ turbulence. This is in agreement\n with recent observations that deviations from self-similar pair dispersion\n statistics are rare events.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1503.03452",
        "title": "Design and implementation of an Android application (MobilitApp+) to\nanalyze the mobility patterns of citizens in the Metropolitan Region of\nBarcelona",
        "abstract": "In our project we have designed an Android application to obtain mobility\n data of the citizens in the metropolitan area of Barcelona. Our implementation\n synchronously obtains in background on the one hand, periodic location updates\n and, on the other hand, the type of activity citizens are doing. At the end of\n the day, all this data is processed and sent to a server where are stored to\n obtain mobility patterns from citizens that could help to improve the current\n transportation infrastructure. MobilitApp is fully functional and stable\n although the results can be improved in some situations. In future releases we\n will implement machine learning technics to obtain significant improvements,\n especially in the activity recognition modules.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2101.03805",
        "title": "A Conflict-Based Search Framework for Multi-Objective Multi-Agent Path\nFinding",
        "abstract": "Conventional multi-agent path planners typically compute an ensemble of paths\n while optimizing a single objective, such as path length. However, many\n applications may require multiple objectives, say fuel consumption and\n completion time, to be simultaneously optimized during planning and these\n criteria may not be readily compared and sometimes lie in competition with each\n other. The goal of the problem is thus to find a Pareto-optimal set of\n solutions instead of a single optimal solution. Naively applying existing\n multi-objective search algorithms, such as multi-objective A* (MOA*), to\n multi-agent path finding may prove to be inefficient as the dimensionality of\n the search space grows exponentially with the number of agents. This article\n presents an approach named Multi-Objective Conflict-Based Search (MO-CBS) that\n attempts to address this so-called curse of dimensionality by leveraging prior\n Conflict-Based Search (CBS), a well-known algorithm for single-objective\n multi-agent path finding, and principles of dominance from multi-objective\n optimization literature. We also develop several variants of MO-CBS to improve\n its performance. We prove that MO-CBS and its variants can compute the entire\n Pareto-optimal set. Numerical results show that MO-CBS outperforms MOM*, a\n recently developed state-of-the-art multi-objective multi-agent planner.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0708.3295",
        "title": "Recent progress on the manipulation of single atoms in optical tweezers\nfor quantum computing",
        "abstract": "This paper summarizes our recent progress towards using single rubidium atoms\n trapped in an optical tweezer to encode quantum information. We demonstrate\n single qubit rotations on this system and measure the coherence of the qubit.\n We move the quantum bit over distances of tens of microns and show that the\n coherence is reserved. We also transfer a qubit atom between two tweezers and\n show no loss of coherence. Finally, we describe our progress towards\n conditional entanglement of two atoms by photon emission and two-photon\n interferences.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1710.05800",
        "title": "A rapidly expanding Bose-Einstein condensate: an expanding universe in\nthe lab",
        "abstract": "We study the dynamics of a supersonically expanding ring-shaped Bose-Einstein\n condensate both experimentally and theoretically. The expansion redshifts\n long-wavelength excitations, as in an expanding universe. After expansion,\n energy in the radial mode leads to the production of bulk topological\n excitations -- solitons and vortices -- driving the production of a large\n number of azimuthal phonons and, at late times, causing stochastic persistent\n currents. These complex nonlinear dynamics, fueled by the energy stored\n coherently in one mode, are reminiscent of a type of \"preheating\" that may have\n taken place at the end of inflation.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1712.06133",
        "title": "Critical graph of a polynomial quadratic differential related to a\nSchr\\\"odinger equation with quartic potential",
        "abstract": "In this paper, we study the weak asymptotic in the plane of some wave\n functions resulting from the WKB techniques applied to a Shrodinger equation\n with quartic oscillator and having some boundary condition. In first step, we\n make transformations of our problem to obtain a Heun equation satisfied by the\n polynomial part of the WKB wave functions .Especially , we investigate the\n properties of the Cauchy transform of the root counting measure of a re-scaled\n solutions of the Schrodinger equation, to obtain a quadratic algebraic equation\n of the form $\\mathcal{C}^{2}\\left( z\\right) +r\\left( z\\right) \\mathcal{C}\\left(\n z\\right) +s\\left( z\\right) =0$, where $r,s$ are also polynomials. In second\n step, we discuss the existence of solutions (as Cauchy transform of a signed\n measures) of this algebraic equation.This problem remains to describe the\n critical graph of a related 4-degree polynomial quadratic differential\n $-p\\left( z\\right) dz^{2}$. In particular, we discuss the existence(and their\n number) of finite critical trajectories of this quadratic differential.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1007.4685",
        "title": "Generalized Zeta Function Regularization and the Multiplicative Anomaly",
        "abstract": "A brief survey of the zeta function regularization and multiplicative anomaly\n issues when the associated zeta function of fluctuation operator is the regular\n at the origin (regular case) as well as when it is singular at the origin\n (singular case) is presented. In the singular case, new results for the\n multiplicative anomaly are presented",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1711.10115",
        "title": "The Smallest Shape Spaces. III. Triangles in 2- and 3-d",
        "abstract": "This is an innovative treatise on triangles, resting upon 1) 3-body problem\ntechniques including mass-weighted relative Jacobi coordinates. 2) Part I's\ndetailed layer-by-layer topological and geometrical study of Kendall-type shape\nspaces - configuration spaces of all possible shapes - which, for triangles,\nare (pieces of) spheres. 3) Hopf mathematics. Triangles are moreover\nprototypical through being the smallest models which carry relative-angle as\nwell as length-ratio information. Both 1) and 3) produce insightful new\nversions of Heron's formula, 3)'s simultaneously providing new foundations for\n2). Medians, and regular triangles bounding between tall and flat triangles,\nalso play prominent roles. Right triangles form three kissing cap-circles on\nthe shape sphere, from which a shape-theoretic answer to the well-known\nconundrum of what is the probability that a triangle is obtuse very readily\nfollows: 3/4. The differential-geometric aspects of this answer moreover\ngeneralize to numerous variant problems.\nHopf mathematics additionally gives a general bundle section interpretation\nto Kendall's iconic spherical blackboard of vertex-unlablelled\nmirror-image-identified triangles, and of its two variants where one of these\ntwo conditions are dropped. We attribute a monopole to each of these spaces and\nto the full shape sphere, one due to Dirac, one to Iwai and the other two are\nnew to this paper. We finally make insightful comparison of triangles in 2-$d$\nwith a) Part II's 4 points on the line. b) Triangles in 3-$d$, which are\nparticularly significant as the smallest model exhibiting stratification.\nStratified manifold-sheaf pairs - sheaves adding useful local and global\nstructure to general bundles - lie at the heart of Shape Theory's future\ndevelopment.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2405.18531",
        "title": "Difference-in-Discontinuities: Estimation, Inference and Validity Tests",
        "abstract": "This paper investigates the econometric theory behind the newly developed\n difference-in-discontinuities design (DiDC). Despite its increasing use in\n applied research, there are currently limited studies of its properties. The\n method combines elements of regression discontinuity (RDD) and\n difference-in-differences (DiD) designs, allowing researchers to eliminate the\n effects of potential confounders at the discontinuity. We formalize the\n difference-in-discontinuity theory by stating the identification assumptions\n and proposing a nonparametric estimator, deriving its asymptotic properties and\n examining the scenarios in which the DiDC has desirable bias properties when\n compared to the standard RDD. We also provide comprehensive tests for one of\n the identification assumption of the DiDC. Monte Carlo simulation studies show\n that the estimators have good performance in finite samples. Finally, we\n revisit Grembi et al. (2016), that studies the effects of relaxing fiscal rules\n on public finance outcomes in Italian municipalities. The results show that the\n proposed estimator exhibits substantially smaller confidence intervals for the\n estimated effects.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1804.09826",
        "title": "Mirror Symmetry of quantum Yang-Mills vacua and cosmological\nimplications",
        "abstract": "We find an argument related to the existence of a Z_2-symmetry for the\n renormalization group flow derived from the bare Yang-Mills Lagrangian, and\n show that the cancellation of the vacuum energy may arise motivated both from\n the renormalization group flow solutions and the effective Yang-Mills action.\n In the framework of the effective Savvidy's action, two Mirror minima are\n allowed, with exactly equal and hold opposite sign energy densities. At the\n cosmological level, we explore the stability of the electric and magnetic\n attractor solutions, both within and beyond the perturbation theory, and find\n that thanks to these latter the cancellation between the electric and the\n magnetic vacua components is achieved at macroscopic space and time\n separations. This implies the disappearance of the conformal anomaly in the\n classical limit of an effective Yang-Mills theory. In this picture, the\n tunneling probability from the Mirror vacua to the other vacua is exponentially\n suppressed in the quantum non-thermal state --- similarly to what happens for\n electroweak instantonic tunneling solutions. Specifically, we show that, in a\n dynamical Friedmann-Lema\\^itre-Robertson-Walker (FLRW) cosmological background,\n the Nielsen-Olsen argument --- on the instability of uniform chromo-electric\n and chromo-magnetic Mirror vacua --- is subtly violated. The chromo-magnetic\n and chromo-electric uniform vacua are unstable only at asymptotic times, when\n the attractor to a zero energy density is already reached. The two vacua can\n safely decay into one anisotropic vacuum that has zero energy-density. We also\n discover a new surprising pattern of solitonic and anti-solitonic space-like\n solutions, which are sourced by the Yang-Mills dynamics in FLRW. We dub such\n non-perturbative configurations, which are directly related to dynamical\n cancellation mechanism of the vacuum energy, as {\\it chronons}, or\n $\\chi$-solutions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2103.02205",
        "title": "Gradual Fine-Tuning for Low-Resource Domain Adaptation",
        "abstract": "Fine-tuning is known to improve NLP models by adapting an initial model\n trained on more plentiful but less domain-salient examples to data in a target\n domain. Such domain adaptation is typically done using one stage of\n fine-tuning. We demonstrate that gradually fine-tuning in a multi-stage process\n can yield substantial further gains and can be applied without modifying the\n model or learning objective.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1812.01123",
        "title": "Negative differential conductance effect and electrical anisotropy of 2D\nZrB2 monolayers",
        "abstract": "Two-dimensional (2D) metal-diboride ZrB2 monolayers was predicted\n theoretically as a stable new electronic material [A. Lopez-Bezanilla, Phys.\n Rev. Mater., 2018, 2, 011002 (R)]. Here, we investigate its electronic\n transport properties along the zigzag (z-ZrB2) and armchair (a-ZrB2)\n directions, using the density functional theory and non-equilibrium Green's\n function methods. Under low biases, the 2D ZrB2 shows a similar electrical\n transport along zigzag and armchair directions as electric current propagates\n mostly via the metallic Zr-Zr bonds. However, it shows an electrical anistropy\n under high biases, and its I-V curves along zigzag and armchair directions\n diverge as the bias voltage is higher than 1.4 V, as more directional B-B\n transmission channels are opened. Importantly, both z-ZrB2 and a-ZrB2 show a\n pronounced negative differential conductance (NDC) effect and hence they can be\n promising for the use in NDC-based nanodevices.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.03081",
        "title": "SD-AETO: Service Deployment Enabled Adaptive Edge Task Offloading in MEC",
        "abstract": "In recent years, edge computing, as an important pillar for future networks,\n has been developed rapidly. Task offloading is a key part of edge computing\n that can provide computing resources for resource-constrained devices to run\n computing-intensive applications, accelerate computing speed and save energy.\n An efficient and feasible task offloading scheme can not only greatly improve\n the quality of experience (QoE) but also provide strong support and assistance\n for 5G/B5G networks, the industrial Internet of Things (IIoT), computing\n networks and so on. To achieve these goals, this paper proposes an adaptive\n edge task offloading scheme assisted by service deployment (SD-AETO) focusing\n on the optimization of the energy utilization ratio (EUR) and the processing\n latency. In the pre-implementation stage of the SD-AETO scheme, a service\n deployment scheme is invoked to assist with task offloading considering each\n service's popularity. The optimal service deployment scheme is obtained by\n using the approximate deployment graph (AD-graph). Furthermore, a task\n scheduling and queue offloading design procedure is proposed to complete the\n SD-AETO scheme based on the task priority. The task priority is generated by\n the corresponding service popularity and task offloading direction. Finally, we\n analyze our SD-AETO scheme and compare it with related approaches, and the\n results show that our scheme has a higher edge offloading rate and lower\n resource consumption for massive task scenarios in the edge network.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/9505058",
        "title": "A Soluble Model of Four-Fermion Interactions in de Sitter Space",
        "abstract": "We consider the theory of four-fermion interactions with N-component fermions\n in de Sitter space. It is found that the effective potential for a composite\n operator in the theory is calculable in the leading order of the 1/N expansion.\n The resulting effective potential is analyzed by varying both the four-fermion\n coupling constant and the curvature of the space-time. The critical curvature\n at which the dynamically generated fermion mass disappears is found to exist\n and is calculated analytically. The dynamical fermion mass is expressed as a\n function of the space-time curvature.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ph/0210325",
        "title": "Forward-Backward Multiplicity Correlations in Symmetric and Asymmetric\nHigh Energy Collisions",
        "abstract": "Forward-backward correlations are explored within the two-component clan\n model of multiparticle production. It is found that existing data are well\n described, and, in hh collisions, that clans must be allowed to leak particles\n from one hemisphere to the other. General formulae given for the symmetric case\n are then extended to the asymmetric one, which is relevant for pA and AB\n collisions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0810.4752",
        "title": "Statistical Learning Theory: Models, Concepts, and Results",
        "abstract": "Statistical learning theory provides the theoretical basis for many of\n today's machine learning algorithms. In this article we attempt to give a\n gentle, non-technical overview over the key ideas and insights of statistical\n learning theory. We target at a broad audience, not necessarily machine\n learning researchers. This paper can serve as a starting point for people who\n want to get an overview on the field before diving into technical details.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1107.5376",
        "title": "On the participant-spectator matter and thermalization of neutron-rich\nsystems in heavy-ion collisions",
        "abstract": "We study the participant-spectator matter at the energy of vanishing flow for\n neutron-rich systems. Our study reveals similar behaviour of\n articipant-spectator for neutron-rich systems as for stable systems and also\n points towards nearly mass independence behaviour of participant-spectator\n matter for neutron-rich systems at the energy of vanishing flow. We also study\n the thermalization reached in the reactions of neutron-rich systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1908.03429",
        "title": "Non-mesonic decay of the $\\eta$-mesic $^{3}\\hspace{-0.03cm}\\mbox{He}$\nvia $pd\\rightarrow(^{3}\\hspace{-0.03cm}\\mbox{He}$-$\\eta)_{bound}\\rightarrow$\n$^{3}\\hspace{-0.03cm}\\mbox{He} 2\\gamma (6\\gamma$) reaction",
        "abstract": "In this article a theoretical model for the $\\eta$-mesic\n $^{3}\\hspace{-0.03cm}\\mbox{He}$ non-mesonic decay channels is presented. We\n present the resultant relative momentum distribution of bound\n $^{3}\\hspace{-0.03cm}\\mbox{He}$-$\\eta$ as well as in-medium branching ratios of\n $\\eta\\rightarrow 2\\gamma$ and $\\eta\\rightarrow 3\\pi^0$, which are crucial for\n the Monte Carlo simulations of measured processes and thus for the experimental\n data interpretation. As an example we also apply the model for the estimation\n of the detection efficiency of the WASA-at-COSY detector.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1412.8654",
        "title": "Entropic forces in a non-equilibrium system: Flocks of birds",
        "abstract": "When birds come together to form a flock, the distribution of their\n individual velocities narrows around the mean velocity of the flock. We argue\n that, in a broad class of models for the joint distribution of positions and\n velocities, this narrowing generates an entropic force that opposes the\n cohesion of the flock. The strength of this force depends strongly on the\n nature of the interactions among birds: if birds are coupled to a fixed number\n of neighbors, the entropic forces are weak, while if they couple to all other\n birds within a fixed distance, the entropic forces are sufficient to tear a\n flock apart. Similar entropic forces should occur in other non-equilibrium\n systems. For the joint distribution of protein structures and amino-acid\n sequences, these forces favor the occurrence of \"highly designable\" structures.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2003.06954",
        "title": "Bounded-Rational Pursuit-Evasion Games",
        "abstract": "We present a framework that incorporates the idea of bounded rationality into\n dynamic stochastic pursuit-evasion games. The solution of a stochastic game is\n characterized, in general, by its (Nash) equilibria in feedback form. However,\n computing these Nash equilibrium strategies may require extensive computational\n resources. In this paper, the agents are modeled as bounded rational entities\n having limited computational resources. We illustrate the framework by applying\n it to a pursuit-evasion game between two vehicles in a stochastic wind field,\n where both the pursuer and the evader are bounded rational. We show how such a\n game may be analyzed by properly casting it as an iterative sequence of\n finite-state Markov Decision Processes (MDPs). Leveraging tools and algorithms\n from cognitive hierarchy theory (\"level-$k$ thinking\") we compute the solution\n of the ensuing discrete game, while taking into consideration the rationality\n level of each agent. We also present an online algorithm for each agent to\n infer its opponent rationality level.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1610.04185",
        "title": "Le Potier's strange duality, quot schemes, and multiple point formulas\nfor del Pezzo surfaces",
        "abstract": "We study Le Potier's strange duality on del Pezzo surfaces using quot schemes\n to construct independent sections of theta line bundles on moduli spaces of\n sheaves, one of which is the Hilbert scheme of n points. For n at most 7, we\n use multiple point formulas to count the length of the quot scheme, which\n agrees with the dimension of the space of sections on the Hilbert scheme. When\n the surface is the projective plane and n is arbitrary, we use nice resolutions\n of general stable sheaves to show that the quot schemes that arise are finite\n and reduced. Combining our results, we obtain a lower bound on the rank of the\n strange duality map, as well as evidence that the map is injective when n is at\n most 7.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2002.00205",
        "title": "Deep segmental phonetic posterior-grams based discovery of\nnon-categories in L2 English speech",
        "abstract": "Second language (L2) speech is often labeled with the native, phone\n categories. However, in many cases, it is difficult to decide on a categorical\n phone that an L2 segment belongs to. These segments are regarded as\n non-categories. Most existing approaches for Mispronunciation Detection and\n Diagnosis (MDD) are only concerned with categorical errors, i.e. a phone\n category is inserted, deleted or substituted by another. However,\n non-categorical errors are not considered. To model these non-categorical\n errors, this work aims at exploring non-categorical patterns to extend the\n categorical phone set. We apply a phonetic segment classifier to generate\n segmental phonetic posterior-grams (SPPGs) to represent phone segment-level\n information. And then we explore the non-categories by looking for the SPPGs\n with more than one peak. Compared with the baseline system, this approach\n explores more non-categorical patterns, and also perceptual experimental\n results show that the explored non-categories are more accurate with increased\n confusion degree by 7.3% and 7.5% under two different measures. Finally, we\n preliminarily analyze the reason behind those non-categories.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2107.04082",
        "title": "Improved Language Identification Through Cross-Lingual Self-Supervised\nLearning",
        "abstract": "Language identification greatly impacts the success of downstream tasks such\n as automatic speech recognition. Recently, self-supervised speech\n representations learned by wav2vec 2.0 have been shown to be very effective for\n a range of speech tasks. We extend previous self-supervised work on language\n identification by experimenting with pre-trained models which were learned on\n real-world unconstrained speech in multiple languages and not just on English.\n We show that models pre-trained on many languages perform better and enable\n language identification systems that require very little labeled data to\n perform well. Results on a 26 languages setup show that with only 10 minutes of\n labeled data per language, a cross-lingually pre-trained model can achieve over\n 89.2% accuracy.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2204.08462",
        "title": "CapillaryX: A Software Design Pattern for Analyzing Medical Images in\nReal-time using Deep Learning",
        "abstract": "Recent advances in digital imaging, e.g., increased number of pixels\n captured, have meant that the volume of data to be processed and analyzed from\n these images has also increased. Deep learning algorithms are state-of-the-art\n for analyzing such images, given their high accuracy when trained with a large\n data volume of data. Nevertheless, such analysis requires considerable\n computational power, making such algorithms time- and resource-demanding. Such\n high demands can be met by using third-party cloud service providers. However,\n analyzing medical images using such services raises several legal and privacy\n challenges and does not necessarily provide real-time results. This paper\n provides a computing architecture that locally and in parallel can analyze\n medical images in real-time using deep learning thus avoiding the legal and\n privacy challenges stemming from uploading data to a third-party cloud\n provider. To make local image processing efficient on modern multi-core\n processors, we utilize parallel execution to offset the resource-intensive\n demands of deep neural networks. We focus on a specific medical-industrial case\n study, namely the quantifying of blood vessels in microcirculation images for\n which we have developed a working system. It is currently used in an\n industrial, clinical research setting as part of an e-health application. Our\n results show that our system is approximately 78% faster than its serial system\n counterpart and 12% faster than a master-slave parallel system architecture.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0612633",
        "title": "Transport properties of n-type ultrananocrystalline diamond films",
        "abstract": "We investigate transport properties of ultrananocrystalline diamond films for\n a broad range of temperatures. Addition of nitrogen during plasma-assisted\n growth increases the conductivity of ultrananocrystalline diamond films by\n several orders of magnitude. We show that films produced at low concentration\n of nitrogen in the plasma are very resistive and electron transport occurs via\n a variable range hopping mechanism while in films produced at high nitrogen\n concentration the electron states become delocalized and the transport\n properties of ultrananocrystalline diamond films can be described using the\n Boltzmann formalism. We discuss the critical concentration of carriers at which\n the metal to insulator transition in ultrananocrystalline diamond films occurs\n and compare our results with available experimental data.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/0608121",
        "title": "String Gas Cosmology and Structure Formation",
        "abstract": "It has recently been shown that a Hagedorn phase of string gas cosmology may\n provide a causal mechanism for generating a nearly scale-invariant spectrum of\n scalar metric fluctuations, without the need for an intervening period of de\n Sitter expansion. A distinctive signature of this structure formation scenario\n would be a slight blue tilt of the spectrum of gravitational waves. In this\n paper we give more details of the computations leading to these results.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1909.08160",
        "title": "Left-invariant CR structures on 3-dimensional Lie groups",
        "abstract": "The systematic study of CR manifolds originated in two pioneering 1932 papers\n of \\'Elie Cartan. In the first, Cartan classifies all homogeneous CR\n 3-manifolds, the most well-known case of which is a one-parameter family of\n left-invariant CR structures on $\\mathrm{SU}_2 = S^3$, deforming the standard\n `spherical' structure. In this paper, mostly expository, we illustrate and\n clarify Cartan's results and methods by providing detailed classification\n results in modern language for four 3-dimensional Lie groups. In particular, we\n find that $\\mathrm{SL}_2(\\mathbb{R})$ admits two one-parameter families of\n left-invariant CR structures, called the elliptic and hyperbolic families,\n characterized by the incidence of the contact distribution with the null cone\n of the Killing metric. Low dimensional complex representations of\n $\\mathrm{SL}_2(\\mathbb{R})$ provide CR embedding or immersions of these\n structures. The same methods apply to all other three-dimensional Lie groups\n and are illustrated by descriptions of the left-invariant CR structures for\n $\\mathrm{SU}_2$, the Heisenberg group, and the Euclidean group.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2005.04394",
        "title": "Threshold-Based Fast Successive-Cancellation Decoding of Polar Codes",
        "abstract": "Fast SC decoding overcomes the latency caused by the serial nature of the SC\n decoding by identifying new nodes in the upper levels of the SC decoding tree\n and implementing their fast parallel decoders. In this work, we first present a\n novel sequence repetition node corresponding to a particular class of bit\n sequences. Most existing special node types are special cases of the proposed\n sequence repetition node. Then, a fast parallel decoder is proposed for this\n class of node. To further speed up the decoding process of general nodes\n outside this class, a threshold-based hard-decision-aided scheme is introduced.\n The threshold value that guarantees a given error-correction performance in the\n proposed scheme is derived theoretically. Analysis and hardware implementation\n results on a polar code of length $1024$ with code rates $1/4$, $1/2$, and\n $3/4$ show that our proposed algorithm reduces the required clock cycles by up\n to $8\\%$, and leads to a $10\\%$ improvement in the maximum operating frequency\n compared to state-of-the-art decoders without tangibly altering the\n error-correction performance. In addition, using the proposed threshold-based\n hard-decision-aided scheme, the decoding latency can be further reduced by\n $57\\%$ at $\\mathrm{E_b}/\\mathrm{N_0} = 5.0$~dB.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/0207104",
        "title": "Cosmological Spacetimes from Negative Tension Brane Backgrounds",
        "abstract": "We identify a time-dependent class of metrics with potential applications to\n cosmology, which emerge from negative-tension branes. The cosmology is based on\n a general class of solutions to Einstein-dilaton-Maxwell theory, presented in\n {hep-th/0106120}. We argue that solutions with hyperbolic or planar symmetry\n describe the gravitational interactions of a pair of negative-tension\n $q$-branes. These spacetimes are static near each brane, but become\n time-dependent and expanding at late epoch -- in some cases asymptotically\n approaching flat space. We interpret this expansion as being the spacetime's\n response to the branes' presence. The time-dependent regions provide explicit\n examples of cosmological spacetimes with past horizons and no past naked\n singularities. The past horizons can be interpreted as S-branes. We prove that\n the singularities in the static regions are repulsive to time-like geodesics,\n extract a cosmological `bounce' interpretation, compute the explicit charge and\n tension of the branes, analyse the classical stability of the solution (in\n particular of the horizons) and study particle production, deriving a general\n expression for Hawking's temperature as well as the associated entropy.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0811.2249",
        "title": "Solutions of Polynomial Systems Derived from the Steady Cavity Flow\nProblem",
        "abstract": "We propose a general algorithm to enumerate all solutions of a\n zero-dimensional polynomial system with respect to a given cost function. The\n algorithm is developed and is used to study a polynomial system obtained by\n discretizing the steady cavity flow problem in two dimensions. The key\n technique on which our algorithm is based is to solve polynomial optimization\n problems via sparse semidefinite programming relaxations (SDPR), which has been\n adopted successfully to solve reaction-diffusion boundary value problems\n recently. The cost function to be minimized is derived from discretizing the\n fluid's kinetic energy. The enumeration algorithm's solutions are shown to\n converge to the minimal kinetic energy solutions for SDPR of increasing order.\n We demonstrate the algorithm with SDPR of first and second order on polynomial\n systems for different scenarios of the cavity flow problem and succeed in\n deriving the $k$ smallest kinetic energy solutions. The question whether these\n solutions converge to solutions of the steady cavity flow problem is discussed,\n and we pose a conjecture for the minimal energy solution for increasing\n Reynolds number.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2109.08408",
        "title": "Finite-Size scaling analysis of many-body localization transition in\nquasi-periodic spin chains",
        "abstract": "We analyze the finite-size scaling of the average gap-ratio and the\n entanglement entropy across the many-body localization (MBL) transition in one\n dimensional Heisenberg spin-chain with quasi-periodic (QP) potential. By using\n the recently introduced cost-function approach, we compare different scenarios\n for the transition using exact diagonalization of systems up to 22 lattice\n sites. Our findings suggest that the MBL transition in the QP Heisenberg chain\n belongs to the class of Berezinskii-Kosterlitz-Thouless (BKT) transition, the\n same as in the case of uniformly disordered systems as advocated in recent\n studies. Moreover, we observe that the critical disorder strength shows a clear\n sub-linear drift with the system-size as compared to the linear drift seen in\n random disordered models, suggesting that the finite-size effects in the MBL\n transition for the QP systems are less severe than that in the random\n disordered scenario. Moreover, deep in the ergodic regime, we find an\n unexpected double-peak structure of distribution of on-site magnetizations that\n can be traced back to the strong correlations present in the QP potential.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2308.01315",
        "title": "Minimal model for the flat bands in copper-substituted lead phosphate\napatite: Strong diamagnetism from multi-orbital physics",
        "abstract": "The claims that a copper-substituted lead apatite, denoted as\n CuPb$_9$(PO$_4$)$_6$OH$_2$, could be a room-temperature superconductor have led\n to an intense research activity. While other research groups did not confirm\n these claims, and the hope of realizing superconductivity in this compound has\n all but vanished, other findings have emerged which motivate further work on\n this material. In fact, Density Functional Theory (DFT) calculations indicate\n the presence of two nearly flat bands near the Fermi level, which are known to\n host strongly correlated physics. In order to facilitate the theoretical study\n of the intriguing physics associated with these two flat bands, we propose a\n minimal tight-binding model which reproduces their main features. We then\n calculate the orbital magnetic susceptibility of our two-band model and find a\n large diamagnetic response which arises due to the multi-orbital nature of the\n bands and which could provide an explanation for the strong diamagnetism\n reported in experiments.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/0103136",
        "title": "Superbranes and Super Born-Infeld Theories from Nonlinear Realizations",
        "abstract": "We describe, on a few instructive examples, a systematic way of deducing the\n superfield equations of motion of superbranes in the approach of partial\n breaking of global supersymmetry (PBGS) from the nonlinear-realizations\n formalism. For D-branes these equations simultaneously represent the\n appropriate supersymmetric Born-Infeld theories. We also discuss how to\n construct an off-shell superfield action for the $N=2, d=4$ Dirac-Born-Infeld\n theory corresponding to the partial supersymmetry breaking $N=4 \\to N=2$ in\n $d=4$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1201.5254",
        "title": "Functionality in single-molecule devices: Model calculations and\napplications of the inelastic electron tunneling signal in molecular\njunctions",
        "abstract": "We analyze how functionality could be obtained within single-molecule devices\n by using a combination of non-equilibrium Green's functions and ab-initio\n calculations to study the inelastic transport properties of single-molecule\n junctions. First we apply a full non-equilibrium Green's function technique to\n a model system with electron-vibration coupling. We show that the features in\n the inelastic electron tunneling spectra (IETS) of the molecular junctions are\n virtually independent of the nature of the molecule-lead contacts. Since the\n contacts are not easily reproducible from one device to another, this is a very\n useful property. The IETS signal is much more robust versus modifications at\n the contacts and hence can be used to build functional nanodevices. Second, we\n consider a realistic model of a organic conjugated molecule. We use ab-initio\n calculations to study how the vibronic properties of the molecule can be\n controlled by an external electric field which acts as a gate voltage. The\n control, through the gate voltage, of the vibron frequencies and (more\n importantly) of the electron-vibron coupling enables the construction of\n functionality: non-linear amplification and/or switching is obtained from the\n IETS signal within a single-molecule device.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0807.2130",
        "title": "Direct observation of the e3ffect of isotope-induced disorder on the\nexciton binding energy in LiHxD1-x mixed crystals",
        "abstract": "The results of a quantitative study of the renormalization of the binding\n energy of the Wannier-Mott exciton by the isotope effect are presented.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2010.10568",
        "title": "On the diversity of asymmetries in gapped protoplanetary disks",
        "abstract": "Protoplanetary disks with large inner dust cavities are thought to host\n massive planetary or substellar companions. These disks show asymmetries and\n rings in the millimeter continuum, caused by dust trapping in pressure bumps,\n and potentially vortices or horseshoes. The origin of the asymmetries and their\n diversity remains unclear. We present a comprehensive study of 16 disks for\n which the gas surface density profile has been constrained by CO isotopologue\n data. We compare the azimuthal extents of the dust continuum profiles with the\n local gas surface density in each disk, and find that the asymmetries\n correspond to higher Stokes numbers or low gas surface density. We discuss\n which asymmetric structures can be explained by a horseshoe, a vortex or spiral\n density waves. Second, we reassess the gas gap radii from the $^{13}$CO maps,\n which are about a factor 2 smaller than the dust ring radii, suggesting that\n companions in these disks are in the brown dwarf mass regime ($\\sim 15-50\n M_{\\rm Jup}$) or in the Super-Jovian mass regime ($\\sim 3-15 M_{\\rm Jup}$) on\n eccentric orbits. This is consistent with the estimates from contrast curves on\n companion mass limits. These curves rule out (sub)stellar companions ($q>$0.05)\n for the majority of the sample at the gap location, but it remains possible at\n even smaller radii. Third, we find that spiral arms in scattered light images\n are primarily detected around high luminosity stars with disks with wide gaps,\n which can be understood by the dependence of the spiral arm pitch angle on disk\n temperature and companion mass.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1106.4538",
        "title": "Chiral Nonet Mixing in pi pi Scattering",
        "abstract": "Pion pion scattering is studied in a generalized linear sigma model which\ncontains two scalar nonets (one of quark-antiquark type and the other of\ndiquark-antidiquark type) and two corresponding pseudoscalar nonets. An\ninteresting feature concerns the mixing of the four isosinglet scalar mesons\nwhich yield poles in the scattering amplitude. Some realism is introduced by\nenforcing exact unitarity via the K-matrix method.\nIt is shown that a reasonable agreement with experimental data is obtained up\nto about 1 GeV. The poles in the unitarized scattering amplitude are studied in\nsome detail. The lowest pole clearly represents the sigma meson (or f0(600))\nwith a mass and decay width around 500 MeV. The second pole invites comparison\nwith the f0(980) which has a mass around 1 GeV and decay width around 100 MeV.\nThe third and fourth poles, resemble some of the isosinglet state in the\ncomplicated 1-2 GeV region. Some comparison is made to the situation in the\nusual SU(3) linear sigma model with a single scalar nonet.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1707.03071",
        "title": "Bright triplet excitons in lead halide perovskites",
        "abstract": "Nanostructured semiconductors emit light from electronic states known as\n excitons[1]. According to Hund's rules[2], the lowest energy exciton in organic\n materials should be a poorly emitting triplet state. Analogously, the lowest\n exciton level in all known inorganic semiconductors is believed to be optically\n inactive. These 'dark' excitons (into which the system can relax) hinder\n light-emitting devices based on semiconductor nanostructures. While strategies\n to diminish their influence have been developed[3-5], no materials have been\n identified in which the lowest exciton is bright. Here we show that the lowest\n exciton in quasi-cubic lead halide perovskites is optically active. We first\n use the effective-mass model and group theory to explore this possibility,\n which can occur when the strong spin-orbit coupling in the perovskite\n conduction band is combined with the Rashba effect [6-10]. We then apply our\n model to CsPbX3 (X=Cl,Br,I) nanocrystals[11], for which we measure size- and\n composition-dependent fluorescence at the single-nanocrystal level. The bright\n character of the lowest exciton immediately explains the anomalous\n photon-emission rates of these materials, which emit 20 and 1,000 times\n faster[12] than any other semiconductor nanocrystal at room[13-16] and\n cryogenic[17] temperatures, respectively. The bright exciton is further\n confirmed by detailed analysis of the fine structure in low-temperature\n fluorescence spectra. For semiconductor nanocrystals[18], which are already\n used in lighting[19,20], lasers[21,22], and displays[23], these optically\n active excitons can lead to materials with brighter emission and enhanced\n absorption. More generally, our results provide criteria for identifying other\n semiconductors exhibiting bright excitons with potentially broad implications\n for optoelectronic devices.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1906.05248",
        "title": "Multitask Learning for Network Traffic Classification",
        "abstract": "Traffic classification has various applications in today's Internet, from\n resource allocation, billing and QoS purposes in ISPs to firewall and malware\n detection in clients. Classical machine learning algorithms and deep learning\n models have been widely used to solve the traffic classification task. However,\n training such models requires a large amount of labeled data. Labeling data is\n often the most difficult and time-consuming process in building a classifier.\n To solve this challenge, we reformulate the traffic classification into a\n multi-task learning framework where bandwidth requirement and duration of a\n flow are predicted along with the traffic class. The motivation of this\n approach is twofold: First, bandwidth requirement and duration are useful in\n many applications, including routing, resource allocation, and QoS\n provisioning. Second, these two values can be obtained from each flow easily\n without the need for human labeling or capturing flows in a controlled and\n isolated environment. We show that with a large amount of easily obtainable\n data samples for bandwidth and duration prediction tasks, and only a few data\n samples for the traffic classification task, one can achieve high accuracy. We\n conduct two experiment with ISCX and QUIC public datasets and show the efficacy\n of our approach.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2401.08025",
        "title": "Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using\nSelf-Imagination",
        "abstract": "The potential of Vision-Language Models (VLMs) often remains underutilized in\n handling complex text-based problems, particularly when these problems could\n benefit from visual representation. Resonating with humans' ability to solve\n complex text-based problems by (1) creating a visual diagram from the problem\n and (2) deducing what steps they need to take to solve it, we propose\n Self-Imagine. We leverage a single Vision-Language Model (VLM) to generate a\n structured representation of the question using HTML, then render the HTML as\n an image, and finally use the same VLM to answer the question using both the\n question and the image. Our approach does not require any additional training\n data or training. We evaluate our approach on three mathematics tasks and nine\n general-purpose reasoning tasks using state-of-the-art (LLAVA-1.5 and GEMINI\n PRO) VLMs. Our approach boosts the performance of LLAVA-1.5 and GEMINI PRO on\n all math tasks (on average GSM8K: +3.1%; ASDIV: +3.2%; SVAMP: +6.9%) and the\n majority of the general-purpose reasoning tasks by 3.2% to 6.0% on average.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1804.05464",
        "title": "On Gradient-Based Learning in Continuous Games",
        "abstract": "We formulate a general framework for competitive gradient-based learning that\n encompasses a wide breadth of multi-agent learning algorithms, and analyze the\n limiting behavior of competitive gradient-based learning algorithms using\n dynamical systems theory. For both general-sum and potential games, we\n characterize a non-negligible subset of the local Nash equilibria that will be\n avoided if each agent employs a gradient-based learning algorithm. We also shed\n light on the issue of convergence to non-Nash strategies in general- and\n zero-sum games, which may have no relevance to the underlying game, and arise\n solely due to the choice of algorithm. The existence and frequency of such\n strategies may explain some of the difficulties encountered when using gradient\n descent in zero-sum games as, e.g., in the training of generative adversarial\n networks. To reinforce the theoretical contributions, we provide empirical\n results that highlight the frequency of linear quadratic dynamic games (a\n benchmark for multi-agent reinforcement learning) that admit global Nash\n equilibria that are almost surely avoided by policy gradient.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2308.06445",
        "title": "SGX-MR-Prot: Efficient and Developer-Friendly Access-Pattern Protection\nin Trusted Execution Environments",
        "abstract": "Trusted Execution Environments, such as Intel SGX, use hardware supports to\n ensure the confidentiality and integrity of applications against a compromised\n cloud system. However, side channels like access patterns remain for\n adversaries to exploit and obtain sensitive information. Common approaches use\n oblivious programs or primitives, such as ORAM, to make access patterns\n oblivious to input data, which are challenging to develop. This demonstration\n shows a prototype SGX-MR-Prot for efficiently protecting access patterns of\n SGX-based data-intensive applications and minimizing developers' efforts.\n SGX-MR-Prot uses the MapReduce framework to regulate application dataflows to\n reduce the cost of access-pattern protection and hide the data oblivious\n details from SGX developers. This demonstration will allow users to intuitively\n understand the unique contributions of the framework-based protection approach\n via interactive exploration and visualization.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.15771",
        "title": "First search for the absorption of fermionic dark matter with the\nPandaX-4T experiment",
        "abstract": "Compared with the signature of dark matter elastic scattering off nuclei, the\n absorption of fermionic dark matter by nuclei opens up a new searching channel\n for light dark matter with a characteristic monoenergetic signal. In this\n Letter, we explore the $95.0$-day data from the PandaX-4T commissioning run and\n report the first dedicated searching results of the fermionic dark matter\n absorption signal through a neutral current process. No significant signal was\n found, and the lowest limit on the dark matter-nucleon interaction cross\n section is set to be $1.5\\times10^{-50}$ cm$^2$ for a fermionic dark matter\n mass of $40$ MeV/$c^2$ with 90\\% confidence level.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2401.03624",
        "title": "The role of anharmonicity in single-molecule spin-crossover",
        "abstract": "We exploit the system-bath paradigm to investigate anharmonicity effects of\n vibrations on spin-crossover (SCO) in a single molecule. Focusing on weak\n coupling, we use the linear response approximation to deal with the vibrational\n bath and propagate the Redfield master equation to obtain the equilibrium high\n spin fraction. We take both the anharmonicity in the bath potentials and the\n nonlinearity in the spin-vibration coupling into account and find a strong\n interplay between these two effects. Further, we show that the SCO in a single\n molecule is always a gradual transition and the anharmonicity-induced phonon\n drag greatly affects the transition behavior.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1007.2432",
        "title": "Chiral Interactions of Histidine in a Hydrated Vermiculite",
        "abstract": "Recent work suggests a link between chiral asymmetry in the amino acid\n iso-valine extracted from the Murchison meteorite and the extent of hydrous\n alteration. We present the results of neutron scattering experiments on an\n exchanged, 1-dimensionally ordered n-propyl ammonium vermiculite clay. The\n vermiculite gel has a (001) d-spacing of order 5nm at the temperature and\n concentration of the experiments and the d-spacing responds sensitively to\n changes in concentration, temperature and electronic environment. The data show\n that isothermal addition of D-histidine or L-histidine solutions produces\n shifts in the d-spacing that are different for each enantiomer. This chiral\n specificity is of interest for the question of whether clays could have played\n an important role in the origin of biohomochirality.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2005.00864",
        "title": "Nonlinear analysis of charge-pump phase-locked loop: the hold-in and\npull-in ranges",
        "abstract": "In this paper a fairly complete mathematical model of CP-PLL, which reliable\n enough to serve as a tool for credible analysis of dynamical properties of\n these circuits, is studied. We refine relevant mathematical definitions of the\n hold-in and pull-in ranges related to the local and global stability. Stability\n analysis of the steady state for the charge-pump phase locked loop is\n non-trivial: straight-forward linearization of available CP-PLL models may lead\n to incorrect conclusions, because the system is not smooth near the steady\n state and may experience overload. In this work necessary details for local\n stability analysis are presented and the hold-in range is computed. An upper\n estimate of the pull-in range is obtained via the analysis of limit cycles. The\n study provided an answer to Gardner's conjecture on the similarity of transient\n responses of CP-PLL and equivalent classical PLL and to conjectures on the\n infinite pull-in range of CP-PLL with proportionally-integrating filter.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/gr-qc/0303090",
        "title": "Gauge Invariant Variables in Two-Parameter Nonlinear Perturbations",
        "abstract": "The procedure to find gauge invariant variables for two-parameter nonlinear\n perturbations in general relativity is considered. For each order metric\n perturbation, we define the variable which is defined by the appropriate\n combination with lower order metric perturbations. Under the gauge\n transformation, this variable is transformed in the manner similar to the gauge\n transformation of the linear order metric perturbation. We confirm this up to\n third order. This implies that gauge invariant variables for higher order\n metric perturbations can be found by using a procedure similar to that for\n linear order metric perturbations. We also derive gauge invariant combinations\n for the perturbation of an arbitrary physical variable, other than the\n spacetime metric, up to third order.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2008.01569",
        "title": "Doubly-hidden scalar heavy molecules and tetraquarks states from QCD at\nNLO",
        "abstract": "Alerted by the recent LHCb discovery of exotic hadrons in the range (6.2 --\n 6.9) GeV, we present new results for the doubly-hidden scalar heavy $(\\bar QQ)\n (Q\\bar Q)$ charm and beauty molecules using the inverse Laplace transform sum\n rule (LSR) within stability criteria and including the Next-to-Leading Order\n (NLO) factorized perturbative and $\\langle G^3\\rangle$ gluon condensate\n corrections. We also critically revisit and improve existing Lowest Order (LO)\n QCD spectral sum rules (QSSR) estimates of the $({ \\bar Q \\bar Q})(QQ)$\n tetraquarks analogous states. In the example of the anti-scalar-scalar\n molecule, we separate explicitly the contributions of the factorized and\n non-factorized contributions to LO of perturbative QCD and to the\n $\\langle\\alpha_sG^2\\rangle$ gluon condensate contributions in order to disprove\n some criticisms on the (mis)uses of the sum rules for four-quark currents. We\n also re-emphasize the importance to include PT radiative corrections for heavy\n quark sum rules in order to justify the (ad hoc) definition and value of the\n heavy quark mass used frequently at LO in the literature. Our LSR results for\n tetraquark masses summarized in Table II are compared with the ones from ratio\n of moments (MOM) at NLO and results from LSR and ratios of MOM at LO (Table\n IV). The LHCb broad structure around (6.2 --6.7) GeV can be described by the\n $\\overline{\\eta}_{c}{\\eta}_{c}$, $\\overline{J/\\psi}{J/\\psi}$ and\n $\\overline{\\chi}_{c1}{\\chi}_{c1}$ molecules or/and their analogue tetraquark\n scalar-scalar, axial-axial and vector-vector lowest mass ground states. The\n peak at (6.8--6.9) GeV can be likely due to a $\\overline{\\chi}_{c0}{\\chi}_{c0}$\n molecule or/and a pseudoscalar-pseudoscalar tetraquark state. Similar analysis\n is done for the scalar beauty states whose masses are found to be above the\n $\\overline\\eta_b\\eta_b$ and $\\overline\\Upsilon(1S)\\Upsilon(1S)$ thresholds.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ph/9906317",
        "title": "Next-to-Leading Order QCD corrections to the Lifetime Difference of\n$B_s$ Mesons",
        "abstract": "In this talk we present a calculation of the dacay rate difference in the\n neutral $B_s-\\bar{B}_s$ system, $\\Delta \\Gamma_{B_s}$, in next-to-leading order\n (NLO) QCD. We find a sizeable decrease compared to leading-order (LO)\n estimates: $ (\\Delta \\Gamma/\\Gamma)_{B_s} = (f_{B_s}/210 MeV)^2 [0.006 B(m_b) +\n 0.150 B_S(m_b) - 0.0063]$ in terms of the bag parameters $B$ and $B_S$ in the\n NDR scheme. We put special emphasize on the theoretical and physical\n implications of this quantity.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1703.04413",
        "title": "Topological Conjugacy of Non-hyperbolic Linear Flows",
        "abstract": "The topological equivalence classification for linear flows on $\\mathbb{R}^n$\n had been completely solved by Kuiper and independently Ladis in 1973. However,\n Ladis' proof was published in a Russian journal which isn't easily available,\n Kuiper's proof is more topological and a little bit subtle. Aiming at\n topological conjugacy classification, mainly based on the ideas of Kuiper, we\n introduce other techniques and try to present an elementary and self-contained\n proof just using linear algebra and elementary topology.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2307.03413",
        "title": "Unsupervised Hyperspectral and Multispectral Images Fusion Based on the\nCycle Consistency",
        "abstract": "Hyperspectral images (HSI) with abundant spectral information reflected\n materials property usually perform low spatial resolution due to the hardware\n limits. Meanwhile, multispectral images (MSI), e.g., RGB images, have a high\n spatial resolution but deficient spectral signatures. Hyperspectral and\n multispectral image fusion can be cost-effective and efficient for acquiring\n both high spatial resolution and high spectral resolution images. Many of the\n conventional HSI and MSI fusion algorithms rely on known spatial degradation\n parameters, i.e., point spread function, spectral degradation parameters,\n spectral response function, or both of them. Another class of deep\n learning-based models relies on the ground truth of high spatial resolution HSI\n and needs large amounts of paired training images when working in a supervised\n manner. Both of these models are limited in practical fusion scenarios. In this\n paper, we propose an unsupervised HSI and MSI fusion model based on the cycle\n consistency, called CycFusion. The CycFusion learns the domain transformation\n between low spatial resolution HSI (LrHSI) and high spatial resolution MSI\n (HrMSI), and the desired high spatial resolution HSI (HrHSI) are considered to\n be intermediate feature maps in the transformation networks. The CycFusion can\n be trained with the objective functions of marginal matching in single\n transform and cycle consistency in double transforms. Moreover, the estimated\n PSF and SRF are embedded in the model as the pre-training weights, which\n further enhances the practicality of our proposed model. Experiments conducted\n on several datasets show that our proposed model outperforms all compared\n unsupervised fusion methods. The codes of this paper will be available at this\n address: https: //github.com/shuaikaishi/CycFusion for reproducibility.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1809.05612",
        "title": "On the Homology of the Space of Curves Immersed in The Sphere with\nCurvature Constrained to a Prescribed Interval",
        "abstract": "While the topology of the space of all smooth immersed curves on the\n $2$-sphere $\\mathbb{S}^2$ that start and end at given points in given\n directions is well known, it is an open problem to understand the homotopy type\n of its subspaces consisting of the curves whose geodesic curvatures are\n constrained to a prescribed proper open interval. In this article we prove\n that, under certain circumstances for endpoints and end directions, these\n subspaces are not homotopically equivalent to the whole space. Moreover, we\n give an explicit construction of exotic generators for some homotopy and\n cohomology groups. It turns out that the dimensions of these generators depend\n on endpoints and end directions. A version of the h-principle is used to prove\n these results.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1609.01368",
        "title": "Product BMO, little BMO and Riesz Commutators in the Bessel setting",
        "abstract": "In this paper, we study the product BMO space, little bmo space and their\n connections with the corresponding commutators associated with Bessel operators\n studied by Weinstein, Huber, and by Muckenhoupt-Stein. We first prove that the\n product BMO space in the Bessel setting can be used to prove the boundedness of\n the iterated commutators with the Bessel Riesz transforms. We next study the\n little $\\rm bmo$ space in this Bessel setting and obtain the equivalent\n characterization of this space in terms of commutators. We further show that in\n analogy with the classical setting, the little $\\rm bmo$ space is a proper\n subspace of the product $\\rm BMO$ space. These extend the previous related\n results studied by Cotlar-Sadosky and Ferguson-Sadosky on the bidisc to the\n Bessel setting.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ex/0106103",
        "title": "Test of CPT and Lorentz invariance from muonium spectroscopy",
        "abstract": "Following a suggestion of Kostelecky et al. we have evaluated a test of CPT\n and Lorentz invariance from the microwave spectroscopy of muonium. Hamiltonian\n terms beyond the standard model violating CPT and Lorentz invariance would\n contribute frequency shifts $\\delta\\nu_{12}$ and $\\delta\\nu_{34}$ to $\\nu_{12}$\n and $\\nu_{34}$, the two transitions involving muon spin flip, which were\n precisely measured in ground state muonium in a strong magnetic field of 1.7 T.\n The shifts would be indicated by anti-correlated oscillations in $\\nu_{12}$ and\n $\\nu_{34}$ at the earth's sidereal frequency. No time dependence was found in\n $\\nu_{12}$ or $\\nu_{34}$ at the level of 20 Hz, limiting the size of some CPT\n and Lorentz violating parameters at the level of $2\\times10^{-23}$ GeV,\n representing Planck scale sensitivity and an order of magnitude improvement in\n sensitivity over previous limits for the muon.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1103.3167",
        "title": "Fidelity decay in interacting two-level boson systems: Freezing and\nrevivals",
        "abstract": "We study the fidelity decay in the $k$-body embedded ensembles of random\n matrices for bosons distributed in two single-particle states, considering the\n reference or unperturbed Hamiltonian as the one-body terms and the diagonal\n part of the $k$-body embedded ensemble of random matrices, and the perturbation\n as the residual off-diagonal part of the interaction. We calculate the\n ensemble-averaged fidelity with respect to an initial random state within\n linear response theory to second order on the perturbation strength, and\n demonstrate that it displays the freeze of the fidelity. During the freeze, the\n average fidelity exhibits periodic revivals at integer values of the Heisenberg\n time $t_H$. By selecting specific $k$-body terms of the residual interaction,\n we find that the periodicity of the revivals during the freeze of fidelity is\n an integer fraction of $t_H$, thus relating the period of the revivals with the\n range of the interaction $k$ of the perturbing terms. Numerical calculations\n confirm the analytical results.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2206.06028",
        "title": "Compact active vibration isolation and tilt stabilization for a portable\nhigh-precision atomic gravimeter",
        "abstract": "In high-precision atomic gravimeters, a rest mass is needed to provide a\n gravity reference, which is typically the retro-reflecting mirror of the Raman\n laser beams that addresses the two hyperfine ground states of the specific\n free-falling atomic test mass. We constructed a compact active feedback control\n system for the retro-reflecting mirror that provides vibration isolation in the\n vertical axis as well as two rotation stabilization in the horizontal plane.\n The active feedback control provides vertical vibration reduction of up to a\n factor of 300 in the frequency range of 0.03 to 10 Hz and tilt stabilization of\n approximately $\\pm$ 1 $\\mu$rad in the noisy lab environment. This system has\n enabled high-precision gravity measurements on a portable gravimeter with\n sensitivity reaching $6.4\\times 10^{-8}$ g/$\\sqrt{\\text{Hz}}$ and resolution of\n $2.8\\times 10^{-9}$ g after an integration time of 4000 s.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1701.06037",
        "title": "The Hessian of quantized Ding functionals and its asymptotic behavior",
        "abstract": "We compute the Hessian of quantized Ding functionals and give an elementary\n proof for the convexity of quantized Ding functionals along Bergman geodesics\n from the view point of projective geometry. We study also the asymptotic\n behavior of the Hessian using the Berezin-Toeplitz quantization.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1612.00103",
        "title": "Review on Hadron Spectroscopy",
        "abstract": "I review some of the lattice results on spectroscopy and resonances in the\n past years. For the conventional hadron spectrum computations, focus has been\n put on the isospin breaking effects, QED effects, and simulations near the\n physical pion mass point. I then go through several single-channel scattering\n studies within L\\\"uscher formalism, a method that has matured over the past few\n years. The topics cover light mesons and also the charmed mesons, with the\n latter case intimately related to the recently discovered exotic $XYZ$\n particles. Other possible related formalisms that are available on the market\n are also discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.06118",
        "title": "CHITNet: A Complementary to Harmonious Information Transfer Network for\nInfrared and Visible Image Fusion",
        "abstract": "Current infrared and visible image fusion (IVIF) methods go to great lengths\n to excavate complementary features and design complex fusion strategies, which\n is extremely challenging. To this end, we rethink the IVIF outside the box,\n proposing a complementary to harmonious information transfer network (CHITNet).\n It reasonably transfers complementary information into harmonious one, which\n integrates both the shared and complementary features from two modalities.\n Specifically, to skillfully sidestep aggregating complementary information in\n IVIF, we design a mutual information transfer (MIT) module to mutually\n represent features from two modalities, roughly transferring complementary\n information into harmonious one. Then, a harmonious information acquisition\n supervised by source image (HIASSI) module is devised to further ensure the\n complementary to harmonious information transfer after MIT. Meanwhile, we also\n propose a structure information preservation (SIP) module to guarantee that the\n edge structure information of the source images can be transferred to the\n fusion results. Moreover, a mutual promotion training paradigm (MPTP) with\n interaction loss is adopted to facilitate better collaboration among MIT,\n HIASSI and SIP. In this way, the proposed method is able to generate fused\n images with higher qualities. Extensive experimental results demonstrate the\n superiority of our CHITNet over state-of-the-art algorithms in terms of visual\n quality and quantitative evaluations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2011.10211",
        "title": "Portable Magnetic Particle Spectrometer (MPS) for Future Rapid and\nWash-free Bioassays",
        "abstract": "Nowadays, there is an increasing demand for more accessible routine\n diagnostics for patients with respect to high accuracy, ease of use, and low\n cost. However, the quantitative and high accuracy bioassays in large hospitals\n and laboratories usually require trained technicians and equipment that is both\n bulky and expensive. In addition, the multi-step bioassays and long turnaround\n time could severely affect the disease surveillance and control especially in\n pandemics such as influenza and COVID-19. In view of this, a portable,\n quantitative bioassay device will be valuable in regions with scarce medical\n resources and help relieve burden on local healthcare systems. Herein, we\n introduce the MagiCoil diagnostic device, an inexpensive, portable,\n quantitative and rapid bioassay platform based on magnetic particle\n spectrometer (MPS) technique. MPS detects the dynamic magnetic responses of\n magnetic nanoparticles (MNPs) and uses the harmonics from oscillating MNPs as\n metrics for sensitive and quantitative bioassays. This device does not require\n trained technicians to operate and employs a fully automatic, one-step,\n wash-free assay with user friendly smartphone interface. Using a\n streptavidin-biotin binding system as a model, we show that the detection limit\n of the current portable device for streptavidin is 64 nM (equal to 5.12 pmole).\n In addition, this MPS technique is very versatile and allows for the detection\n of different diseases just by changing the surface modifications on MNPs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2402.15478",
        "title": "Transformers are Expressive, But Are They Expressive Enough for\nRegression?",
        "abstract": "Transformers have become pivotal in Natural Language Processing,\n demonstrating remarkable success in applications like Machine Translation and\n Summarization. Given their widespread adoption, several works have attempted to\n analyze the expressivity of Transformers. Expressivity of a neural network is\n the class of functions it can approximate. A neural network is fully expressive\n if it can act as a universal function approximator. We attempt to analyze the\n same for Transformers. Contrary to existing claims, our findings reveal that\n Transformers struggle to reliably approximate smooth functions, relying on\n piecewise constant approximations with sizable intervals. The central question\n emerges as: \"Are Transformers truly Universal Function Approximators?\" To\n address this, we conduct a thorough investigation, providing theoretical\n insights and supporting evidence through experiments. Theoretically, we prove\n that Transformer Encoders cannot approximate smooth functions. Experimentally,\n we complement our theory and show that the full Transformer architecture cannot\n approximate smooth functions. By shedding light on these challenges, we\n advocate a refined understanding of Transformers' capabilities.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0005189",
        "title": "On the quantum model of gravitational electrodynamics",
        "abstract": "It is shown that application of dynamic flows concept in 4-dimensional\n Euclidean space makes possible to form Minkowski space and to formulate the\n generalized variational problem of electrodynamics and gravi- dynamics. It is\n shown that 1-dimensional (cylindrical) factorization of 4-dimensional Euclidean\n space provides a quantization of ths model.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2008.09808",
        "title": "On the emergent System Mass Function: the contest between accretion and\nfragmentation",
        "abstract": "We propose a new model for the evolution of a star cluster's System Mass\n Function (SMF). The model involves both turbulent fragmentation and competitive\n accretion. Turbulent fragmentation creates low-mass seed proto-systems (i.e.\n single and multiple protostars). Some of these low-mass seed proto-systems then\n grow by competitive accretion to produce the high-mass power-law tail of the\n SMF. Turbulent fragmentation is relatively inefficient, in the sense that the\n creation of low-mass seed proto-systems only consumes a fraction, $\\sim 23\\%$\n (at most $\\sim 50\\%$), of the mass available for star formation. The remaining\n mass is consumed by competitive accretion. Provided the accretion rate onto a\n proto-system is approximately proportional to its mass ($dm/dt \\propto m$), the\n SMF develops a power-law tail at high masses with the Salpeter slope ($\\sim\n -2.3$). If the rate of supply of mass accelerates, the rate of proto-system\n formation also accelerates, as appears to be observed in many clusters.\n However, even if the rate of supply of mass decreases, or ceases and then\n resumes, the SMF evolves homologously, retaining the same overall shape, and\n the high-mass power-law tail simply extends to ever higher masses until the\n supply of gas runs out completely. The Chabrier SMF can be reproduced very\n accurately if the seed proto-systems have an approximately log-normal mass\n distribution with median mass $\\sim 0.11 {\\rm M}_{_\\odot}$ and logarithmic\n standard deviation $\\sigma_{\\log_{10}(M/{\\rm M}_\\odot)}\\sim 0.47$).",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1407.2192",
        "title": "An ensemble Kushner-Stratonovich-Poisson filter for recursive estimation\nin nonlinear dynamical systems",
        "abstract": "Despite the numerous applications that may be expeditiously modelled by\n counting processes, stochastic filtering strategies involving Poisson-type\n observations still remain somewhat poorly developed. In this work, we propose a\n Monte Carlo stochastic filter for recursive estimation in the context of\n linear/nonlinear dynamical systems with Poisson-type measurements. A key aspect\n of the present development is the filter-update scheme, derived from an\n ensemble approximation of the time-discretized nonlinear filtering equation,\n modified to account for Poisson-type measurements. Specifically, the additive\n update through a gain-like correction term, empirically approximated from the\n innovation integral in the filtering equation, eliminates the problem of\n particle collapse encountered in many conventional particle filters. Through a\n few numerical demonstrations, the versatility of the proposed filter is brought\n forth, first with application to filtering problems with diffusive or\n Poisson-type measurements and then to an automatic control problem wherein the\n extremization of the associated cost functional is achieved simply by an\n appropriate redefinition of the innovation process.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0303032",
        "title": "Selection criteria for targets of asteroseismic campaigns",
        "abstract": "Various dedicated satellite projects are underway or in advanced stages of\n planning to perform high-precision, long duration time series photometry of\n stars, with the purpose of using the frequencies of stellar oscillations to put\n new constraints on the internal structure of stars. It is known (cf. Brown, et\n al. 1994) that the effectiveness of oscillation frequencies in constraining\n stellar model parameters is significantly higher if classical parameters such\n as effective temperature, and luminosity are known with high precision. In\n order to optimize asteroseismic campaigns it is therefore useful to select\n targets from among candidates for which good spectroscopic and astrometric data\n already exists. This paper presents selection criteria, as well as\n redeterminations of stellar luminosity and reddening for stars satisfying these\n criteria.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2101.10093",
        "title": "Single-Crystal NMR Spectroscopy",
        "abstract": "Single-crystal (SC) NMR spectroscopy is a solid-state NMR method that has\n been used since the early days of NMR to study the magnitude and orientation of\n tensorial nuclear spin interactions in solids. This review first presents the\n field of SC NMR instrumentation, then provides a survey of software for\n analysis of SC NMR data, and finally it highlights selected applications of SC\n NMR in various fields of research. The aim of the last part is not to provide a\n complete review of all SC NMR literature but to provide examples that\n demonstrate interesting applications of SC NMR.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1306.0930",
        "title": "Stationary States and Asymptotic Behaviour of Aggregation Models with\nNonlinear Local Repulsion",
        "abstract": "We consider a continuum aggregation model with nonlinear local repulsion\n given by a degenerate power-law diffusion with general exponent. The steady\n states and their properties in one dimension are studied both analytically and\n numerically, suggesting that the quadratic diffusion is a critical case. The\n focus is on finite-size, monotone and compactly supported equilibria. We also\n investigate numerically the long time asymptotics of the model by simulations\n of the evolution equation. Issues such as metastability and local/ global\n stability are studied in connection to the gradient flow formulation of the\n model.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2102.05593",
        "title": "Quantum Variational Optimization of Ramsey Interferometry and Atomic\nClocks",
        "abstract": "We discuss quantum variational optimization of Ramsey interferometry with\n ensembles of $N$ entangled atoms, and its application to atomic clocks based on\n a Bayesian approach to phase estimation. We identify best input states and\n generalized measurements within a variational approximation for the\n corresponding entangling and decoding quantum circuits. These circuits are\n built from basic quantum operations available for the particular sensor\n platform, such as one-axis twisting, or finite range interactions. Optimization\n is defined relative to a cost function, which in the present study is the\n Bayesian mean square error of the estimated phase for a given prior\n distribution, i.e. we optimize for a finite dynamic range of the\n interferometer. In analogous variational optimizations of optical atomic\n clocks, we use the Allan deviation for a given Ramsey interrogation time as the\n relevant cost function for the long-term instability. Remarkably, even\n low-depth quantum circuits yield excellent results that closely approach the\n fundamental quantum limits for optimal Ramsey interferometry and atomic clocks.\n The quantum metrological schemes identified here are readily applicable to\n atomic clocks based on optical lattices, tweezer arrays, or trapped ions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1402.2369",
        "title": "Discovery of new low-excitation planetary nebulae",
        "abstract": "We report a multi-wavelength study of four new planetary nebula (PN)\n candidates selected from the INT/WFC Photometric Ha Survey of the Northern\n Galactic Plane (IPHAS) and Deep Sky Hunter (DSH) catalogues. We present\n mid-resolution optical spectra of these PNs. The PN status of our sample was\n confirmed by optical narrow-band images and mid-resolution spectra. Based on\n the locations of these objects in the log (Ha/[N II]) versus log (Ha/[S II])\n diagnostic diagram, we conclude that these sources are evolved lowexcitation\n PNs. The optical and infrared appearances of these newly discovered PNs are\n discussed. Three of the new nebulae studied here are detected in infrared and\n have low infrared-to-radio flux ratios, probably suggesting that they are\n evolved. Furthermore, we derive the dynamical ages and distances of these\n nebulae and study the spectral energy distribution for one of them with\n extensive infrared archival data.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2204.07470",
        "title": "Explaining the 96 GeV Di-photon Anomaly in a Generic 2HDM Type-III",
        "abstract": "Motivated by results recently reported by the CMS Collaboration about an\n excess in the di-photon spectrum at about 96 GeV, especially when combined with\n another long-standing anomaly at the same value in the $b\\bar b$ invariant mass\n spectrum in four-jet events collected at LEP, we show that a possible\n explanation to both phenomena can be found at 1$\\sigma$ level in a generic\n 2-Higgs Doublet Model (2HDM) of Type-III in presence of a specific Yukawa\n texture, wherein Lepton Flavour Violating (LFV) (neutral) currents are induced\n at tree level. Bounds from Higgs data play a major role in limiting the\n parameter space of this scenario, yet we find solutions with $m_H = 125$ GeV\n and $m_h = 96$ GeV consistent with current theoretical and experimental bounds.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1902.07904",
        "title": "Filamentous Active Matter: Band Formation, Bending, Buckling, and\nDefects",
        "abstract": "Motor proteins drive persistent motion and self-organisation of cytoskeletal\n filaments. However, state-of-the-art microscopy techniques and continuum\n modelling approaches focus on large length and time scales. Here, we perform\n component-based computer simulations of polar filaments and molecular motors\n linking microscopic interactions and activity to self-organisation and dynamics\n from the two-filament level up to the mesoscopic domain level. Dynamic filament\n crosslinking and sliding, and excluded-volume interactions promote formation of\n bundles at small densities, and of active polar nematics at high densities. A\n buckling-type instability sets the size of polar domains and the density of\n topological defects. We predict a universal scaling of the active diffusion\n coefficient and the domain size with activity, and its dependence on parameters\n like motor concentration and filament persistence length. Our results provide a\n microscopic understanding of cytoplasmic streaming in cells and help to develop\n design strategies for novel engineered active materials.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2206.15324",
        "title": "Topological superconductivity enhanced by exceptional points",
        "abstract": "Majorana zero modes (MZMs) emerge as edge states in topological\n superconductors and are promising for topological quantum computation, but\n their detection has so far been elusive. Here we show that non-Hermiticity can\n be used to obtain dramatically more robust MZMs. The enhanced properties appear\n as a result of an extreme instability of exceptional points to\n superconductivity, such that even a vanishingly small superconducting order\n parameter already opens a large energy gap, produces well-localized MZMs, and\n leads to strong superconducting pair correlations. Our work thus illustrates\n the large potential of enhancing topological superconductivity using\n non-Hermitian exceptional points.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1911.09426",
        "title": "KPZ statistics of second class particles in ASEP via mixing",
        "abstract": "We consider the asymmetric simple exclusion process on $\\mathbb{Z}$ with a\n single second class particle initially at the origin. The first class particles\n form two rarefaction fans which come together at the origin, where the large\n time density jumps from $0$ to $1$. We are interested in $X(t)$, the position\n of the second class particle at time $t$. We show that, under the KPZ $1/3$\n scaling, $X(t)$ is asymptotically distributed as the difference of two\n independent, $\\mathrm{GUE}$-distributed random variables.The key part of the\n proof is to show that $X(t)$ equals, up to a negligible term, the difference of\n a random number of holes and particles, with the randomness built up by ASEP\n itself. This provides a KPZ analogue to the 1994 result of Ferrari and Fontes\n \\cite{FF94b}, where this randomness comes from the initial data and leads to\n Gaussian limit laws.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2104.01120",
        "title": "Linear Systems can be Hard to Learn",
        "abstract": "In this paper, we investigate when system identification is statistically\n easy or hard, in the finite sample regime. Statistically easy to learn linear\n system classes have sample complexity that is polynomial with the system\n dimension. Most prior research in the finite sample regime falls in this\n category, focusing on systems that are directly excited by process noise.\n Statistically hard to learn linear system classes have worst-case sample\n complexity that is at least exponential with the system dimension, regardless\n of the identification algorithm. Using tools from minimax theory, we show that\n classes of linear systems can be hard to learn. Such classes include, for\n example, under-actuated or under-excited systems with weak coupling among the\n states. Having classified some systems as easy or hard to learn, a natural\n question arises as to what system properties fundamentally affect the hardness\n of system identifiability. Towards this direction, we characterize how the\n controllability index of linear systems affects the sample complexity of\n identification. More specifically, we show that the sample complexity of\n robustly controllable linear systems is upper bounded by an exponential\n function of the controllability index. This implies that identification is easy\n for classes of linear systems with small controllability index and potentially\n hard if the controllability index is large. Our analysis is based on recent\n statistical tools for finite sample analysis of system identification as well\n as a novel lower bound that relates controllability index with the least\n singular value of the controllability Gramian.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0806.0787",
        "title": "Power reductivity over an arbitrary base",
        "abstract": "Our starting point is Mumford's conjecture, on representations of Chevalley\n groups over fields, as it is phrased in the preface of \"Geometric Invariant\n Theory\". After extending the conjecture appropriately, we show that it holds\n over an arbitrary commutative base ring. We thus obtain the first fundamental\n theorem of invariant theory (often referred to as Hilbert's fourteenth problem)\n over an arbitrary Noetherian ring. We also prove results on the Grosshans\n graded deformation of an algebra in the same generality. We end with tentative\n finiteness results for rational cohomology over the integers.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2006.04393",
        "title": "The Gaia-ESO Survey: an extremely Li-rich giant in the globular cluster\nNGC 1261",
        "abstract": "Lithium rich stars in globular clusters are rare. In fact, only 14 have been\n found so far, in different evolutionary phases from dwarfs to giants. Different\n mechanisms have been proposed to explain this enhancement, but it is still an\n open problem. Using spectra collected within the Gaia-ESO Survey, obtained with\n the GIRAFFE spectrograph at the ESO Very Large Telescope, we present the\n discovery of the first Li-rich star in the cluster NGC 1261, the second star\n known in the red giant branch bump phase. The star shows an extreme Li\n overabundance of A(Li)_LTE=3.92\\pm0.14, corresponding to A(Li)_NLTE=3.40 dex.\n We propose that the Li enhancement is caused by fresh Li production through an\n extra mixing process (sometimes referred to as {\\em cool bottom burning}) or\n could be a pre-existing Li overabundance resulting from binary mass transfer,\n likely from a red giant branch star, because of the low barium abundance. To\n unambiguously explain the Li enhancement in globular cluster stars, however, a\n reliable determination of the abundance of key species like Be, 6Li, 12C/13C,\n and several s-process elements is required, as well as detailed modeling of\n chromospheric activity indicators.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1406.4497",
        "title": "A Chandra-HETG VIEW OF MCG +8-11-11",
        "abstract": "We present a spectral analysis of the 118 ks High Energy Transmission\n Gratings (HETG) observation of the X-ray bright Seyfert 1.5 galaxy MCG\n +8-11-11, in conjunction with 100 ks of archival Suzaku data, aimed at\n investigating the signatures of warm absorption and Compton reflection reported\n from previous Suzaku and XMM-Newton studies of the source. Contrary to previous\n results, we find that warm absorption is not required by the data. Instead, we\n report upper limits on absorption lines that are below previous (marginal)\n detections. Fe Ka line emission is clearly detected and is likely resolved with\n sigma ~ 0.02 keV with the HETG data. We applied self-consistent, broadband\n spectral-fitting models to the HETG and Suzaku data to investigate this and\n other signatures of distant absorption and reflection. Utilizing in particular\n the MYTorus model, we find that the data are consistent with reprocessing by a\n distant, neutral torus that is Compton thick and out of the line-of-sight.\n However, we do not find compelling evidence of a relativistically-broadened\n Fe-K emission line, which is often expected from type 1 AGN. This is consistent\n with some, although not all, previous studies of MCG +8-11-11. A well-measured\n edge is identified by the HETG near 0.5 keV, indicating neutral absorption in\n the line of sight that is consistent with galactic absorption; however, the\n absorption may be partially intrinsic to the source. The HETG data are\n consistent with the presence of a soft excess, a feature that may be missed by\n considering the Suzaku data alone.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2405.09534",
        "title": "Learning-Based Compress-and-Forward Schemes for the Relay Channel",
        "abstract": "The relay channel, consisting of a source-destination pair along with a\n relay, is a fundamental component of cooperative communications. While the\n capacity of a general relay channel remains unknown, various relaying\n strategies, including compress-and-forward (CF), have been proposed. In CF, the\n relay forwards a quantized version of its received signal to the destination.\n Given the correlated signals at the relay and destination, distributed\n compression techniques, such as Wyner--Ziv coding, can be harnessed to utilize\n the relay-to-destination link more efficiently. Leveraging recent advances in\n neural network-based distributed compression, we revisit the relay channel\n problem and integrate a learned task-aware Wyner--Ziv compressor into a\n primitive relay channel with a finite-capacity out-of-band relay-to-destination\n link. The resulting neural CF scheme demonstrates that our compressor recovers\n binning of the quantized indices at the relay, mimicking the optimal asymptotic\n CF strategy, although no structure exploiting the knowledge of source\n statistics was imposed into the design. The proposed neural CF, employing\n finite order modulation, operates closely to the rate achievable in a primitive\n relay channel with a Gaussian codebook. We showcase the advantages of\n exploiting the correlated destination signal for relay compression through\n various neural CF architectures that involve end-to-end training of the\n compressor and the demodulator components. Our learned task-oriented\n compressors provide the first proof-of-concept work toward interpretable and\n practical neural CF relaying schemes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.07685",
        "title": "The residue-counts of $x^2+a/x$ modulo a prime",
        "abstract": "For a prime $p>3$ and $a\\in \\Bbb Z$ with $p\\nmid a$ let $V_p(x^2+\\frac ax)$\n be the residue-counts of $x^2+\\frac ax$ modulo $p$ as $x$ runs over\n $1,2,\\ldots,p-1$. In this paper, we obtain an explicit formula for\n $V_p(x^2+\\frac ax)$, which is concerned with cubic residues and binary\n quadratic forms.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1512.07593",
        "title": "Regularity of distributions of Wigner integrals",
        "abstract": "Wigner integrals and the corresponding Wigner chaos were introduced by P.\nBiane and R. Speicher in 1998 as a non-commutative counterpart of classical\nWiener-It\\^o integrals and the corresponding Wiener-It\\^o chaos, respectively,\nin free probability.\nIn the classical case, a famous result of I. Shigekawa states that\nnon-trivial elements in the finite Wiener-It\\^o chaos have an absolutely\ncontinuous distribution. We provide here a first contribution to such\nregularity questions for Wigner integrals by showing that the distribution of\nnon-trivial elements in the finite Wigner chaos cannot have atoms. This answers\na question of I. Nourdin and G. Peccati.\nFor doing so, we establish the notion of directional gradients in the context\nof the free Malliavin calculus. These directional gradients bridge between free\nMalliavin calculus and the theory of non-commutative derivations as initiated\nby D. Voiculescu and Y. Dabrowski. Methods recently invented by R. Speicher, M.\nWeber, and the author for treating similar questions in the case of finitely\nmany variables are extended, such that they apply to directional gradients.\nThis approach also excludes zero-divisors for the considered elements in the\nfinite Wigner chaos.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2109.05515",
        "title": "One-dimensional purely Lee-Huang-Yang fluids dominated by quantum\nfluctuations in two-component Bose-Einstein condensates",
        "abstract": "Lee-Huang-Yang (LHY) fluids are an exotic quantum matter dominated purely by\n quantum fluctuations. Recently, the three-dimensional LHY fluids were observed\n in ultracold atoms experiments, while their low-dimensional counterparts have\n not been well known. Herein, based on the Gross-Pitaevskii equation of\n one-dimensional LHY quantum fluids in two-component Bose-Einstein condensates,\n we reveal analytically and numerically the formation, properties, and dynamics\n of matter-wave structures therein. Considering a harmonic trap, approximate\n analytical results are obtained based on variational approximation, and\n higher-order nonlinear localized modes with nonzero nodes are constructed\n numerically. Stability regions of all the LHY nonlinear localized modes are\n identified by linear-stability analysis and direct perturbed numerical\n simulations. Movements and oscillations of single localized mode, and\n collisions between two modes, under the influence of different initial kicks\n are also studied in dynamical evolutions. The predicted results are available\n to quantum-gas experiments, providing a new insight into LHY physics in\n low-dimensional settings.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2207.03769",
        "title": "Laser-guided lightning",
        "abstract": "Electric currents circulating between charged clouds and the earth surface\n during lightning discharges are responsible for considerable damages and\n casualties. It is therefore important to develop better protection methods in\n addition to the traditional Franklin rod. Here we present the first\n demonstration that filaments formed by short and intense laser pulses can guide\n lightning discharges over considerable distances. We believe that this\n experimental breakthrough will lead to progress in lightning protection and\n lightning physics. An experimental campaign was carried out on the S\\\"antis\n Mountain in Northeastern Switzerland during the Summer of 2021 with a high\n repetition rate terawatt laser. The guiding of an upward negative lightning\n leader over a distance of 50 m was recorded by two separate high-speed cameras.\n The guiding of negative lightning leaders by laser filaments was corroborated\n in three other instances by VHF interferometric measurements, and the number of\n X-ray bursts detected during guided lightning events was significantly\n increased. While this research field has been very active for more than 20\n years with many research groups around the world working to achieve this goal,\n this result demonstrates lightning guiding by lasers, which may lead to the\n development of a laser lightning rod. This work paves the way for new\n atmospheric applications of ultrashort lasers and represents a significant step\n forward in the development of a laser based lightning protection for airports,\n launchpads or large infrastructures.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1903.03233",
        "title": "Some extremal results on K_{s,t}-free graphs",
        "abstract": "For graphs $H$ and $F$, let $\\text{ex}(n,H,F)$ be the maximum possible number\n of copies of $H$ in an $F$-free graph on $n$ vertices. The study of this\n function, which generalizes the well-known Tur\\'{a}n number of graphs, was\n systematically studied by Alon and Shikhelman recently. In this paper, we show\n that for any $m$ and $t\\ge 2m-3\\ge3$,\n \\[\\text{ex}(n,K_{m},K_{2,t})=\\Theta(n^{\\frac{3}{2}}).\\] This result improves\n some results of Alon and Shikhelman (J. Combin. Theory Ser. B, 121:146-172,\n 2016). We also study the $k$-partite $K_{s,t}$-free graph, we show that for any\n $k\\ge3$ and $t\\ge(k-1)(s-1)!+1$, \\[\\text{ex}_{\\chi\\le\n k}(n,K_{s,t})\\ge\\frac{k-1}{2k}n^{2-1/s}+o(n^{2-1/s}).\\] Moreover, we give a new\n construction of $3$-partite $K_{2,2t+1}$-free graphs with many edges.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0405140",
        "title": "VERA Observation of the W49N H2O Maser Outburst in 2003 October",
        "abstract": "We report on a strong outburst of the W49N H2O maser observed with VERA.\n Single-dish monitoring with VERA 20 m telescopes detected a strong outburst of\n the maser feature at V_LSR = -30.7 km/s in 2003 October. The outburst had a\n duration of ~100 days and a peak intensity of 7.9 x 10^4 Jy, being one of the\n strongest outbursts in W49N observed so far. VLBI observations with the VERA\n array were also carried out near to the maximum phase of the outburst, and the\n outburst spot was identified in the VLBI map. While the map was in good\n agreement with previous studies, showing three major concentrations of maser\n spots, we found a newly formed arc-like structure in the central maser\n concentration, which may be a shock front powered by a forming star or a star\n cluster. The outburst spot was found to be located on the arc-like structure,\n indicating a possible connection of the present outburst to a shock phenomenon.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1801.02252",
        "title": "Spaces with polynomial hulls that contain no analytic discs",
        "abstract": "Extensions of the notions of polynomially and rationally convex hulls are\n introduced. Using these notions, a generalization of a result of Duval and\n Levenberg on polynomially convex hulls containing no analytic discs is\n presented. As a consequence it is shown that there exists a Cantor set $X$ in\n ${\\mathbb C}^3$ with a nontrivial polynomially convex hull that contains no\n analytic discs. Using this Cantor set, it is shown that there exist arcs and\n curves in ${\\mathbb C}^4$ with nontrivial polynomially convex hulls that\n contain no analytic discs. This answers a question raised a few years ago by\n Bercovici and can be regarded as a partial answer to a question raised by\n Wermer over 60 years ago. More generally, it is shown that every uncountable,\n compact subspace of a Euclidean space can be embedded as a subspace $X$ of\n ${\\mathbb C}^N$, for some N, in such a way as to have a nontrivial polynomially\n convex hull that contains no analytic discs. In the case when the topological\n dimension of the space is at most one, $X$ can be chosen so as to have the\n stronger property that $P(X)$ has a dense set of invertible elements.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1511.06571",
        "title": "A cut-cell finite volume - finite element coupling approach for\nfluid-structure interaction in compressible flow",
        "abstract": "We present a loosely coupled approach for the solution of fluid-structure\n interaction problems between a compressible flow and a deformable structure.\n The method is based on staggered Dirichlet-Neumann partitioning. The interface\n motion in the Eulerian frame is accounted for by a conservative cut-cell\n Immersed Boundary method. The present approach enables sub-cell resolution by\n considering individual cut-elements within a single fluid cell, which\n guarantees an accurate representation of the time-varying solid interface. The\n cut-cell procedure inevitably leads to non-matching interfaces, demanding for a\n special treatment. A Mortar method is chosen in order to obtain a conservative\n and consistent load transfer. We validate our method by investigating\n two-dimensional test cases comprising a shock-loaded rigid cylinder and a\n deformable panel. Moreover, the aeroelastic instability of a thin plate\n structure is studied with a focus on the prediction of flutter onset. Finally,\n we propose a three-dimensional fluid-structure interaction test case of a\n flexible inflated thin shell interacting with a shock wave involving large and\n complex structural deformations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.01921",
        "title": "Causal Scoring Medical Image Explanations: A Case Study On Ex-vivo\nKidney Stone Images",
        "abstract": "On the promise that if human users know the cause of an output, it would\n enable them to grasp the process responsible for the output, and hence provide\n understanding, many explainable methods have been proposed to indicate the\n cause for the output of a model based on its input. Nonetheless, little has\n been reported on quantitative measurements of such causal relationships between\n the inputs, the explanations, and the outputs of a model, leaving the\n assessment to the user, independent of his level of expertise in the subject.\n To address this situation, we explore a technique for measuring the causal\n relationship between the features from the area of the object of interest in\n the images of a class and the output of a classifier. Our experiments indicate\n improvement in the causal relationships measured when the area of the object of\n interest per class is indicated by a mask from an explainable method than when\n it is indicated by human annotators. Hence the chosen name of Causal\n Explanation Score (CaES)",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/9704056",
        "title": "Pseudoscalar Conversion and Gamma-Rays from Supernovae",
        "abstract": "A light pseudoscalar coupled to two photons would be copiously emitted by the\n core of a supernova and part of this flux would be converted to gamma-rays by\n the galactic magnetic field. Measurements on the SN1987A gamma-ray flux by the\n Gamma-Ray Spectrometer on the Solar Maximum Mission satellite imply stringents\n bounds on such process. The improved generation of satellite-borne detectors,\n like EGRET or the projegeneration of satellite-borne detectors, like EGRET or\n the project GLAST, could be able to detect a pseudoscalar-to-photon signal from\n a nearby supernova.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2006.02794",
        "title": "Generalized Ordered Set Partitions",
        "abstract": "In this paper, we consider ordered set partitions obtained by imposing\n conditions on the size of the lists, and such that the first $r$ elements are\n in distinct blocks, respectively. We introduce a generalization of the Lah\n numbers. For this new combinatorial sequence we derive its exponential\n generating function, some recurrence relations, and combinatorial identities.\n We prove and present results using combinatorial arguments, generating\n functions, the symbolic method and Riordan arrays. For some specific cases we\n provide a combinatorial interpretation for the inverse matrix of the\n generalized Lah numbers by means of two families of posets.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1302.0703",
        "title": "Two Particle-Hole Excitations in Charged Current Quasielastic\nAntineutrino--Nucleus Scattering",
        "abstract": "We evaluate the quasielastic and multinucleon contributions to the\n antineutrino nucleus scattering cross section and compare our results with the\n recent MiniBooNE data. We use a local Fermi gas model that includes RPA\n correlations and gets the multinucleon part from a systematic many body\n expansion of the $W$ boson selfenergy in the nuclear medium. The same model had\n been quite successful for the neutrino cross section and contains no new\n parameters. We have also analysed the relevance of 2p2h events for the\n antineutrino energy reconstruction.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.05887",
        "title": "Bottleneck Matching in the Plane",
        "abstract": "We present an algorithm for computing a bottleneck matching in a set of\n $n=2\\ell$ points in the plane, which runs in $O(n^{\\omega/2}\\log n)$\n deterministic time, where $\\omega\\approx 2.37$ is the exponent of matrix\n multiplication.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1711.01582",
        "title": "A symmetrizable extension of polyconvex thermoelasticity and\napplications to zero-viscosity limits and weak-strong uniqueness",
        "abstract": "We embed the equations of polyconvex thermoviscoelasticity into an augmented,\n symmetrizable, hyperbolic system and derive a relative entropy identity in the\n extended variables. Following the relative entropy formulation, we prove the\n convergence from thermoviscoelasticity with Newtonian viscosity and Fourier\n heat conduction to smooth solutions of the system of adiabatic thermoelasticity\n as both parameters tend to zero. Also, convergence from thermoviscoelasticity\n to smooth solutions of thermoelasticity in the zero-viscosity limit. Finally,\n we establish a weak-strong uniqueness result for the equations of adiabatic\n thermoelasticity in the class of entropy weak solutions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2204.13553",
        "title": "A Synthetic Roman Space Telescope High-Latitude Time-Domain Survey:\nSupernovae in the Deep Field",
        "abstract": "NASA will launch the Nancy Grace Roman Space Telescope (Roman) in the second\n half of this decade, which will allow for a generation-defining measurement of\n dark energy through multiple probes, including Type Ia supernovae (SNe Ia). To\n improve decisions on survey strategy, we have created the first simulations of\n realistic Roman images that include artificial SNe Ia injected as point sources\n in the images. Our analysis combines work done on Roman simulations for weak\n gravitational lensing studies as well as catalog-level simulations of SN\n samples. We have created a time series of images over two years containing\n $\\sim$ 1,050 SNe Ia, covering a 1 square degree subarea of a planned 5 square\n degree deep survey. We have released these images publicly for community use\n along with input catalogs of all injected sources. We create secondary products\n from these images by generating coadded images and demonstrating recovery of\n transient sources using image subtraction. We perform first-use analyses on\n these images in order to measure galaxy-detection efficiency, point\n source-detection efficiency, and host-galaxy association biases. The simulated\n images can be found here:\n https://roman.ipac.caltech.edu/sims/SN_Survey_Image_sim.html.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/math/0612795",
        "title": "Locality of continuous Hamiltonian flows and Lagrangian intersections\nwith the conormal of open subsets",
        "abstract": "In this paper, we prove that if a continuous Hamiltonian flow fixes the\n points in an open subset $U$ of a symplectic manifold $(M,\\omega)$, then its\n associated Hamiltonian is constant at each moment on $U$. As a corollary, we\n prove that the Hamiltonian of compactly supported continuous Hamiltonian flows\n is unique both on a compact $M$ with smooth boundary $\\del M$ and on a\n non-compact manifold bounded at infinity. An essential tool for the proof of\n the locality is the Lagrangian intersection theorem for the conormals of open\n subsets proven by Kasturirangan and the author, combined with Viterbo's scheme\n that he introduced in the proof of uniqueness of the Hamiltonian on a closed\n manifold \\cite{viterbo2}. We also prove the converse of the theorem which\n localizes a previously known global result in symplectic topology.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.05908",
        "title": "The Temporal and Spatial Behaviors of CME Occurrence Rate at Different\nLatitudes",
        "abstract": "The statistical study of the Coronal Mass Ejections (CMEs) is a hot topic in\n solar physics. To further reveal the temporal and spatial behaviors of the CMEs\n at different latitudes and heights, we analyzed the correlation and phase\n relationships between the occurrence rate of CMEs, the Coronal Brightness Index\n (CBI), and the 10.7-cm solar radio flux (F10.7). We found that the occurrence\n rate of the CMEs correlates with CBI relatively stronger at high latitudes\n (>=60) than at low latitudes (<=50). At low latitudes, the occurrence rate of\n the CMEs correlates relatively weaker with CBI than F10.7. There is a\n relatively stronger correlation relationship between CMEs, F10.7, and CBI\n during Solar Cycle 24(SC24) than Solar Cycle 23 (SC23). During SC23, the\n high-latitude CME occurrence rate lags behind F10.7 by three months, and during\n SC24, the low-latitude CME occurrence rate leads to the low-latitude CBI by one\n month. The correlation coefficient values turn out to be larger when the very\n faint CMEsare removed from the samples of the CDAW catalog. Based on our\n results, we may speculate that the source regions of the high/low-latitude CMEs\n may vary in height, and the process of magnetic energy accumulation and\n dissipation is from the lower to the upper atmosphere of the Sun. The temporal\n offsets between different indicators could help us better understand the\n physical processes responsible for the solar-terrestrial interactions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0407610",
        "title": "Dynamical Mass Estimates for Two Luminous Young Stellar Clusters in\nMessier 83",
        "abstract": "Using new data from the UVES spectrograph on the ESO Very Large Telescope and\n archive images from the Hubble Space Telescope, we have measured projected\n velocity dispersions and structural parameters for two bright young star\n clusters in the nearby spiral galaxy NGC 5236. One cluster is located near the\n nuclear starburst of NGC 5236, at a projected distance of 440 pc from the\n centre, while the other is located in the disk of the galaxy at a projected\n galactocentric distance of 2.3 kpc. We estimate virial masses for the two\n clusters of (4.2+/-0.7)x10^5 Msun and (5.2+/-0.8)x10^5 Msun and ages (from\n broad-band photometry) of 10^(7.1+/-0.2} years and 10^(8.0+/-0.1) years,\n respectively. Comparing the observed mass-to-light (M/L) ratios with simple\n stellar population models, we find that the data for both clusters are\n consistent with a Kroupa-type stellar mass function (MF). In particular, we\n rule out any MF with a significantly lower M/L ratio than the Kroupa MF, such\n as a Salpeter-like MF truncated at a mass of 1 Msun or higher. These clusters\n provide a good illustration of the fact that massive, globular cluster-like\n objects (\"super star clusters\") can form at the present epoch even in the disks\n of seemingly normal, undisturbed spiral galaxies.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1802.00604",
        "title": "Monaural Speech Enhancement using Deep Neural Networks by Maximizing a\nShort-Time Objective Intelligibility Measure",
        "abstract": "In this paper we propose a Deep Neural Network (DNN) based Speech Enhancement\n (SE) system that is designed to maximize an approximation of the Short-Time\n Objective Intelligibility (STOI) measure. We formalize an approximate-STOI cost\n function and derive analytical expressions for the gradients required for DNN\n training and show that these gradients have desirable properties when used\n together with gradient based optimization techniques. We show through\n simulation experiments that the proposed SE system achieves large improvements\n in estimated speech intelligibility, when tested on matched and unmatched\n natural noise types, at multiple signal-to-noise ratios. Furthermore, we show\n that the SE system, when trained using an approximate-STOI cost function\n performs on par with a system trained with a mean square error cost applied to\n short-time temporal envelopes. Finally, we show that the proposed SE system\n performs on par with a traditional DNN based Short-Time Spectral Amplitude\n (STSA) SE system in terms of estimated speech intelligibility. These results\n are important because they suggest that traditional DNN based STSA SE systems\n might be optimal in terms of estimated speech intelligibility.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2206.14042",
        "title": "Pressure-dependent structure of BaZrO$_3$ crystals as determined by\nRaman Spectroscopy",
        "abstract": "The structure of dielectric perovskite BaZrO$_3$, long known to be cubic at\n room temperature without any structural phase transition with variation of\n temperature, has been recently disputed to have different ground state\n structures with lower symmetries involving octahedra rotation. The\n pressure-dependent Raman scattering measurements can identify the hierarchy of\n energetically-adjacent polymorphs, helping in turn understand its ground state\n structure at atmospheric pressure. Here, Raman scattering spectra of\n high-quality BaZrO$_3$ single crystals grown by the optical floating zone\n method are investigated in a pressure range from 1 atm to 42 GPa. First, based\n on the analyses of the infrared and Raman spectra measured at the atmosphere,\n it is found that all observed vibrational modes can be assigned according to\n the cubic $Pm\\bar{3}m$ structure. In addition, by applying pressure, two\n structural phase transitions are found at 8.4 and 19.2 GPa, one from the cubic\n to the rhombohedral $R\\bar{3}c$ phase and the other from the rhombohedral to\n the tetragonal $I4/mcm$ phase. Based on the two pressure-induced structural\n phase transitions, the true ground state structure of BaZrO$_3$ at room\n temperature and ambient pressure is corroborated to be cubic while the\n rhombohedral phase is the closest second.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0905.2411",
        "title": "Color Distributions, Number and Mass Densities of Massive Galaxies at\n1.5 < z < 3: Comparing Observations with Merger Simulations",
        "abstract": "We present a comparison between the observed color distribution, number and\n mass density of massive galaxies at 1.5 < z < 3 and a model by Hopkins et al.\n that relates the quasar and galaxy population on the basis of gas-rich mergers.\n In order to test the hypothesis that quiescent red galaxies are formed after a\n gas-rich merger involving quasar activity, we confront photometry of massive (M\n > 4x10^10 Msun) galaxies extracted from the FIRES, GOODS-South, and MUSYC\n surveys, together spanning an area of 496 arcmin^2, with synthetic photometry\n from hydrodynamical merger simulations. As in the Hopkins et al. (2006b) model,\n we use the observed quasar luminosity function to estimate the merger rate. We\n find that the synthetic U-V and V-J colors of galaxies that had a quasar phase\n in their past match the colors of observed galaxies that are best characterized\n by a quiescent stellar population. At z ~ 2.6, the observed number and mass\n density of quiescent red galaxies with M > 4x10^10 Msun is consistent with the\n model in which every quiescent massive galaxy underwent a quasar phase in the\n past. At z ~ 1.9, 2.8 times less quiescent galaxies are observed than predicted\n by the model as descendants of higher redshift quasars. The merger model also\n predicts a large number of galaxies undergoing merger-driven star formation. We\n find that the predicted number and mass density accounts for 30-50% of the\n observed massive star-forming galaxies. However, their colors do not match\n those of observed star-forming galaxies. In particular, the colors of dusty red\n galaxies are not reproduced by the simulations. Several possible origins of\n this discrepancy are discussed. The observational constraints on the validity\n of the model are currently limited by cosmic variance and uncertainties in\n stellar population synthesis and radiative transfer.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1701.05379",
        "title": "ALPs Effective Field Theory and Collider Signatures",
        "abstract": "We study the leading effective interactions between the Standard Model fields\n and a generic singlet CP-odd (pseudo)Goldstone boson. Two possible frameworks\n for electroweak symmetry breaking are considered: linear and non-linear. For\n the latter case, the basis of leading effective operators is determined and\n compared with that for the linear expansion. Associated phenomenological\n signals at colliders are explored for both scenarios, deriving new bounds and\n analyzing future prospects, including LHC and High Luminosity LHC\n sensitivities. Mono-$Z$, mono-$W$, $W$-photon plus missing energy and on-shell\n top final states are most promising signals expected in both frameworks. In\n addition, non-standard Higgs decays and mono-Higgs signatures are especially\n prominent and expected to be dominant in non-linear realizations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1904.01127",
        "title": "Cyberthreat Detection from Twitter using Deep Neural Networks",
        "abstract": "To be prepared against cyberattacks, most organizations resort to security\n information and event management systems to monitor their infrastructures.\n These systems depend on the timeliness and relevance of the latest updates,\n patches and threats provided by cyberthreat intelligence feeds. Open source\n intelligence platforms, namely social media networks such as Twitter, are\n capable of aggregating a vast amount of cybersecurity-related sources. To\n process such information streams, we require scalable and efficient tools\n capable of identifying and summarizing relevant information for specified\n assets. This paper presents the processing pipeline of a novel tool that uses\n deep neural networks to process cybersecurity information received from\n Twitter. A convolutional neural network identifies tweets containing\n security-related information relevant to assets in an IT infrastructure. Then,\n a bidirectional long short-term memory network extracts named entities from\n these tweets to form a security alert or to fill an indicator of compromise.\n The proposed pipeline achieves an average 94% true positive rate and 91% true\n negative rate for the classification task and an average F1-score of 92% for\n the named entity recognition task, across three case study infrastructures.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1805.08697",
        "title": "Cepheids in Eclipsing Binaries. What and How We Can Learn About Them",
        "abstract": "Eclipsing binary systems with pulsating components offer a unique possibility\n to accurately measure the most important parameters of pulsating stars, to\n study their evolution, and to test the pulsation theory. I will show what we\n can learn about the pulsating stars from the analysis of such systems and how\n we can do it. Special attention will be paid to the mass, radius, p-factor, and\n distance determination. Although the core of the method is based on the\n observations of double-lined eclipsing spectroscopic binaries, with the help of\n the pulsation theory, it is possible to measure absolute parameters for\n single-lined binaries also.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.15489",
        "title": "A Flexible Cryptographic Infrastructure for High-security SDR-based\nSystems",
        "abstract": "Military software defined radio (SDR) systems are a major factor in future\n network-centric operations due to their flexibility and support for more\n capable radio communications systems. The inherent nature of software-based\n systems requires a more complex auxiliary infrastructure and multiple\n independent levels of security compared with typical systems: Secure booting of\n the SDR device, cryptographically signed software, real time operating platform\n software as well as radio applications. This technology raises new challenges\n with respect to the management. The largest impact on SDR deployments is due to\n the auxiliary cryptographic infrastructure for the security of the software\n life cycle and the cyclic update of the keys. Compared to conventional radio\n devices, the SDR system with the cryptographic infrastructure described in this\n paper reaches a higher security level and is more flexible. The advantage is\n the possibility to deploy trunked radio system and further waveforms, such as\n coalition wideband, which will be standardized in the future. Also it is\n possible to update cryptographic mechanisms. In this work, we analyze the\n requirements for a high secure SDR deployment and model the life cycle of the\n components of a deployed SDR node based on the Joint Program Executive Office\n (JPEO) Software Communication Architecture (SCA).",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1410.7441",
        "title": "Diagonality and idempotents with applications to problems in operator\ntheory and frame theory",
        "abstract": "We prove that a nonzero idempotent is zero-diagonal if and only if it is not\na Hilbert-Schmidt perturbation of a projection, along with other useful\nequivalences. Zero-diagonal operators are those whose diagonal entries are\nidentically zero in some basis.\nWe also prove that any bounded sequence appears as the diagonal of some\nidempotent operator, thereby providing a characterization of inner products of\ndual frame pairs in infinite dimensions. Furthermore, we show that any\nabsolutely summable sequence whose sum is a positive integer appears as the\ndiagonal of a finite rank idempotent.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2005.02572",
        "title": "Artificial intelligence real-time prediction and physical interpretation\nof atomic binding energies in nano-scale metal clusters",
        "abstract": "Single atomic sites often determine the functionality and performance of\n materials, such as catalysts, semi-conductors or enzymes. Computing and\n understanding the properties of such sites is therefore a crucial component of\n the rational materials design process. Because of complex electronic effects at\n the atomic level, atomic site properties are conventionally derived from\n computationally expensive first-principle calculations, as this level of theory\n is required to achieve relevant accuracy. In this study, we present a widely\n applicable machine learning (ML) approach to compute atomic site properties\n with high accuracy in real time. The approach works well for complex\n non-crystalline atomic structures and therefore opens up the possibility for\n high-throughput screenings of nano-materials, amorphous systems and materials\n interfaces. Our approach includes a robust featurization scheme to transform\n atomic structures into features which can be used by common machine learning\n models. Performing a genetic algorithm (GA) based feature selection, we show\n how to establish an intuitive physical interpretation of the structure-property\n relations implied by the ML models. With this approach, we compute atomic site\n stabilities of metal nanoparticles ranging from 3-55 atoms with mean absolute\n errors in the range of 0.11-0.14 eV in real time. We also establish the\n chemical identity of the site as most important factor in determining atomic\n site stabilities, followed by structural features like bond distances and\n angles. Both, the featurization and GA feature selection functionality are\n published in open-source python modules. With this method, we enable the\n efficient rational design of highly specialized real-world nano-catalysts\n through data-driven materials screening.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0507176",
        "title": "Simulating field-aligned diffusion of a cosmic ray gas",
        "abstract": "The macroscopic behaviour of cosmic rays in turbulent magnetic fields is\n discussed. An implementation of anisotropic diffusion of cosmic rays with\n respect to the magnetic field in a non-conservative, high-order,\n finite-difference magnetohydrodynamic code is discussed. It is shown that the\n standard implementation fails near singular X-points of the magnetic field,\n which are common if the field is random. A modification to the diffusion model\n for cosmic rays is described and the resulting telegraph equation (implemented\n by solving a dynamic equation for the diffusive flux of cosmic rays) is used;\n it is argued that this modification may better describe the physics of cosmic\n ray diffusion. The present model reproduces several processes important for the\n propagation and local confinement of cosmic rays, including spreading\n perpendicular to the local large-scale magnetic field, controlled by the\n random-to-total magnetic field ratio, and the balance between cosmic ray\n pressure and magnetic tension. Cosmic ray diffusion is discussed in the context\n of a random magnetic field produced by turbulent dynamo action. It is argued\n that energy equipartition between cosmic rays and other constituents of the\n interstellar medium do not necessarily imply that cosmic rays play a\n significant role in the balance of forces.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1207.5442",
        "title": "Security Analysis of a Password-Based Authentication Protocol Proposed\nto IEEE 1363",
        "abstract": "In recent years, several protocols for password-based authenticated key\n exchange have been proposed. These protocols aim to be secure even though the\n sample space of passwords may be small enough to be enumerated by an off-line\n adversary. In Eurocrypt 2000, Bellare, Pointcheval and Rogaway (BPR) presented\n a model and security definition for authenticated key exchange. They claimed\n that in the ideal-cipher model (random oracles), the two-flow protocol at the\n core of Encrypted Key Exchange (EKE) is secure. Bellare and Rogaway suggested\n several instantiations of the ideal cipher in their proposal to the IEEE\n P1363.2 working group. Since then there has been an increased interest in\n proving the security of password-based protocols in the ideal-cipher model. For\n example, Bresson, Chevassut, and Pointcheval have recently showed that the\n One-Encryption-Key-Exchange (OEKE) protocol is secure in the ideal cipher\n model. In this paper, we present examples of real (NOT ideal) ciphers\n (including naive implementations of the instantiations proposed to IEEE\n P1363.2) that would result in broken instantiations of the idealised AuthA\n protocol and OEKE protocol. Our result shows that the AuthA protocol can be\n instantiated in an insecure way, and that there are no well defined (let alone\n rigorous) ways to distinguish between secure and insecure instantiations. Thus,\n without a rigorous metric for ideal-ciphers, the value of provable security in\n ideal cipher model is limited.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2012.13958",
        "title": "Impurities in systems of noninteracting trapped fermions",
        "abstract": "We study the properties of spin-less non-interacting fermions trapped in a\n confining potential in one dimension but in the presence of one or more\n impurities which are modelled by delta function potentials. We use a method\n based on the single particle Green's function. For a single impurity placed in\n the bulk, we compute the density of the Fermi gas near the impurity. Our\n results, in addition to recovering the Friedel oscillations at large distance\n from the impurity, allow the exact computation of the density at short\n distances. We also show how the density of the Fermi gas is modified when the\n impurity is placed near the edge of the trap in the region where the\n unperturbed system is described by the Airy gas. Our method also allows us to\n compute the effective potential felt by the impurity both in the bulk and at\n the edge. In the bulk this effective potential is shown to be a universal\n function only of the local Fermi wave vector, or equivalently of the local\n fermion density. When the impurity is placed near the edge of the Fermi gas,\n the effective potential can be expressed in terms of Airy functions. For an\n attractive impurity placed far outside the support of the fermion density, we\n show that an interesting transition occurs where a single fermion is pulled out\n of the Fermi sea and forms a bound state with the impurity. This is a quantum\n analogue of the well-known Baik-Ben Arous-P\\'ech\\'e (BBP) transition, known in\n the theory of spiked random matrices. The density at the location of the\n impurity plays the role of an order parameter. We also consider the case of two\n impurities in the bulk and compute exactly the effective force between them\n mediated by the background Fermi gas.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2210.11098",
        "title": "The definable content of homological invariants II: \\v{C}ech cohomology\nand homotopy classification",
        "abstract": "This is the second installment in a series of papers applying descriptive set\ntheoretic techniques to both analyze and enrich classical functors from\nhomological algebra and algebraic topology. In it, we show that the \\v{C}ech\ncohomology functors $\\check{\\mathrm{H}}^n$ on the category of locally compact\nseparable metric spaces each factor into (i) what we term their definable\nversion, a functor $\\check{\\mathrm{H}}^n_{\\mathrm{def}}$ taking values in the\ncategory $\\mathsf{GPC}$ of groups with a Polish cover (a category first\nintroduced in this work's predecessor), followed by (ii) a forgetful functor\nfrom $\\mathsf{GPC}$ to the category of groups. These definable cohomology\nfunctors powerfully refine their classical counterparts: we show that they are\ncomplete invariants, for example, of the homotopy types of mapping telescopes\nof $d$-spheres or $d$-tori for any $d\\geq 1$, and, in contrast, that there\nexist uncountable families of pairwise homotopy inequivalent mapping telescopes\nof either sort on which the classical cohomology functors are constant. We then\napply the functors $\\check{\\mathrm{H}}^n_{\\mathrm{def}}$ to show that a seminal\nproblem in the development of algebraic topology, namely Borsuk and Eilenberg's\n1936 problem of classifying, up to homotopy, the maps from a solenoid\ncomplement $S^3\\backslash\\Sigma$ to the $2$-sphere, is essentially hyperfinite\nbut not smooth.\nIn the course of this work, we record Borel definable versions of a number of\nclassical results bearing on both the combinatorial and homotopical\nformulations of \\v{C}ech cohomology; in aggregate, this work may be regarded as\nlaying foundations for the descriptive set theoretic study of the homotopy\nrelation on the space of maps from a locally compact Polish space to a\npolyhedron, a relation which embodies a substantial variety of classification\nproblems arising throughout mathematics.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2206.13486",
        "title": "Hardness of almost embedding simplicial complexes in $\\mathbb{R}^d$, II",
        "abstract": "A map $f: K \\to \\mathbb{R}^d$ of a simplicial complex is an almost embedding\n if $f(\\sigma) \\cap f(\\tau) = \\varnothing$ whenever $\\sigma, \\tau$ are disjoint\n simplices of $K$. Fix integers $d,k \\geqslant 2$ such that $k+2 \\leqslant d\n \\leqslant\\frac{3k}2+1$. Assuming that the \"preimage of a cycle is a cycle\" we\n prove $\\mathbf{NP}$-hardness of the algorithmic problem of recognition of\n almost embeddability of finite $k$-dimensional complexes in $\\mathbb{R}^d$.\n Assuming that $\\mathbf{P} \\ne \\mathbf{NP}$ (and that the \"preimage of a cycle\n is a cycle\") we prove that the embedding obstruction is incomplete for\n $k$-dimensional complexes in $\\mathbb{R}^d$ using configuration spaces. Our\n proof generalizes the Skopenkov-Tancer proof of this result for $d =\n \\frac{3k}{2} + 1$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1912.01471",
        "title": "Exploring Supersymmetric CP Violation after LHC Run 2 with Electric\nDipole Moments and B Observables",
        "abstract": "We consider the prospects for measuring distinctive signatures of the\n CP-violating phases in the minimal supersymmetric extension of the Standard\n Model (MSSM) in light of the limits on sparticle masses from searches at the\n LHC. We use the CPsuperH code to evaluate model predictions and scan the\n parameter space using a geometric approach that maximizes CP-violating\n observables subject to the current upper limits on electric dipole moments\n (EDMs). We focus on the possible CP-violating asymmetry $A_{\\rm CP}$ in $b \\to\n s \\gamma$ decay and on a possible CP-violating contribution to the $B_s -\n \\overline{B}_s$ mass difference $\\Delta M^{NP}_{B_s}$, as well as future\n measurements of the EDMs of the proton, neutron and electron. We find that the\n current LHC and EDM limits are consistent with values of $A_{\\rm CP}$, $\\Delta\n M^{NP}_{B_s}$ and the proton EDM that are measurable with the Belle-II\n detector, LHCb and a proposed measurement of the proton EDM using a storage\n ring, respectively. Measurement of a non-zero proton EDM would constrain\n $A_{\\rm CP}$ significantly, but it and a CP-violating contribution to $\\Delta\n M^{NP}_{B_s}$ could still be measurable, along with neutron and electron EDMs.\n A more accurate measurement of $A_{\\rm CP}$ with the current central value\n would favour stop and chargino masses within reach of future LHC runs as well\n as a potentially measurable value of $\\Delta M^{NP}_{B_s}$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0605097",
        "title": "Decoherence and Quantum Walks: anomalous diffusion and ballistic tails",
        "abstract": "The common perception is that strong coupling to the environment will always\nrender the evolution of the system density matrix quasi-classical (in fact,\ndiffusive) in the long time limit. We present here a counter-example, in which\na particle makes quantum transitions between the sites of a d-dimensional\nhypercubic lattice whilst strongly coupled to a bath of two-level systems which\n'record' the transitions. The long-time evolution of an initial wave packet\nis found to be most unusual: the mean square displacement of the particle\ndensity matrix shows long-range ballitic behaviour, but simultaneously a kind\nof weakly-localised behaviour near the origin. This result may have important\nimplications for the design of quantum computing algorithms, since it describes\na class of quantum walks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2106.04857",
        "title": "Positivity of the Poincar\\'e bundle on the moduli space of vector\nbundles and its applications",
        "abstract": "We prove that the normalized Poincar\\'e bundle on the moduli space of stable\n rank $r$ vector bundles with a fixed determinant on a smooth projective curve\n $X$ induces a family of nef vector bundles on the moduli space. Two\n applications follow. We show that when the genus of $X$ is large, the derived\n category of $X$ is embedded into the derived category of the moduli space for\n arbitrary rank and coprime degree, which extends the results of Narasimhan,\n Fonarev-Kuznetsov, and Belmans-Mukhopadhyay. As the second application, we\n construct a family of ACM bundles on the moduli space. A key ingredient of our\n proof is the investigation of birational geometry of the moduli spaces of\n parabolic bundles.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0809.3081",
        "title": "Undetermined states: how to find them and their applications",
        "abstract": "We investigate the undetermined sets consisting of two-level, multi-partite\n pure quantum states, whose reduced density matrices give absolutely no\n information of their original states. Two approached of finding these quantum\n states are proposed. One is to establish the relation between codewords of the\n stabilizer quantum error correction codes (SQECCs) and the undetermined states.\n The other is to study the local complementation rules of the graph states. As\n an application, the undetermined states can be exploited in the quantum secret\n sharing scheme. The security is guaranteed by their undetermineness.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2403.02671",
        "title": "On the convergence of conditional gradient method for unbounded\nmultiobjective optimization problems",
        "abstract": "This paper focuses on developing a conditional gradient algorithm for\n multiobjective optimization problems with an unbounded feasible region. We\n employ the concept of recession cone to establish the well-defined nature of\n the algorithm. The asymptotic convergence property and the iteration-complexity\n bound are established under mild assumptions. Numerical examples are provided\n to verify the algorithmic performance.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2306.08092",
        "title": "Bayesian Mixture Modelling with Ranked Set Samples",
        "abstract": "We consider the Bayesian estimation of the parameters of a finite mixture\n model from independent order statistics arising from imperfect ranked set\n sampling designs. As a cost-effective method, ranked set sampling enables us to\n incorporate easily attainable characteristics, as ranking information, into\n data collection and Bayesian estimation. To handle the special structure of the\n ranked set samples, we develop a Bayesian estimation approach exploiting the\n Expectation-Maximization (EM) algorithm in estimating the ranking parameters\n and Metropolis within Gibbs Sampling to estimate the parameters of the\n underlying mixture model. Our findings show that the proposed RSS-based\n Bayesian estimation method outperforms the commonly used Bayesian counterpart\n using simple random sampling. The developed method is finally applied to\n estimate the bone disorder status of women aged 50 and older.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1502.01504",
        "title": "Feedback based Reputation on top of the Bitcoin Blockchain",
        "abstract": "The ability to assess the reputation of a member in a web community is a need\n addressed in many different ways according to the many different stages in\n which the nature of communities has evolved over time. In the case of\n reputation of goods/services suppliers, the solutions available to prevent the\n feedback abuse are generally reliable but centralized under the control of few\n big Internet companies. In this paper we show how a decentralized and\n distributed feedback management system can be built on top of the Bitcoin\n blockchain",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/9505072",
        "title": "Self-Similarity and Localization",
        "abstract": "The localized eigenstates of the Harper equation exhibit universal\n self-similar fluctuations once the exponentially decaying part of a wave\n function is factorized out. For a fixed quantum state, we show that the whole\n localized phase is characterized by a single strong coupling fixed point of the\n renormalization equations. This fixed point also describes the generalized\n Harper model with next nearest neighbor interaction below a certain threshold.\n Above the threshold, the fluctuations in the generalized Harper model are\n described by a strange invariant set of the renormalization equations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1001.4244",
        "title": "A centerless representation of the Virasoro algebra associated with the\nunitary circular ensemble",
        "abstract": "We consider the 2-dimensional Toda lattice tau functions\n $\\tau_n(t,s;\\eta,\\theta)$ deforming the probabilities $\\tau_n(\\eta,\\theta)$\n that a randomly chosen matrix from the unitary group U(n), for the Haar\n measure, has no eigenvalues within an arc $(\\eta,\\theta)$ of the unit circle.\n We show that these tau functions satisfy a centerless Virasoro algebra of\n constraints, with a boundary part in the sense of Adler, Shiota and van\n Moerbeke. As an application, we obtain a new derivation of a differential\n equation due to Tracy and Widom, satisfied by these probabilities, linking it\n to the Painleve VI equation.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0610477",
        "title": "Reconstructing compositions",
        "abstract": "We consider the problem of reconstructing compositions of an integer from\n their subcompositions, which was raised by Raykova (albeit disguised as a\n question about layered permutations). We show that every composition w of n\\ge\n 3k+1 can be reconstructed from its set of k-deletions, i.e., the set of all\n compositions of n-k contained in w. As there are compositions of 3k with the\n same set of k-deletions, this result is best possible.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2110.04471",
        "title": "Provably Efficient Black-Box Action Poisoning Attacks Against\nReinforcement Learning",
        "abstract": "Due to the broad range of applications of reinforcement learning (RL),\n understanding the effects of adversarial attacks against RL model is essential\n for the safe applications of this model. Prior theoretical works on adversarial\n attacks against RL mainly focus on either observation poisoning attacks or\n environment poisoning attacks. In this paper, we introduce a new class of\n attacks named action poisoning attacks, where an adversary can change the\n action signal selected by the agent. Compared with existing attack models, the\n attacker's ability in the proposed action poisoning attack model is more\n restricted, which brings some design challenges. We study the action poisoning\n attack in both white-box and black-box settings. We introduce an adaptive\n attack scheme called LCB-H, which works for most RL agents in the black-box\n setting. We prove that the LCB-H attack can force any efficient RL agent, whose\n dynamic regret scales sublinearly with the total number of steps taken, to\n choose actions according to a policy selected by the attacker very frequently,\n with only sublinear cost. In addition, we apply LCB-H attack against a popular\n model-free RL algorithm: UCB-H. We show that, even in the black-box setting, by\n spending only logarithm cost, the proposed LCB-H attack scheme can force the\n UCB-H agent to choose actions according to the policy selected by the attacker\n very frequently.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1711.03250",
        "title": "Switchable Ferroelectric Photovoltaic Effects in Epitaxial Thin Films of\nh-RFeO3 having Narrow Optical Band Gaps",
        "abstract": "Ferroelectric photovoltaics (FPVs) have drawn much attention owing to their\n high stability, environmental safety, anomalously high photovoltages, coupled\n with reversibly switchable photovoltaic responses. However, FPVs suffer from\n extremely low photocurrents, which is primarily due to their wide band gaps.\n Here, we present a new class of FPVs by demonstrating switchable ferroelectric\n photovoltaic effects using hexagonal ferrite (h-RFeO3) thin films having narrow\n band gaps of ~1.2 eV, where R denotes rare-earth ions. FPVs with narrow band\n gaps suggests their potential applicability as photovoltaic and optoelectronic\n devices. The h-RFeO3 films further exhibit reasonably large ferroelectric\n polarizations, which possibly reduces a rapid recombination rate of the\n photo-generated electron-hole pairs. The power conversion efficiency (PCE) of\n h-RFeO3 thin-film devices is sensitive on the magnitude of polarization. In the\n case of h-TmFeO3 (h-TFO) thin film, the measured PCE is twice as large as that\n of the BiFeO3 thin film, a prototypic FPV. We have further shown that the\n switchable photovoltaic effect dominates over the unswitchable internal field\n effect arising from the net built-in potential. This work thus demonstrates a\n new class of FPVs towards high-efficiency solar cell and optoelectronic\n applications.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1407.8239",
        "title": "Conductance behavior in nanowires with spin-orbit interaction -- A\nnumerical study",
        "abstract": "We consider electronic transport through semiconducting nanowires (W) with\n spin-orbit interaction (SOI), in a hybrid N-W-N setup where the wire is\n contacted by normal-metal leads (N). We investigate the conductance behavior of\n the system as a function of gate and bias voltage, magnetic field, wire length,\n temperature, and disorder. The transport calculations are performed numerically\n and are based on standard recursive Green's function techniques. In particular,\n we are interested in understanding if and how it is possible to deduce the\n strength of the SOI from the transport behavior. This is a very relevant\n question since so far no clear experimental observation in that direction has\n been produced. We find that the smoothness of the electrostatic potential\n profile between the contacts and the wire plays a crucial role, and we show\n that in realistic regimes the N-W-N setup may mask the effects of SOI, and a\n trivial behavior with apparent vanishing SOI is observed. We identify an\n optimal parameter regime, with neither too smooth nor too abrupt potentials,\n where the signature of SOI is best visible, with and without Fabry-Perot\n oscillations, and is most resilient to disorder and temperature effects.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1009.0910",
        "title": "Fundamental solitons in discrete lattices with a delayed nonlinear\nresponse",
        "abstract": "The formation of unstaggered localized modes in dynamical lattices can be\n supported by the interplay of discreteness and nonlinearity with a finite\n relaxation time. In rapidly responding nonlinear media, on-site discrete\n solitons are stable, and their broad inter-site counterparts are marginally\n stable, featuring a virtually vanishing real instability eigenvalue. The\n solitons become unstable in the case of the slowly relaxing nonlinearity. The\n character of the instability alters with the increase of the delay time, which\n leads to a change in the dynamics of unstable discrete solitons. They form\n robust localized breathers in rapidly relaxing media, and decay into\n oscillatory diffractive pattern in the lattices with a slow nonlinear response.\n Marginally stable solitons can freely move across the lattice.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2405.12680",
        "title": "On a universal characterisation of $p$-typical Witt vectors",
        "abstract": "For a prime $p$ and a commutative ring $R$ with unity, let $W(R)$ denote the\n ring of $p$-typical Witt vectors. The ring $W(R)$ is endowed with a\n Verschiebung operator $W(R)\\xrightarrow{V}W(R)$ and a Teichm\\\"{u}ller map\n $R\\xrightarrow{\\langle \\ \\rangle}W(R)$. One of the properties satisfied by $V,\n \\langle \\ \\rangle$ is that the map $R \\to W(R)$ given by $x\\mapsto V\\langle\n x^p\\rangle - p\\langle x \\rangle$ is an additive map. In this paper we show that\n for $p\\neq 2$, this property essentially characterises the functor $W$. Unlike\n other characterisations, this only uses the group structure on $W(R)$ and hence\n is suitable for generalising to the non-commutative setup. We give a\n conjectural characterisation of Hesselholt's functor of $p$-typical Witt\n vectors using a universal property for $p\\neq 2$. Moreover we provide evidence\n for this conjecture.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1602.06019",
        "title": "Unraveling nonadiabatic ionization and Coulomb potential effects in\nstrong-field photoelectron holography",
        "abstract": "Strong field photoelectron holography has been proposed as a means for\n interrogating the spatial and temporal information of electrons and ions in a\n dynamic system. After ionization, part of the electron wave packet may directly\n go to the detector (the reference wave), while another part may be driven back\n to the ion where it scatters off (the signal wave). The interference hologram\n of the two waves may be used to retrieve the target information. However,\n unlike conventional optical holography, the propagations of electron wave\n packets are affected by the Coulomb potential as well as by the laser field. In\n addition, electrons are emitted over the whole laser pulse duration, thus\n multiple interferences may occur. In this work, we used a generalized\n quantum-trajectory Monte Carlo method to investigate the effect of Coulomb\n potential and the nonadiabatic subcycle ionization on the photoelectron\n hologram. We showed that photoelectron hologram can be well described only when\n the nonadiabatic effect in ionization is accounted for, and Coulomb potential\n can be neglected only in the tunnel ionization regime. Our results help\n establishing photoelectron holography for probing spatial and dynamic\n properties of atoms and molecules.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1408.1927",
        "title": "A Search of The Four-Color Theorem and its Higher Dimensional\nGeneralization",
        "abstract": "Four-Color Theorem has secret in its logical proof and actual operating. In\n this paper we will give a proof of Four-Color Theorem based on Kuratowski's\n Theorem using some induction argument and give a description of the most\n complicated coloring map, a simple proof of Kuratowski's Theorem using Euler\n charateristic is also presented. We also conjecture the higher dimensional\n generalization of Four-Color Theorem.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/gr-qc/0412135",
        "title": "Varying Couplings in Electroweak Theory",
        "abstract": "We extend the theory of Kimberly and Magueijo for the spacetime variation of\n the electroweak couplings in the unified Glashow-Salam-Weinberg model of the\n electroweak interaction to include quantum corrections. We derive the effective\n quantum-corrected dilaton evolution equations in the presence of a background\n cosmological matter density that is composed of weakly interacting and\n non-weakly-interacting non-relativistic dark-matter components.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1505.02331",
        "title": "The Atiyah-Bott formula for the cohomology of the moduli space of\nbundles on a curve",
        "abstract": "This paper is a companion of the paper \"Weil's conjecture for function\n fields\" by J. Lurie and the author. We present a different exposition of\n essentially the same algebro-geometric proof of the Atiyah-Bott for the\n cohomology of Bun(G), which subsequently leads to the proof of the Tamagawa\n number formula for the volume of the automorphic space for function fields.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2303.06752",
        "title": "High energy nuclear physics meets Machine Learning",
        "abstract": "Though being seemingly disparate and with relatively new intersection, high\n energy nuclear physics and machine learning have already begun to merge and\n yield interesting results during the last few years. It's worthy to raise the\n profile of utilizing this novel mindset from machine learning in high energy\n nuclear physics, to help more interested readers see the breadth of activities\n around this intersection. The aim of this mini-review is to introduce to the\n community the current status and report an overview of applying machine\n learning for high energy nuclear physics, to present from different aspects and\n examples how scientific questions involved in high energy nuclear physics can\n be tackled using machine learning.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2102.06295",
        "title": "Realizations of Isostatic Material Frameworks",
        "abstract": "This paper studies the set of equivalent realizations of isostatic frameworks\n in two dimensions, and algorithms for finding all such realizations. We show\n that an isostatic framework has an even number of equivalent realizations that\n preserve edge lengths and connectivity. We enumerate the complete set of\n equivalent realizations for a toy framework with pinned boundary in two\n dimensions and study the impact of boundary length on the emergence of these\n realizations. To ameliorate the computational complexity of finding a solution\n to a large multivariate quadratic system corresponding to the constraints;\n alternative methods - based on constraint reduction and distance-based covering\n map or Cayley parameterization of the search space - are presented. The\n application of these methods is studied on atomic clusters, a model\n two-dimensional glasses, and jamming.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1911.09664",
        "title": "Time dependence of few-body F\\\"{o}rster interactions among ultracold\nRydberg atoms",
        "abstract": "Rubidium Rydberg atoms in either $|m_j|$-sublevel of the $36p_{3/2}$ state\n can exchange energy via Stark-tuned F\\\"{o}rster resonances, including two-,\n three-, and four-body dipole-dipole interactions. Three-body interactions of\n this type were first reported and categorized by Faoro, \\textit{et al.}~[Nat.\\\n Commun.\\ \\textbf{6}, 8173 (2015)] and their Borromean nature was confirmed by\n Tretyakov, \\textit{et al.}~[Phys.\\ Rev.\\ Lett. \\textbf{119}, 173402 (2017)]. We\n report the time dependence of the $N$-body F\\\"{o}rster resonance $N\\times\n 36p_{3/2,|m_j|=1/2}\\rightarrow 36s_{1/2}+37s_{1/2}+(N-2)\\times\n 36p_{3/2,|m_j|=3/2}$, for $N=2,3$, and 4, by measuring the fraction of\n initially excited atoms that end up in the $37s_{1/2}$ state as a function of\n time. The essential features of these interactions are captured in an\n analytical model that includes only the many-body matrix elements and\n neighboring atom distribution. A more sophisticated simulation reveals the\n importance of beyond-nearest-neighbor interactions and of always-resonant\n interactions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1008.0023",
        "title": "Supertropical matrix algebra III: Powers of matrices and generalized\neigenspaces",
        "abstract": "We investigate powers of supertropical matrices, with special attention to\n the role of the coefficients of the supertropical characteristic polynomial\n (especially the supertropical trace) in controlling the rank of a power of a\n matrix. This leads to a Jordan-type decomposition of supertropical matrices,\n together with a generalized eigenspace decomposition of a power of an arbitrary\n supertropical matrix.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2107.13539",
        "title": "On square-free and radical factorizations and existence of some divisors",
        "abstract": "We discuss various square-free and radical factorizations and existence of\n some divisors in monoids in the context of: atomicity, ascending chain\n condition for principal ideals, a pre-Schreier property, a greatest common\n divisor property and a greatest common divisor for sets property.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0210322",
        "title": "Connections, local subgroupoids,and a holonomy Lie groupoid of a line\nbundle gerbe",
        "abstract": "Our main aim is to associate a holonomy Lie groupoid to the connective\n structure of an abelian gerbe. The construction has analogies with a procedure\n for the holonomy Lie groupoid of a foliation, in using a locally Lie groupoid\n and a globalisation procedure. We show that path connections and 2-holonomy on\n line bundles may be formulated using the notion of a connection pair on a\n double category, due to Brown-Spencer, but now formulated in terms of double\n groupoids using the thin fundamental groupoids introduced by\n Caetano-Mackaay-Picken. To obtain a locally Lie groupoid to which globalisation\n applies, we use methods of local subgroupoids as developed by Brown-$\\dot{\\rm\n I}$\\c{c}en-Mucuk.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1703.10642",
        "title": "Deep Neural Network Optimized to Resistive Memory with Nonlinear\nCurrent-Voltage Characteristics",
        "abstract": "Artificial Neural Network computation relies on intensive vector-matrix\n multiplications. Recently, the emerging nonvolatile memory (NVM) crossbar array\n showed a feasibility of implementing such operations with high energy\n efficiency, thus there are many works on efficiently utilizing emerging NVM\n crossbar array as analog vector-matrix multiplier. However, its nonlinear I-V\n characteristics restrain critical design parameters, such as the read voltage\n and weight range, resulting in substantial accuracy loss. In this paper,\n instead of optimizing hardware parameters to a given neural network, we propose\n a methodology of reconstructing a neural network itself optimized to resistive\n memory crossbar arrays. To verify the validity of the proposed method, we\n simulated various neural network with MNIST and CIFAR-10 dataset using two\n different specific Resistive Random Access Memory (RRAM) model. Simulation\n results show that our proposed neural network produces significantly higher\n inference accuracies than conventional neural network when the synapse devices\n have nonlinear I-V characteristics.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2103.16065",
        "title": "General local energy-preserving integrators for solving multi-symplectic\nHamiltonian PDEs",
        "abstract": "In this paper we propose and investigate a general approach to constructing\n local energy-preserving algorithms which can be of arbitrarily high order in\n time for solving Hamiltonian PDEs. This approach is based on the temporal\n discretization using continuous Runge-Kutta-type methods, and the spatial\n discretization using pseudospectral methods or Gauss--Legendre collocation\n methods. The local energy conservation law of our new schemes is analyzed in\n detail. The effectiveness of the novel local energy-preserving integrators is\n demonstrated by coupled nonlinear Schr\\\"odinger equations and 2D nonlinear\n Schr\\\"odinger equations with external fields. Our new schemes are compared with\n some classical multi-symplectic and symplectic schemes in numerical\n experiments. The numerical results show the remarkable \\emph{long-term}\n behaviour of our new schemes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1411.2520",
        "title": "Local-global compatibility for regular algebraic cuspidal automorphic\nrepresentation when $\\ell \\neq p$",
        "abstract": "We prove the compatibility of local and global Langlands correspondences for\n $GL_n$ up to semisimplification for the Galois representations constructed by\n Harris-Lan-Taylor-Thorne and Scholze. More precisely, let $r_p(\\pi)$ denote an\n $n$-dimensional $p$-adic representation of the Galois group of a CM field $F$\n attached to a regular algebraic cuspidal automorphic representation $\\pi$ of\n $GL_n(\\mathbb{A}_F)$. We show that the restriction of $r_p(\\pi)$ to the\n decomposition group of a place $v\\nmid p$ of $F$ corresponds up to\n semisimplification to $rec(\\pi_v)$, the image of $\\pi_v$ under the local\n Langlands correspondence. Furthermore, we can show that the monodromy of the\n associated Weil-Deligne representation of $.r_p(\\pi)|_{G_{F_v}}$ is `more\n nilpotent' than the monodromy of $rec(\\pi_v)$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2103.06292",
        "title": "Relative depolarization of the black hole photon ring in GRMHD models of\nSgr A* and M87*",
        "abstract": "Using general relativistic magnetohydrodynamic simulations of accreting black\n holes, we show that a suitable subtraction of the linear polarization per pixel\n from total intensity images can enhance the photon ring features. We find that\n the photon ring is typically a factor of $\\simeq 2$ less polarized than the\n rest of the image. This is due to a combination of plasma and general\n relativistic effects, as well as magnetic turbulence. When there are no other\n persistently depolarized image features, adding the subtracted residuals over\n time results in a sharp image of the photon ring. We show that the method works\n well for sample, viable GRMHD models of Sgr A* and M87*, where measurements of\n the photon ring properties would provide new measurements of black hole mass\n and spin, and potentially allow for tests of the \"no-hair\" theorem of general\n relativity.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2003.09473",
        "title": "Exploring the nature and synchronicity of early cluster formation in the\nLarge Magellanic Cloud V: Multiple Populations in ancient Globular Clusters",
        "abstract": "We examine four ancient Large Magellanic Cloud (LMC) globular clusters (GCs)\n for evidence of multiple stellar populations using the Advanced Camera for\n Surveys and Wide Field Camera 3 on the Hubble Space Telescope Programme\n GO-14164. NGC 1466, NGC 1841, and NGC 2257 all show evidence for a redder,\n secondary population along the main-sequence. Reticulum does not show evidence\n for the presence of a redder population, but this GC has the least number of\n stars and Monte Carlo simulations indicate that the sample of main sequence\n stars is too small to robustly infer whether a redder population exists in this\n cluster. The second, redder, population of the other three clusters constitutes\n $\\sim30-40\\%$ of the total population along the main-sequence. This brings the\n total number of ancient LMC GCs with known split or broadened main-sequences to\n five. However, unlike for Hodge 11 and NGC 2210 (see arXiv:1904.01434), none of\n the clusters show evidence for multiple populations in the horizontal branch.\n We also do not find evidence of a second population along the Red Giant Branch\n (RGB).",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2103.08171",
        "title": "Stochastic Integrals and Gelfand Integration in Fr\\'echet Spaces",
        "abstract": "We provide a detailed analysis of the Gelfand integral on Fr\\'echet spaces,\n showing among other things a Vitali theorem, dominated convergence and a Fubini\n result. Furthermore, the Gelfand integral commutes with linear operators. The\n Skorohod integral is conveniently expressed in terms of a Gelfand integral on\n Hida distribution space, which forms our prime motivation and example. We\n extend several results of Skorohod integrals to a general class of pathwise\n Gelfand integrals. For example, we provide generalizations of the\n Hida-Malliavin derivative and extend the integration-by-parts formula in\n Malliavin Calculus. A Fubini-result is also shown, based on the commutative\n property of Gelfand integrals with linear operators. Finally, our studies give\n the motivation for two existing definitions of stochastic Volterra integration\n in Hida space.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1309.6168",
        "title": "Lumped Parameter Modeling of a Quantum Optics Circuit and Decisive Test\nfor Time-Symmetric Physics",
        "abstract": "This paper showed how a simple lumped parameter model of a circuit can yield\n correct quantum mechanical predictions of its behavior, even when there is\n quantum entanglement between components of that circuit. It addresses an\n important example, the circuit of the original Bell's Theorem experiments for\n ideal polarizers. Correct predictions emerge from two alternative simple but\n time-symmetric models based on classical Markov Random Field across space time.\n Exact agreement here does not violate Bell's Theorem itself, because the\n interplay between initial and final outcomes in these calculations does not\n fall within the CHSH definition of time forwards causality. Both models raise\n interesting questions for future research. The final section discusses several\n possible directions for following up on these results, both in lumped system\n modeling and in more general approaches. The final section proposed a new\n experiment with three-photon entanglement which could tell us which is true,\n local realistic MRF models and time-symmetric physics, or conventional\n predictions assuming the usual collapse of the wave function. The appendix\n worked out what the conventional predictions would be for the proposed\n experiment, and also gives a simple master equation version of the collapse\n assumption which does not involve metaphysical observers. Section A.4 gives the\n prediction for the new models.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2001.09703",
        "title": "Unconstrained Biometric Recognition: Summary of Recent SOCIA Lab.\nResearch",
        "abstract": "The development of biometric recognition solutions able to work in visual\n surveillance conditions, i.e., in unconstrained data acquisition conditions and\n under covert protocols has been motivating growing efforts from the research\n community. Among the various laboratories, schools and research institutes\n concerned about this problem, the SOCIA: Soft Computing and Image Analysis\n Lab., of the University of Beira Interior, Portugal, has been among the most\n active in pursuing disruptive solutions for obtaining such extremely ambitious\n kind of automata. This report summarises the research works published by\n elements of the SOCIA Lab. in the last decade in the scope of biometric\n recognition in unconstrained conditions. The idea is that it can be used as\n basis for someone wishing to entering in this research topic.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1708.04748",
        "title": "When the cookie meets the blockchain: Privacy risks of web payments via\ncryptocurrencies",
        "abstract": "We show how third-party web trackers can deanonymize users of\n cryptocurrencies. We present two distinct but complementary attacks. On most\n shopping websites, third party trackers receive information about user\n purchases for purposes of advertising and analytics. We show that, if the user\n pays using a cryptocurrency, trackers typically possess enough information\n about the purchase to uniquely identify the transaction on the blockchain, link\n it to the user's cookie, and further to the user's real identity. Our second\n attack shows that if the tracker is able to link two purchases of the same user\n to the blockchain in this manner, it can identify the user's entire cluster of\n addresses and transactions on the blockchain, even if the user employs\n blockchain anonymity techniques such as CoinJoin. The attacks are passive and\n hence can be retroactively applied to past purchases. We discuss several\n mitigations, but none are perfect.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/q-bio/0508039",
        "title": "Stimulus-invariant processing and spectrotemporal reverse correlation in\nprimary auditory cortex",
        "abstract": "The spectrotemporal receptive field (STRF) provides a versatile and\n integrated, spectral and temporal, functional characterization of single cells\n in primary auditory cortex (AI). In this paper, we explore the origin of, and\n relationship between, different ways of measuring and analyzing an STRF. We\n demonstrate that STRFs measured using a spectrotemporally diverse array of\n broadband stimuli -- such as dynamic ripples, spectrotemporally white noise,\n and temporally orthogonal ripple combinations (TORCs) -- are very similar,\n confirming earlier findings that the STRF is a robust linear descriptor of the\n cell. We also present a new deterministic analysis framework that employs the\n Fourier series to describe the spectrotemporal modulations contained in the\n stimuli and responses. Additional insights into the STRF measurements,\n including the nature and interpretation of measurement errors, is presented\n using the Fourier transform, coupled to singular-value decomposition (SVD), and\n variability analyses including bootstrap. The results promote the utility of\n the STRF as a core functional descriptor of neurons in AI.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/quant-ph/0110120",
        "title": "Optimal Evaluation of Generalized Euler Angles with Applications to\nClassical and Quantum Control",
        "abstract": "Given two linearly independent matrices in $so(3)$, $Z_1$ and $Z_2$, every\nrotation matrix $X_f \\in SO(3)$ can be written as the product of alternate\nelements from the one dimensional subgroups corresponding to $Z_1$ and $Z_2$,\nnamely $X_f=e^{Z_1 t_1}e^{Z_2 t_2}e^{Z_1t_3} \\cdot \\cdot \\cdot e^{Z_1t_s}$. The\nparameters $t_i$, $i=1,...,s$ are called {\\it generalized Euler angles}.\nIn this paper, we evaluate the minimum number of factors required for the\nfactorization of $X_f \\in SO(3)$, as a function of $X_f$, and provide an\nalgorithm to determine the generalized Euler angles explicitly. The results can\nbe applied to the bang bang control with minimum number of switches of some\nclassical control systems and of two level quantum systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1903.06335",
        "title": "Orthogonal multiple flag varieties of finite type II : even degree case",
        "abstract": "Let $G$ be the split orthogonal group of degree $2n$ over an arbitrary\n infinite field $\\mathbb{F}$ of chararcteristic not $2$. In this paper, we\n classify multiple flag varieties $G/P_1\\times\\cdots\\times G/P_k$ of finite\n type. Here a multiple flag variety is said to be of finite type if it has a\n finite number of $G$-orbits with respect to the diagonal action of $G$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1508.06713",
        "title": "Encoding the Factorisation Calculus",
        "abstract": "Jay and Given-Wilson have recently introduced the Factorisation (or SF-)\ncalculus as a minimal fundamental model of intensional computation. It is a\ncombinatory calculus containing a special combinator, F, which is able to\nexamine the internal structure of its first argument. The calculus is\nsignificant in that as well as being combinatorially complete it also exhibits\nthe property of structural completeness, i.e. it is able to represent any\nfunction on terms definable using pattern matching on arbitrary normal forms.\nIn particular, it admits a term that can decide the structural equality of any\ntwo arbitrary normal forms.\nSince SF-calculus is combinatorially complete, it is clearly at least as\npowerful as the more familiar and paradigmatic Turing-powerful computational\nmodels of Lambda Calculus and Combinatory Logic. Its relationship to these\nmodels in the converse direction is less obvious, however. Jay and Given-Wilson\nhave suggested that SF-calculus is strictly more powerful than the\naforementioned models, but a detailed study of the connections between these\nmodels is yet to be undertaken.\nThis paper begins to bridge that gap by presenting a faithful encoding of the\nFactorisation Calculus into the Lambda Calculus preserving both reduction and\nstrong normalisation. The existence of such an encoding is a new result. It\nalso suggests that there is, in some sense, an equivalence between the former\nmodel and the latter. We discuss to what extent our result constitutes an\nequivalence by considering it in the context of some previously defined\nframeworks for comparing computational power and expressiveness.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1904.02399",
        "title": "Riemannian Normalizing Flow on Variational Wasserstein Autoencoder for\nText Modeling",
        "abstract": "Recurrent Variational Autoencoder has been widely used for language modeling\n and text generation tasks. These models often face a difficult optimization\n problem, also known as the Kullback-Leibler (KL) term vanishing issue, where\n the posterior easily collapses to the prior, and the model will ignore latent\n codes in generative tasks. To address this problem, we introduce an improved\n Wasserstein Variational Autoencoder (WAE) with Riemannian Normalizing Flow\n (RNF) for text modeling. The RNF transforms a latent variable into a space that\n respects the geometric characteristics of input space, which makes posterior\n impossible to collapse to the non-informative prior. The Wasserstein objective\n minimizes the distance between the marginal distribution and the prior directly\n and therefore does not force the posterior to match the prior. Empirical\n experiments show that our model avoids KL vanishing over a range of datasets\n and has better performances in tasks such as language modeling, likelihood\n approximation, and text generation. Through a series of experiments and\n analysis over latent space, we show that our model learns latent distributions\n that respect latent space geometry and is able to generate sentences that are\n more diverse.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2003.01869",
        "title": "Exactly solvable two-terminal heat engine with asymmetric Onsager\ncoefficients: Origin of the power-efficiency bound",
        "abstract": "An engine producing a finite power at the ideal (Carnot) efficiency is a\n dream engine, which is not prohibited by the thermodynamic second law. Some\n years ago, a two-terminal heat engine with {\\em asymmetric} Onsager\n coefficients in the linear response regime was suggested by Benenti, Saito, and\n Casati [Phys. Rev. Lett. {\\bf 106}, 230602 (2011)], as a prototypical system to\n make such a dream come true with non-divergent system parameter values.\n However, such a system has never been realized in spite of many trials. Here,\n we introduce an exactly solvable two-terminal Brownian heat engine with the\n asymmetric Onsager coefficients in the presence of a Lorenz (magnetic) force.\n Nevertheless, we show that the dream engine regime cannot be accessible even\n with the asymmetric Onsager coefficients, due to an instability keeping the\n engine from reaching its steady state. This is consistent with recent trade-off\n relations between the engine power and efficiency, where the (cyclic)\n steady-state condition is implicitly presumed. We conclude that the\n inaccessibility to the dream engine originates from the steady-state constraint\n on the engine.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2306.08660",
        "title": "Ziv-Zakai-type error bounds for general statistical models",
        "abstract": "I propose Ziv-Zakai-type lower bounds on the Bayesian error for estimating a\n parameter $\\beta:\\Theta \\to \\mathbb R$ when the parameter space $\\Theta$ is\n general and $\\beta(\\theta)$ need not be a linear function of $\\theta$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1704.00128",
        "title": "Bifurcation results for problems with fractional Trudinger-Moser\nnonlinearity",
        "abstract": "By using a suitable topological argument based on cohomological linking and\n by exploiting a Trudinger-Moser inequality in fractional spaces recently\n obtained, we prove existence of multiple solutions for a problem involving the\n nonlinear fractional laplacian and a related critical exponential nonlinearity.\n This extends results in the literature for the N-Laplacian operator.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1410.5005",
        "title": "Theoretical Calculations for Predicted States of Heavy Quarkonium via\nNon-Relativistic Frame Work",
        "abstract": "In this paper, we calculate the mass spectra of heavy quarkonium by using\n matrix Numerov's method to make the predictions of F and G states for further\n experiments. The method gives a very reasonable result which is in a good\n agreement with other methods and with recently published theoretical data. From\n the yielded wave functions we calculate the root mean square radius r_ms and\n \\b{eta} coefficients of heavy quarkonium",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2303.10275",
        "title": "MoRF: Mobile Realistic Fullbody Avatars from a Monocular Video",
        "abstract": "We present a system to create Mobile Realistic Fullbody (MoRF) avatars. MoRF\n avatars are rendered in real-time on mobile devices, learned from monocular\n videos, and have high realism. We use SMPL-X as a proxy geometry and render it\n with DNR (neural texture and image-2-image network). We improve on prior work,\n by overfitting per-frame warping fields in the neural texture space, allowing\n to better align the training signal between different frames. We also refine\n SMPL-X mesh fitting procedure to improve the overall avatar quality. In the\n comparisons to other monocular video-based avatar systems, MoRF avatars achieve\n higher image sharpness and temporal consistency. Participants of our user study\n also preferred avatars generated by MoRF.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2305.08639",
        "title": "Abelian quotients for groups generated by cubed half-twists",
        "abstract": "Below we construct non-cyclic and torsion-free abelian quotients for\n subgroups of braid groups generated by cube powers of half-twists. In the case\n of 3 and 4 strands we compute the abelianization of these groups. Also, we get\n abelianizations of level-3 congruence subgroups of braid groups on 3 and 4\n strands. Moreover, we construct non-cyclic and torsion-free abelian quotients\n for level-3 congruence subgroups of braid groups. The constructions are based\n on the original construction of Dennis Johnson \\cite{DJ}, who computed an\n abelian quotient of the Torelli group.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/0207111",
        "title": "Higher dimensional geometries related to fuzzy odd-dimensional spheres",
        "abstract": "We study $SO(m)$ covariant Matrix realizations of $ \\sum_{i=1}^{m} X_i^2 = 1\n$ for even $m$ as candidate fuzzy odd spheres following hep-th/0101001. As for\nthe fuzzy four sphere, these Matrix algebras contain more degrees of freedom\nthan the sphere itself and the full set of variables has a geometrical\ndescription in terms of a higher dimensional coset. The fuzzy $S^{2k-1} $ is\nrelated to a higher dimensional coset $ {SO(2k) \\over U(1) \\times U(k-1)}$.\nThese cosets are bundles where base and fibre are hermitian symmetric spaces.\nThe detailed form of the generators and relations for the Matrix algebras\nrelated to the fuzzy three-spheres suggests Matrix actions which admit the\nfuzzy spheres as solutions. These Matrix actions are compared with the BFSS,\nIKKT and BMN Matrix models as well as some others. The geometry and\ncombinatorics of fuzzy odd spheres lead to some remarks on the transverse\nfive-brane problem of Matrix theories and the exotic scaling of the entropy of\n5-branes with the brane number.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2101.07935",
        "title": "A quantitative model for a nanoscale switch accurately predicts thermal\nactuation behavior",
        "abstract": "Manipulation of temperature can be used to actuate DNA origami nano-hinges\n containing gold nanoparticles. We develop a physical model of this system that\n uses partition function analysis of the interaction between the nano-hinge and\n nanoparticle to predict the probability that the nano-hinge is open at a given\n temperature. The model agrees well with experimental data and predicts\n experimental conditions that allow the actuation temperature of the nano-hinge\n to be tuned over a range of temperatures from $30$${}^{\\circ}\\mathrm{C}$ to\n $45$${}^{\\circ}\\mathrm{C}$. Additionally, the model reveals surprising physical\n constraints on the system. This combination of physical insight and predictive\n potential is likely to inform future designs that integrate nanoparticles into\n dynamic DNA origami structures. Furthermore, our modeling approach could be\n expanded to consider the incorporation, stability, and actuation of other types\n of functional elements or actuation mechanisms integrated into nucleic acid\n devices.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1707.03620",
        "title": "A universe with no ordinal-definable, stationary, co-stationary subset\nof $\\omega_1$",
        "abstract": "It is shown that the existence of a measurable cardinal is equiconsistent to\n a model of ZFC in which there is no ordinal-definable, stationary, costationary\n subset of $\\omega_1$",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1605.06434",
        "title": "Molecular Dynamics Simulations of Compression-Tension Asymmetry in\nPlasticity of Fe Nanopillars",
        "abstract": "Tension-compression asymmetry is a notable feature of plasticity in bcc\n single crystals. Recent experiments reveal striking differences in the\n plasticity of bcc nanopillars for tension and compression. Here we present\n results from molecular dynamics simulations of nanopillars of bcc Fe in tension\n and compression. We find that a totally different deformation mechanism applies\n in each cases: dislocation glide in compression and twinning in tension. This\n difference explains experimentally-observed asymmetry in the nanopillar\n morphology.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0906.4488",
        "title": "Search for 14.4 keV solar axions emitted in the M1-transition of 57Fe\nnuclei with CAST",
        "abstract": "We have searched for 14.4 keV solar axions or more general axion-like\n particles (ALPs), that may be emitted in the M1 nuclear transition of 57Fe, by\n using the axion-to-photon conversion in the CERN Axion Solar Telescope (CAST)\n with evacuated magnet bores (Phase I). From the absence of excess of the\n monoenergetic X-rays when the magnet was pointing to the Sun, we set\n model-independent constraints on the coupling constants of pseudoscalar\n particles that couple to two photons and to a nucleon g_{a\\gamma} |-1.19\n g_{aN}^{0}+g_{aN}^{3}|<1.36\\times 10^{-16} GeV^{-1} for m_{a}<0.03 eV at the\n 95% confidence level.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2006.16280",
        "title": "The Assembly History of M87 Through Radial Variations in Chemical\nAbundances of its Field Star And Globular Cluster Populations",
        "abstract": "We present an extensive study of spectroscopically-derived chemical\n abundances for M87 and its globular cluster (GC) system. Using observations\n from the Mitchell spectrograph at McDonald, LRIS at Keck, and Hectospec on the\n MMT, we derive new metallicity gradients from $\\sim 2$ to $140$ kpc. We use a\n novel hierarchical statistical framework to simultaneously separate the GC\n system into subpopulations while measuring the metallicity gradients of those\n subpopulations. We create physically-motivated spectral stacks of the GC\n subpopulations by leveraging the output of this statistical framework to\n perform the first application of abundance tagging in a massive ETG to better\n constrain the origins of the GC subpopulations and, thus, the assembly history\n of M87. We find a metal-poor, $\\alpha$-enhanced population of GCs in both in\n the inner and outer halo unanticipated by current cosmological simulations of\n galaxy evolution. We use the remarkably flat metallicity gradients we find for\n both the metal-rich and metal-poor GC subpopulations in the inner halo as\n tentative evidence that some amount of the metal-poor GCs formed directly in\n the halo of M87 at high redshift.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1007.4937",
        "title": "Broad emission lines for negatively spinning black holes",
        "abstract": "We present an extended scheme for the calculation of the profiles of emission\n lines from accretion discs around rotating black holes. The scheme includes\n discs with angular momenta which are parallel and antiparallel with respect to\n the black hole's angular momentum, as both configurations are assumed to be\n stable (King et al., 2005). We discuss line shapes for such discs and present a\n code for modelling observational data with this scheme in X-ray data analysis\n programs. Based on a Green's function approach, an arbitrary radius dependence\n of the disc emissivity and arbitrary limb darkening laws can be easily taken\n into account, while the amount of precomputed data is significantly reduced\n with respect to other available models.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1904.08925",
        "title": "The impact of proportional transaction costs on systematically generated\nportfolios",
        "abstract": "The effect of proportional transaction costs on systematically generated\n portfolios is studied empirically. The performance of several portfolios (the\n index tracking portfolio, the equally-weighted portfolio, the entropy-weighted\n portfolio, and the diversity-weighted portfolio) in the presence of dividends\n and transaction costs is examined under different configurations involving the\n trading frequency, constituent list size, and renewing frequency. Moreover, a\n method to smooth transaction costs is proposed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1103.3452",
        "title": "The characteristics of thermalization of boost-invariant plasma from\nholography",
        "abstract": "We report on the approach towards the hydrodynamic regime of boost-invariant\n N=4 super Yang-Mills plasma at strong coupling starting from various\n far-from-equilibrium states at tau=0. The results are obtained through\n numerical solution of Einstein's equations for the dual geometries, as\n described in detail in the companion article arXiv:1203.0755. Despite the very\n rich far-from-equilibrium evolution, we find surprising regularities in the\n form of clear correlations between initial entropy and total produced entropy,\n as well as between initial entropy and the temperature at thermalization,\n understood as the transition to a hydrodynamic description. For 29 different\n initial conditions that we consider, hydrodynamics turns out to be definitely\n applicable for proper times larger than 0.7 in units of inverse temperature at\n thermalization. We observe a sizable anisotropy in the energy-momentum tensor\n at thermalization, which is nevertheless entirely due to hydrodynamic effects.\n This suggests that effective thermalization in heavy ion collisions may occur\n significantly earlier than true thermalization.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1207.4244",
        "title": "Cloaking and imaging at the same time",
        "abstract": "In this letter, we propose a conceptual device to perform subwavelength\n imaging with positive refraction. The key to this proposal is that a drain is\n no longer a must for some cases. What's more, this device is an isotropic\n omnidirectional cloak with a perfect electric conductor hiding region and shows\n versatile illusion optical effects. Numerical simulations are performed to\n verify the functionalities.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2111.12420",
        "title": "CircuitFlow: A Domain Specific Language for Dataflow Programming (with\nappendices)",
        "abstract": "Dataflow applications, such as machine learning algorithms, can run for days,\n making it desirable to have assurances that they will work correctly. Current\n tools are not good enough: too often the interactions between tasks are not\n type-safe, leading to undesirable run-time errors. This paper presents a new\n declarative Haskell Embedded DSL (eDSL) for dataflow programming: CircuitFlow.\n Defined as a Symmetric Monoidal Preorder (SMP) on data that models dependencies\n in the workflow, it has a strong mathematical basis, refocusing on how data\n flows through an application, resulting in a more expressive solution that not\n only catches errors statically, but also achieves competitive run-time\n performance. In our preliminary evaluation, CircuitFlow outperforms the\n industry-leading Luigi library of Spotify by scaling better with the number of\n inputs. The innovative creation of CircuitFlow is also of note, exemplifying\n how to create a modular eDSL whose semantics necessitates effects, and where\n storing complex type information for program correctness is paramount.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0912.5193",
        "title": "Ranking relations using analogies in biological and information networks",
        "abstract": "Analogical reasoning depends fundamentally on the ability to learn and\n generalize about relations between objects. We develop an approach to\n relational learning which, given a set of pairs of objects\n $\\mathbf{S}=\\{A^{(1)}:B^{(1)},A^{(2)}:B^{(2)},\\ldots,A^{(N)}:B ^{(N)}\\}$,\n measures how well other pairs A:B fit in with the set $\\mathbf{S}$. Our work\n addresses the following question: is the relation between objects A and B\n analogous to those relations found in $\\mathbf{S}$? Such questions are\n particularly relevant in information retrieval, where an investigator might\n want to search for analogous pairs of objects that match the query set of\n interest. There are many ways in which objects can be related, making the task\n of measuring analogies very challenging. Our approach combines a similarity\n measure on function spaces with Bayesian analysis to produce a ranking. It\n requires data containing features of the objects of interest and a link matrix\n specifying which relationships exist; no further attributes of such\n relationships are necessary. We illustrate the potential of our method on text\n analysis and information networks. An application on discovering functional\n interactions between pairs of proteins is discussed in detail, where we show\n that our approach can work in practice even if a small set of protein pairs is\n provided.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1703.05324",
        "title": "Velocity-Density Correlations from the cosmicflows-3 Distance Catalog\nand the 2MASS Redshift Survey",
        "abstract": "The peculiar velocity of a mass tracer is on average aligned with the dipole\n modulation of the surrounding mass density field. We present a first\n measurement of the correlation between radial peculiar velocities of objects in\n the cosmicflows-3 catalog and the dipole moment of the 2MRS galaxy distribution\n in concentric spherical shells centered on these objects. Limiting the analysis\n to cosmicflows-3 objects with distances of $100 \\rm Mpc h^{-1}$, the\n correlation function is detected at a confidence level $> 4\\sigma$. The\n measurement is found consistent with the standard $\\Lambda$CDM model at $<\n 1.7\\sigma$ level. We formally derive the constraints\n $0.32<\\Omega^{0.55}\\sigma_8<0.48$ ($68\\% $ confidence level) or equivalently\n $0.34<\\Omega^{0.55}/b<0.52$, where $b$ is the galaxy bias factor. Deeper and\n improved peculiar velocity catalogs will substantially reduce the\n uncertainties, allowing tighter constraints from this type of correlations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0601507",
        "title": "p-adic cohomology",
        "abstract": "This is a survey of some recent developments concerning the p-adic cohomology\n of algebraic varieties over fields of positive characteristic and local fields\n of mixed characteristic, plus some related areas like p-adic Hodge theory.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1912.05626",
        "title": "Impact of Source to Drain Tunneling on the Ballistic Performance of Ge,\nGaSb, and GeSn Nanowire p-MOSFETs",
        "abstract": "We investigated the effect of material choice and orientation in limiting\n source to drain tunneling (SDT) in nanowire (NW) p-MOSFETs. Si, Ge, GaSb, and\n Ge0.96Sn0.04 nanowire MOSFETs (NWFETs) were simulated using rigorous ballistic\n quantum transport simulations. To properly account for the non-parabolicity and\n anisotropy of the valence band the k.p method was used. For each material, a\n set of six different transport/confinement directions were simulated to\n identify the direction with the highest ON-current (ION ). For Ge, GaSb, and\n GeSn [001]/110/-110 oriented NWFETs showed the best ON-state performance,\n compared to other orientations. Our simulation results show that, despite\n having a higher percentage of SDT in OFF-state than silicon, GaSb\n [001]/110/-110 NWFET can outperform Si NWFETs. We further examined the role of\n doping in limiting SDT and demonstrated that the ON-state performance of Ge and\n GeSn NWFETs could be improved by reducing the doping in the source/drain (S/D)\n extension regions. Finally, we analyzed the impact of increased injection\n velocity in [ [001]/110/-110 oriented GaSb and GeSn NWFETs, as a result of the\n application of uniaxial compressive stress, and showed that when compared at a\n fixed OFF-current (IOFF) with unstrained NWFETs, uniaxial compressive stress\n deteriorates the ON-state performancedue to an increase in OFF-state SDT\n current component.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1004.5323",
        "title": "Geometrization of Trace Formulas",
        "abstract": "Following our joint work arXiv:1003.4578 with Robert Langlands, we make the\n first steps toward developing geometric methods for analyzing trace formulas in\n the case of the function field of a curve defined over a finite field. We also\n suggest a conjectural framework of geometric trace formulas for curves defined\n over the complex field, which exploits the categorical version of the geometric\n Langlands correspondence.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2002.05220",
        "title": "Importance of non-flow background on the chiral magnetic wave search",
        "abstract": "An observable sensitive to the chiral magnetic wave (CMW) is the charge\n asymmetry dependence of the $\\pi^{-}$ and $\\pi^{+}$ anisotropic flow\n difference, $\\Delta v_{n}(A_{\\rm ch})$. We show that, due to non-flow\n correlations, the flow measurements by the Q-cumulant method using all charged\n particles as reference introduce a trivial linear term to $\\Delta v_{n}(A_{\\rm\n ch})$. The trivial slope contribution to the triangle flow difference $\\Delta\n v_{3}(A_{\\rm ch})$ can be negative if the non-flow is dominated by back-to-back\n pairs. This can explain the observed negative $\\Delta v_{3}(A_{\\rm ch})$ slope\n in the preliminary STAR data. We further find that the non-flow correlations\n give rise to additional backgrounds to the slope of $\\Delta v_{2}(A_{\\rm ch})$\n from the competition among different pion sources and from the larger\n multiplicity dilution to $\\pi^{+}$ ($\\pi^{-}$) at positive (negative) $A_{\\rm\n ch}$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1701.05305",
        "title": "Random Forest Missing Data Algorithms",
        "abstract": "Random forest (RF) missing data algorithms are an attractive approach for\n dealing with missing data. They have the desirable properties of being able to\n handle mixed types of missing data, they are adaptive to interactions and\n nonlinearity, and they have the potential to scale to big data settings.\n Currently there are many different RF imputation algorithms but relatively\n little guidance about their efficacy, which motivated us to study their\n performance. Using a large, diverse collection of data sets, performance of\n various RF algorithms was assessed under different missing data mechanisms.\n Algorithms included proximity imputation, on the fly imputation, and imputation\n utilizing multivariate unsupervised and supervised splitting---the latter class\n representing a generalization of a new promising imputation algorithm called\n missForest. Performance of algorithms was assessed by ability to impute data\n accurately. Our findings reveal RF imputation to be generally robust with\n performance improving with increasing correlation. Performance was good under\n moderate to high missingness, and even (in certain cases) when data was missing\n not at random.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2302.12002",
        "title": "Master's Thesis: Out-of-distribution Detection with Energy-based Models",
        "abstract": "Today, deep learning is increasingly applied in security-critical situations\n such as autonomous driving and medical diagnosis. Despite its success, the\n behavior and robustness of deep networks are not fully understood yet, posing a\n significant risk. In particular, researchers recently found that neural\n networks are overly confident in their predictions, even on data they have\n never seen before. To tackle this issue, one can differentiate two approaches\n in the literature. One accounts for uncertainty in the predictions, while the\n second estimates the underlying density of the training data to decide whether\n a given input is close to the training data, and thus the network is able to\n perform as expected.In this thesis, we investigate the capabilities of EBMs at\n the task of fitting the training data distribution to perform detection of\n out-of-distribution (OOD) inputs. We find that on most datasets, EBMs do not\n inherently outperform other density estimators at detecting OOD data despite\n their flexibility. Thus, we additionally investigate the effects of\n supervision, dimensionality reduction, and architectural modifications on the\n performance of EBMs. Further, we propose Energy-Prior Network (EPN) which\n enables estimation of various uncertainties within an EBM for classification,\n bridging the gap between two approaches for tackling the OOD detection problem.\n We identify a connection between the concentration parameters of the Dirichlet\n distribution and the joint energy in an EBM. Additionally, this allows\n optimization without a held-out OOD dataset, which might not be available or\n costly to collect in some applications. Finally, we empirically demonstrate\n that Energy-Prior Network (EPN) is able to detect OOD inputs, datasets shifts,\n and adversarial examples. Theoretically, EPN offers favorable properties for\n the asymptotic case when inputs are far from the training data.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0705.4335",
        "title": "Effective Theory for Trapped Few-Fermion Systems",
        "abstract": "We apply the general principles of effective field theories to the\n construction of effective interactions suitable for few- and many-body\n calculations in a no-core shell model framework. We calculate the spectrum of\n systems with three and four two-component fermions in a harmonic trap. In the\n unitary limit, we find that three-particle results are within 10% of known\n semi-analytical values even in small model spaces. The method is very general,\n and can be readily extended to other regimes, more particles, different species\n (e.g., protons and neutrons in nuclear physics), or more-component fermions (as\n well as bosons). As an illustration, we present calculations of the\n lowest-energy three-fermion states away from the unitary limit and find a\n possible inversion of parity in the ground state in the limit of trap size\n large compared to the scattering length. Furthermore, we investigate the lowest\n positive-parity states for four fermions, although we are limited by the\n dimensions we can currently handle in this case.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0711.2322",
        "title": "Relativistic corrections to the static potential at O(1/m) and O(1/m^2)",
        "abstract": "We investigate the relativistic corrections to the static potential, i.e. the\n O(1/m) potential and the O(1/m^2) velocity-dependent potentials, in SU(3)\n lattice gauge theory. They are important ingredients of potential\n nonrelativistic QCD for heavy quarkonium. Utilizing the multi-level algorithm,\n we obtain remarkably clean signals of these potentials up to r=0.9 fm. We\n observe long range nonperturbative contributions to these corrections.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1311.6120",
        "title": "The circle method and bounds for $L$-functions - IV: Subconvexity for\ntwists of $GL(3)$ $L$-functions - B",
        "abstract": "Let $\\pi$ be a $SL(3,\\mathbb Z)$ Hecke-Maass cusp form satisfying the\n Ramanujan conjecture and the Selberg-Ramanujan conjecture, and let $\\chi$ be a\n primitive Dirichlet character modulo $M$, which we assume to be prime for\n simplicity. We will prove the following subconvex bound $$\n L\\left(\\tfrac{1}{2},\\pi\\otimes\\chi\\right)\\ll_{\\pi,\\varepsilon}\n M^{\\frac{3}{4}-\\frac{1}{1612}+\\varepsilon}. $$",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1304.3597",
        "title": "Percolation on interdependent networks with a fraction of antagonistic\ninteractions",
        "abstract": "Recently, the percolation transition has been characterized on interacting\n networks both in presence of interdependent and antagonistic interactions. Here\n we characterize the phase diagram of the percolation transition in two Poisson\n interdependent networks with a percentage q of antagonistic nodes. We show that\n this system can present a bistability of the steady state solutions, and both\n first, and second order phase transitions. In particular, we observe a\n bistability of the solutions in some regions of the phase space also for a\n small fraction of antagonistic interactions 0<q<0.4. Moreover, we show that a\n fraction q>q_c=2/3 of antagonistic interactions is necessary to strongly reduce\n the region in phase-space in which both networks are percolating. This last\n result suggests that interdependent networks are robust to the presence of\n antagonistic interactions. Our approach can be extended to multiple networks,\n and to complex boolean rules for regulating the percolation phase transition.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1712.05400",
        "title": "Itinerant quantum multi-criticality of two dimensional Dirac fermions",
        "abstract": "We analyze emergent quantum multi-criticality for strongly interacting,\n massless Dirac fermions in two spatial dimensions ($d=2$) within the framework\n of Gross-Neveu-Yukawa models, by considering the competing order parameters\n that give rise to fully gapped (insulating or superconducting) ground states.\n We focus only on those competing orders, which can be rotated into each other\n by generators of an exact or emergent chiral symmetry of massless Dirac\n fermions, and break $O(S_1)$ and $O(S_2)$ symmetries in the ordered phase.\n Performing a renormalization group analysis by using the $\\epsilon=(3-d)$\n expansion scheme, we show that all the coupling constants in the critical\n hyperplane flow toward a new attractive fixed point, supporting an\n \\emph{enlarged} $O(S_1+S_2)$ chiral symmetry. Such a fixed point acts as an\n exotic quantum multi-critical point (MCP), governing the \\emph{continuous}\n semimetal-insulator as well as insulator-insulator (for example antiferromagnet\n to valence bond solid) quantum phase transitions. In comparison with the lower\n symmetric semimetal-insulator quantum critical points, possessing either\n $O(S_1)$ or $O(S_2)$ chiral symmetry, the MCP displays enhanced correlation\n length exponents, and anomalous scaling dimensions for both fermionic and\n bosonic fields. We discuss the scaling properties of the ratio of bosonic and\n fermionic masses, and the increased dc resistivity at the MCP. By computing the\n scaling dimensions of different local fermion bilinears in the particle-hole\n channel, we establish that most of the four fermion operators or generalized\n density-density correlation functions display faster power law decays at the\n MCP compared to the free fermion and lower symmetric itinerant quantum critical\n points. Possible generalization of this scenario to higher dimensional Dirac\n fermions is also outlined.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1210.6120",
        "title": "An effective approach to the solution of a system of nonlinear\ndifferential equations in partial derivatives",
        "abstract": "There are few approaches to the solution of a system of nonlinear\n differential equations in partial derivatives, for example $\\cite{NK87} -\n \\cite{EK98}$. In our paper we propose an approach that was used to solve the\n Navier-Stokes equations in three dimensional space. This solution is described\n in details in article \"Existence, uniqueness and smoothness of solution for 3D\n Navier-Stokes equations with any smooth initial velocity\" $\\cite{TT12}$. The\n authors expect that it can be successfully applied to other systems of\n nonlinear differential equations in partial derivatives.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1904.06830",
        "title": "ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging",
        "abstract": "Grasping and manipulating objects is an important human skill. Since\n hand-object contact is fundamental to grasping, capturing it can lead to\n important insights. However, observing contact through external sensors is\n challenging because of occlusion and the complexity of the human hand. We\n present ContactDB, a novel dataset of contact maps for household objects that\n captures the rich hand-object contact that occurs during grasping, enabled by\n use of a thermal camera. Participants in our study grasped 3D printed objects\n with a post-grasp functional intent. ContactDB includes 3750 3D meshes of 50\n household objects textured with contact maps and 375K frames of synchronized\n RGB-D+thermal images. To the best of our knowledge, this is the first\n large-scale dataset that records detailed contact maps for human grasps.\n Analysis of this data shows the influence of functional intent and object size\n on grasping, the tendency to touch/avoid 'active areas', and the high frequency\n of palm and proximal finger contact. Finally, we train state-of-the-art image\n translation and 3D convolution algorithms to predict diverse contact patterns\n from object shape. Data, code and models are available at\n https://contactdb.cc.gatech.edu.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0702559",
        "title": "Cellular Dynamical Mean Field Theory of the Periodic Anderson Model",
        "abstract": "We develop a cluster dynamical mean field theory of the periodic Anderson\nmodel in three dimensions, taking a cluster of two sites as a basic reference\nframe. The mean field theory displays the basic features of the Doniach phase\ndiagram: a paramagnetic Fermi liquid state, an antiferromagnetic state and a\ntransition between them.\nIn contrast with spin density wave theories, the transition is accompanied by\na large increase of the effective mass everywhere on the Fermi surface and a\nsubstantial change of the Fermi surface shape across the transition. To\nunderstand the nature and the origin of the phases near the transition, we\ninvestigate the paramagnetic solution underlying the antiferromagnetic state,\nand identify the transition as a point where the $f$ electrons decouple from\nthe conduction electrons undergoing an orbitally selective Mott transition.\nThis point turns out to be intimately related to the two impurity Kondo model\nquantum critical point. In this regime, non local correlations become important\nand result in significant changes in the photoemission spectra and the de\nHaas-van Alphen frequencies. The transition involves considerable $f$ spectral\nweight transfer from the Fermi level to its immediate vicinity, rather than to\nthe Hubbard bands as in single site DMFT.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1406.3734",
        "title": "Critical Behavior of the SDW Transition in Underdoped Ba(Fe1-xCox)2As2\n(x <=0.05): 75As NMR Investigation",
        "abstract": "We investigate the nature of the SDW (Spin Density Wave) transition in the\n underdoped regime of an iron-based high Tc superconductor Ba(Fe1-xCox)2As2 by\n 75As NMR, with primary focus on a composition with x = 0.02 (T_SDW = 99 K).We\n demonstrate that critical slowing down toward the three dimensional SDW\n transition sets in at the tetragonal to orthorhombic structural phase\n transition, Ts = 105 K, suggesting strong interplay between structural\n distortion and spin correlations. In the critical regime between Ts and T_SDW,\n the dynamical structure factor of electron spins S(q,Wn) measured with the\n longitudinal NMR relaxation rate 1/T1 exhibits a divergent behavior obeying a\n power law, 1/T1~S(q, Wn)~(T/T_SDW-1)^a with the critical exponent a ~ 0.33.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2005.11672",
        "title": "The local balance laws for energy, momentum and entropy: how they came\ninto being, and what was their destiny",
        "abstract": "The historical process of the genesis of the extensive or substance-like\n quantities took place in two steps. First, global conservation or\n non-conservation was discovered. Only later did it become possible to formulate\n the balance locally in the form of a continuity equation. This process can be\n clearly seen in energy, momentum, and entropy. After a long and intricate\n history, the quantitative description of the local balance has been achieved\n for all of the three quantities in a surprisingly short period of time around\n the turn of the 19th to the 20th century. The new ideas could have simplified\n considerably the teaching of energy, momentum, and entropy. However, in all\n three cases, today's language of physics remained essentially the same as it\n was at the time when a local balancing was not yet possible.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0509662",
        "title": "Killing vector fields with twistor derivative",
        "abstract": "Motivated by the possible characterization of Sasakian manifolds in terms of\n twistor forms, we give the complete classification of compact Riemannian\n manifolds carrying a Killing vector field whose covariant derivative (viewed as\n a 2-form) is a twistor form.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1802.07669",
        "title": "On the convergence of partial sums with respect to Vilenkin system on\nthe martingale Hardy spaces",
        "abstract": "In this paper we derive characterizations of boundedness of the subsequences\n of partial sums with respect to Vilenkin system on the martingale Hardy spaces\n when $ 0<p<1 $. Moreover, we find necessary and sufficient conditions for the\n modulus of continuity of $f\\in H_{p}$ martingales, which provide convergence of\n subsequences of partial sums on the martingale Hardy spaces. It is also proved\n that these results are the best possible in a special sense. As applications,\n both some well-known and new results are pointed out.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0410496",
        "title": "3-D Photoionization Structure and Distances of Planetary Nebulae II.\nMenzel 1",
        "abstract": "We present the results of a spatio-kinematic study of the planetary nebula\n Menzel 1 using spectro-photometric mapping and a 3-D photoionization code. We\n create several 2-D emission line images from our long-slit spectra, and use\n these to derive the line fluxes for 15 lines, the Halpha/Hbeta extinction map,\n and the [SII] line ratio density map of the nebula. We use our photoionization\n code constrained by these data to derive the three-dimensional nebular\n structure and ionizing star parameters of Menzel 1 by simultaneously fitting\n the integrated line intensities, the density map, and the observed morphologies\n in several lines, as well as the velocity structure. Using theoretical\n evolutionary tracks of intermediate and low mass stars, we derive a mass for\n the central star of 0.63+-0.05 Msolar. We also derive a distance of 1050+_150\n pc to Menzel 1.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0903.4299",
        "title": "Token Ring Project",
        "abstract": "Ring topology is a simple configuration used to connect processes that\n communicate among themselves. A number of network standards such as token ring,\n token bus, and FDDI are based on the ring connectivity. This article will\n develop an implementation of a ring of processes that communicate among\n themselves via pipe links. The processes are nodes in the ring. Each process\n reads from its standard input and writes in its standard output. N-1 process\n redirects the its standard output to a standard input of the process through a\n pipe. When the ring-structure is designed, the project can be extended to\n simulate networks or to implement algorithms for mutual exclusion.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1301.6639",
        "title": "Defects, Super-Poincar\\'{e} line bundle and Fermionic T-duality",
        "abstract": "Topological defects are interfaces joining two conformal field theories, for\nwhich the energy momentum tensor is continuous across the interface. A class of\nthe topological defects is provided by the interfaces separating two bulk\nsystems each described by its own Lagrangian, where the two descriptions are\nrelated by a discrete symmetry.\nIn this paper we elaborate on the cases in which the discrete symmetry is a\nbosonic or a fermionic T- duality. We review how the equations of motion\nimposed by the defect encode the general bosonic T- duality transformations for\ntoroidal compactifications. We generalize this analysis in some detail to the\ncase of topological defects allowed in coset CFTs, in particular to those\ncosets where the gauged group is either an axial or vector U(1). This is\ndiscussed in both the operator and Lagrangian approaches. We proceed to\nconstruct a defect encoding a fermionic T-duality. We show that the fermionic\nT-duality is implemented by the Super-Poincar\\'{e} line bundle. The observation\nthat the exponent of the gauge invariant flux on a defect is a kernel of the\nFourier-Mukai transform of the Ramond-Ramond fields, is generalized to a\nfermionic T-duality. This is done via a fiberwise integration on\nsupermanifolds.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2301.01662",
        "title": "Unimodular approaches to the cosmological constant problem",
        "abstract": "We review selected aspects of unimodular gravity and we discuss its viability\n as a solution of the old cosmological constant problem. In unimodular gravity\n the cosmological constant is promoted to a global degree of freedom. We\n highlight the importance of correctly setting up its initial data in order to\n achieve a resolution of the cosmological constant problem on a semi-classical\n level. We review recent path integral analysis of quantum aspects of unimodular\n gravity to note that the semi-classical findings carry over to the quantum\n level as well. We point out that a resolution of the problem inherently relies\n on a global constraint on the space-time four-volume. This makes the theory\n closely related to the vacuum energy sequester, which operates in a similar\n way. We discuss possible avenues of extending unimodular gravity that preserve\n the resolution of the cosmological constant problem.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2402.05219",
        "title": "Photochemically-induced acousto-optics in gases",
        "abstract": "Acousto-optics consists of launching acoustic waves in a medium (usually a\n crystal) in order to modulate its refractive index and create a tunable optical\n grating. In this article, we present the theoretical basis of a new scheme to\n generate acousto-optics in a gas, where the acoustic waves are initiated by the\n localized absorption (and thus gas heating) of spatially-modulated UV light, as\n was demonstrated in Y. Michine and H. Yoneda, Commun. Phys. 3, 24 (2020). We\n identify the chemical reactions initiated by the absorption of UV light via the\n photodissociation of ozone molecules present in the gas, and calculate the\n resulting temperature increase in the gas as a function of space and time.\n Solving the Euler fluid equations shows that the modulated, isochoric heating\n initiates a mixed acoustic/entropy wave in the gas, whose high-amplitude\n density (and thus refractive index) modulation can be used to manipulate a\n high-power laser. We calculate that diffraction efficiencies near 100% can be\n obtained using only a few millimeters of gas containing a few percent ozone\n fraction at room temperature, with UV fluences of less than 100 mJ/cm2,\n consistent with the experimental measurements. Our analysis suggests possible\n ways to optimize the diffraction efficiency by changing the buffer gas\n composition. Gases have optics damage thresholds two to three orders of\n magnitude beyond those of solids; these optical elements should therefore be\n able to manipulate kJ-class lasers.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0808.2142",
        "title": "An Abundance of Heterotic Vacua",
        "abstract": "We explicitly construct the largest dataset to date of heterotic vacua\n arising from stable vector bundles on Calabi-Yau threefolds. Focusing on\n elliptically fibered Calabi-Yau manifolds with spectral cover bundles, we show\n that the number of heterotic models with non-zero number of generations is\n finite. We classify these models according to the complex base of their\n Calabi-Yau threefold and to the unification gauge group that they preserve in\n four dimensions. This database of the order of $10^7$ models, which includes\n potential Standard Model candidates, is subjected to some preliminary\n statistical analyses. The additional constraint that there should be three net\n generations of particles gives a dramatic reduction of the number of vacua.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2103.11476",
        "title": "Under pressure: Hydrogel swelling in a granular medium",
        "abstract": "Hydrogels hold promise in agriculture as reservoirs of water in dry soil,\n potentially alleviating the burden of irrigation. However, confinement in soil\n can markedly reduce the ability of hydrogels to absorb water and swell,\n limiting their widespread adoption. Unfortunately, the underlying reason\n remains unknown. By directly visualizing the swelling of hydrogels confined in\n three-dimensional granular media, we demonstrate that the extent of hydrogel\n swelling is determined by the competition between the force exerted by the\n hydrogel due to osmotic swelling and the confining force transmitted by the\n surrounding grains. Furthermore, the medium can itself be restructured by\n hydrogel swelling, as set by the balance between the osmotic swelling force,\n the confining force, and intergrain friction. Together, our results provide\n quantitative principles to predict how hydrogels behave in confinement,\n potentially improving their use in agriculture as well as informing other\n applications such as oil recovery, construction, mechanobiology, and\n filtration.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/physics/0312126",
        "title": "Guiding optical flows by photonic crystal slabs made of dielectric\ncylinders",
        "abstract": "We investigate the electromagnetic propagation in two-dimensional photonic\n crystals, formed by parallel dielectric cylinders embedded a uniform medium.\n The frequency band structure is computed using the standard plane-wave\n expansion method, while the propagation and scattering of the electromagnetic\n waves are calculated by the multiple scattering theory. It is shown that within\n partial bandgaps, the waves tend to bend away from the forbidden directions.\n Such a property may render novel applications in manipulating optical flows. In\n addition, the relevance with the imaging by flat photonic crystal slabs will\n also be discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1503.06482",
        "title": "Optimal binomial, Poisson, and normal left-tail domination for sums of\nnonnegative random variables",
        "abstract": "Let $X_1,\\dots,X_n$ be independent nonnegative random variables (r.v.'s),\n with $S_n:=X_1+\\dots+X_n$ and finite values of $s_i:=E X_i^2$ and $m_i:=E\n X_i>0$. Exact upper bounds on $E f(S_n)$ for all functions $f$ in a certain\n class $\\mathcal{F}$ of nonincreasing functions are obtained, in each of the\n following settings: (i) $n,m_1,\\dots,m_n,s_1,\\dots,s_n$ are fixed; (ii) $n$,\n $m:=m_1+\\dots+m_n$, and $s:=s_1+\\dots+s_n$ are fixed; (iii)~only $m$ and $s$\n are fixed. These upper bounds are of the form $E f(\\eta)$ for a certain r.v.\n $\\eta$. The r.v. $\\eta$ and the class $\\mathcal{F}$ depend on the choice of one\n of the three settings. In particular, $(m/s)\\eta$ has the binomial distribution\n with parameters $n$ and $p:=m^2/(ns)$ in setting (ii) and the Poisson\n distribution with parameter $\\lambda:=m^2/s$ in setting (iii). One can also let\n $\\eta$ have the normal distribution with mean $m$ and variance $s$ in any of\n these three settings. In each of the settings, the class $\\mathcal{F}$\n contains, and is much wider than, the class of all decreasing exponential\n functions. As corollaries of these results, optimal in a certain sense upper\n bounds on the left-tail probabilities $P(S_n\\le x)$ are presented, for any real\n $x$. In fact, more general settings than the ones described above are\n considered. Exact upper bounds on the exponential moments $E\\exp\\{hS_n\\}$ for\n $h<0$, as well as the corresponding exponential bounds on the left-tail\n probabilities, were previously obtained by Pinelis and Utev. It is shown that\n the new bounds on the tails are substantially better.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2305.11522",
        "title": "DSFNet: Dual Space Fusion Network for Occlusion-Robust 3D Dense Face\nAlignment",
        "abstract": "Sensitivity to severe occlusion and large view angles limits the usage\n scenarios of the existing monocular 3D dense face alignment methods. The\n state-of-the-art 3DMM-based method, directly regresses the model's\n coefficients, underutilizing the low-level 2D spatial and semantic information,\n which can actually offer cues for face shape and orientation. In this work, we\n demonstrate how modeling 3D facial geometry in image and model space jointly\n can solve the occlusion and view angle problems. Instead of predicting the\n whole face directly, we regress image space features in the visible facial\n region by dense prediction first. Subsequently, we predict our model's\n coefficients based on the regressed feature of the visible regions, leveraging\n the prior knowledge of whole face geometry from the morphable models to\n complete the invisible regions. We further propose a fusion network that\n combines the advantages of both the image and model space predictions to\n achieve high robustness and accuracy in unconstrained scenarios. Thanks to the\n proposed fusion module, our method is robust not only to occlusion and large\n pitch and roll view angles, which is the benefit of our image space approach,\n but also to noise and large yaw angles, which is the benefit of our model space\n method. Comprehensive evaluations demonstrate the superior performance of our\n method compared with the state-of-the-art methods. On the 3D dense face\n alignment task, we achieve 3.80% NME on the AFLW2000-3D dataset, which\n outperforms the state-of-the-art method by 5.5%. Code is available at\n https://github.com/lhyfst/DSFNet.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1810.02747",
        "title": "Vanishing Theorem for the de Rham Complex of Unitary Local System",
        "abstract": "We will prove a Kodaira-Nakano type of vanishing theorem for the logarithmic\n de Rham complex of unitary local system. We will then study the weight\n filtration on the logarithmic de Rham complex, and prove a stronger statement\n for the associated graded complex.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/0303075",
        "title": "Superfield T-duality rules",
        "abstract": "A geometric treatment of T-duality as an operation which acts on differential\n forms in superspace allows us to derive the complete set of T-duality\n transformation rules which relate the superfield potentials of D=10 type IIA\n supergravity with those of type IIB supergravity including Ramond-Ramond\n superfield potentials and fermionic supervielbeins. We show that these rules\n are consistent with the superspace supergravity constraints.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2005.04344",
        "title": "Physical Security in the Post-quantum Era: A Survey on Side-channel\nAnalysis, Random Number Generators, and Physically Unclonable Functions",
        "abstract": "Over the past decades, quantum technology has seen consistent progress, with\n notable recent developments in the field of quantum computers. Traditionally,\n this trend has been primarily seen as a serious risk for cryptography; however,\n a positive aspect of quantum technology should also be stressed. In this\n regard, viewing this technology as a resource for honest parties rather than\n adversaries, it may enhance not only the security, but also the performance of\n specific cryptographic schemes. While considerable effort has been devoted to\n the design of quantum-resistant and quantum-enhanced schemes, little effort has\n been made to understanding their physical security. Physical security deals\n with the design and implementation of security measures fulfilling the\n practical requirements of cryptographic primitives, which are equally essential\n for classic and quantum ones. This survey aims to draw greater attention to the\n importance of physical security, with a focus on secure key generation and\n storage as well as secure execution. More specifically, the possibility of\n performing side-channel analysis in the quantum world is discussed and compared\n to attacks launched in the classic world. Besides, proposals for quantum random\n number generation and quantum physically unclonable functions are compared to\n their classic counterparts and further analyzed to give a better understanding\n of their features, advantages, and shortcomings. Finally, seen from these three\n perspectives, this survey provides an outlook for future research in this\n direction.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2404.12371",
        "title": "Long-lived oscillations of false and true vacuum states in neutral atom\nsystems",
        "abstract": "Metastable false vacuum states arise in a range of quantum systems and can be\n observed in various dynamical scenarios, including decay, bubble nucleation,\n and long-lived oscillations. False vacuum phenomenology has been examined in\n quantum many-body systems, notably in 1D ferromagnetic Ising spin systems and\n superfluids. In this paper, we study long-lived oscillations of false and true\n vacuum states in 1D antiferromagnetic neutral atom chains with long-range\n Rydberg interactions. We use a staggered local detuning field to achieve\n confinement. Using theoretical and numerical models, we identify novel spectral\n signatures of quasiparticle oscillations distinct to antiferromagnetic neutral\n atom systems and interpret them using a classical energy model of deconfinement\n from Rydberg tails. Finally, we evaluate the experimental accessibility of our\n proposed setup on current neutral-atom platforms and discuss experimental\n feasibility and constraints.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2103.12272",
        "title": "Construction of explicit symplectic integrators in general relativity.\nIII. Reissner-Nordstrom-(anti)-de Sitter black holes",
        "abstract": "We give a possible splitting method to a Hamiltonian for the description of\n charged particles moving around the Reissner-Nordstrom-(anti)-de Sitter black\n hole with an external magnetic field. This Hamiltonian can be separated into\n six analytical solvable pieces, whose solutions are explicit functions of\n proper time. In this case, second- and fourth-order explicit symplectic\n integrators are easily available. They exhibit excellent long-term behavior in\n maintaining the boundness of Hamiltonian errors regardless of ordered or\n chaotic orbits if appropriate step-sizes are chosen. Under some circumstances,\n an increase of positive cosmological constant gives rise to strengthening the\n extent of chaos from the global phase space; namely, chaos of charged particles\n occurs easily for the accelerated expansion of the universe. However, an\n increase of the magnitude of negative cosmological constant does not. The\n different contributions on chaos are because the cosmological constant acts as\n a repulsive force in the Reissner-Nordstrom-de Sitter black hole, but an\n attractive force in the Reissner-Nordstrom-anti-de Sitter black hole.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2303.08020",
        "title": "Gauge equivalence between 1+1 rational Calogero-Moser field theory and\nhigher rank Landau-Lifshitz equation",
        "abstract": "In this paper we study 1+1 field generalization of the rational $N$-body\n Calogero-Moser model. We show that this model is gauge equivalent to some\n special higher rank matrix Landau-Lifshitz equation. The latter equation is\n described in terms of ${\\rm GL}_N$ rational $R$-matrix, which turns into the\n 11-vertex $R$-matrix in the $N=2$ case. The rational $R$-matrix satisfies the\n associative Yang-Baxter equation, which underlies construction of the Lax pair\n for the Zakharov-Shabat equation. The field analogue of the IRF-Vertex\n transformation is proposed. It allows to compute explicit change of variables\n between the field Calogero-Moser model and the Landau-Lifshitz equation.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2403.03413",
        "title": "Online Learning and Control Synthesis for Reachable Paths of Unknown\nNonlinear Systems",
        "abstract": "In this paper, we present a novel method to drive a nonlinear system to a\n desired state, with limited a priori knowledge of its dynamic model: local\n dynamics at a single point and the bounds on the rate of change of these\n dynamics. This method synthesizes control actions by utilizing locally learned\n dynamics along a trajectory, based on data available up to that moment, and\n known proxy dynamics, which can generate an underapproximation of the unknown\n system's true reachable set. An important benefit to the contributions of this\n paper is the lack of knowledge needed to execute the presented control method.\n We establish sufficient conditions to ensure that a controlled trajectory\n reaches a small neighborhood of any provably reachable state within a short\n time horizon, with precision dependent on the tunable parameters of these\n conditions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1902.03843",
        "title": "A Sundaram type bijection for $\\mathrm{SO}(2k+1)$: vacillating tableaux\nand pairs consisting of a standard Young tableau and an orthogonal\nLittlewood-Richardson tableau",
        "abstract": "We present a bijection between vacillating tableaux and pairs consisting of a\n standard Young tableau and an orthogonal Littlewood-Richardson tableau for the\n special orthogonal group $\\mathrm{SO}(2k+1)$. This bijection is motivated by\n the direct-sum-decomposition of the $r$th tensor power of the defining\n representation of $\\mathrm{SO}(2k+1)$. To formulate it, we use Kwon's\n orthogonal Littlewood-Richardson tableaux and introduce new alternative\n tableaux they are in bijection with. Moreover we use a suitably defined descent\n set for vacillating tableaux to determine the quasi-symmetric expansion of the\n Frobenius characters of the isotypic components.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2102.04754",
        "title": "Bayesian Transformer Language Models for Speech Recognition",
        "abstract": "State-of-the-art neural language models (LMs) represented by Transformers are\n highly complex. Their use of fixed, deterministic parameter estimates fail to\n account for model uncertainty and lead to over-fitting and poor generalization\n when given limited training data. In order to address these issues, this paper\n proposes a full Bayesian learning framework for Transformer LM estimation.\n Efficient variational inference based approaches are used to estimate the\n latent parameter posterior distributions associated with different parts of the\n Transformer model architecture including multi-head self-attention, feed\n forward and embedding layers. Statistically significant word error rate (WER)\n reductions up to 0.5\\% absolute (3.18\\% relative) and consistent perplexity\n gains were obtained over the baseline Transformer LMs on state-of-the-art\n Switchboard corpus trained LF-MMI factored TDNN systems with i-Vector speaker\n adaptation. Performance improvements were also obtained on a cross domain LM\n adaptation task requiring porting a Transformer LM trained on the Switchboard\n and Fisher data to a low-resource DementiaBank elderly speech corpus.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0107377",
        "title": "Resonant X-Ray Scattering from the Quadrupolar Ordering Phase of CeB_6",
        "abstract": "We theoretically investigate the origin of the resonant x-ray scattering\n (RXS) signal near the Ce $L_{III}$ absorption edge in the quadrupolar ordering\n phase of CeB$_6$, considering the intersite interaction between the $\\Gamma_8$\n states in the initial state. The anisotropic charge distribution of the $4f$\n states modulates the $5d$ states through the intra-atomic Coulomb interaction\n and thereby generates a large RXS superlattice intensity. The temperature and\n magnetic field dependence indicates that the induced dipolar and octupolar\n orders have little influence on the RXS spectra, in good agreement with the\n recent experiment.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2304.06087",
        "title": "High-Fidelity, Frequency-Flexible Two-Qubit Fluxonium Gates with a\nTransmon Coupler",
        "abstract": "We propose and demonstrate an architecture for fluxonium-fluxonium two-qubit\n gates mediated by transmon couplers (FTF, for fluxonium-transmon-fluxonium).\n Relative to architectures that exclusively rely on a direct coupling between\n fluxonium qubits, FTF enables stronger couplings for gates using\n non-computational states while simultaneously suppressing the static\n controlled-phase entangling rate ($ZZ$) down to kHz levels, all without\n requiring strict parameter matching. Here we implement FTF with a flux-tunable\n transmon coupler and demonstrate a microwave-activated controlled-Z (CZ) gate\n whose operation frequency can be tuned over a 2 GHz range, adding frequency\n allocation freedom for FTF's in larger systems. Across this range,\n state-of-the-art CZ gate fidelities were observed over many bias points and\n reproduced across the two devices characterized in this work. After optimizing\n both the operation frequency and the gate duration, we achieved peak CZ\n fidelities in the 99.85-99.9\\% range. Finally, we implemented model-free\n reinforcement learning of the pulse parameters to boost the mean gate fidelity\n up to $99.922\\pm0.009\\%$, averaged over roughly an hour between scheduled\n training runs. Beyond the microwave-activated CZ gate we present here, FTF can\n be applied to a variety of other fluxonium gate schemes to improve gate\n fidelities and passively reduce unwanted $ZZ$ interactions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1405.1463",
        "title": "Mixed quantum states in higher categories",
        "abstract": "There are two ways to describe the interaction between classical and quantum\n information categorically: one based on completely positive maps between\n Frobenius algebras, the other using symmetric monoidal 2-categories. This paper\n makes a first step towards combining the two. The integrated approach allows a\n unified description of quantum teleportation and classical encryption in a\n single 2-category, as well as a universal security proof applicable\n simultaneously to both scenarios.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0806.0068",
        "title": "An exploratory analysis of the transient and long term behaviour of\nsmall 3D perturbations in the circular cylinder wake",
        "abstract": "An initial-value problem for arbitrary small 3D vorticity perturbations\n imposed on a free shear flow is considered. The viscous perturbation equations\n are then combined in terms of the vorticity and velocity, and are solved by\n means of a combined Laplace-Fourier transform in the plane normal to the basic\n flow. This treatment allows for a simplification of the governing equations\n such that it is possible to observe long transients, that can last hundreds\n time scales. This result would not be possible over an acceptable lapse of time\n by carrying out a direct numerical integration of the linearized Navier-Stokes\n equations. The exploration is done with respect to physical inputs as the angle\n of obliquity, the symmetry of the perturbation and the streamwise damping rate.\n The base flow is an intermediate section of the growing 2D circular cylinder\n wake where the entrainment process is still active. Two Reynolds numbers of the\n order of the critical value for the onset of the first instability are\n considered. The early transient evolution offers very different scenarios for\n which we present a summary for particular cases. For example, for amplified\n perturbations, we have observed two kinds of transients, namely (1) a monotone\n amplification and (2) a sequence of growth - decrease - final growth. In the\n latter case, if the initial condition is an asymmetric oblique or longitudinal\n perturbation, the transient clearly shows an initial oscillatory time scale.\n Furthermore, the more a perturbation is longitudinally confined the more it is\n amplified in time. The long-term behavior of two-dimensional disturbances shows\n excellent agreement with a recent two-dimensional spatio-temporal multiscale\n modal analysis and with laboratory data concerning the frequency and wave\n length of the parallel vortex shedding in the cylinder wake.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1802.00958",
        "title": "Arbitrarily accurate twin composite $\\pi$ pulse sequences",
        "abstract": "We present three classes of symmetric broadband composite pulse sequences.\n The composite phases are given by analytic formulas (rational fractions of\n $\\pi$) valid for any number of constituent pulses. The transition probability\n is expressed by simple analytic formulas and the order of pulse area error\n compensation grows linearly with the number of pulses. Therefore, any desired\n compensation order can be produced by an appropriate composite sequence; in\n this sense, they are arbitrarily accurate. These composite pulses perform\n equally well or better than previously published ones. Moreover, the current\n sequences are more flexible as they allow total pulse areas of arbitrary\n integer multiples of $\\pi$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1502.08047",
        "title": "$L^2$ estimates for the $\\bar \\partial$ operator",
        "abstract": "This is a survey article about $L^2$ estimates for the $\\bar \\partial$\n operator. After a review of the basic approach that has come to be called the\n \"Bochner-Kodaira Technique\", the focus is on twisted techniques and their\n applications to estimates for $\\bar \\partial$, to $L^2$ extension theorems, and\n to other problems in complex analysis and geometry, including invariant metric\n estimates and the $\\bar \\partial$-Neumann Problem.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1005.2199",
        "title": "On rational homology disk smoothings of valency 4 surface singularities",
        "abstract": "Thanks to the recent work of Bhupal, Stipsicz, Szabo, and the author, one has\na complete list of resolution graphs of weighted homogeneous complex surface\nsingularities admitting a rational homology disk (\"QHD\") smoothing, i.e., one\nwith Milnor number 0. They fall into several classes, the most interesting of\nwhich are the three classes whose resolution dual graph has central vertex with\nvalency 4. We give a uniform \"quotient construction\" of the QHD smoothings for\nthese classes; it is an explicit Q-Gorenstein smoothing, yielding a precise\ndescription of the Milnor fibre and its non-abelian fundamental group. This had\nalready been done for two of these classes in a previous paper; what is new\nhere is the construction of the third class, which is far more difficult. In\naddition, we explain the existence of two different QHD smoothings for the\nfirst class.\nWe also prove a general formula for the dimension of a QHD smoothing\ncomponent for a rational surface singularity. A corollary is that for the\nvalency 4 cases, such a component has dimension 1 and is smooth. Another\ncorollary is that \"most\" H-shaped resolution graphs cannot be the graph of a\nsingularity with a QHD smoothing. This result, plus recent work of\nBhupal-Stipsicz, is evidence for a general\nConjecture: The only complex surface singularities with a QHD smoothing are\nthe (known) weighted homogeneous examples.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0505544",
        "title": "Boxicity and Treewidth",
        "abstract": "In this paper, we relate the seemingly unrelated concepts of treewidth and\n boxicity. Our main result is that, for any graph G, boxicity(G) <= treewidth(G)\n + 2. We also show that this upper bound is (almost) tight. Our result leads to\n various interesting consequences, like bounding the boxicity of many well known\n graph classes, such as chordal graphs, circular arc graphs, AT-free graphs,\n co--comparability graphs etc. All our bounds are shown to be tight up to small\n constant factors. An algorithmic consequence of our result is a linear time\n algorithm to construct a box representation for graphs of bounded treewidth in\n a space of constant dimension. We also show many structural results as a\n consequence. In particular, we show that, if the boxicity of a graph is b >= 3,\n then there exists a simple cycle of length at least b-3 as well as an induced\n cycle of length at least floor of (log(b-2) to the base Delta) + 2, where Delta\n is its maximum degree. We also relate boxicity with the cardinality of minimum\n vertex cover, minimum feedback vertex cover etc. Another structural consequence\n is that, for any fixed planar graph H, there is a constant c(H) such that, if\n boxicity(G) >= c(H) then H is a minor of G.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1911.08491",
        "title": "The Pristine survey X: a large population of low-metallicity stars\npermeates the Galactic disk",
        "abstract": "The orbits of the least chemically enriched stars open a window on the\n formation of our Galaxy when it was still in its infancy. The common picture is\n that these low-metallicity stars are distributed as an isotropic,\n pressure-supported component since these stars were either accreted from the\n early building blocks of the assembling Milky Way, or were later brought by the\n accretion of faint dwarf galaxies. Combining the metallicities and radial\n velocities from the Pristine and LAMOST surveys and Gaia DR2 parallaxes and\n proper motions for an unprecedented large and unbiased sample of very\n metal-poor stars at $[Fe/H]\\leq-2.5$ we show that this picture is incomplete.\n This sample shows strong statistical evidence (at the $5.0\\sigma$ level) of\n asymmetry in their kinematics, favouring prograde motion. Moreover, we find\n that $31\\%$ of the stars that currently reside in the disk do not venture\n outside of the disk plane throughout their orbit. The discovery of this\n population implies that a significant fraction of stars with iron abundances\n $[Fe/H]\\leq-2.5$ formed within or concurrently with the Milky Way disk and that\n the history of the disk was quiet enough to allow them to retain their\n disk-like orbital properties.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ph/0009140",
        "title": "Single chargino production at linear colliders",
        "abstract": "We study the single chargino production $e^+ e^- \\to \\tilde \\chi^{\\pm}\n \\mu^{\\mp}$ at linear colliders which occurs through the $\\l_{121}$ R-parity\n violating coupling constant. We focus on the final state containing 4 leptons\n and some missing energy. The largest background is \\susyq and can be reduced\n using the initial beam polarization and some cuts based on the specific\n kinematics of the single chargino production. Assuming the highest allowed\n supersymmetric background, a center of mass energy of $\\sqrt s=500GeV$ and a\n luminosity of ${\\cal L}=500fb^{-1}$, the sensitivities on the $\\l_{121}$\n coupling constant obtained from the single chargino production study improve\n the low-energy experimental limit over a range of $\\Delta m_{\\tilde \\nu}\n \\approx 500GeV$ around the sneutrino resonance, and reach values of $\\sim\n 10^{-4}$ at the $\\tilde \\nu$ pole. The single chargino production also allows\n to reconstruct the $\\tilde \\chi_1^{\\pm}$, $\\tilde \\chi_2^{\\pm}$ and $\\tilde\n \\nu$ masses. The initial state radiation plays a fundamental role in this\n study.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.13620",
        "title": "Optical Continuum Reverberation in the Dwarf Seyfert Nucleus of NGC 4395",
        "abstract": "The nearby dwarf spiral galaxy NGC 4395 contains a broad-lined active\n galactic nucleus (AGN) of exceptionally low luminosity powered by accretion\n onto a central black hole of very low mass ($\\sim10^4-10^5$ M$_\\odot$). In\n order to constrain the size of the optical continuum emission region through\n reverberation mapping, we carried out high-cadence photometric monitoring of\n NGC 4395 in the $griz$ filter bands on two consecutive nights in 2022 April\n using the four-channel MuSCAT3 camera on the Faulkes Telescope North at\n Haleakal\\={a} Observatory. Correlated variability across the $griz$ bands is\n clearly detected, and the $r$, $i$, and $z$ band light curves show lags of\n $8.4^{+1.0}_{-1.1}$, $14.2^{+1.2}_{-1.4}$, and $20.4^{+2.0}_{-2.1}$ minutes\n with respect to the $g$ band when measured using the full-duration light\n curves. When lags are measured for each night separately, the Night 2 data\n exhibit lower cross-correlation amplitudes and shorter lags than the Night 1\n light curves. Using the full-duration lags, we find that the lag-wavelength\n relationship is consistent with the $\\tau\\propto\\lambda^{4/3}$ dependence found\n for more luminous AGN. Combining our results with continuum lags measured for\n other objects, the lag between $g$ and $z$ band scales with optical continuum\n luminosity as $\\tau_{gz} \\propto L^{0.56\\pm0.05}$, similar to the scaling of\n broad-line region size with luminosity, reinforcing recent evidence that\n diffuse continuum emission from the broad-line region may contribute\n substantially to optical continuum variability and reverberation lags.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2109.10316",
        "title": "Direct and clean loading of nanoparticles into optical traps at millibar\npressures",
        "abstract": "Nanoparticles levitated by optical fields under vacuum conditions have\n applications in quantum science, the study of nanothermodynamics and precision\n sensing. Existing techniques for loading optical traps require ambient\n conditions, and often involve dispersion in liquids, which can contaminate\n delicate optics and lead to enhanced optical absorption and heating. Here we\n present a clean, dry and generic mechanism for directly loading optical traps\n at pressures down to 1\\,mbar, exploiting Laser Induced Acoustic Desorption. Our\n method allows rapid and efficient trapping, and is also suitable for\n site-selective loading of nanofabricated particles grown on a silicon\n substrate.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2301.01014",
        "title": "Trichotomy Theorem for Prescribed Scalar and Mean Curvatures on Compact\nManifolds with Boundaries",
        "abstract": "In this article, we give results of prescribing scalar and mean curvature\n functions for metrics either pointwise conformal or conformally equivalent to a\n Riemannian metric that is equipped on a compact manifold with boundary, with\n dimensions at least $ 3 $. The results are classified by the sign of the first\n eigenvalue of the conformal Laplacian. This leads to a \"Trichotomy Theorem\" in\n terms of both scalar and mean curvature functions, which is a full extension of\n the \"Trichotomy Theorem\" given by Kazdan and Warner. We also discuss\n prescribing Gauss and geodesic curvature problems on compact Riemann surfaces\n with boundary for metrics either pointwise conformal or conformally equivalent\n to the original metric, provided that the Euler characteristic is negative. The\n key step is a general version of monotone iteration scheme which handle the\n zeroth order nonlinear term on the boundary conditions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2005.03219",
        "title": "$L^{q}$-error estimates for approximation of irregular functionals of\nrandom vectors",
        "abstract": "Avikainen showed that, for any $p,q \\in [1,\\infty)$, and any function $f$ of\n bounded variation in $\\mathbb{R}$, it holds that\n $\\mathbb{E}[|f(X)-f(\\widehat{X})|^{q}] \\leq C(p,q)\n \\mathbb{E}[|X-\\widehat{X}|^{p}]^{\\frac{1}{p+1}}$, where $X$ is a\n one-dimensional random variable with a bounded density, and $\\widehat{X}$ is an\n arbitrary random variable. In this article, we will provide multi-dimensional\n versions of this estimate for functions of bounded variation in\n $\\mathbb{R}^{d}$, Orlicz--Sobolev spaces, Sobolev spaces with variable\n exponents, and fractional Sobolev spaces. The main idea of our arguments is to\n use the Hardy--Littlewood maximal estimates and pointwise characterizations of\n these function spaces. We apply our main results to analyze the numerical\n approximation for some irregular functionals of the solution of stochastic\n differential equations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2112.03961",
        "title": "Edge Clique Covers in Graphs with Independence Number Two: a Special\nCase",
        "abstract": "The edge clique cover number $ecc(G)$ of a graph $G$ is the size of the\n smallest set of complete subgraphs whose union covers all edges of $G$. It has\n been conjectured that all the simple graphs with independence number two\n satisfy $ecc(G)\\leq n$. First, we present a class of graphs containing edges\n difficult to cover but that satisfy the conjecture. Second, we describe a large\n class of graphs $G$ such that $ecc(G)\\leq \\frac{3}{2}n$. This class is easy to\n characterize.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2111.11761",
        "title": "Confronting quantum-corrected teleparallel cosmology with observations",
        "abstract": "It has been shown that at the semi-classical order, gravitational theories\n with quantum fluctuations can be effectively recast as modified theories of\n gravity with non-minimal gravity-matter couplings. We proceed from an\n observational perspective and see whether such quantum fluctuations can leave\n imprints on the late Universe. Within the teleparallel formulation, we\n investigate a representative model in this general class of modified\n gravitational theories inlaid with quantum fluctuations, and determine the\n cosmological parameters by using compiled late-time data sets. Furthermore, we\n assess the statistical significance of such quantum corrections compared to the\n standard cosmological model. The results mildly favor the inclusion of quantum\n corrections with a negative density parameter supporting a phantom-like dark\n energy. This edge is not sufficient to rule out either models but it supports\n the consideration of quantum corrections in a cosmological setting.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2102.09372",
        "title": "Mobile Apps Prioritizing Privacy, Efficiency and Equity: A Decentralized\nApproach to COVID-19 Vaccination Coordination",
        "abstract": "In this early draft, we describe a decentralized, app-based approach to\n COVID-19 vaccine distribution that facilitates zero knowledge verification,\n dynamic vaccine scheduling, continuous symptoms reporting, access to aggregate\n analytics based on population trends and more. To ensure equity, our solution\n is developed to work with limited internet access as well. In addition, we\n describe the six critical functions that we believe last mile vaccination\n management platforms must perform, examine existing vaccine management systems,\n and present a model for privacy-focused, individual-centric solutions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.09942",
        "title": "OptiRoute: A Heuristic-assisted Deep Reinforcement Learning Framework\nfor UAV-UGV Collaborative Route Planning",
        "abstract": "Unmanned aerial vehicles (UAVs) are capable of surveying expansive areas, but\n their operational range is constrained by limited battery capacity. The\n deployment of mobile recharging stations using unmanned ground vehicles (UGVs)\n significantly extends the endurance and effectiveness of UAVs. However,\n optimizing the routes of both UAVs and UGVs, known as the UAV-UGV cooperative\n routing problem, poses substantial challenges, particularly with respect to the\n selection of recharging locations. Here in this paper, we leverage\n reinforcement learning (RL) for the purpose of identifying optimal recharging\n locations while employing constraint programming to determine cooperative\n routes for the UAV and UGV. Our proposed framework is then benchmarked against\n a baseline solution that employs Genetic Algorithms (GA) to select rendezvous\n points. Our findings reveal that RL surpasses GA in terms of reducing overall\n mission time, minimizing UAV-UGV idle time, and mitigating energy consumption\n for both the UAV and UGV. These results underscore the efficacy of\n incorporating heuristics to assist RL, a method we refer to as\n heuristics-assisted RL, in generating high-quality solutions for intricate\n routing problems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1409.8165",
        "title": "Theory for Baryon Number and Dark Matter at the LHC",
        "abstract": "We investigate the possibility to test the simplest theory for spontaneous\n baryon number violation at the Large Hadron Collider. In this context the\n baryon number is a local gauge symmetry spontaneously broken at the low scale\n through the Brout-Englert-Higgs mechanism. This theory predicts the existence\n of a leptophobic neutral gauge boson and a fermionic dark matter candidate with\n baryon number. We study the gauge boson and Higgs decays, and explore the\n connection between collider signatures and constraints coming from dark matter\n experiments. We point out an upper bound on the symmetry breaking scale using\n the relic density constraints which tells us that this model can be tested or\n ruled out at current or future collider experiments.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1505.01738",
        "title": "Radiative Neutrino Mass Models",
        "abstract": "In this short review, we see some typical models in which light neutrino\n masses are generated at the loop level. These models involve new Higgs bosons\n whose Yukawa interactions with leptons are constrained by the neutrino\n oscillation data. Predictions about flavor structures of $\\ell \\to\n \\overline{\\ell}_1 \\ell_2 \\ell_3$ and leptonic decays of new Higgs bosons via\n the constrained Yukawa interactions are briefly summarized in order to utilize\n such Higgs as a probe of $\\nu$ physics.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2210.03102",
        "title": "Ambiguous Images With Human Judgments for Robust Visual Event\nClassification",
        "abstract": "Contemporary vision benchmarks predominantly consider tasks on which humans\n can achieve near-perfect performance. However, humans are frequently presented\n with visual data that they cannot classify with 100% certainty, and models\n trained on standard vision benchmarks achieve low performance when evaluated on\n this data. To address this issue, we introduce a procedure for creating\n datasets of ambiguous images and use it to produce SQUID-E (\"Squidy\"), a\n collection of noisy images extracted from videos. All images are annotated with\n ground truth values and a test set is annotated with human uncertainty\n judgments. We use this dataset to characterize human uncertainty in vision\n tasks and evaluate existing visual event classification models. Experimental\n results suggest that existing vision models are not sufficiently equipped to\n provide meaningful outputs for ambiguous images and that datasets of this\n nature can be used to assess and improve such models through model training and\n direct evaluation of model calibration. These findings motivate large-scale\n ambiguous dataset creation and further research focusing on noisy visual data.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/physics/0211096",
        "title": "Scanning the structure of ill-known spaces: Part 1. Founding principles\nabout mathematical constitution of space",
        "abstract": "Necessary and sufficient conditions allowing a previously unknown space to be\n explored through scanning operators are reexamined with respect to measure\n theory. Generalized conceptions of distances and dimensionality evaluation are\n proposed, together with their conditions of validity and range of application\n to topological spaces. The existence of a Boolean lattice with fractal\n properties originating from nonwellfounded properties of the empty set is\n demonstrated. This lattice provides a substrate with both discrete and\n continuous properties, from which existence of physical universes can be\n proved, up to the function of conscious perception. Spacetime emerges as an\n ordered sequence of mappings of closed 3-D Ponicare sections of a topological\n 4-space provided by the lattice. The possibility of existence of spaces with\n fuzzy dimension or with adjoined parts with decreasing dimensions is raised,\n together with possible tools for their study. The work provides the\n introductory foundations supporting a new theory of space whose physical\n predictions (suppressing the opposition of quantum and relativistic approaches)\n and experimental proofs are presented in details in Parts 2 and 3 of the study.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1912.04787",
        "title": "Deformations of representations of fundamental groups of complex\nvarieties",
        "abstract": "We describe locally the representation varieties of fundamental groups for\n smooth complex varieties at representations coming from the monodromy of a\n variation of mixed Hodge structure. Given such a manifold $X$ and such a linear\n representation $\\rho$ of its fundamental group $\\pi_1(X,x)$, we use the theory\n of Goldman-Millson and pursue our previous work that combines mixed Hodge\n theory with derived deformation theory to construct a mixed Hodge structure on\n the formal local ring $\\widehat{\\mathcal{O}}_\\rho$ to the representation\n variety of $\\pi_1(X,x)$ at $\\rho$. Then we show how a weighted-homogeneous\n presentation of $\\widehat{\\mathcal{O}}_\\rho$ is induced directly from a\n splitting of the weight filtration of its mixed Hodge structure. In this way we\n recover and generalize theorems of Eyssidieux-Simpson ($X$ compact) and of\n Kapovich-Millson ($\\rho$ finite).",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2109.04913",
        "title": "Adjoint Differentiation for generic matrix functions",
        "abstract": "We derive a formula for the adjoint $\\overline{A}$ of a square-matrix\n operation of the form $C=f(A)$, where $f$ is holomorphic in the neighborhood of\n each eigenvalue. We then apply the formula to derive closed-form expressions in\n particular cases of interest such as the case when we have a spectral\n decomposition $A=UDU^{-1}$, the spectrum cut-off $C=A_+$ and the Nearest\n Correlation Matrix routine. Finally, we explain how to simplify the computation\n of adjoints for regularized linear regression coefficients.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ph/0609274",
        "title": "The Standard Model: Alchemy and Astrology",
        "abstract": "An brief unconventional review of Standard Model physics, containing no\n plots.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2001.11108",
        "title": "D2M: Dynamic Defense and Modeling of Adversarial Movement in Networks",
        "abstract": "Given a large enterprise network of devices and their authentication history\n (e.g., device logons), how can we quantify network vulnerability to lateral\n attack and identify at-risk devices? We systematically address these problems\n through D2M, the first framework that models lateral attacks on enterprise\n networks using multiple attack strategies developed with researchers,\n engineers, and threat hunters in the Microsoft Defender Advanced Threat\n Protection group. These strategies integrate real-world adversarial actions\n (e.g., privilege escalation) to generate attack paths: a series of compromised\n machines. Leveraging these attack paths and a novel Monte-Carlo method, we\n formulate network vulnerability as a probabilistic function of the network\n topology, distribution of access credentials and initial penetration point. To\n identify machines at risk to lateral attack, we propose a suite of five fast\n graph mining techniques, including a novel technique called AnomalyShield\n inspired by node immunization research. Using three real-world authentication\n graphs from Microsoft and Los Alamos National Laboratory (up to 223,399\n authentications), we report the first experimental results on network\n vulnerability to lateral attack, demonstrating D2M's unique potential to\n empower IT admins to develop robust user access credential policies.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1908.10529",
        "title": "Semiclassical inverse spectral problem for elastic Love waves in\nisotropic media",
        "abstract": "We analyze the inverse spectral problem on the half line associated with\n elastic surface waves. Here, we focus on Love waves. Under certain generic\n conditions, we establish uniqueness and present a reconstruction scheme for the\n S- wavespeed with multiple wells from the semiclassical spectrum of these\n waves.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2307.04897",
        "title": "Spin-EPR-pair separation by conveyor-mode single electron shuttling in\nSi/SiGe",
        "abstract": "Long-ranged coherent qubit coupling is a missing function block for scaling\n up spin qubit based quantum computing solutions. Spin-coherent conveyor-mode\n electron-shuttling could enable spin quantum-chips with scalable and sparse\n qubit-architecture. Its key feature is the operation by only few easily\n tuneable input terminals and compatibility with industrial gate-fabrication.\n Single electron shuttling in conveyor-mode in a 420 nm long quantum bus has\n been demonstrated previously. Here we investigate the spin coherence during\n conveyor-mode shuttling by separation and rejoining an Einstein-Podolsky-Rosen\n (EPR) spin-pair. Compared to previous work we boost the shuttle velocity by a\n factor of 10000. We observe a rising spin-qubit dephasing time with the longer\n shuttle distances due to motional narrowing and estimate the spin-shuttle\n infidelity due to dephasing to be 0.7 % for a total shuttle distance of nominal\n 560 nm. Shuttling several loops up to an accumulated distance of 3.36 $\\mu$m,\n spin-entanglement of the EPR pair is still detectable, giving good perspective\n for our approach of a shuttle-based scalable quantum computing architecture in\n silicon.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2211.09975",
        "title": "Explicit bounds for large gaps between squarefree integers",
        "abstract": "We obtain explicit forms of the current best known asymptotic upper bounds\n for gaps between squarefree integers. In particular we show, for any $x \\ge 2$,\n that every interval of the form $(x, x + 11x^{1/5}\\log x]$ contains a\n squarefree integer. The constant 11 can be improved further, if $x$ is assumed\n to be larger than a (very) large constant.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1511.01566",
        "title": "DEMONIC programming: a computational language for single-particle\nequilibrium thermodynamics, and its formal semantics",
        "abstract": "Maxwell's Demon, 'a being whose faculties are so sharpened that he can follow\nevery molecule in its course', has been the centre of much debate about its\nabilities to violate the second law of thermodynamics. Landauer's hypothesis,\nthat the Demon must erase its memory and incur a thermodynamic cost, has become\nthe standard response to Maxwell's dilemma, and its implications for the\nthermodynamics of computation reach into many areas of quantum and classical\ncomputing. It remains, however, still a hypothesis. Debate has often centred\naround simple toy models of a single particle in a box. Despite their\nsimplicity, the ability of these systems to accurately represent thermodynamics\n(specifically to satisfy the second law) and whether or not they display\nLandauer Erasure, has been a matter of ongoing argument. The recent\nNorton-Ladyman controversy is one such example.\nIn this paper we introduce a programming language to describe these simple\nthermodynamic processes, and give a formal operational semantics and program\nlogic as a basis for formal reasoning about thermodynamic systems. We formalise\nthe basic single-particle operations as statements in the language, and then\nshow that the second law must be satisfied by any composition of these basic\noperations. This is done by finding a computational invariant of the system. We\nshow, furthermore, that this invariant requires an erasure cost to exist within\nthe system, equal to kTln2 for a bit of information: Landauer Erasure becomes a\ntheorem of the formal system. The Norton-Ladyman controversy can therefore be\nresolved in a rigorous fashion, and moreover the formalism we introduce gives a\nset of reasoning tools for further analysis of Landauer erasure, which are\nprovably consistent with the second law of thermodynamics.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1609.09574",
        "title": "Multiple-scale analysis on the radiation within the coupled KdV\nequations",
        "abstract": "A multiple scale model of the nonlinearly coupled KdV equations is\n established to predict mechanism of interaction of equatorial Rossby waves and\n barotropic waves in certain case. Analytically, predicted precursor radiation\n is a centrosymmetric object and is shown in excellent quantitative agreement\n with numerical simulations; furthermore, the multiple scale model elucidates\n the salient mechanisms of the interaction of solitary waves and the mechanism\n for radiation. While the atmosphere-ocean science community is very interested\n in theoretical studies of tropical wave interactions and in developing reduced\n dynamical models that can explain some key features of equatorial phenomena,\n our analytic predictions quantitively explain formation of radiation during\n interaction in Biello's model beyond qualitative level.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1011.3221",
        "title": "Reflected backward doubly stochastic differential equations with\ndiscontinuous generator",
        "abstract": "In this note, we study one-dimensional reflected backward doubly stochastic\n differential equations (RBDSDEs) with one continuous barrier and discontinuous\n generator (left-or right-continuous). By a comparison theorem establish here\n for RBDSDEs, we provide a minimal or a maximal solution to RBDSDEs",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2403.08957",
        "title": "Effect of Deviations from General Relativity on Searches for\nGravitational Wave Microlensing and Type II Strong Lensing",
        "abstract": "As the gravitational wave detector network is upgraded and the sensitivity of\n the detectors improves, novel scientific avenues open for exploration. For\n example, tests of general relativity will become more accurate as smaller\n deviations can be probed. Additionally, the detection of lensed gravitational\n waves becomes more likely. However, these new avenues could also interact with\n each other, and a gravitational wave event presenting deviations from general\n relativity could be mistaken for a lensed one. Here, we explore how\n phenomenological deviations from general relativity or binaries of exotic\n compact objects could impact those lensing searches focusing on a single event.\n We consider strong lensing, millilensing, and microlensing and find that\n certain phenomenological deviations from general relativity may be mistaken for\n all of these types of lensing. Therefore, our study shows that future candidate\n lensing events would need to be carefully examined to avoid a false claim of\n lensing where instead a deviation from general relativity has been seen.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2204.12739",
        "title": "Analysis of the data from photoelectric gas polarimeters",
        "abstract": "We review the tools and procedures for the analysis of the data collected by\n X-ray photoelectric gas polarimeters, like the ones on-board the Imaging X-ray\n Polarimetry Explorer (IXPE). Although many of such tools are in principle\n common with polarimeters working at other energy bands, the peculiar\n characteristics and performance of these devices require a specific approach.\n We will start from the analysis of the raw data read-out from this kind of\n instruments, that is, the image of the track of the photoelectron. We will\n briefly present how such images are processed with highly-specialized\n algorithms to extract all the information collected by the instrument. These\n include energy, time of arrival and, possibly, absorption point of the photon,\n in addition to the initial direction of emission of the photoelectron. The last\n is the quantity relevant for polarimetry, and we will present different methods\n to obtain the polarization degree and angle from it. A simple method, used\n extensively especially during the development phase of X-ray photoelectric gas\n polarimeters, is based on the construction and fitting of the azimuthal\n distribution of the photoelectrons. We will discuss that there are several\n reasons to prefer an analysis based on Stokes parameters, especially when one\n wants to analyze measurements of real, i.e., not laboratory, sources. These are\n quantities commonly used at all wavelengths because they are additive, and then\n operations like background subtraction or the application of calibration are\n trivial to apply. We will summarize how Stokes parameters can be used to adapt\n current spectroscopy software based on forward folding fitting to perform\n spectro-polarimetry. Moreover, we will derive how to properly associate the\n statistical uncertainty on a polarimetry measurement and the relation with\n another statistical indicator, which is in the minimum detectable polarization.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1405.2646",
        "title": "Consecuencias geom\\'etricas y din\\'amicas de la m\\'ultiple adsorci\\'on\nde Litio y otros alcalinos en poliacenos, poli-parafenilenos y hojuelas de\ngrafeno",
        "abstract": "We study the spontaneous symmetry breaking due to adsorption of Lithium atoms\n on polyacenes and aromatic molecules consistent on carbon rings with edges\n closed by bond of hydrogen atoms. Hartree Fock and DFT calculations were made\n for polyacenes, poly-para-phenyls and carbon sheets of sizes that show\n properties of graphene. As a result, the spontaneous symmetry breaking on\n polyacenes due to the adsorption of multiple pairs of Lithium atoms on opposite\n sides find expla- nation on the Peierls distortion. We also found that there\n are no sponta- neous symmetry breaking due to adsorption of other alkalines in\n polyacenes and the only case showing a distortion of polyacene is when a pair\n of Sodium atoms is adsorbed on it. Although there were previous studies about\n adsorption of metals on lar- ge sheet of carbon, we initially wanted to\n simplify the problem so that allowed us to obtain a better understanding of the\n reasons for symmetry breaking and then extend it to complex structures, for\n which, we found that the Peierls distortion is observable in small flakes, with\n appropriate symmetry, but can not be generalized to larger flakes and neither\n graphene strips as the minimum energy states of these cases do not correspond\n to this distortion. As an application on the methods we have used in the\n present work, a me- chanism for the decomposition reactions of chloromethane,\n dichloromethane and formyl chloride on graphene surface is proposed. For this,\n we calculate the reactions on the graphene surface with a Lithium atom absorbed\n on the center of it at the opposite side and we found intermediate production\n of radicals is reduced.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1606.00268",
        "title": "Certain Chromatic Sums of Some Cycle Related Graph Classes",
        "abstract": "Let $\\mathcal{C} = \\{c_1,c_2, c_3, \\ldots,c_k\\}$ be a certain type of proper\n $k$-colouring of a given graph $G$ and $\\theta(c_i)$ denote the number of times\n a particular colour $c_i$ is assigned to the vertices of $G$. Then, the\n colouring sum of a given graph $G$ with respect to the colouring $\\cC$, denoted\n by $\\omega_{\\cC}(G)$, is defined to be $\\omega(\\cC) =\n \\sum\\limits_{i=1}^{k}i\\,\\theta(c_i)$. The colouring sums such as\n $\\chi$-chromatic sum, $\\chi^+$-chromatic sum, $b$-chromatic sum,\n $b^+$-chromatic sum etc. are some of these types of colouring sums that have\n been studied recently. Motivated by these studies on certain chromatic sums of\n graphs, in this paper, we study certain chromatic sums for some standard cycle\n related graphs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1307.5000",
        "title": "Weyl composition of symbols in large dimension",
        "abstract": "This paper is concerned with the Weyl composition of symbols in large\n dimension. We specify a class of symbols in order to estimate the Weyl symbol\n of the product of two Weyl $h-$pseudodifferential operators, with constants\n independent of the dimension. The proof includes a regularized and a hybrid\n compositions together with a decomposition formula. We also analyze in this\n context the remainder term of the semiclassical expansion of the Weyl\n composition.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1906.03625",
        "title": "Soft-ranking Label Encoding for Robust Facial Age Estimation",
        "abstract": "Automatic facial age estimation can be used in a wide range of real-world\n applications. However, this process is challenging due to the randomness and\n slowness of the aging process. Accordingly, in this paper, we propose a\n comprehensive framework aimed at overcoming the challenges associated with\n facial age estimation. First, we propose a novel age encoding method, referred\n to as 'Soft-ranking', which encodes two important properties of facial age,\n i.e., the ordinal property and the correlation between adjacent ages.\n Therefore, Soft-ranking provides a richer supervision signal for training deep\n models. Moreover, we also carefully analyze existing evaluation protocols for\n age estimation, finding that the overlap in identity between the training and\n testing sets affects the relative performance of different age encoding\n methods. Finally, since existing face databases for age estimation are\n generally small, deep models tend to suffer from an overfitting problem. To\n address this issue, we propose a novel regularization strategy to encourage\n deep models to learn more robust features from facial parts for age estimation\n purposes. Extensive experiments indicate that the proposed techniques improve\n the age estimation performance; moreover, we achieve state-of-the-art\n performance on the three most popular age databases, $i.e.$, Morph II,\n CLAP2015, and CLAP2016.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1306.4986",
        "title": "Dynamics of thermalization and decoherence of a nanoscale system",
        "abstract": "We study the decoherence and thermalization dynamics of a nanoscale system\n coupled nonperturbatively to a fully quantum-mechanical bath. The system is\n prepared out of equilibrium in a pure state of the complete system. We propose\n a random matrix model and show analytically that there are two robust temporal\n regimes in the approach of the system to equilibrium --- an initial Gaussian\n decay followed by an exponential tail, consistent with numerical results on\n small interacting lattices [S. Genway, A.F. Ho and D.K.K. Lee, Phys. Rev. Lett.\n 105, 260402 (2010)]. Furthermore, the system decays towards a Gibbs ensemble in\n accordance with the eigenstate thermalization hypothesis.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1909.12608",
        "title": "Charge Induced Acceleration Noise in the LISA Gravitational Reference\nSensor",
        "abstract": "The presence of free charge on isolated proof-masses, such as those within\n space-borne gravitational reference sensors, causes a number of spurious forces\n which will give rise to associated acceleration noise. A complete discusssion\n of each charge induced force and its linear acceleration noise is presented.\n The resulting charge acceleration noise contributions to the LISA mission are\n evaluated using the LISA Pathfinder performance and design. It is shown that\n one term is largely dominant but that a full budget should be maintained for\n LISA and future missions due to the large number of possible contributions and\n their dependence on different sensor parameters.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.11765",
        "title": "Privacy-Preserving In-Context Learning with Differentially Private\nFew-Shot Generation",
        "abstract": "We study the problem of in-context learning (ICL) with large language models\n (LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak\n or regurgitate the private examples demonstrated in the prompt. We propose a\n novel algorithm that generates synthetic few-shot demonstrations from the\n private dataset with formal differential privacy (DP) guarantees, and show\n empirically that it can achieve effective ICL. We conduct extensive experiments\n on standard benchmarks and compare our algorithm with non-private ICL and\n zero-shot solutions. Our results demonstrate that our algorithm can achieve\n competitive performance with strong privacy levels. These results open up new\n possibilities for ICL with privacy protection for a broad range of\n applications.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1705.09934",
        "title": "Probing various formulations of macrorealism for unsharp quantum\nmeasurements",
        "abstract": "Standard Leggett and Garg inequalities (SLGIs) were formulated for testing\n the incompatibility between the classical worldview of macrorealism and quantum\n mechanics. In recent times, various other formulations, such as Wigner form of\n LGIs (WLGIs), entropic LGIs (ELGIs) and the no-signaling in time (NSIT)\n condition have also been proposed. It is also recently argued that no set of\n SLGIs can provide the necessary and sufficient conditions for macrorealism but\n a suitable conjunction of NSIT conditions provides the same. In this paper, we\n first provide a comparative study of the various formulations of LGIs for\n testing macrorealism pertaining to the two different unsharp measurements.\n While the violations of WLGIs are more robust than SLGIs and ELGIs for\n spin-POVMs, here we demonstrate that for the case of biased POVMs, the quantum\n violations of both SLGIs and ELGIs provide the same robustness as WLGIs.\n Importantly, the violations of all formulations of LGIs can be achieved for\n \\textit{any non-zero value} of unsharpness parameter. We have also studied the\n connection between LGIs and NSIT conditions. Further, we investigate the role\n of the joint measurability of the POVMs in the violation of LGIs and found that\n there is no generic connection.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1104.2939",
        "title": "Subexponential convergence for information aggregation on regular trees",
        "abstract": "We consider the decentralized binary hypothesis testing problem on trees of\nbounded degree and increasing depth. For a regular tree of depth t and\nbranching factor k>=2, we assume that the leaves have access to independent and\nidentically distributed noisy observations of the 'state of the world' s.\nStarting with the leaves, each node makes a decision in a finite alphabet M,\nthat it sends to its parent in the tree. Finally, the root decides between the\ntwo possible states of the world based on the information it receives.\nWe prove that the error probability vanishes only subexponentially in the\nnumber of available observations, under quite general hypotheses. More\nprecisely the case of binary messages, decay is subexponential for any decision\nrule. For general (finite) message alphabet M, decay is subexponential for\n'node-oblivious' decision rules, that satisfy a mild irreducibility condition.\nIn the latter case, we propose a family of decision rules with close-to-optimal\nasymptotic behavior.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0708.1616",
        "title": "Robustness of the second law of thermodynamics under generalizations of\nthe maximum entropy method",
        "abstract": "It is shown that the laws of thermodynamics are extremely robust under\n generalizations of the form of entropy. Using the Bregman-type relative\n entropy, the Clausius inequality is proved to be always valid. This implies\n that thermodynamics is highly universal and does not rule out consistent\n generalization of the maximum entropy method.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1711.06740",
        "title": "Information Gathering with Peers: Submodular Optimization with\nPeer-Prediction Constraints",
        "abstract": "We study a problem of optimal information gathering from multiple data\n providers that need to be incentivized to provide accurate information. This\n problem arises in many real world applications that rely on crowdsourced data\n sets, but where the process of obtaining data is costly. A notable example of\n such a scenario is crowd sensing. To this end, we formulate the problem of\n optimal information gathering as maximization of a submodular function under a\n budget constraint, where the budget represents the total expected payment to\n data providers. Contrary to the existing approaches, we base our payments on\n incentives for accuracy and truthfulness, in particular, {\\em peer-prediction}\n methods that score each of the selected data providers against its best peer,\n while ensuring that the minimum expected payment is above a given threshold. We\n first show that the problem at hand is hard to approximate within a constant\n factor that is not dependent on the properties of the payment function.\n However, for given topological and analytical properties of the instance, we\n construct two greedy algorithms, respectively called PPCGreedy and\n PPCGreedyIter, and establish theoretical bounds on their performance w.r.t. the\n optimal solution. Finally, we evaluate our methods using a realistic crowd\n sensing testbed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2006.02829",
        "title": "The Enclaveless Competition Game",
        "abstract": "For a subset $S$ of vertices in a graph $G$, a vertex $v \\in S$ is an enclave\n of $S$ if $v$ and all of its neighbors are in $S$, where a neighbor of $v$ is a\n vertex adjacent to $v$. A set $S$ is enclaveless if it does not contain any\n enclaves. The enclaveless number $\\Psi(G)$ of $G$ is the maximum cardinality of\n an enclaveless set in $G$. As first observed in 1997 by Slater [J. Res. Nat.\n Bur. Standards 82 (1977), 197--202], if $G$ is a graph with $n$ vertices, then\n $\\gamma(G) + \\Psi(G) = n$ where $\\gamma(G)$ is the well-studied domination\n number of $G$. In this paper, we continue the study of the\n competition-enclaveless game introduced in 2001 by Phillips and Slater [Graph\n Theory Notes N. Y. 41 (2001), 37--41] and defined as follows. Two players take\n turns in constructing a maximal enclaveless set $S$, where one player,\n Maximizer, tries to maximize $|S|$ and one player, Minimizer, tries to\n minimize~$|S|$. The competition-enclaveless game number $\\Psi_g^+(G)$ of $G$ is\n the number of vertices played when Maximizer starts the game and both players\n play optimally. We study among other problems the conjecture that if $G$ is an\n isolate-free graph of order $n$, then $\\Psi_g^+(G) \\ge \\frac{1}{2}n$. We prove\n this conjecture for regular graphs and for claw-free graphs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.07501",
        "title": "Multi-parameter perturbations for the space-periodic heat equation",
        "abstract": "This paper is divided into three parts. The first part focuses on periodic\n layer heat potentials, demonstrating their smooth dependence on regular\n perturbations of the support of integration. In the second part, we present an\n application of the results from the first part. Specifically, we consider a\n transmission problem for the heat equation in a periodic two-phase composite\n material and we show that the solution depends smoothly on the shape of the\n transmission interface, boundary data, and conductivity parameters. Finally, in\n the last part of the paper, we fix all parameters except for the contrast\n parameter and outline a strategy to deduce an explicit expansion of the\n solution using a Neumann-type series.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1808.03533",
        "title": "Measuring azimuthal and radial modes of photons",
        "abstract": "With the emergence of the field of quantum communications, the appropriate\n choice of photonic degrees of freedom used for encoding information is of\n paramount importance. Highly precise techniques for measuring the polarisation,\n frequency, and arrival time of a photon have been developed. However, the\n transverse spatial degree of freedom still lacks a measurement scheme that\n allows the reconstruction of its full transverse structure with a simple\n implementation and a high level of accuracy. Here we show a method to measure\n the azimuthal and radial modes of Laguerre-Gaussian beams with a greater than\n 99% accuracy, using a single phase screen. We compare our technique with\n previous commonly used methods and demonstrate the significant improvements it\n presents for quantum key distribution and state tomography of high-dimensional\n quantum states of light. Moreover, our technique can be readily extended to any\n arbitrary family of spatial modes, such as mutually unbiased bases,\n Hermite-Gauss, and Ince-Gauss. Our scheme will significantly enhance existing\n quantum and classical communication protocols that use the spatial structure of\n light, as well as enable fundamental experiments on spatial-mode entanglement\n to reach their full potential.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.14972",
        "title": "Tropical Positivity and Determinantal Varieties",
        "abstract": "We initiate the study of positive-tropical generators as positive analogues\n of the concept of tropical bases. Applying this to the tropicalization of\n determinantal varieties, we develop criteria for characterizing their positive\n part. We focus on the study of low-rank matrices, in particular matrices of\n rank 2 and 3. Moreover, in the case square-matrices of corank 1, we fully\n classify the signed tropicalization of the determinantal variety, even beyond\n the positive part.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cs/0510071",
        "title": "A Simple Cooperative Diversity Method Based on Network Path Selection",
        "abstract": "Cooperative diversity has been recently proposed as a way to form virtual\n antenna arrays that provide dramatic gains in slow fading wireless\n environments. However most of the proposed solutions require distributed\n space-time coding algorithms, the careful design of which is left for future\n investigation if there is more than one cooperative relay. We propose a novel\n scheme, that alleviates these problems and provides diversity gains on the\n order of the number of relays in the network. Our scheme first selects the best\n relay from a set of M available relays and then uses this best relay for\n cooperation between the source and the destination. We develop and analyze a\n distributed method to select the best relay that requires no topology\n information and is based on local measurements of the instantaneous channel\n conditions. This method also requires no explicit communication among the\n relays. The success (or failure) to select the best available path depends on\n the statistics of the wireless channel, and a methodology to evaluate\n performance for any kind of wireless channel statistics, is provided.\n Information theoretic analysis of outage probability shows that our scheme\n achieves the same diversity-multiplexing tradeoff as achieved by more complex\n protocols, where coordination and distributed space-time coding for M nodes is\n required, such as those proposed in [7]. The simplicity of the technique,\n allows for immediate implementation in existing radio hardware and its adoption\n could provide for improved flexibility, reliability and efficiency in future 4G\n wireless systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1008.3570",
        "title": "Temperature-induced crossovers in the static roughness of a\none-dimensional interface",
        "abstract": "At finite temperature and in presence of disorder, a one-dimensional elastic\n interface displays different scaling regimes at small and large lengthscales.\n Using a replica approach and a Gaussian Variational Method (GVM), we explore\n the consequences of a finite interface width $\\xi$ on the small-lengthscale\n fluctuations. We compute analytically the static roughness $B(r)$ of the\n interface as a function of the distance $r$ between two points on the\n interface. We focus on the case of short-range elasticity and random-bond\n disorder. We show that for a finite width $\\xi$ two temperature regimes exist.\n At low temperature, the expected thermal and random-manifold regimes,\n respectively for small and large scales, connect via an intermediate `modified'\n Larkin regime, that we determine. This regime ends at a temperature-independent\n characteristic `Larkin' length. Above a certain `critical' temperature that we\n identify, this intermediate regime disappears. The thermal and random-manifold\n regimes connect at a single crossover lengthscale, that we compute. This is\n also the expected behavior for zero width. Using a directed polymer\n description, we also study via a second GVM procedure and generic scaling\n arguments, a modified toy model that provides further insights on this\n crossover. We discuss the relevance of the two GVM procedures for the roughness\n at large lengthscale in those regimes. In particular we analyze the scaling of\n the temperature-dependent prefactor in the roughness $B(r)\\sim T^{2\n \\text{\\thorn}} r^{2 \\zeta}$ and its corresponding exponent $\\text{\\thorn}$. We\n briefly discuss the consequences of those results for the quasistatic creep law\n of a driven interface, in connection with previous experimental and numerical\n studies.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0511332",
        "title": "VIMOS-IFU survey of z~0.2 massive galaxy clusters. I. Observations of\nthe strong lensing cluster Abell 2667",
        "abstract": "(abridged) We present extensive multi-color imaging and low resolution VIMOS\n Integral Field Unit spectroscopic observations of the X-ray luminous cluster\n Abell 2667 (z=0.233). An extremely bright giant gravitational arc (z=1.0334) is\n easily identified as part of a triple image system and other fainter multiple\n images are also revealed by the HST-WFPC2 images. The VIMOS-IFU observations\n cover a field of view of 54'' x 54'' and enable us to determine the redshift of\n all galaxies down to V=22.5. Furthermore, redshifts could be identified for\n some sources down to V=23.2. In particular we identify 21 cluster members in\n the cluster inner region, from which we derive a velocity dispersion of\n \\sigma=960 km/s, corresponding to a total mass of 7.1 x 10^{13} solar masses\n within a 110 kpc radius. Using the multiple images constraints and priors on\n the mass distribution of cluster galaxy halos we construct a detailed lensing\n mass model leading to a total mass of 2.9 x 10^{13} solar masses within the\n Einstein radius (16 arcsec). The lensing mass and dynamical mass are in good\n agreement although the dynamical one is much less accurate. Comparing these\n measurements with published X-ray analysis, is however less conclusive.\n Although the X-ray temperature matches the dynamical and lensing estimates, the\n published NFW mass model derived from the X-ray measurement with its small\n concentration of c ~3 can not account for the large Einstein radius observed in\n this cluster. A larger concentration of ~6 would however match the strong\n lensing measurements. These results are likely reflecting the complex structure\n of the cluster mass distribution, underlying the importance of panchromatic\n studies from small to large scale in order to better understand cluster\n physics.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1805.06710",
        "title": "The optical characteristics of the dust of sungrazing comet C/2012 S1\n(ISON) observed at large heliocentric distances",
        "abstract": "We present an analysis of the photometric and spectroscopic data of the comet\n C/2012 S1 (ISON) observed at the heliocentric distances of 6.21\\div 4.81 AU.\n The photometric observations were made with the 60-cm Zeiss-600 telescope\n (ICAMER, peak Terskol, Russia) and the spectroscopic observations were\n performed using the SCORPIO-2 focal reducer mounted in the prime focus of the\n 6-m BTA telescope (SAO RAS, Russia). We analyse the B, V and R-band images to\n describe the dusty cometary coma and to investigate its brightness, colours and\n dust production rate. The spectra cover the wavelength range of 3600\\div 7070\n \\AA. No emissions which are expected in this wavelength region were detected\n above the 3$\\sigma$ level. The continuum shows a reddening effect with the\n normalized gradient of reflectivity along dispersion of (9.3$\\pm$1.1)% per 1000\n \\AA. A dust-loss rate was derived using the obtained values and under the\n different model assumptions. Our simulations clearly indicate that to retrieve\n dust production from the observational Af$\\rho$ parameter is an ambiguous task.\n The result of such a procedure is strongly dependent on dynamical (e.g.\n effective density and cross-section) as well as optical (e.g. scattering\n coefficient and phase function) characteristics of dust grains. A variation of\n the mentioned parameters can lead to dramatic changes in the evaluation of mass\n production. We demonstrate that the dynamic and optical properties are\n interconnected via the microscopic properties of dust grains (effective size\n and porosity).",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1912.13155",
        "title": "LHC probes of the 10 TeV scale",
        "abstract": "The usual range of new particle masses, up to a few TeV, searched for at the\n LHC may be substantially extended if ultraheavy diquark particles exist. A\n diquark scalar, $S_{uu}$, that interacts perturbatively with two up quarks may\n be as heavy as 10 TeV and would still produce tens of spectacular events at the\n 14 TeV LHC. It is shown here that an ultraheavy $S_{uu}$ could be discovered\n through final states of very high energy in various channels, especially if the\n diquark can decay into other new heavy particles. Examples include cascade\n decays of $S_{uu}$ via a second scalar produced in pairs, which leads to two\n dijet resonances, or to more exotic signals with top quarks, Higgs bosons,\n electroweak bosons, and high-$p_T$ jets. Another possibility is that the\n diquark decays into a vectorlike quark of multi-TeV mass and a top or up quark.\n Signal events include one or two highly boosted top quarks and a Higgs boson or\n a $Z$, without counterparts containing top antiquarks. Similarly, direct decays\n of the diquark into $tj$ or $tt$ with leptonic top decays involve only\n positively charged leptons.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.10688",
        "title": "Integration of Programmable Diffraction with Digital Neural Networks",
        "abstract": "Optical imaging and sensing systems based on diffractive elements have seen\n massive advances over the last several decades. Earlier generations of\n diffractive optical processors were, in general, designed to deliver\n information to an independent system that was separately optimized, primarily\n driven by human vision or perception. With the recent advances in deep learning\n and digital neural networks, there have been efforts to establish diffractive\n processors that are jointly optimized with digital neural networks serving as\n their back-end. These jointly optimized hybrid (optical+digital) processors\n establish a new \"diffractive language\" between input electromagnetic waves that\n carry analog information and neural networks that process the digitized\n information at the back-end, providing the best of both worlds. Such hybrid\n designs can process spatially and temporally coherent, partially coherent, or\n incoherent input waves, providing universal coverage for any spatially varying\n set of point spread functions that can be optimized for a given task, executed\n in collaboration with digital neural networks. In this article, we highlight\n the utility of this exciting collaboration between engineered and programmed\n diffraction and digital neural networks for a diverse range of applications. We\n survey some of the major innovations enabled by the push-pull relationship\n between analog wave processing and digital neural networks, also covering the\n significant benefits that could be reaped through the synergy between these two\n complementary paradigms.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1011.2064",
        "title": "The mass-loss rates and molecular abundances of S-type AGB stars",
        "abstract": "The S-type stars are believed to have a C/O-ratio close to unity (within a\n few percent). They are considered to represent an intermediate evolutionary\n stage as AGB stars evolve from oxygen-rich M-type stars into carbon stars. As\n possible transition objects the S-type stars could give important clues to the\n mass-loss mechanism(s) and to the chemical evolution along the AGB. Using\n observations of circumstellar radio line emission in combination with a\n detailed radiative transfer analysis, we have estimated mass-loss rates and\n abundances of chemically important molecules (SiO, HCN) for a sample of 40\n S-type AGB stars. The results will be compared to previous results for M-type\n and carbon stars.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1304.2384",
        "title": "Logical Fuzzy Optimization",
        "abstract": "We present a logical framework to represent and reason about fuzzy\n optimization problems based on fuzzy answer set optimization programming. This\n is accomplished by allowing fuzzy optimization aggregates, e.g., minimum and\n maximum in the language of fuzzy answer set optimization programming to allow\n minimization or maximization of some desired criteria under fuzzy environments.\n We show the application of the proposed logical fuzzy optimization framework\n under the fuzzy answer set optimization programming to the fuzzy water\n allocation optimization problem.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1807.07818",
        "title": "Wireless Multi-Sensor Networks for Smart Cities: A Prototype System with\nStatistical Data Analysis",
        "abstract": "As urbanization proceeds at an astonishing rate, cities have to continuously\n improve their solutions that affect the safety, health and overall wellbeing of\n their residents. Smart city projects worldwide build on advanced sensor,\n information and communication technologies to help dealing with issues like air\n pollution, waste management, traffic optimization, and energy efficiency. The\n paper reports about the prototype of a smart city initiative in Budapest which\n applies various sensors installed on the public lighting system and a\n cloud-based analytical module. While the installed wireless multi-sensor\n network gathers information about a number of stressors, the module integrates\n and statistically processes the data. The module can handle inconsistent,\n missing and noisy data and can extrapolate the measurements in time and space,\n namely, it can create short-term forecasts and smoothed maps, both accompanied\n by reliability estimates. The resulting database uses geometric representations\n and can serve as an information centre for public services.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1903.08799",
        "title": "The pure cohomology of multiplicative quiver varieties",
        "abstract": "To a quiver $Q$ and choices of nonzero scalars $q_i$, non-negative integers\n $\\alpha_i$, and integers $\\theta_i$ labeling each vertex $i$,\n Crawley-Boevey--Shaw associate a \"multiplicative quiver variety\"\n $\\mathcal{M}_\\theta^q(\\alpha)$, a trigonometric analogue of the Nakajima quiver\n variety associated to $Q$, $\\alpha$, and $\\theta$. We prove that the pure\n cohomology, in the Hodge-theoretic sense, of the stable locus\n $\\mathcal{M}_\\theta^q(\\alpha)^s$ is generated as a $\\mathbb{Q}$-algebra by the\n tautological characteristic classes. In particular, the pure cohomology of\n genus $g$ twisted character varieties of $GL_n$ is generated by tautological\n classes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1309.6585",
        "title": "Next-to-leading order QCD corrections to five jet production at the LHC",
        "abstract": "We present theoretical predictions for five jet production in proton-proton\n collisions at next-to-leading order accuracy in QCD. Inclusive as well as\n differential observables are studied for collision energies of 7 and 8 TeV. In\n general the next-to-leading order corrections stabilize the theoretical\n predictions with respect to scale variations. In case of the inclusive jet\n cross sections, we compare with experimental data where possible and find\n reasonable agreement. We observe that the four-to-three and five-to-four jet\n ratios show better perturbative convergence than the known three-to-two ratio\n and are promising candidates for future alpha_s measurements. Furthermore, we\n present a detailed analysis of uncertainties related to parton distribution\n functions. The full colour virtual matrix elements used in the computation were\n obtained with the NJet package, a publicly available library for the evaluation\n of one-loop amplitudes in massless QCD.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1412.3512",
        "title": "The equidistribution of some length three vincular patterns on\n$S_n(132)$",
        "abstract": "In 2012 B\\'ona showed the rather surprising fact that the cumulative number\n of occurrences of the classical patterns $231$ and $213$ are the same on the\n set of permutations avoiding $132$, beside the pattern based statistics $231$\n and $213$ do not have the same distribution on this set. Here we show that if\n it is required for the symbols playing the role of $1$ and $3$ in the\n occurrences of $231$ and $213$ to be adjacent, then the obtained statistics are\n equidistributed on the set of $132$-avoiding permutations. Actually, expressed\n in terms of vincular patterns, we prove the following more general results: the\n statistics based on the patterns $b-ca$, $b-ac$ and $ba-c$, together with other\n statistics, have the same joint distribution on $S_n(132)$, and so do the\n patterns $bc-a$ and $c-ab$; and up to trivial transformations, these statistics\n are the only based on length three proper (not classical nor adjacent) vincular\n patterns which are equidistributed on a set of permutations avoiding a\n classical length three pattern.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ex/0511002",
        "title": "Prospects for Antineutrino Running at MiniBooNE",
        "abstract": "We outline a program of antineutrino cross-section measurements necessary for\n the next generation of neutrino oscillation experiments, that can be performed\n with one year of data at MiniBooNE. We describe three independent methods of\n constraining wrong-sign (neutrino) backgrounds in an antineutrino beam, and\n their application to the MiniBooNE antineutrino cross section measurements.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1812.11678",
        "title": "Microscopic states of Kerr black holes from boundary-bulk correspondence",
        "abstract": "It was claimed by the author that black holes can be considered as\n topological insulators. They both have boundary modes and those boundary modes\n can be described by an effective BF theory. In this paper, we analyze the\n boundary modes on the horizon of black holes with the methods developed for\n topological insulator. Firstly the BTZ black hole is analysed, and the results\n are compatible with the previous works. Then we generalize those results to\n Kerr black holes. Some new results are obtained: dimensionless right- and\n left-temperature can be defined and have well behaviors both in Schwarzschild\n limit $a\\rightarrow 0$ and extremal limit $a\\rightarrow M$. Upon the Kerr/CFT\n correspondence, we can associate a central charge $c=12 M r_+$ with an\n arbitrary Kerr black hole if a dual CFT exists. We can identify the microstates\n of the Kerr black hole with the quantum states of this scalar field. From this\n identification we can count the number of microstates of the Kerr black hole\n and give the Bekenstein-Hawking area law for the entropy.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2301.08508",
        "title": "Connecting transonic buffet with incompressible low-frequency\noscillations on aerofoils",
        "abstract": "Self-sustained low-frequency flow unsteadiness over rigid aerofoils in the\n transonic regime is referred to as transonic buffet. Although the exact\n physical mechanisms underlying this phenomenon are unclear, it is generally\n assumed to be unique to the transonic regime. This assumption is shown to be\n incorrect here by performing large-eddy simulations of flow over a NACA0012\n profile for a wide range of flow conditions. At zero incidence and sufficiently\n high freestream Mach numbers, M, transonic buffet occurs with shock waves\n present in the flow. However, self-sustained oscillations that occur at similar\n frequencies are observed at lower M for which shock waves are absent and the\n entire flow field remains subsonic at all times. At higher incidences, the\n oscillations are sustained at progressively lower M. Oscillations were observed\n for M as low as 0.3, where compressibility effects are small. A spectral proper\n orthogonal decomposition shows that the spatial structure of these oscillations\n (i.e., mode shapes) are essentially the same for all cases. These results\n indicate that buffet on aerofoils does not necessarily require the presence of\n shock waves. Furthermore, the trend seen with increasing incidence angles\n suggests that transonic buffet on aerofoils and low-frequency oscillations\n reported in the incompressible regime (Zaman et al., 1989, J. Fluid Mech., vol.\n 202, pp. 403--442) have similar origins. Thus, models which rely specifically\n on shock waves to explain transonic buffet are incorrect. These insights could\n be useful in understanding the origins of ``transonic\" buffet and reformulating\n mitigation strategies by shifting the focus away from shock waves.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2206.00565",
        "title": "LiYbSe2: Frustrated Magnetism in a New Pyrochlore Lattice",
        "abstract": "Three-dimensionally (3D) frustrated magnets generally exist in the magnetic\n diamond and pyrochlore lattices, in which quantum fluctuations suppress\n magnetic orders and generate highly entangled ground states (GS). LiYbSe2 in a\n previously unreported pyrochlore lattice was discovered from LiCl flux growth.\n Distinct from the quantum spin liquid (QSL) candidate NaYbSe2 hosting a perfect\n triangular lattice of Yb3+, LiYbSe2 crystallizes in the cubic pyrochlore\n structure with space group Fd-3m (No. 227). The Yb3+ ions in LiYbSe2 are\n arranged on a network of corner-sharing tetrahedra, which is particularly\n susceptible to geometrical frustration. According to our temperature-dependent\n magnetic susceptibility measurements, the dominant antiferromagnetic\n interaction in LiYbSe2 is expected to appear around 8 K. However, no long-range\n magnetic order is detected in thermomagnetic measurements above 70 mK. Specific\n heat measurements also show magnetic correlations shifting with applied\n magnetic field with a degree of missing entropy that may be related to the\n slight mixture of Yb3+ on the Li site. Such magnetic frustration of Yb3+ is\n rare in pyrochlore structures. Thus, LiYbSe2 shows promises in intrinsically\n realizing disordered quantum states like QSL in pyrochlore structures.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2306.06534",
        "title": "K-Tensors: Clustering Positive Semi-Definite Matrices",
        "abstract": "This paper introduces $K$-Tensors, a novel self-consistent clustering\n algorithm designed to cluster positive semi-definite (PSD) matrices by their\n eigenstructures. Clustering PSD matrices is crucial across various fields,\n including computer and biomedical sciences. Traditional clustering methods,\n which often involve matrix vectorization, tend to overlook the inherent PSD\n characteristics, thereby discarding valuable shape and eigenstructural\n information. To preserve this essential shape and eigenstructral information,\n our approach incorporates a unique distance metric that respects the PSD nature\n of the data. We demonstrate that $K$-Tensors is not only self-consistent but\n also reliably converges to a local optimum. Through numerical studies, we\n further validate the algorithm's effectiveness and explore its properties in\n detail.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/0410178",
        "title": "Topological strings and their physical applications",
        "abstract": "We give an introductory review of topological strings and their application\n to various aspects of superstrings and supersymmetric gauge theories. This\n review includes developing the necessary mathematical background for\n topological strings, such as the notions of Calabi-Yau manifold and toric\n geometry, as well as physical methods developed for solving them, such as\n mirror symmetry, large N dualities, the topological vertex and quantum foam. In\n addition, we discuss applications of topological strings to N=1,2\n supersymmetric gauge theories in 4 dimensions as well as to BPS black hole\n entropy in 4 and 5 dimensions. (These are notes from lectures given by the\n second author at the 2004 Simons Workshop in Mathematics and Physics.)",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1009.5456",
        "title": "Shear Banding from lattice kinetic models with competing interactions",
        "abstract": "Soft Glassy Materials, Non Linear Rheology, Lattice Kinetic models,\n frustrated phase separation} We present numerical simulations based on a\n Boltzmann kinetic model with competing interactions, aimed at characterizating\n the rheological properties of soft-glassy materials. The lattice kinetic model\n is shown to reproduce typical signatures of driven soft-glassy flows in\n confined geometries, such as Herschel-Bulkley rheology, shear-banding and\n histeresys. This lends further credit to the present lattice kinetic model as a\n valuable tool for the theoretical/computational investigation of the rheology\n of driven soft-glassy materials under confinement.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.07236",
        "title": "Combating COVID-19 using Generative Adversarial Networks and Artificial\nIntelligence for Medical Images: A Scoping Review",
        "abstract": "This review presents a comprehensive study on the role of GANs in addressing\n the challenges related to COVID-19 data scarcity and diagnosis. It is the first\n review that summarizes the different GANs methods and the lungs images datasets\n for COVID-19. It attempts to answer the questions related to applications of\n GANs, popular GAN architectures, frequently used image modalities, and the\n availability of source code. This review included 57 full-text studies that\n reported the use of GANs for different applications in COVID-19 lungs images\n data. Most of the studies (n=42) used GANs for data augmentation to enhance the\n performance of AI techniques for COVID-19 diagnosis. Other popular applications\n of GANs were segmentation of lungs and super-resolution of the lungs images.\n The cycleGAN and the conditional GAN were the most commonly used architectures\n used in nine studies each. 29 studies used chest X-Ray images while 21 studies\n used CT images for the training of GANs. For majority of the studies (n=47),\n the experiments were done and results were reported using publicly available\n data. A secondary evaluation of the results by radiologists/clinicians was\n reported by only two studies. Conclusion: Studies have shown that GANs have\n great potential to address the data scarcity challenge for lungs images of\n COVID-19. Data synthesized with GANs have been helpful to improve the training\n of the Convolutional Neural Network (CNN) models trained for the diagnosis of\n COVID-19. Besides, GANs have also contributed to enhancing the CNNs performance\n through the super-resolution of the images and segmentation. This review also\n identified key limitations of the potential transformation of GANs based\n methods in clinical applications.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2010.08862",
        "title": "Mad Science is Provably Hard: Puzzles in Hearthstone's Boomsday Lab are\nNP-hard",
        "abstract": "We consider the computational complexity of winning this turn (mate-in-1 or\n \"finding lethal\") in Hearthstone as well as several other single turn puzzle\n types introduced in the Boomsday Lab expansion. We consider three natural\n generalizations of Hearthstone (in which hand size, board size, and deck size\n scale) and prove the various puzzle types in each generalization NP-hard.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2401.05828",
        "title": "Optimized Distillation Profiles for Heavy-Light Spectroscopy",
        "abstract": "It has been demonstrated that distillation profiles can be employed to build\n optimized quarkonium interpolators for spectroscopy calculations in lattice\n QCD. We test their usefulness for heavy-light systems on (3+1)-flavor ensembles\n with mass-degenerate light and a charm quark in the sea in preparation for a\n future $D\\bar{D}$-scattering analysis. The additional cost of light inversions\n naturally leads to the question if knowledge of optimal profiles can be used to\n avoid superfluous computations. We show such optimal profiles for different\n lattice sizes and pion masses and discuss general trends. Furthermore, we\n discuss the handling of momenta in this framework.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1405.2651",
        "title": "Single atom anisotropic magnetoresistance on a topological insulator\nsurface",
        "abstract": "We demonstrate single atom anisotropic magnetoresistance on the surface of a\n topological insulator, arising from the interplay between the helical\n spin-momentum-locked surface electronic structure and the hybridization of the\n magnetic adatom states. Our first-principles quantum transport calculations\n based on density functional theory for Mn on Bi$_2$Se$_3$ elucidate the\n underlying mechanism. We complement our findings with a two dimensional model\n valid for both single adatoms and magnetic clusters, which leads to a proposed\n device setup for experimental realization. Our results provide an explanation\n for the conflicting scattering experiments on magnetic adatoms on topological\n insulator surfaces, and reveal the real space spin texture around the magnetic\n impurity.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1704.08085",
        "title": "Collisions and drag in debris discs with eccentric parent belts",
        "abstract": "Context: High-resolution images of circumstellar debris discs reveal\noff-centred rings that indicate past or ongoing perturbation, possibly caused\nby secular gravitational interaction with unseen stellar or substellar\ncompanions. The purely dynamical aspects of this departure from radial symmetry\nare well understood. However, the observed dust is subject to additional forces\nand effects, most notably collisions and drag. Aims: To complement the studies\nof dynamics, we therefore aim to understand how new asymmetries are created by\nthe addition of collisional evolution and drag forces, and existing ones\nstrengthened or overridden. Methods: We augmented our existing numerical code\n\"Analysis of Collisional Evolution\" (ACE) by an azimuthal dimension, the\nlongitude of periapse. A set of fiducial discs with global eccentricities\nranging from 0 to 0.4 is evolved over giga-year timescales. Size distribution\nand spatial variation of dust are analysed and interpreted. The basic impact of\nbelt eccentricity on spectral energy distributions (SEDs) and images is\ndiscussed.\nResults: We find features imposed on characteristic timescales. First,\nradiation pressure defines size cutoffs that differ between periapse and\napoapse, resulting in an asymmetric halo. The differences in size distribution\nmake the observable asymmetry of the halo depend on wavelength. Second,\ncollisional equilibrium prefers smaller grains on the apastron side of the\nparent belt, reducing the effect of pericentre glow and the overall asymmetry.\nThird, Poynting-Robertson drag fills the region interior to an eccentric belt\nsuch that the apastron side is more tenuous. Interpretation and prediction of\nthe appearance in scattered light is problematic when spatial and size\ndistribution are coupled.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2111.07877",
        "title": "Bidirectional, Analog Current Source Benchmarked with Gray\nMolasses-Assisted Stray Magnetic Field Compensation",
        "abstract": "In ultracold-atom and ion experiments, flexible control of the direction and\n amplitude of a uniform magnetic field is necessary. It is achieved almost\n exclusively by controlling the current flowing through coils surrounding the\n experimental chamber. Here, we present the design and characterization of a\n modular, analog electronic circuit that enables three-dimensional control of a\n magnetic field via the amplitude and direction of a current flowing through\n three perpendicular pairs of coils. Each pair is controlled by one module, and\n we are able to continuously change the current flowing thorough the coils in\n the $\\pm$4 A range using analog waveforms such that smooth crossing through\n zero as the current's direction changes is possible. With the electrical\n current stability at the 10$^{-5}$ level, the designed circuit enables\n state-of-the-art ultracold experiments. As a benchmark, we use the circuit to\n compensate stray magnetic fields that hinder efficient sub-Doppler cooling of\n alkali atoms in gray molasses. We demonstrate how such compensation can be\n achieved without actually measuring the stray fields present, thus speeding up\n the process of optimization of various laser cooling stages.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ph/9905354",
        "title": "Generating and Calculating One-loop Feynman Diagrams with FeynArts,\nFormCalc, and LoopTools",
        "abstract": "A set of programs is presented for automatically generating and calculating\n Feynman diagrams. Diagrams are generated with FeynArts, then algebraically\n simplified using a combination of Mathematica and FORM implemented in the\n package FormCalc, and finally evaluated numerically using the LoopTools\n package. FormCalc works either in dimensional regularization or in constrained\n differential renormalization, the latter of which is equivalent at the one-loop\n level to regularization by dimensional reduction. FormCalc combines the speed\n of FORM with the powerful instruction set of Mathematica, and the latter\n greatly eases further processing of the results (e.g. selecting or modifying\n terms). The output is in a form well suited for numerical evaluation, which is\n then straightforward using the implementations of the one-loop integrals in\n LoopTools.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.07122",
        "title": "Double-GEM based thermal neutron detector prototype",
        "abstract": "The Helium-3 shortage and the growing interest in neutron science constitute\n a driving factor in developing new neutron detection technologies. In this\n work, we report the development of a double-GEM detector prototype that uses a\n $^{10}$B$_4$C layer as a neutron converter material. GEANT4 simulations were\n performed predicting an efficiency of 3.14(10) %, agreeing within 2.7 $\\sigma$\n with the experimental and analytic detection efficiencies obtained by the\n detector when tested in a 41.8 meV thermal neutron beam. The detector is\n position sensitive, equipped with a 256+256 strip readout connected to\n resistive chains, and achieves a spatial resolution better than 3 mm. The gain\n stability over time was also measured with a fluctuation of about 0.2 %h$^{-1}$\n of the signal amplitude. A simple data acquisition with only 5 electronic\n channels is sufficient to operate this detector.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/physics/0401072",
        "title": "Liquid Xe scintillation calorimetry and Xe optical properties",
        "abstract": "The optical properties of LXe in the vacuum ultra violet (VUV), determining\n the performance of a scintillation calorimeter, are discussed in detail. The\n available data, measured in a wider spectral region from visible to UV light,\n and in a large range of Xe densities, from gas to liquid, are examined. It is\n shown that this information can be used for deriving the LXe optical properties\n in the VUV. A comparison is made with the few direct measurements in LXe for\n VUV light resulting from the LXe excitation by ionizing particles. A useful\n relation is obtained which connects the Rayleigh scattering length to the\n refractive index in LXe.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0907.1559",
        "title": "Holographic entanglement entropy of the BTZ black hole",
        "abstract": "We investigate quantum entanglement of gravitational configurations in 3D AdS\n gravity using the AdS/CFT correspondence. We derive explicit formulas for the\n holographic entanglement entropy (EE) of the BTZ black hole, conical\n singularities and regularized AdS$_{3}$. The leading term in the large\n temperature expansion of the holographic EE of the BTZ black hole reproduces\n exactly its Bekenstein-Hawking entropy S_BH, whereas the subleading term\n behaves as ln S_BH. We also show that the leading term of the holographic EE\n for the BTZ black hole can be obtained from the large temperature expansion of\n the partition function of a broad class of 2D CFTs on the torus. This result\n indicates that black hole EE is not a fundamental feature of the underlying\n theory of quantum gravity but emerges when the semiclassical notion of\n spacetime geometry is used to describe the black hole.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ph/9709396",
        "title": "Semileptonic B decays to excited charm mesons",
        "abstract": "Exclusive semileptonic B decays to the lightest excited charmed mesons are\n investigated at order $\\Lambda_{QCD}/m_Q$ in the heavy quark effective theory.\n At zero recoil, $\\Lambda_{QCD}/m_Q$ corrections to the matrix elements of the\n weak currents can be written in terms of the leading Isgur-Wise functions for\n the corresponding transition and meson mass splittings. The differential decay\n rates are predicted, including $\\Lambda_{QCD}/m_Q$ corrections with some model\n dependence away from zero recoil. Applications to B decay sum rules and\n factorization are presented.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1802.01148",
        "title": "Spectral Characterizations of Solvability and Stability for Delay\nDifferential-Algebraic Equations",
        "abstract": "The solvability and stability analysis of linear time invariant systems of\n delay differential-algebraic equations (DDAEs) is analyzed. The behavior\n approach is applied to DDAEs in order to establish characterizations of their\n solvability in terms of spectral conditions. Furthermore, examples are\n delivered to demonstrate that the eigenvalue-based approach to analyze the\n exponential stability of dynamical system is only valid for a special class of\n DDAEs, namely non-advanced. Then, a new concept of weak stability is proposed\n and studied for DDAEs whose matrix coefficients pairwise commute.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1208.4731",
        "title": "Solution of two-center time-dependent Dirac equation in spherical\ncoordinates: Application of the multipole expansion of the electron-nuclei\ninteraction",
        "abstract": "A non-perturbative approach to the solution of the time-dependent, two-center\n Dirac equation is presented with a special emphasis on the proper treatment of\n the potential of the nuclei. In order to account for the full multipole\n expansion of this potential, we express eigenfunctions of the two-center\n Hamiltonian in terms of well-known solutions of the \"monopole\" problem that\n employs solely the spherically-symmetric part of the interaction. When combined\n with the coupled-channel method, such a wavefunction-expansion technique allows\n for an accurate description of the electron dynamics in the field of moving\n ions for a wide range of internuclear distances. To illustrate the\n applicability of the proposed approach, the probabilities of the K- as well as\n L- shell ionization of hydrogen-like ions in the course of nuclear alpha-decay\n and slow ion-ion collisions have been calculated.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1503.05898",
        "title": "Oxygen, {\\alpha}-element and iron abundance distributions in the inner\npart of the Galactic thin disc",
        "abstract": "We derived elemental abundances in 27 Cepheids, the great majority situated\n within a zone of Galactocentric distances ranging from 5 to 7 kpc. One star of\n our sample, SU Sct, has a Galactocentric distance of about 3 kpc, and thus\n falls in a poorly investigated region of the inner thin disc. Our new results,\n combined with data on abundances in the very central part of our Galaxy taken\n from literature, show that iron, magnesium, silicon, sulfur, calcium and\n titanium LTE abundance radial distributions, as well as NLTE distribution of\n oxygen reveal a plateau-like structure or even positive abundance gradient in\n the region extending from the Galactic center to about 5 kpc.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1106.1232",
        "title": "A reduction from parity games to simple stochastic games",
        "abstract": "Games on graphs provide a natural model for reactive non-terminating systems.\n In such games, the interaction of two players on an arena results in an\n infinite path that describes a run of the system. Different settings are used\n to model various open systems in computer science, as for instance turn-based\n or concurrent moves, and deterministic or stochastic transitions. In this\n paper, we are interested in turn-based games, and specifically in deterministic\n parity games and stochastic reachability games (also known as simple stochastic\n games). We present a simple, direct and efficient reduction from deterministic\n parity games to simple stochastic games: it yields an arena whose size is\n linear up to a logarithmic factor in size of the original arena.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2108.12621",
        "title": "Circular strings in Kerr-$AdS_5$ black holes",
        "abstract": "The quest for extension of holographic correspondence to the case of finite\n temperature naturally includes Kerr-AdS black holes and their field theory\n duals. We probe the five-dimensional Kerr-AdS space time by pulsating strings.\n First we find particular pulsating string solutions and then semi-classically\n quantize the theory. For the string with large values of energy, we use the\n Bohr-Sommerfeld analysis to find the energy of the string as a function of a\n large quantum number. We obtain the wave function of the problem and thoroughly\n study the corrections to the energy, which according to the holographic\n dictionary are related to anomalous dimensions of certain operators in the dual\n gauge theory. The interpretation of results from holographic point of view is\n not straightforward since the dual theory is at finite temperature.\n Nevertheless, near or at conformal point the expressions can be thought of as\n the dispersion relations of stationary states.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1010.3870",
        "title": "Colour fields in gauge invariant quenched SU(3) Lattice QCD",
        "abstract": "he colour fields, created by static sources belonging to different SU(3)\n representations, from the 3 to the 27, are computed in quenched SU(3) lattice\n QCD, in a 24^3 x 48 lattice at beta=6.2 and a=0.07261(85) fm. We utilize the\n technique of generalized Wilson Loops to localize the sources, correlated with\n plaquettes to measure the respective colour fields. We investigate the Casimir\n scaling of the fields, measured in the static potentials by Bali. We also study\n the coherence length, comparing with the dual Ginzburg-Landau approach. With\n the penetration and coherence lengths we determined the Ginzburg-Landau\n dimensionless parameter, this result is consistent with a type II\n superconductor picture, and with an effective dual gluon mass of 0.905 +- 0.163\n GeV.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2308.12882",
        "title": "LCANets++: Robust Audio Classification using Multi-layer Neural Networks\nwith Lateral Competition",
        "abstract": "Audio classification aims at recognizing audio signals, including speech\n commands or sound events. However, current audio classifiers are susceptible to\n perturbations and adversarial attacks. In addition, real-world audio\n classification tasks often suffer from limited labeled data. To help bridge\n these gaps, previous work developed neuro-inspired convolutional neural\n networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA)\n in the first layer (i.e., LCANets) for computer vision. LCANets learn in a\n combination of supervised and unsupervised learning, reducing dependency on\n labeled samples. Motivated by the fact that auditory cortex is also sparse, we\n extend LCANets to audio recognition tasks and introduce LCANets++, which are\n CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that\n LCANets++ are more robust than standard CNNs and LCANets against perturbations,\n e.g., background noise, as well as black-box and white-box attacks, e.g.,\n evasion and fast gradient sign (FGSM) attacks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0303153",
        "title": "Riemannian geometry over different normed division algebra",
        "abstract": "We develop a unifed theory to study geometry of manifolds with different\nholonomy groups.\nThey are classified by (1) real, complex, quaternion or octonion number they\nare defined over and (2) being special or not. Specialty is an orientation with\nrespect to the corresponding normed algebra A. For example, special Riemannian\nA-manifolds are oriented Riemannian, Calabi-Yau, Hyperkahler and G_2-manifolds\nrespectively.\nFor vector bundles over such manifolds, we introduce (special) A-connections.\nThey include holomorphic, Hermitian Yang-Mills, Anti-Self-Dual and\nDonaldson-Thomas connections. Similarly we introduce (special) A/2-Lagrangian\nsubmanifolds as maximally real submanifolds. They include (special) Lagrangian,\ncomplex Lagrangian, Cayley and (co-)associative submanifolds.\nWe also discuss geometric dualities from this viewpoint: Fourier\ntransformations on A-geometry for flat tori and a conjectural SYZ mirror\ntransformation from (special) A-geometry to (special) A/2-Lagrangian geometry\non mirror special A-manifolds.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2108.08738",
        "title": "An experimental setup to generate narrowband bi-photons via four-wave\nmixing in cold atoms",
        "abstract": "We present our recently-built experimental setup designed to generate\n near-infrared and narrow-band correlated photon pairs by inducing four-wave\n mixing in a cold gas of $^{87}$Rb atoms confined in a magneto-optical trap. The\n experimental setup and its automation and control approach are described in\n detail. A characterization of the optical density of the atomic ensemble as\n well as the basic statistical measurements of the generated light are reported.\n The non-classical nature of the photons pairs is confirmed by observing a\n violation of Cauchy-Schwarz inequality by a factor of 5.6 $\\times 10^5$ in a\n Hanbury Brown - Twiss interferometer. A $1/e$ coherence time for the heralded,\n idler photons of $4.4 \\pm 0.1$ ns is estimated from our observations. We are\n able to achieve a value of $10^{4}$ s$^{-1}$ pair-detection-rate, which results\n in a spectral brightness of 280 (MHz s)$^{-1}$. The combination of high\n brightness and narrow-band spectrum makes this photon-pair source a viable tool\n in fundamental studies of quantum states and opens the door to use them in\n quantum technologies.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2401.09689",
        "title": "Probing quantum geometry through optical conductivity and magnetic\ncircular dichroism",
        "abstract": "Probing ground-state quantum geometry and topology through optical response\n is not only of fundamental interest, but it can also offer several practical\n advantages. Here, using first-principles calculations on antiferromagnetic\n topological insulator MnBi$_2$Te$_4$ thin films, we demonstrate how the\n generalized optical weight arising from the absorptive part of the optical\n conductivity can be used to probe the ground state quantum geometry and\n topology. We show that three septuple layers MnBi$_2$Te$_4$ exhibit an enhanced\n almost perfect magnetic circular dichroism for a narrow photon energy window in\n the infrared region. We calculate the quantum weight in a few septuple layers\n MnBi$_2$Te$_4$ and show that it far exceeds the lower bound provided by the\n Chern number. Our results suggest that the well-known optical methods are\n powerful tools for probing the ground state quantum geometry and topology.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1806.02030",
        "title": "Improving Performance Models for Irregular Point-to-Point Communication",
        "abstract": "Parallel applications are often unable to take full advantage of emerging\n parallel architectures due to scaling limitations, which arise due to\n inter-process communication. Performance models are used to analyze the sources\n of communication costs. However, traditional models for point-to-point\n communication fail to capture the full cost of many irregular operations, such\n as sparse matrix methods. In this paper, a node-aware based model is presented.\n Furthermore, the model is extended to include communication queue search time\n as well as an additional parameter estimating network contention. The resulting\n model is applied to a variety of irregular communication patterns throughout\n matrix operations, displaying improved accuracy over traditional models.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2305.10565",
        "title": "Measurement Based Evaluation and Mitigation of Flood Attacks on a LAN\nTest-Bed",
        "abstract": "The IoT is vulnerable to network attacks, and Intrusion Detection Systems\n (IDS) can provide high attack detection accuracy and are easily installed in\n IoT Servers. However, IDS are seldom evaluated in operational conditions which\n are seriously impaired by attack overload. Thus a Local Area Network testbed is\n used to evaluate the impact of UDP Flood Attacks on an IoT Server, whose first\n line of defence is an accurate IDS. We show that attacks overload the\n multi-core Server and paralyze its IDS. Thus a mitigation scheme that detects\n attacks rapidly, and drops packets within milli-seconds after the attack\n begins, is proposed and experimentally evaluated.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1602.04339",
        "title": "Mathematical Theory Exploration in Theorema: Reduction Rings",
        "abstract": "In this paper we present the first-ever computer formalization of the theory\nof Gr\\\"obner bases in reduction rings, which is an important theory in\ncomputational commutative algebra, in Theorema. Not only the formalization, but\nalso the formal verification of all results has already been fully completed by\nnow; this, in particular, includes the generic implementation and correctness\nproof of Buchberger's algorithm in reduction rings. Thanks to the seamless\nintegration of proving and computing in Theorema, this implementation can now\nbe used to compute Gr\\\"obner bases in various different domains directly within\nthe system. Moreover, a substantial part of our formalization is made up solely\nby \"elementary theories\" such as sets, numbers and tuples that are themselves\nindependent of reduction rings and may therefore be used as the foundations of\nfuture theory explorations in Theorema.\nIn addition, we also report on two general-purpose Theorema tools we\ndeveloped for an efficient and convenient exploration of mathematical theories:\nan interactive proving strategy and a \"theory analyzer\" that already proved\nextremely useful when creating large structured knowledge bases.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2311.16385",
        "title": "A formula for the overlap between Generalized Coherent States of any\nrank one simple Lie algebra",
        "abstract": "We provide a formula for computing the overlap between two Generalized\n Coherent States of any rank one simple Lie algebra. Then, we apply our formula\n to spin coherent states (i.e. $\\mathfrak{su}(2)$ algebra), pseudo-spin coherent\n states (i.e. $\\mathfrak{su}(1,1)$ algebra), and the\n $\\mathfrak{sl}(2,\\mathbb{R})$ subalgebras of Virasoro. In all these examples,\n we show the emergence of a semi-classical behaviour from the set of coherent\n states and verify that it always happens when some parameter, depending on the\n algebra and its representation, becomes large.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1812.11752",
        "title": "Cusps, Congruence Groups and Monstrous Dessins",
        "abstract": "We study general properties of the dessins d'enfants associated with the\n Hecke congruence subgroups $\\Gamma_0(N)$ of the modular group\n $\\mathrm{PSL}_2(\\mathbb{R})$. The definition of the $\\Gamma_0(N)$ as the\n stabilisers of couples of projective lattices in a two-dimensional vector space\n gives an interpretation of the quotient set\n $\\Gamma_0(N)\\backslash\\mathrm{PSL}_2(\\mathbb{R})$ as the projective lattices\n $N$-hyperdistant from a reference one, and hence as the projective line over\n the ring $\\mathbb{Z}/N\\mathbb{Z}$. The natural action of\n $\\mathrm{PSL}_2(\\mathbb{R})$ on the lattices defines a dessin d'enfant\n structure, allowing for a combinatorial approach to features of the classical\n modular curves, such as the torsion points and the cusps. We tabulate the\n dessins d'enfants associated with the $15$ Hecke congruence subgroups of genus\n zero, which arise in Moonshine for the Monster sporadic group.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2403.19230",
        "title": "Optomechanical cavities based on epitaxial GaP on nominally\n(001)-oriented Si",
        "abstract": "Gallium phosphide (GaP) has recently received considerable attention as a\n suitable material for building photonic integrated circuits due to its\n remarkable optical and piezoelectric properties. Usually, GaP is grown\n epitaxially on III-V substrates to keep its crystallinity and later transferred\n to silicon wafers for further processing. Here, an alternative promising route\n for the fabrication of optomechanical (OM) cavities on GaP epitaxially grown on\n nominally (001)-oriented Si is introduced by using a two-step process\n consisting of a low-temperature etching of GaP followed by selective etching of\n the underneath silicon. The low-temperature (-30 $^o$C) during the dry-etching\n of GaP hinders the lateral etching rate, preserving the pattern with a\n deviation between the design and the pattern in the GaP layer lower than 5 %,\n avoiding the complex process of transferring and bonding a GaP wafer to a\n silicon-on-insulator wafer. To demonstrate the quality and feasibility of the\n proposed fabrication route, suspended OM cavities are fabricated and\n experimentally characterized. The cavities show optical quality factors between\n 10$^3$ and 10$^4$, and localized mechanical resonances at frequencies around\n 3.1 GHz. Both optical and mechanical resonances are close to those previously\n reported on crystalline GaP structures. These results suggest a simple and\n low-cost way to build GaP-based photonic devices directly integrated on\n industry-standard Si(001) photonic wafers.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-lat/9607064",
        "title": "Polyakov Loops and Magnetic Screening from Monopoles in SU(2) Lattice\nGauge Theory",
        "abstract": "We present results from magnetic monopoles in $SU(2)$ lattice gauge theory at\n finite temperature. The lattices are $16^{3}\\times N_{t}$, for\n $N_{t}=4,6,8,12$, at $\\beta=2.5115$. Quantities discussed are: the spacial\n string tension, Polyakov loops, and the screening of timelike and spacelike\n magnetic currents.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0709.2711",
        "title": "Population synthesis at short wavelengths and spectrophotometric\ndiagnostic tools for galaxy evolution",
        "abstract": "Taking advantage of recent important advances in the calculation of\n high-resolution spectral grids of stellar atmospheres at short wavelengths, and\n their implementation for population synthesis models, we briefly review here\n some special properties of ultraviolet emission in SSPs, and discuss their\n potential applications for identifying and tuning up effective diagnostic tools\n to probe distinctive evolutionary properties of early-type galaxies and other\n evolved stellar systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0008277",
        "title": "Quantum Films Adsorbed on Graphite: Third and Fourth Helium Layers",
        "abstract": "Using a path-integral Monte Carlo method for simulating superfluid quantum\n films, we investigate helium layers adsorbed on a substrate consisting of\n graphite plus two solid helium layers. Our results for the promotion densities\n and the dependence of the superfluid density on coverage are in agreement with\n experiment. We can also explain certain features of the measured heat capacity\n as a function of temperature and coverage.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1507.07534",
        "title": "Unified model for cosmic rays above $10^{17}$ eV and the diffuse\ngamma-ray and neutrino backgrounds",
        "abstract": "We investigate how the extragalactic proton component derived within the\n \"escape model\" can be explained by astrophysical sources. We consider as\n possible cosmic ray (CR) sources normal/starburst galaxies and radio-loud\n active galactic nuclei (AGN). We find that the contribution to the total\n extragalactic proton flux from normal and starburst galaxies is only\n subdominant and does not fit the spectral shape deduced in the escape model. In\n the case of radio-loud AGN, we show that the complete extragalactic proton\n spectrum can be explained by a single source population, BL Lac/FR I, for any\n of the potential acceleration sites in these sources. We calculate the diffuse\n neutrino and $\\gamma$-ray fluxes produced by these CR protons interacting with\n gas inside their sources. For a spectral slope of CRs close to $\\alpha=2.1-2.2$\n as suggested by shock acceleration, we find that these UHECR sources contribute\n the dominant fraction of both the isotropic $\\gamma$-ray background and of the\n extragalactic part of the astrophysical neutrino signal observed by IceCube.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0402354",
        "title": "Quantum dynamics of two capacitively coupled superconducting islands via\nJosephson junctions",
        "abstract": "In this paper, we consider a system consisting of two capacitively coupled\n superconducting islands via Josephson junctions. We show that it can be reduced\n to two coupling harmonic oscillators under certain conditions, and solved\n exactly in terms of a displacing transformation, a beam-splitter-like\n transformation, and a squeezing transformation. It is found that the system\n evolves by a rotated-squeezed-coherent state when the system is initially in a\n coherent state. Quantum dynamics of the Cooper pairs in the two superconducting\n islands is investigated. It is shown that the number of the Cooper pairs in the\n two islands evolves periodically.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2305.11056",
        "title": "PETAL: Physics Emulation Through Averaged Linearizations for Solving\nInverse Problems",
        "abstract": "Inverse problems describe the task of recovering an underlying signal of\n interest given observables. Typically, the observables are related via some\n non-linear forward model applied to the underlying unknown signal. Inverting\n the non-linear forward model can be computationally expensive, as it often\n involves computing and inverting a linearization at a series of estimates.\n Rather than inverting the physics-based model, we instead train a surrogate\n forward model (emulator) and leverage modern auto-grad libraries to solve for\n the input within a classical optimization framework. Current methods to train\n emulators are done in a black box supervised machine learning fashion and fail\n to take advantage of any existing knowledge of the forward model. In this\n article, we propose a simple learned weighted average model that embeds\n linearizations of the forward model around various reference points into the\n model itself, explicitly incorporating known physics. Grounding the learned\n model with physics based linearizations improves the forward modeling accuracy\n and provides richer physics based gradient information during the inversion\n process leading to more accurate signal recovery. We demonstrate the efficacy\n on an ocean acoustic tomography (OAT) example that aims to recover ocean sound\n speed profile (SSP) variations from acoustic observations (e.g. eigenray\n arrival times) within simulation of ocean dynamics in the Gulf of Mexico.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1801.06816",
        "title": "Preferential Attachment Graphs with Planted Communities",
        "abstract": "A variation of the preferential attachment random graph model of Barab\\'asi\n and Albert is defined that incorporates planted communities. The graph is built\n progressively, with new vertices attaching to the existing ones one-by-one. At\n every step, the incoming vertex is randomly assigned a label, which represents\n a community it belongs to. This vertex then chooses certain vertices as its\n neighbors, with the choice of each vertex being proportional to the degree of\n the vertex multiplied by an affinity depending on the labels of the new vertex\n and a potential neighbor. It is shown that the fraction of half-edges attached\n to vertices with a given label converges almost surely for some classes of\n affinity matrices. In addition, the empirical degree distribution for the set\n of vertices with a given label converges to a heavy tailed distribution, such\n that the tail decay parameter can be different for different communities. Our\n proof method may be of independent interest, both for the classical Barab\\'asi\n -Albert model and for other possible extensions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1605.09485",
        "title": "Band Structure and Topological Properties of Graphene in a Superlattice\nSpin Exchange Field",
        "abstract": "We analyze the energy spectrum of graphene in the presence of spin-orbit\n coupling and a unidirectionally periodic Zeeman field, focusing on the\n stability and location of Dirac points it may support. It is found that the\n Dirac points at the $K$ and $K'$ points are generically moved to other\n locations in the Brillouin zone, but that they remain present when the Zeeman\n field $\\vec{\\Delta}(x)$ integrates to zero within a unit cell. A large variety\n of locations for the Dirac points is shown to be possible: when $\\vec\\Delta\n \\parallel \\hat{z}$ they are shifted from their original locations along the\n direction perpendicular to the superlattice axis, while realizations of\n $\\vec\\Delta(x)$ that rotate periodically move the Dirac points to locations\n that can reflect the orbit of the rotating electron spin as it moves through a\n unit cell. When a uniform Zeeman field is applied in addition to a periodic\n $\\vec\\Delta \\parallel \\hat{z}$ integrating to zero, the system can be brought\n into a metallic, Dirac semimetal, or insulating state, depending on the\n direction of the uniform field. The latter is shown to be an anomalous quantum\n Hall insulator.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2304.13771",
        "title": "Some Problems Concerning Quantum Channels and Entropies",
        "abstract": "Fundamental limits on communication rates over quantum channels are given by\n mathematical expressions involving entropic formulas. Often, it is unclear if\n these expressions are computable. This thesis describes contributions to the\n study of optimizing and approximating entropic formulas over relevant subsets\n of quantum states. It also describes progress on a quantum erasure simulation\n problem in the high noise regime.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1302.4957",
        "title": "Learning Bayesian Networks: A Unification for Discrete and Gaussian\nDomains",
        "abstract": "We examine Bayesian methods for learning Bayesian networks from a combination\n of prior knowledge and statistical data. In particular, we unify the approaches\n we presented at last year's conference for discrete and Gaussian domains. We\n derive a general Bayesian scoring metric, appropriate for both domains. We then\n use this metric in combination with well-known statistical facts about the\n Dirichlet and normal--Wishart distributions to derive our metrics for discrete\n and Gaussian domains.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1711.05698",
        "title": "Efficient Verification of Multi-Property Designs (The Benefit of Wrong\nAssumptions) (Extended Version)",
        "abstract": "We consider the problem of efficiently checking a set of safety properties\n P1,....,Pk of one design. We introduce a new approach called JA-verification\n where JA stands for \"Just-Assume\" (as opposed to \"assume-guarantee\"). In this\n approach, when proving property Pi, one assumes that every property Pj for j!=i\n holds. The process of proving properties either results in showing that\n P1,....,Pk hold without any assumptions or finding a \"debugging set\" of\n properties. The latter identifies a subset of failed properties that cause\n failure of other properties. The design behaviors that cause the properties in\n the debugging set to fail must be fixed first. Importantly, in our approach,\n there is no need to prove the assumptions used. We describe the theory behind\n our approach and report experimental results that demonstrate substantial gains\n in performance, especially in the cases where a small debugging set exists.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0810.4411",
        "title": "Compression as a tool to detect Bose glass in cold atoms experiments",
        "abstract": "We suggest that measuring the variation of the radius of an atomic cloud when\n the harmonic tap confinement is varied make it possible to monitor the\n disappearance of the insulating Mott phase of an ultracold atomic gas trapped\n in a disordered optical lattice. This paves the way for an unambiguous\n identification of a Bose glass phase in the system.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1604.00524",
        "title": "Colocalising subcategories of modules over finite group schemes",
        "abstract": "The Hom closed colocalising subcategories of the stable module category of a\n finite group scheme are classified. This complements the classification of the\n tensor closed localising subcategories in our previous work. Both\n classifications involve pi-points in the sense of Friedlander and Pevtsova. We\n identify for each pi-point an endofinite module which both generates the\n corresponding minimal localising subcategory and cogenerates the corresponding\n minimal colocalising subcategory.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2308.03564",
        "title": "New series of multi-parametric solutions to GYBE: quantum gates and\nintegrability",
        "abstract": "We obtain two series of spectral parameter dependent solutions to the\n generalized Yang-Baxter equations (GYBE), for definite types of $N_1^2\\times\n N_2^2$ matrices with general dimensions $N_1$ and $N_2$. Appropriate extensions\n are presented for the inhomogeneous GYBEs. The first series of the solutions\n includes as particular cases the $X$-shaped trigonometric braiding matrices.\n For construction of the second series the colored and graded permutation\n operators are defined, and multi-spectral parameter Yang-Baxterization is\n performed. For some examples the corresponding integrable models are discussed.\n The unitary solutions existing in these two series can be considered as\n generalizations of the multipartite Bell matrices in the quantum information\n theory.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1712.02828",
        "title": "On the second largest component of random hyperbolic graphs",
        "abstract": "We show that in the random hyperbolic graph model as formalized by Gugelmann\n et al. in the most interesting range of $\\frac12 < \\alpha < 1$ the size of the\n second largest component is $\\Theta((\\log n)^{1/(1-\\alpha)})$, thus answering a\n question of Bode et al. We also show that for $\\alpha=\\frac12$ with constant\n probability the corresponding size is $\\Theta(\\log n)$, whereas for $\\alpha=1$\n it is $\\Omega(n^{b})$ for some $b > 0$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1111.0736",
        "title": "Branching instability in the flux creep regime of type-II\nsuperconductors",
        "abstract": "We study theoretically the space-time evolution of the thermal and\n electromagnetic perturbation in a superconductor with a nonlinear\n current-voltage characteristics in the flux creep regime. On the basis of a\n linear analysis of a set of differential equations describing small\n perturbations of temperature and electromagnetic field, it is found that under\n some conditions a branching instability may occur in a superconductor sample.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2405.11555",
        "title": "Strong magnetic field inside degenerate relativistic plasma and the\nimpacts on the neutrino transport in Core-Collapse Supernovae",
        "abstract": "We study the impacts of magnetic field on the neutrino transport inside\n core-collapse supernovae (CCSNe). Magnetic field quantizes the momentum of\n electrons and positrons, resulting in the modification of weak-interaction\n cross sections and the chemical potentials of electrons and positrons. We\n include these changes in the leakage scheme of neutrino transport and perform\n 1D CCSN simulations with GR1D, assuming the postbounce magnetic field strength\n of $10^{16-17}$ G. The results show that the neutrino opacities are enhanced\n due to the amplified interaction rates, resulting in a larger neutrinosphere.\n This further reduces the peak value of neutrino luminosities and their decay\n rates since neutrinos stay longer inside the neutrinosphere. Meanwhile, the\n neutrino mean energies are smaller shortly after bounce and reach their peak\n values at later times. As these neutrino properties are crucial in subsequent\n nucleosynthesis processes, including the $\\nu$p-process, $\\nu$-process, and\n $r$-process, our findings suggest that the magnetic field may leave discernible\n marks on the abundance pattern of nucleosynthesis in CCSN.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0806.1961",
        "title": "Broadband frequency mode entanglement in waveguided PDC",
        "abstract": "We report the observation of beatings of the coincidence event rate in a\n Hong-Ou-Mandel interference (HOMI) between signal and idler photons from a\n parametric downconversion (PDC) process inside a multi-mode KTP waveguide. As\n explanation we introduce bi-photonic states entangled in their broadband\n frequency modes generated by waveguide mode triples and propose a suitable\n entanglement detection scheme.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2307.07952",
        "title": "Theoretical proposal for the experimental realization of realignment\noperation",
        "abstract": "Realignment operation has a significant role in detecting bound as well as\n free entanglement. Just like partial transposition, it is also based on\n permutations of the matrix elements. However, the physical implementation of\n realignment operation is not known yet. In this letter, we address the problem\n of experimental realization of realignment operation and to achieve this aim,\n we propose a theoretical proposal for the same. We first show that after\n applying the realignment operation on a bipartite state, the resulting matrix\n can be expressed in terms of the partial transposition operation along with\n column interchange operations. We observed that these column interchange\n operations forms a permutation matrix which can be implemented via SWAP\n operator acting on the density matrix. This mathematical framework is used to\n exactly determine the first moment of the realignment matrix experimentally.\n This has been done by showing that the first moment of the realignment matrix\n can be expressed as the expectation value of a SWAP operator which indicates\n the possibility of its measurement. Further, we have provided an estimation of\n the higher order realigned moments in terms of the first realigned moment and\n thus pave a way to estimate the higher order moments experimentally. Next, we\n develop moments based entanglement detection criteria that detect positive\n partial transpose entangled states (PPTES) as well as negative partial\n transpose entangled states (NPTES). Moreover, we define a new matrix\n realignment operation for three-qubit states and have devised an entanglement\n criteria that is able to detect three-qubit fully entangled states. We have\n developed various methods and techniques in the detection of bipartite and\n tripartite entangled states that may be realized in the current technology.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1311.2799",
        "title": "Aggregation of Affine Estimators",
        "abstract": "We consider the problem of aggregating a general collection of affine\n estimators for fixed design regression. Relevant examples include some commonly\n used statistical estimators such as least squares, ridge and robust least\n squares estimators. Dalalyan and Salmon (2012) have established that, for this\n problem, exponentially weighted (EW) model selection aggregation leads to sharp\n oracle inequalities in expectation, but similar bounds in deviation were not\n previously known. While results indicate that the same aggregation scheme may\n not satisfy sharp oracle inequalities with high probability, we prove that a\n weaker notion of oracle inequality for EW that holds with high probability.\n Moreover, using a generalization of the newly introduced $Q$-aggregation scheme\n we also prove sharp oracle inequalities that hold with high probability.\n Finally, we apply our results to universal aggregation and show that our\n proposed estimator leads simultaneously to all the best known bounds for\n aggregation, including $\\ell_q$-aggregation, $q \\in (0,1)$, with high\n probability.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1711.02881",
        "title": "Topological and statistical properties of nonlinear force-free fields",
        "abstract": "We use our semi-analytic solution of the nonlinear force-free field equation\n to construct three-dimensional magnetic fields that are applicable to the solar\n corona and study their statistical properties for estimating the degree of\n braiding exhibited by these fields. We present a new formula for calculating\n the winding number and compare it with the formula for the crossing number. The\n comparison is shown for a toy model of two helices and for realistic cases of\n nonlinear force-free fields; conceptually the formulae are nearly the same but\n the resulting distributions calculated for a given topology can be different.\n We also calculate linkages, which are useful topological quantities that are\n independent measures of the contribution of magnetic braiding to the total free\n energy and relative helicity of the field. Finally, we derive new analytical\n bounds for the free energy and relative helicity for the field configurations\n in terms of the linking number. These bounds will be of utility in estimating\n the braided energy available for nano-flares or for eruptions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2006.10754",
        "title": "Islands for Reflected Entropy",
        "abstract": "Recent work has demonstrated the need to include contributions from\n entanglement islands when computing the entanglement entropy in QFT states\n coupled to regions of semiclassical gravity. We propose a new formula for the\n reflected entropy that includes additional contributions from such islands. We\n derive this formula from the gravitational path integral by finding additional\n saddles that include generalized replica wormholes. We also demonstrate that\n our covariant formula satisfies all the inequalities required of the reflected\n entropy. We use this formula in various examples that demonstrate its relevance\n in illustrating the structure of multipartite entanglement that are invisible\n to the entropies.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ph/9907378",
        "title": "QCD-based description of one-particle inclusive B decays",
        "abstract": "We discuss one-particle inclusive B decays in the limit of heavy b and c\n quarks. Using the large-N_C limit we factorize the non-leptonic matrix\n elements, and we employ a short distance expansion. Modeling the remaining\n nonperturbative matrix elements we obtain predictions for various decay\n channels and compare them with existing data.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2009.07940",
        "title": "Scattering Amplitudes and Soft Theorems in Multi-Flavor Galileon\nTheories",
        "abstract": "In this paper, we initiate the study of multi-flavor Galileon theories using\n the methods of scattering amplitudes. We explore this topic from different\n perspectives and extend the techniques employed so far mainly in the\n single-flavor case. This includes soft theorems, generalized soft theorems with\n non-trivial right-hand side, Galileon dualities, soft bootstrap and bonus\n relations. We demonstrate new properties on two examples, the multi-flavor U(N)\n Galileon and the three-flavor U(2)/U(1) Galileon.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1706.08527",
        "title": "Combinatorics and Topology of Kawai-Lewellen-Tye Relations",
        "abstract": "We revisit the relations between open and closed string scattering amplitudes\n discovered by Kawai, Lewellen, and Tye (KLT). We show that they emerge from the\n underlying algebro-topological identities known as the twisted period\n relations. In order to do so, we formulate tree-level string theory amplitudes\n in the language of twisted de Rham theory. There, open string amplitudes are\n understood as pairings between twisted cycles and cocycles. Similarly, closed\n string amplitudes are given as a pairing between two twisted cocycles. Finally,\n objects relating the two types of string amplitudes are the $\\alpha'$-corrected\n bi-adjoint scalar amplitudes recently defined by the author [arXiv:1610.04230].\n We show that they naturally arise as intersection numbers of twisted cycles. In\n this work we focus on the combinatorial and topological description of twisted\n cycles relevant for string theory amplitudes. In this setting, each twisted\n cycle is a polytope, known in combinatorics as the associahedron, together with\n an additional structure encoding monodromy properties of string integrals. In\n fact, this additional structure is given by higher-dimensional generalizations\n of the Pochhammer contour. An open string amplitude is then computed as an\n integral of a logarithmic form over an associahedron. We show that the inverse\n of the KLT kernel can be calculated from the knowledge of how pairs of\n associahedra intersect one another in the moduli space. In the field theory\n limit, contributions from these intersections localize to vertices of the\n associahedra, giving rise to the bi-adjoint scalar partial amplitudes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1412.1752",
        "title": "Cross-correlation Aided Transport in Stochastically Driven Accretion\nFlows",
        "abstract": "Origin of linear instability resulting in rotating sheared accretion flows\n has remained a controversial subject for long. While some explanations of such\n non-normal transient growth of disturbances in the Rayleigh stable limit were\n available for magnetized accretion flows, similar instabilities in absence of\n magnetic perturbations remained unexplained. This dichotomy was resolved in two\n recent publications by Chattopadhyay, {\\it et al} where it was shown that such\n instabilities, especially for non-magnetized accretion flows, were introduced\n through interaction of the inherent stochastic noise in the system (even a\n \\enquote{cold} accretion flow at 3000K is too \\enquote{hot} in the statistical\n parlance and is capable of inducing strong thermal modes) with the underlying\n Taylor-Couette flow profiles. Both studies, however, excluded the additional\n energy influx (or efflux) that could result from nonzero cross-correlation of a\n noise perturbing the velocity flow, say, with the noise that is driving the\n vorticity flow (or equivalently the magnetic field and magnetic vorticity flow\n dynamics). In this article we show that nonzero noise cross-correlations\n essentially renormalize the strength of temporal correlations. Apart from an\n overall boost in the energy rate (both for spatial and temporal correlations,\n and hence in the ensemble averaged energy spectra), this results in mutual\n competition in growth rates of affected variables often resulting in\n suppression of oscillating Alfven waves at small times while leading to faster\n saturations at relatively longer time scales. The effects are seen to be more\n pronounced with magnetic field fluxes where the noise cross-correlation\n magnifies the strength of the field concerned.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1710.00947",
        "title": "VIDOSAT: High-dimensional Sparsifying Transform Learning for Online\nVideo Denoising",
        "abstract": "Techniques exploiting the sparsity of images in a transform domain have been\n effective for various applications in image and video processing. Transform\n learning methods involve cheap computations and have been demonstrated to\n perform well in applications such as image denoising and medical image\n reconstruction. Recently, we proposed methods for online learning of\n sparsifying transforms from streaming signals, which enjoy good convergence\n guarantees, and involve lower computational costs than online synthesis\n dictionary learning. In this work, we apply online transform learning to video\n denoising. We present a novel framework for online video denoising based on\n high-dimensional sparsifying transform learning for spatio-temporal patches.\n The patches are constructed either from corresponding 2D patches in successive\n frames or using an online block matching technique. The proposed online video\n denoising requires little memory, and offers efficient processing. Numerical\n experiments compare the performance to the proposed video denoising scheme but\n fixing the transform to be 3D DCT, as well as prior schemes such as dictionary\n learning-based schemes, and the state-of-the-art VBM3D and VBM4D on several\n video data sets, demonstrating the promising performance of the proposed\n methods.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.15613",
        "title": "Quasibound states of scalar fields in the consistent 4D\nEinstein-Gauss-Bonnet-(Anti-)de Sitter gravity",
        "abstract": "We examine the interaction between massless scalar fields and the\n gravitational field generated by a black hole solution that was recently\n obtained in the consistent well-defined 4-dimensional Einstein-Gauss-Bonnet\n gravity with a cosmological constant. In order to do this, we calculate\n quasibound state frequencies of scalar fields for the spherically symmetric\n black hole in the consistent 4-dimensional Einstein-Gauss-Bonnet-de Sitter and\n Anti-de Sitter theories. The expression for the quasibound states is obtained\n by using the polynomial condition associated to the Heun functions, and their\n values are overdamped. We also demonstrate the stability of the systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0403738",
        "title": "Nanothermodynamics:a generic approach to material properties at\nnanoscale",
        "abstract": "Granular and nanoscale materials containing a relatively small number of\n constituents have been studied to discover how their properties differ from\n their macroscopic counterparts. These studies are designed to test how far the\n known macroscopic approaches such as thermodynamics may be applicable in these\n cases. A brief review of the recent literature on these topics is given as a\n motivation to introduce a generic approach called nanothermodynamics. An\n important feature that must be incorporated into the theory is the non-additive\n property because of the importance of surface contributions to the physics of\n these systems. This is achieved by incorporating fluctuations into the theory\n right from the start. There are currently two approaches to incorporate this\n property. Hill (and further elaborated more recently with Chamberlin) initiated\n an approach by modifying the thermodynamic relations by taking into account the\n surface effects. We generalize Boltzmann-Gibbs statistical mechanics by\n relaxing the additivity properties of thermodynamic quantities to include\n nonextensive features of such systems. An outline of this generalization of the\n macroscopic thermodynamics to nano-systems will be given here.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2207.06417",
        "title": "Regular charged black hole in massive gravity as heat engine",
        "abstract": "We examine the effect of addition of graviton mass on the efficiency of a\n regular charged black hole (RCB). In doing so, we make few comments on the\n critical behaviour of the system and calculate the relevant thermodynamic\n quantities such as entropy, Hawkings temperature and heat capacity. We confirm\n that graviton mass has a positive effect on the efficacy of such a heat engine\n under consideration.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1706.08977",
        "title": "A Millimeter Continuum Size-Luminosity Relationship for Protoplanetary\nDisks",
        "abstract": "We present a sub-arcsecond resolution survey of the 340 GHz dust continuum\n emission from 50 nearby protoplanetary disks, based on new and archival\n observations with the Submillimeter Array. The observed visibility data were\n modeled with a simple prescription for the radial surface brightness profile.\n The results were used to extract intuitive, empirical estimates of the emission\n \"size\" for each disk, $R_{\\rm eff}$, defined as the radius that encircles a\n fixed fraction of the total continuum luminosity, $L_{\\rm mm}$. We find a\n significant correlation between the sizes and luminosities, such that $R_{\\rm\n eff} \\propto L_{\\rm mm}^{0.5}$, providing a confirmation and quantitative\n characterization of a putative trend that was noted previously. This\n correlation suggests that these disks have roughly the same average surface\n brightness interior to their given effective radius, ~0.2 Jy arcsec$^{-2}$ (or\n 8 K in brightness temperature). The same trend remains, but the 0.2dex of\n dispersion perpendicular to this relation essentially disappears, when we\n account for the irradiation environment of each disk with a crude approximation\n of the dust temperatures based on the stellar host luminosities. We consider\n two (not mutually exclusive) explanations for the origin of this\n size-luminosity relationship. Simple models of the growth and migration of disk\n solids can account for the observed trend for a reasonable range of initial\n conditions, but only on timescales that are much shorter than the nominal ages\n present in the sample. An alternative scenario invokes optically thick emission\n concentrated on unresolved scales, with filling factors of a few tens of\n percent, that are perhaps manifestations of localized particle traps.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2401.11140",
        "title": "Stability Plasticity Decoupled Fine-tuning For Few-shot end-to-end\nObject Detection",
        "abstract": "Few-shot object detection(FSOD) aims to design methods to adapt object\n detectors efficiently with only few annotated samples. Fine-tuning has been\n shown to be an effective and practical approach. However, previous works often\n take the classical base-novel two stage fine-tuning procedure but ignore the\n implicit stability-plasticity contradiction among different modules.\n Specifically, the random re-initialized classifiers need more plasticity to\n adapt to novel samples. The other modules inheriting pre-trained weights demand\n more stability to reserve their class-agnostic knowledge. Regular fine-tuning\n which couples the optimization of these two parts hurts the model\n generalization in FSOD scenarios. In this paper, we find that this problem is\n prominent in the end-to-end object detector Sparse R-CNN for its\n multi-classifier cascaded architecture. We propose to mitigate this\n contradiction by a new three-stage fine-tuning procedure by introducing an\n addtional plasticity classifier fine-tuning(PCF) stage. We further design the\n multi-source ensemble(ME) technique to enhance the generalization of the model\n in the final fine-tuning stage. Extensive experiments verify that our method is\n effective in regularizing Sparse R-CNN, outperforming previous methods in the\n FSOD benchmark.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0705.2373",
        "title": "Bekenstein Bound and Spectral Geometry",
        "abstract": "In this letter it is proposed to study the Bekenstein's $\\xi(4)$ calculation\n of the $S/E$ bound for more general geometries. It is argued that, using some\n relations among eigenvalues obtained in the context of Spectral Geometry, it is\n possible to estimate $\\xi(4)$ without an exact analytical knowledge of the\n spectrum. Finally it is claimed that isospectrality can define a class of\n domains with the same ratio $S/E$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1308.1305",
        "title": "Carbon coating of the SPS dipole chambers",
        "abstract": "The Electron Multipacting (EM) phenomenon is a limiting factor for the\n achievement of high luminosity in accelerators for positively charged particles\n and for the performance of RF devices. At CERN, the Super Proton Synchrotron\n (SPS) must be upgraded in order to feed the Large Hadron Collider (LHC) with 25\n ns bunch spaced beams. At such small bunch spacing, EM may limit the\n performance of the SPS and consequently that of the LHC. To mitigate this\n phenomenon CERN is developing a carbon thin film coating with low Secondary\n Electron Yield (SEY) to coat the internal walls of the SPS dipoles beam pipes.\n This paper presents the progresses in the coating technology, the performance\n of the carbon coatings and the strategy for a large scale production.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1005.2009",
        "title": "HAT-P-16b: A 4 Mj Planet Transiting A Bright Star On An Eccentric Orbit",
        "abstract": "We report the discovery of HAT-P-16b, a transiting extrasolar planet orbiting\n the V = 10.8 mag F8 dwarf GSC 2792-01700, with a period P = 2.775960 +-\n 0.000003 d, transit epoch Tc = 2455027.59293 +- 0.00031 (BJD), and transit\n duration 0.1276 +- 0.0013 d. The host star has a mass of 1.22 +- 0.04 Msun,\n radius of 1.24 +- 0.05 Rsun, effective temperature 6158 +-80 K, and metallicity\n [Fe/H] = +0.17 +- 0.08. The planetary companion has a mass of 4.193 +- 0.094\n MJ, and radius of 1.289 +- 0.066 RJ yielding a mean density of 2.42 +- 0.35\n g/cm3. Comparing these observed characteristics with recent theoretical models,\n we find that HAT-P-16b is consistent with a 1 Gyr H/He-dominated gas giant\n planet. HAT-P-16b resides in a sparsely populated region of the mass{radius\n diagram and has a non-zero eccentricity of e = 0.036 with a significance of 10\n sigma.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2402.00530",
        "title": "Superfiltering: Weak-to-Strong Data Filtering for Fast\nInstruction-Tuning",
        "abstract": "Instruction tuning is critical to improve LLMs but usually suffers from\n low-quality and redundant data. Data filtering for instruction tuning has\n proved important in improving both the efficiency and performance of the tuning\n process. But it also leads to extra cost and computation due to the involvement\n of LLMs in this process. To reduce the filtering cost, we study Superfiltering:\n Can we use a smaller and weaker model to select data for finetuning a larger\n and stronger model? Despite the performance gap between weak and strong\n language models, we find their highly consistent capability to perceive\n instruction difficulty and data selection results. This enables us to use a\n much smaller and more efficient model to filter the instruction data used to\n train a larger language model. Not only does it largely speed up the data\n filtering, but the filtered-data-finetuned LLM achieves even better performance\n on standard benchmarks. Extensive experiments validate the efficacy and\n efficiency of our approach.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1111.0345",
        "title": "Searching for Unmodeled Sources Using the Earth Occultation Data from\nthe Fermi GBM",
        "abstract": "Employing the 12 NaI detectors in the Fermi GBM, the Earth Occultation\n Technique (EOT) can be used to measure the fluxes of x-ray and gamma-ray\n sources. Each time a source passes behind the Earth (or emerges from behind the\n Earth), a step-like feature is produced in the detector count rate. With a\n predefined catalog of source positions, the times of the occultation steps can\n be calculated, the individual steps fit, and the fluxes derived. However, in\n order to find new sources and generate a complete catalog, a method is needed\n for generating an image of the sky. An imaging algorithm has been developed to\n generate all-sky images using the GBM data. Here we present imaging results\n from ~2.5 years of data in the 12-25 keV and 100-300 keV energy bands.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1501.01983",
        "title": "Neutrino-nucleus interactions and the short-range structure of nuclei",
        "abstract": "Improvements in theoretical modeling of Short Range structures and phenomena,\n and comparisons with data, will require sustained collaboration between nuclear\n theorists and neutrino experimentalists. The extensive history of studying this\n area of nuclear physics in electron- and hadron-scattering experiments, coupled\n with the transformative capabilities of LArTPCs to identify neutrinos, will\n provide a ripe opportunity for new discoveries that will further our\n understanding of the nucleus.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1611.06400",
        "title": "The puzzle of the CNO isotope ratios in AGB carbon stars",
        "abstract": "Previous determinations of the oxygen isotopic ratios in AGB carbon stars\n were at odds with the existing theoretical predictions. We aim to redetermine\n the oxygen ratios in these stars using new spectral analysis tools and further\n develop discussions on the carbon and nitrogen isotopic ratios in order to\n elucidate this problem. Oxygen isotopic ratios were derived from spectra in the\n K-band in a sample of galactic AGB carbon stars of different spectral types and\n near solar metallicity. Synthetic spectra calculated in LTE with spherical\n carbon-rich atmosphere models and updated molecular line lists were used. The\n CNO isotope ratios derived in a homogeneous way, were compared with theoretical\n predictions for low-mass (1.5-3 M_o) AGB stars computed with the FUNS code\n assuming extra mixing both during the RGB and AGB phases. For most of the stars\n the 16O/17O/18O ratios derived are in good agreement with theoretical\n predictions confirming that, for AGB stars, are established using the values\n reached after the FDU according to the initial stellar mass. This fact, as far\n as the oxygen isotopic ratios are concerned, leaves little space for the\n operation of any extra mixing mechanism during the AGB phase. Nevertheless, for\n a few stars with large 16O/17O/18O, the operation of such a mechanism might be\n required, although their observed 12C/13C and 14N/15N ratios would be difficult\n to reconcile within this scenario. Furthermore, J-type stars tend to have lower\n 16O/17O ratios than the normal carbon stars, as already indicated in previous\n studies. Excluding these peculiar stars, AGB carbon stars occupy the same\n region as pre-solar type I oxide grains in a 17O/16O vs. 18O/16O diagram,\n showing little spread. This reinforces the idea that these grains were probably\n formed in low-mass stars during the previous O-rich phases.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2012.04871",
        "title": "A note on truncated degenerate Bell polynomials",
        "abstract": "The aim of this paper is to introduce truncated degenerate Bell polynomials\n and numbers and to investigate some of their properties. In more detail, we\n obtain explicit expressions, identities involving other special polynomials,\n integral representations, Dobinski-like formula and expressions of the\n generating function in terms of differential operators and linear incomplete\n gamma function. In addition, we introduce truncated degenerate modified Bell\n polynomials and numbers and get similar results for those polynomials. As an\n application of our results, we show that the truncated degenerate Bell numbers\n can be expressed as a finite sum involving moments of a beta random variable\n with certain parameters.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1503.04111",
        "title": "Asymptotic behavior of Palais-Smale sequences associated with fractional\nYamabe type equations",
        "abstract": "In this paper, we analyze the asymptotic behavior of Palais-Smale sequences\n associated with fractional Yamabe type equations on an asymptotically\n hyperbolic Riemannian manifold. We prove that Palais-Smale sequences can be\n decomposed into the solution of the limit equation plus a finite number of\n bubbles, which are the rescaling of the fundamental solutions to the fractional\n Yamabe equation on Euclidean space. We also verify the non-interfering fact for\n multi-bubbles.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0107625",
        "title": "Pseudogap formation of four-layer BaRuO$_3$ and its electrodynamic\nresponse changes",
        "abstract": "We investiaged the optical properties of four-layer BaRuO$_{3}$, which shows\n a fermi-liquid-like behavior at low temperature. Its optical conductivity\n spectra clearly displayed the formation of a pseudogap and the development of a\n coherent peak with decreasing temperature. Temperature-dependences of the\n density $n$ and the scattering rate $1/\\tau$ of the coherent component were\n also derived. As the temperature decreases, both $n$ and $1/\\tau$ decrease for\n four-layer BaRuO$_{3}$. These electrodynamic responses were compared with those\n of nine-layer BaRuO$_{3}$, which also shows a pseudogap formation but has an\n insulator-like state at low temperature. It was found that the relative rates\n of change of both $n$ and $1/\\tau$ determine either metallic or insulator-like\n responses in the ruthenates. The optical properties of the four-layer ruthenate\n were also compared with those of other pseudogap systems, such as high $T_{c}$\n cuprates and heavy electron systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1501.07623",
        "title": "Energy Deposition Studies for the Hi-Lumi LHC Inner Triplet Magnets",
        "abstract": "A detailed model of the High Luminosity LHC inner triplet region with new\n large-aperture Nb3Sn magnets, field maps, corrector packages, and segmented\n tungsten inner absorbers was built and implemented into the FLUKA and MARS15\n codes. In the optimized configuration, the peak power density averaged over the\n magnet inner cable width is safely below the quench limit. For the integrated\n luminosity of 3000 fb -1, the peak dose in the innermost magnet insulator\n ranges from 20 to 35 MGy. Dynamic heat loads to the triplet magnet cold mass\n are calculated to evaluate the cryogenic capability. In general, FLUKA and MARS\n results are in a very good agreement.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1511.03137",
        "title": "k-way Hypergraph Partitioning via n-Level Recursive Bisection",
        "abstract": "We develop a multilevel algorithm for hypergraph partitioning that contracts\n the vertices one at a time. Using several caching and lazy-evaluation\n techniques during coarsening and refinement, we reduce the running time by up\n to two-orders of magnitude compared to a naive $n$-level algorithm that would\n be adequate for ordinary graph partitioning. The overall performance is even\n better than the widely used hMetis hypergraph partitioner that uses a classical\n multilevel algorithm with few levels. Aided by a portfolio-based approach to\n initial partitioning and adaptive budgeting of imbalance within recursive\n bipartitioning, we achieve very high quality. We assembled a large benchmark\n set with 310 hypergraphs stemming from application areas such VLSI, SAT\n solving, social networks, and scientific computing. We achieve significantly\n smaller cuts than hMetis and PaToH, while being faster than hMetis.\n Considerably larger improvements are observed for some instance classes like\n social networks, for bipartitioning, and for partitions with an allowed\n imbalance of 10%. The algorithm presented in this work forms the basis of our\n hypergraph partitioning framework KaHyPar (Karlsruhe Hypergraph Partitioning).",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1206.0520",
        "title": "Searching for New Physics with Flavor Violating Observables",
        "abstract": "In this talk, I review the status and prospects of several low energy flavor\n observables that are highly sensitive to New Physics effects. In particular I\n discuss the implications for possible New Physics in b --> s transitions coming\n from the recent experimental results on the B_s mixing phase, the branching\n ratio of the rare decay B_s --> mu+mu-, and angular observables in the B --> K*\n mu+mu- decay. Also the recent evidence for direct CP violation in singly\n Cabibbo suppressed charm decays and its interpretation in the context of New\n Physics models is briefly discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/9311075",
        "title": "Regular Representation of the Quantum Heisenberg Double $U_q(sl(2))$,\n$Fun_{q}(SL(2))$ ($q$ is a root of unity)",
        "abstract": "Pairing between the universal enveloping algebra $U_q(sl(2))$ and the algebra\n of functions over $SL_q(2)$ is obtained in explicit terms. The regular\n representation of the quantum double is constructed and investigated. The\n structure of the root subspaces of the Casimir operator is revealed and\n described in terms of $SL_q(2)$ elements.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2203.04114",
        "title": "A study on joint modeling and data augmentation of multi-modalities for\naudio-visual scene classification",
        "abstract": "In this paper, we propose two techniques, namely joint modeling and data\n augmentation, to improve system performances for audio-visual scene\n classification (AVSC). We employ pre-trained networks trained only on image\n data sets to extract video embedding; whereas for audio embedding models, we\n decide to train them from scratch. We explore different neural network\n architectures for joint modeling to effectively combine the video and audio\n modalities. Moreover, data augmentation strategies are investigated to increase\n audio-visual training set size. For the video modality the effectiveness of\n several operations in RandAugment is verified. An audio-video joint mixup\n scheme is proposed to further improve AVSC performances. Evaluated on the\n development set of TAU Urban Audio Visual Scenes 2021, our final system can\n achieve the best accuracy of 94.2% among all single AVSC systems submitted to\n DCASE 2021 Task 1b.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2307.05070",
        "title": "A Logic-Based Analysis of Responsibility",
        "abstract": "This paper presents a logic-based framework to analyze responsibility, which\n I refer to as intentional epistemic act-utilitarian stit theory (IEAUST). To be\n precise, IEAUST is used to model and syntactically characterize various modes\n of responsibility, where by 'modes of responsibility' I mean instances of\n Broersen's three categories of responsibility (causal, informational, and\n motivational responsibility), cast against the background of particular deontic\n contexts. IEAUST is obtained by integrating a modal language to express the\n following components of responsibility on stit models: agency, epistemic\n notions, intentionality, and different senses of obligation. With such a\n language, I characterize the components of responsibility using particular\n formulas. Then, adopting a compositional approach -- where complex modalities\n are built out of more basic ones -- these characterizations of the components\n are used to formalize the aforementioned modes of responsibility.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1604.05495",
        "title": "Deep Saliency with Encoded Low level Distance Map and High Level\nFeatures",
        "abstract": "Recent advances in saliency detection have utilized deep learning to obtain\n high level features to detect salient regions in a scene. These advances have\n demonstrated superior results over previous works that utilize hand-crafted low\n level features for saliency detection. In this paper, we demonstrate that\n hand-crafted features can provide complementary information to enhance\n performance of saliency detection that utilizes only high level features. Our\n method utilizes both high level and low level features for saliency detection\n under a unified deep learning framework. The high level features are extracted\n using the VGG-net, and the low level features are compared with other parts of\n an image to form a low level distance map. The low level distance map is then\n encoded using a convolutional neural network(CNN) with multiple 1X1\n convolutional and ReLU layers. We concatenate the encoded low level distance\n map and the high level features, and connect them to a fully connected neural\n network classifier to evaluate the saliency of a query region. Our experiments\n show that our method can further improve the performance of state-of-the-art\n deep learning-based saliency detection methods.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1209.6005",
        "title": "Imaginary quadratic fields with isomorphic abelian Galois groups",
        "abstract": "In 1976, Onabe discovered that, in contrast to the Neukirch-Uchida results\n that were proved around the same time, a number field $K$ is not completely\n characterized by its absolute abelian Galois group $A_K$. The first examples of\n non-isomorphic $K$ having isomorphic $A_K$ were obtained on the basis of a\n classification by Kubota of idele class character groups in terms of their\n infinite families of Ulm invariants, and did not yield a description of $A_K$.\n In this paper, we provide a direct `computation' of the profinite group $A_K$\n for imaginary quadratic $K$, and use it to obtain many different $K$ that all\n have the same minimal absolute abelian Galois group.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0002405",
        "title": "Recovering the Topology of the Initial Density Fluctuations Using the\nIRAS Point Source Catalogue Redshift Survey",
        "abstract": "We apply the reconstruction technique of Nusser & Dekel (1992) to the\n recently available Point Source Catalogue Redshift Survey (PSCz) in order to\n subtract the phase correlations that are expected to develop in the mild\n non-linear regime of gravitational evolution. We study the evolution of\n isodensity contours defined using an adaptive smoothing algorithm in order to\n minimize the problems derived from the non-comutivity of operators. We study\n the genus curves of the fields thus obtained and concentrate on the evolution\n of the amplitude drops, a meta-statistic able to quantify the level of\n phase-correlation present in the field. In order to test the method and to\n quantify the level of statistical uncertainty, we apply the method to a set of\n mock PSCz catalogues derived from the N-body simulations of two 'standard' CDM\n models, kindly granted to us by the Virgo consortium. We find the method to be\n reliable in recovering the right amplitude drops. When applied to PSCz, the\n level of phase correlations observed is very low on all scales ranging from\n 5h-1Mpc to 60 h-1Mpc, providing support to the theory that structure originated\n from gaussian initial conditions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2304.08602",
        "title": "Crossing Roads of Federated Learning and Smart Grids: Overview,\nChallenges, and Perspectives",
        "abstract": "Consumer's privacy is a main concern in Smart Grids (SGs) due to the\n sensitivity of energy data, particularly when used to train machine learning\n models for different services. These data-driven models often require huge\n amounts of data to achieve acceptable performance leading in most cases to\n risks of privacy leakage. By pushing the training to the edge, Federated\n Learning (FL) offers a good compromise between privacy preservation and the\n predictive performance of these models. The current paper presents an overview\n of FL applications in SGs while discussing their advantages and drawbacks,\n mainly in load forecasting, electric vehicles, fault diagnoses, load\n disaggregation and renewable energies. In addition, an analysis of main design\n trends and possible taxonomies is provided considering data partitioning, the\n communication topology, and security mechanisms. Towards the end, an overview\n of main challenges facing this technology and potential future directions is\n presented.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1910.10872",
        "title": "Man is to Person as Woman is to Location: Measuring Gender Bias in Named\nEntity Recognition",
        "abstract": "We study the bias in several state-of-the-art named entity recognition (NER)\n models---specifically, a difference in the ability to recognize male and female\n names as PERSON entity types. We evaluate NER models on a dataset containing\n 139 years of U.S. census baby names and find that relatively more female names,\n as opposed to male names, are not recognized as PERSON entities. We study the\n extent of this bias in several NER systems that are used prominently in\n industry and academia. In addition, we also report a bias in the datasets on\n which these models were trained. The result of this analysis yields a new\n benchmark for gender bias evaluation in named entity recognition systems. The\n data and code for the application of this benchmark will be publicly available\n for researchers to use.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2312.07095",
        "title": "Dual Brunn-Minkowski inequality for C-star bodies",
        "abstract": "In this paper, we consider the concept of $C$-star body in a fixed pointed\n closed convex cone $C$ and study the dual mixed volume for $C$-star bodies. For\n $C$-star bodies, we establish the corresponding dual Brunn-Minkowski\n inequality, the dual Minkowski inequality and the dual Aleksandrov-Fenchel\n inequality. Our dual Brunn-Minkowski inequality for $C$-star bodies strengthens\n Schneider's Brunn-Minkowski inequality for $C$-coconvex sets.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2011.02272",
        "title": "Trustworthy AI",
        "abstract": "Modern AI systems are reaping the advantage of novel learning methods. With\n their increasing usage, we are realizing the limitations and shortfalls of\n these systems. Brittleness to minor adversarial changes in the input data,\n ability to explain the decisions, address the bias in their training data, high\n opacity in terms of revealing the lineage of the system, how they were trained\n and tested, and under which parameters and conditions they can reliably\n guarantee a certain level of performance, are some of the most prominent\n limitations. Ensuring the privacy and security of the data, assigning\n appropriate credits to data sources, and delivering decent outputs are also\n required features of an AI system. We propose the tutorial on Trustworthy AI to\n address six critical issues in enhancing user and public trust in AI systems,\n namely: (i) bias and fairness, (ii) explainability, (iii) robust mitigation of\n adversarial attacks, (iv) improved privacy and security in model building, (v)\n being decent, and (vi) model attribution, including the right level of credit\n assignment to the data sources, model architectures, and transparency in\n lineage.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0805.3745",
        "title": "Enslaved Phase-Separation Fronts in One-Dimensional Binary Mixtures",
        "abstract": "Phase-separation fronts leave in their wakes morphologies that are\n substantially different from the morphologies formed in homogeneous\n phase-separation. In this paper we focus on fronts in binary mixtures that are\n enslaved phase-separation fronts, i.e. fronts that follow in the wake of a\n control-parameter front. In the one-dimensional case, which is the focus of\n this paper, the formed morphology is deceptively simple: alternating domains of\n a regular size. However, determining the size of these domains as a function of\n the front speed and other system parameters is a non-trivial problem. We\n present an analytical solution for the case where no material is deposited\n ahead of the front and numerical solutions and scaling arguments for more\n general cases. Through these enslaved phase-separation fronts large domains can\n be formed that are practically unattainable in homogeneous one-dimensional\n phase-separation.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2112.15105",
        "title": "Moire Band Structures of the Double twisted Few Layer Graphene",
        "abstract": "Very recently, unconventional superconductivity has been observed in the\n double twisted trilayer graphene (TLG), where three monolayer graphene (MLG)\n are stacked on top of each other with two twist angles [J. M. Park, et al.,\n Nature 590, 249 (2021); Z. Hao, et al., Science 371, 1133 (2021); X. Zhang, et\n al., Phys. Rev. Lett.127, 166802 (2021)]. When some of MLGs in the double\n twisted TLG are replaced by bilayer graphene (BLG), we get a new family of\n double twisted moire heterostructure, namely double twisted few layer graphene\n (DTFLG). In this work, we theoretically investigate the moire band structures\n of the DTFLGs with diverse arrangements of MLG and BLG. We find that, depending\n on the relative rotation direction of the two twist angles (alternate or chiral\n twist) and the middle van der Waals (vdW) layer (MLG or BLG), a general\n (X+Y+Z)-DTFLG can be classified into four categories, i.e. (X+1+Z)-ATFLG,\n (X+2+Z)-ATFLG, (X+1+Z)-CTFLG and (X+2+Z)-CTFLG, each of which has its own\n unique band structure. Here, X, Y, Z denote the three vdW layers, i.e. MLG or\n BLG. Interestingly, the (X+1+Z)-ATFLGs have a pair of perfect flat bands at the\n magic angle about $1.54^\\circ$ coexisting with a pair of linear or parabolic\n bands, which is quite like the double twisted TLG. Meanwhile, when the twist\n angle is smaller than a \"magic angle\" $1.70^\\circ$, the (X+2+Z)-CTFLGs can have\n two isolated narrow bands at $E_f$ with band width less than 5 meV. The\n influence of electric field and the topological features of the moire bands\n have been studied as well. Our work indicates that the DTFLGs, especially the\n (X+1+Z)-ATFLG and (X+2+Z)-CTFLG, are promising platforms to study the moire\n flat band induced novel correlation and topological effects.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1403.7948",
        "title": "Structure of conflict graphs in constraint alignment problems and\nalgorithms",
        "abstract": "We consider the constrained graph alignment problem which has applications in\n biological network analysis. Given two input graphs $G_1=(V_1,E_1),\n G_2=(V_2,E_2)$, a pair of vertex mappings induces an {\\it edge conservation} if\n the vertex pairs are adjacent in their respective graphs. %In general terms The\n goal is to provide a one-to-one mapping between the vertices of the input\n graphs in order to maximize edge conservation. However the allowed mappings are\n restricted since each vertex from $V_1$ (resp. $V_2$) is allowed to be mapped\n to at most $m_1$ (resp. $m_2$) specified vertices in $V_2$ (resp. $V_1$). Most\n of results in this paper deal with the case $m_2=1$ which attracted most\n attention in the related literature. We formulate the problem as a maximum\n independent set problem in a related {\\em conflict graph} and investigate\n structural properties of this graph in terms of forbidden subgraphs. We are\n interested, in particular, in excluding certain wheals, fans, cliques or claws\n (all terms are defined in the paper), which corresponds in excluding certain\n cycles, paths, cliques or independent sets in the neighborhood of each vertex.\n Then, we investigate algorithmic consequences of some of these properties,\n which illustrates the potential of this approach and raises new horizons for\n further works. In particular this approach allows us to reinterpret a known\n polynomial case in terms of conflict graph and to improve known approximation\n and fixed-parameter tractability results through efficiently solving the\n maximum independent set problem in conflict graphs. Some of our new\n approximation results involve approximation ratios that are function of the\n optimal value, in particular its square root; this kind of results cannot be\n achieved for maximum independent set in general graphs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1508.03714",
        "title": "Probabilistic Asynchronous Arbitrary Pattern Formation",
        "abstract": "We propose a new probabilistic pattern formation algorithm for oblivious\n mobile robots that operates inthe ASYNC model. Unlike previous work, our\n algorithm makes no assumptions about the local coordinatesystems of robots (the\n robots do not share a common \"North\" nor a common \"Right\"), yet it preserves\n theability from any initial configuration that contains at least 5 robots to\n form any general pattern (and not justpatterns that satisfy symmetricity\n predicates). Our proposal also gets rid of the previous assumption (in thesame\n model) that robots do not pause while moving (so, our robots really are fully\n asynchronous), and theamount of randomness is kept low -- a single random bit\n per robot per Look-Compute-Move cycle is used.Our protocol consists in the\n combination of two phases, a probabilistic leader election phase, and a\n deterministicpattern formation one. As the deterministic phase does not use\n chirality, it may be of independentinterest in the deterministic context. A\n noteworthy feature of our algorithm is the ability to form patternswith\n multiplicity points (except the gathering case due to impossibility results), a\n new feature in the contextof pattern formation that we believe is an important\n asset of our approach.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1312.4434",
        "title": "Electrical conductivity of the quark-gluon plasma and soft photon\nspectrum in heavy-ion collisions",
        "abstract": "We extract the electrical conductivity $\\sigma_0$ of the quark gluon\n plasma(QGP) and study the effects of magnetic field and chiral anomaly on soft\n photon azimuthal anisotropy, $v_2$, based on the thermal photon spectrum at\n $0.4GeV<p_{\\perp}<0.6GeV$ at RHIC energy. As a basis for our analysis, we\n derive the behavior of retarded photon self energy of a strongly interacting\n neutral plasma in hydrodynamic regime in the presence of magnetic field and\n chiral anomaly. By evolving the resulting soft thermal photon production rate\n over the realistic hydrodynamic background and comparing the results with the\n preliminary data from the PHENIX Collaboration, we found that the electrical\n conductivity at QGP temperature is in the range: $0.4<\\sigma_0/(e^{2}T) <1.1$,\n which is comparable with recent studies on lattice. We also compare the\n contribution from the magnetic field and chiral anomaly to soft thermal photon\n $v_{2}$ with the data. We argue that at LHC, the chiral magnetic wave would\n give negative contribution to photon $v_2$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1804.07890",
        "title": "A Nutritional Label for Rankings",
        "abstract": "Algorithmic decisions often result in scoring and ranking individuals to\ndetermine credit worthiness, qualifications for college admissions and\nemployment, and compatibility as dating partners. While automatic and seemingly\nobjective, ranking algorithms can discriminate against individuals and\nprotected groups, and exhibit low diversity. Furthermore, ranked results are\noften unstable --- small changes in the input data or in the ranking\nmethodology may lead to drastic changes in the output, making the result\nuninformative and easy to manipulate. Similar concerns apply in cases where\nitems other than individuals are ranked, including colleges, academic\ndepartments, or products.\nIn this demonstration we present Ranking Facts, a Web-based application that\ngenerates a \"nutritional label\" for rankings. Ranking Facts is made up of a\ncollection of visual widgets that implement our latest research results on\nfairness, stability, and transparency for rankings, and that communicate\ndetails of the ranking methodology, or of the output, to the end user. We will\nshowcase Ranking Facts on real datasets from different domains, including\ncollege rankings, criminal risk assessment, and financial services.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/physics/0603233",
        "title": "A Prototype PCI-based Data Acquisition System for Cosmic Ray Detection\nBelow 10^18 eV",
        "abstract": "A prototype flash analog-to-digital readout system for cosmic ray detection\n at energies below 10^18 eV has been designed and tested at Columbia University\n Nevis Laboratories. The electronics consist of an FADC module that digitizes 16\n photomultipliers at 40 MHz with 14-bit dynamic range. The module is read out to\n a PC (running Linux) through a PCI interface. Taking advantage of the large\n bandwidth provided by the PCI bus, we have implemented a software-based data\n acquisition system. This note describes the software and electronics, as well\n as preliminary tests carried out using a prototype FADC module.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0708.1991",
        "title": "Conditions for the Trivers-Willard hypothesis to be valid: A minimal\npopulation-genetic model",
        "abstract": "The very insightful Trivers-Willard hypothesis, proposed in the early 1970s,\n states that females in good physiological conditions are more likely to produce\n male offspring, when the variance of reproductive success amongst males is\n high. A number of studies, aimed at its experimental verification, have found\n adequate supportive evidence in its favour. Theoretical investigations,\n however, have been few, perhaps because formulating a population-genetic model\n for describing the Trivers-Willard hypothesis turns out to be surprisingly\n complex. The present study describes a minimal population genetic model to\n explore one specific scenario, viz. how is the preference for a male offspring\n by females in good condition altered when 'g', the proportion of such females\n in the population changes from a low to a high value. As expected, when the\n proportion of such females is low, i.e., for low values of 'g', the\n Trivers-Willard (TW) strategy goes to fixation against the equal investment\n strategy. This holds true up to gmax, a critical value of 'g', above which the\n two strategies coexist, but the proportion of the TW strategy steadily\n decreases as 'g' increases to unity. Similarly, when the effect of well-endowed\n males attaining disproportionately high number of matings is more pronounced,\n the TW strategy is more likely to go to fixation. Interestingly, the success of\n the TW strategy has a complex dependence on the variance in the physiological\n condition of females. If the difference in the two types of conditions is not\n large, TW strategy is favoured, and its success is more likely as the\n difference increases. However, beyond a critical value of the difference, the\n TW strategy is found to be less and less likely to succeed as the difference\n becomes larger. Possible reasons for these effects are discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2008.01033",
        "title": "Experimental evidence of hidden spin polarization in silicon by using\nstrain gradient",
        "abstract": "The centrosymmetric materials with hidden spin polarization are considered to\n be the promising candidates for realization of energy efficient spintronics\n systems and devices. However, the control of hidden spin polarization and\n resulting transport behavior is not well understood. We hypothesized that\n inhomogeneous strain can be the external knob to study and control hidden spin\n polarization. In this work, we demonstrate a strain gradient mediated symmetry\n breaking to discover the hidden spin polarization in centrosymmetric Si\n lattice. The hidden spin polarization gives rise to magnetocrystalline\n anisotropy and local magnetic moment along <111> directions in the Si. The\n local magnetic moment gives rise to spin-acoustic phonon coupling, which is the\n underlying cause of observed spin-Hall effect in both n-Si and p-Si. Discovery\n of hidden magnetic moment in Si not only challenges the fundamental\n understanding of the origin of the magnetism but also presents a giant leap in\n realization of spintronics systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1811.07494",
        "title": "Bulk Spectrum and K-theory for Infinite-Area Topological Quasicrystal",
        "abstract": "The bulk spectrum of a possible Chern insulator on a quasicrystalline lattice\nis examined. The effect of being a 2D insulator seems to override any fractal\nproperties in the spectrum. We compute that the spectrum is either two\ncontinuous bands, or that any gaps other than the main gap are small. After\nmaking estimates on the spectrum, we deduce a finite system size, above which\nthe K-theory must coincide with the K-theory of the infinite system. Knowledge\nof the spectrum and $K$-theory of the infinite-area system will control the\nspectrum and K-theory of sufficiently large finite systems.\nThe relation between finite volume $K$-theory and infinite volume Chern\nnumbers is only proven to begin, for the model under investigation here, for\nsystems on Hilbert space of dimension around 17 million. The real-space method\nbased on the Clifford spectrum allows for computing Chern numbers for systems\non Hilbert space of dimension around 2.7 million. New techniques in numerical\nK-theory are used to equate the K-theory of systems of different sizes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1712.04781",
        "title": "Recollements from Cotorsion Pairs",
        "abstract": "Given a complete hereditary cotorsion pair $(\\mathcal{A},\\mathcal{B})$ in a\nGrothendieck category $\\mathcal{G}$, the derived category\n$\\mathcal{D}(\\mathcal{B})$ of the exact category $\\mathcal{B}$ is defined as\nthe quotient of the category $\\mathrm{Ch}(\\mathcal{B})$, of unbounded complexes\nwith terms in $\\mathcal{B}$, modulo the subcategory $\\widetilde{\\mathcal{B}}$\nconsisting of the acyclic complexes with terms in $\\mathcal{B}$ and cycles in\n$\\mathcal{B}$.\nWe restrict our attention to the cotorsion pairs such that\n$\\widetilde{\\mathcal{B}}$ coincides with the class $ex\\mathcal{B}$ of the\nacyclic complexes of $\\mathrm{Ch}(\\mathcal{G})$ with terms in $\\mathcal{B}$. In\nthis case the derived category $\\mathcal{D}(\\mathcal{B})$ fits into a\nrecollement $\\dfrac{ex\\mathcal{B}}{\\sim}\n\\mathrel{\\substack{\\textstyle\\leftarrow\\textstyle\\rightarrow\\textstyle\\leftarrow}}\n{K(\\mathcal{B})}\n\\mathrel{\\substack{\\textstyle\\leftarrow\\textstyle\\rightarrow\\textstyle\\leftarrow}}\n{\\dfrac{\\mathrm{Ch}(\\mathcal{B})}{ex\\mathcal{B} }}$.\nWe will explore the conditions under which\n$\\mathrm{ex}\\,\\mathcal{B}=\\widetilde{\\mathcal{B}}$ and provide many examples.\nSymmetrically, we prove analogous results for the exact category\n$\\mathcal{A}$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1412.6315",
        "title": "On the environments of type Ia supernovae within host galaxies",
        "abstract": "We present constraints on supernovae type Ia (SNe~Ia) progenitors through an\n analysis of the environments found at the explosion sites of 102 events within\n star-forming host galaxies. H-alpha and GALEX near-UV images are used to trace\n on-going and recent star formation (SF), while broad band B, R, J, K imaging is\n also analysed. Using pixel statistics we find that SNe~Ia show the lowest\n degree of association with H-alpha emission of all supernova types. It is also\n found that they do not trace near-UV emission. As the latter traces SF on\n timescales less than 100 Myr, this rules out any extreme 'prompt' delay-times\n as the dominant progenitor channel of SNe~Ia. SNe~Ia best trace the B-band\n light distribution of their host galaxies. This implies that the population\n within star-forming galaxies is dominated by relatively young progenitors.\n Splitting SNe by their (B-V) colours at maximum light, 'redder' events show a\n higher degree of association to HII regions and are found more centrally within\n hosts. We discuss possible explanations of this result in terms of line of\n sight extinction and progenitor effects. No evidence for correlations between\n SN stretch and environment properties is observed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2208.10527",
        "title": "The Fibonacci decomposition of symmetric Tetranacci polynomials",
        "abstract": "In this manuscript, we introduce (symmetric) Tetranacci polynomials $\\xi_j$\n as a twofold generalization of ordinary Tetranacci numbers, by considering both\n non unity coefficients and generic initial values in their recursive\n definition. The issue of these polynomials arose in condensed matter physics\n and the diagonalization of symmetric Toeplitz matrices having in total four\n non-zero off diagonals. For the latter, the symmetric Tetranacci polynomials\n are the basic entities of the associated eigenvectors; thus, treating the\n recursive structure determines the eigenvalues as well. Subsequently, we\n present a complete closed form expression for any symmetric Tetranacci\n polynomial. The key feature is a decomposition in terms of generalized\n Fibonacci polynomials.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2201.03372",
        "title": "Asymmetry engendered by symmetric kink-antikink scattering in a\ndegenerate two-field model",
        "abstract": "In this paper we analyze the scattering process in a two-field model in\n $(1+1)$-dimensions, with the special property to have several topological\n solutions: i) one with higher rest mass, characterized by a nested defect (lump\n inside a kink), and ii) four others having lower rest mass, degenerated, and\n characterized by a kink inside kink. We investigate kink-antikink symmetric\n scattering, where the kink and antikink have higher rest mass and the same\n initial velocity modulus $v$. The output of scattering presents a wide range of\n behaviors, such as annihilation of the kink-antikink pair, the emission of\n radiation jets, the generation of oscillating pulses and the change of the\n topological sector. We show that the changing of the topological sector is\n favored, and only two of the four sectors are possible as outcomes. Moreover,\n despite the degeneracy in energy, the distribution of the final states is\n asymmetric in the phase space, being an effect of the presence of vibrational\n states.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2401.03546",
        "title": "NovelGym: A Flexible Ecosystem for Hybrid Planning and Learning Agents\nDesigned for Open Worlds",
        "abstract": "As AI agents leave the lab and venture into the real world as autonomous\n vehicles, delivery robots, and cooking robots, it is increasingly necessary to\n design and comprehensively evaluate algorithms that tackle the ``open-world''.\n To this end, we introduce NovelGym, a flexible and adaptable ecosystem designed\n to simulate gridworld environments, serving as a robust platform for\n benchmarking reinforcement learning (RL) and hybrid planning and learning\n agents in open-world contexts. The modular architecture of NovelGym facilitates\n rapid creation and modification of task environments, including multi-agent\n scenarios, with multiple environment transformations, thus providing a dynamic\n testbed for researchers to develop open-world AI agents.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2006.06130",
        "title": "ROOTS: Object-Centric Representation and Rendering of 3D Scenes",
        "abstract": "A crucial ability of human intelligence is to build up models of individual\n 3D objects from partial scene observations. Recent works achieve object-centric\n generation but without the ability to infer the representation, or achieve 3D\n scene representation learning but without object-centric compositionality.\n Therefore, learning to represent and render 3D scenes with object-centric\n compositionality remains elusive. In this paper, we propose a probabilistic\n generative model for learning to build modular and compositional 3D object\n models from partial observations of a multi-object scene. The proposed model\n can (i) infer the 3D object representations by learning to search and group\n object areas and also (ii) render from an arbitrary viewpoint not only\n individual objects but also the full scene by compositing the objects. The\n entire learning process is unsupervised and end-to-end. In experiments, in\n addition to generation quality, we also demonstrate that the learned\n representation permits object-wise manipulation and novel scene generation, and\n generalizes to various settings. Results can be found on our project website:\n https://sites.google.com/view/roots3d",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1909.04846",
        "title": "Covariance Matrix Adaptation Greedy Search Applied to Water Distribution\nSystem Optimization",
        "abstract": "Water distribution system design is a challenging optimisation problem with a\n high number of search dimensions and constraints. In this way, Evolutionary\n Algorithms (EAs) have been widely applied to optimise WDS to minimise cost\n subject whilst meeting pressure constraints. This paper proposes a new hybrid\n evolutionary framework that consists of three distinct phases. The first phase\n applied CMA-ES, a robust adaptive meta-heuristic for continuous optimisation.\n This is followed by an upward-greedy search phase to remove pressure\n violations. Finally, a downward greedy search phase is used to reduce oversized\n pipes. To assess the effectiveness of the hybrid method, it was applied to five\n well-known WDSs case studies. The results reveal that the new framework\n outperforms CMA-ES by itself and other previously applied heuristics on most\n benchmarks in terms of both optimisation speed and network cost.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1803.04092",
        "title": "Estimating Shape of Target Object Moving on Unknown Trajectory by Using\nLocation-Unknown Distance Sensors: Theoretical Framework",
        "abstract": "By using directional distance sensors that have unknown locations, this paper\n proposes a method of estimating the shape of a location-unknown target object\n $T$ moving with unknown speed on an unknown straight line trajectory.\n Regardless of many unknown factors, the proposed method can estimate the shape\n by using each sensor's continuous report of the measured distance to $T$\n without using side information or additional mechanisms such as locations of\n anchor sensors and angle-of-arrival measurements. By using the sensor reports,\n the proposed method estimates (i) the moving speed of $T$, (ii) the length and\n direction of an edge of $T$, and (iii) the order of consecutive edges. As a\n result, we can obtain the shape of $T$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1509.07804",
        "title": "From Statistician to Data Scientist",
        "abstract": "According to a recent report from the European Commission, the world\n generates every minute 1.7 million of billions of data bytes, the equivalent of\n 360,000 DVDs, and companies that build their decision-making processes by\n exploiting these data increase their productivity. The treatment and\n valorization of massive data has consequences on the employment of graduate\n students in statistics. Which additional skills do students trained in\n statistics need to acquire to become data scientists ? How to evolve training\n so that future graduates can adapt to rapid changes in this area, without\n neglecting traditional jobs and the fundamental and lasting foundation for the\n training? After considering the notion of big data and questioning the\n emergence of a \"new\" science: Data Science, we present the current developments\n in the training of engineers in Mathematical and Modeling at INSA Toulouse.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/alg-geom/9509007",
        "title": "Obstruction bundles, semiregularity, and Seiberg-Witten invariants",
        "abstract": "We compare the deformation theory and the analytic structure of the\n Seiberg-Witten moduli spaces of a K\\\"ahler surface to the corresponding\n components of the Hilbert scheme, and show that they are isomorphic. Next we\n show how to compute the invariant in case the moduli space is smooth but not of\n the expected dimension, and apply this study to elliptic surfaces. Finally we\n discuss ruled surfaces, both products and more general ruled surfaces. For\n product ruled surfaces we relate the infinitesimal structure of the moduli\n spaces to Brill-Noether theory and compute the invariant in special cases. For\n more general ruled surfaces, we relate the geometry of the Hilbert scheme to\n properties of stable bundles and give more general computations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2405.20991",
        "title": "Hard Cases Detection in Motion Prediction by Vision-Language Foundation\nModels",
        "abstract": "Addressing hard cases in autonomous driving, such as anomalous road users,\n extreme weather conditions, and complex traffic interactions, presents\n significant challenges. To ensure safety, it is crucial to detect and manage\n these scenarios effectively for autonomous driving systems. However, the rarity\n and high-risk nature of these cases demand extensive, diverse datasets for\n training robust models. Vision-Language Foundation Models (VLMs) have shown\n remarkable zero-shot capabilities as being trained on extensive datasets. This\n work explores the potential of VLMs in detecting hard cases in autonomous\n driving. We demonstrate the capability of VLMs such as GPT-4v in detecting hard\n cases in traffic participant motion prediction on both agent and scenario\n levels. We introduce a feasible pipeline where VLMs, fed with sequential image\n frames with designed prompts, effectively identify challenging agents or\n scenarios, which are verified by existing prediction models. Moreover, by\n taking advantage of this detection of hard cases by VLMs, we further improve\n the training efficiency of the existing motion prediction pipeline by\n performing data selection for the training samples suggested by GPT. We show\n the effectiveness and feasibility of our pipeline incorporating VLMs with\n state-of-the-art methods on NuScenes datasets. The code is accessible at\n https://github.com/KTH-RPL/Detect_VLM.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1311.3778",
        "title": "Twisted reduction in large N QCD with adjoint Wilson fermions",
        "abstract": "The twisted space-time reduced model of large $N$ QCD with various flavours\n of adjoint Wilson fermions is constructed applying the symmetric twist boundary\n conditions with flux $k$. The models with one and two flavours show distinctive\n behaviours. For the two flavor case, the string tension, calculated at $N=289$,\n approaches zero as we decrease the quark mass in a way consistent with the\n theory being governed by an infrared fixed point. In contrast, the string\n tension for the case of a single adjoint Wilson fermion remains finite as the\n quark mass decreases to zero, supporting that this is a confining theory.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0604579",
        "title": "Asymptotics of the Gaussian Curvatures of the Canonical Metric on the\nSurface",
        "abstract": "We study the canonical metric on a compact Riemann surface of genus at least\n two. While it is known that the canonical metric is of nonpositive curvature,\n we show that its Gaussian curvatures are not bounded away from zero nor\n negative infinity when the surface is close to the compactification divisor of\n Riemann's moduli space.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1006.0676",
        "title": "Longitudinal foliation rigidity and Lipschitz-continuous invariant forms\nfor hyperbolic flows",
        "abstract": "In several contexts the defining invariant structures of a hyperbolic\n dynamical system are smooth only in systems of algebraic origin (smooth\n rigidity), and we prove new results of this type for a class of flows. For a\n compact Riemannian manifold and a uniformly quasiconformal transversely\n symplectic Anosov flow we define the longitudinal KAM-cocycle and use it to\n prove a rigidity result: The joint stable/unstable subbundle is\n Zygmund-regular, and higher regularity implies vanishing of the longitudinal\n KAM-cocycle, which in turn implies that the subbundle is Lipschitz-continuous\n and indeed that the flow is smoothly conjugate to an algebraic one. To\n establish the latter, we prove results for algebraic Anosov systems that imply\n smoothness and a special structure for any Lipschitz-continuous invariant\n 1-form. Several features of the reasoning are interesting: The use of exterior\n calculus for Lipschitz-continuous forms, that the arguments for geodesic flows\n and infranilmanifoldautomorphisms are quite different, and the need for mixing\n as opposed to ergodicity in the latter case.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1605.01437",
        "title": "Compound chondrule formation via collision of supercooled droplets",
        "abstract": "We present a novel model showing that compound chondrules are formed by\n collisions of supercooled droplets. This model reproduces two prominent\n observed features of compound chondrules: the nonporphyritic texture and the\n size ratio between two components.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1809.01816",
        "title": "Visual Coreference Resolution in Visual Dialog using Neural Module\nNetworks",
        "abstract": "Visual dialog entails answering a series of questions grounded in an image,\n using dialog history as context. In addition to the challenges found in visual\n question answering (VQA), which can be seen as one-round dialog, visual dialog\n encompasses several more. We focus on one such problem called visual\n coreference resolution that involves determining which words, typically noun\n phrases and pronouns, co-refer to the same entity/object instance in an image.\n This is crucial, especially for pronouns (e.g., `it'), as the dialog agent must\n first link it to a previous coreference (e.g., `boat'), and only then can rely\n on the visual grounding of the coreference `boat' to reason about the pronoun\n `it'. Prior work (in visual dialog) models visual coreference resolution either\n (a) implicitly via a memory network over history, or (b) at a coarse level for\n the entire question; and not explicitly at a phrase level of granularity. In\n this work, we propose a neural module network architecture for visual dialog by\n introducing two novel modules - Refer and Exclude - that perform explicit,\n grounded, coreference resolution at a finer word level. We demonstrate the\n effectiveness of our model on MNIST Dialog, a visually simple yet\n coreference-wise complex dataset, by achieving near perfect accuracy, and on\n VisDial, a large and challenging visual dialog dataset on real images, where\n our model outperforms other approaches, and is more interpretable, grounded,\n and consistent qualitatively.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1810.10267",
        "title": "Thermoelectric generator at optimal power with external and internal\nirreversibilities",
        "abstract": "There are few exact results on optimal power conditions for a thermoelectric\n generator in the presence of both external and internal\n irreversibilities---modelled as non-ideal thermal contacts and Joule heating,\n respectively. Simplified cases, where only one kind of irreversibility is\n assumed, yield some well-known expressions for efficiency at maximum power\n (EMP), such as Curzon-Ahlborn efficiency for endoreversible model. In this\n work, we analyze situations under the simultaneous presence of internal and\n external irreversibilities. To simplify, we neglect heat leaks, and each kind\n of irreversibility is assumed only on the side of one of the thermal contacts.\n We also present the symmetric case---where each kind of irreversibility\n contributes with equal strengths towards the side of each thermal contact. We\n show the bounds satisfied by EMP in each of these regimes and compare its\n properties for thermal impedence matching and close to equilibrium, where we\n find step-wise changes in EMP.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1611.06514",
        "title": "A scenario-based framework for supply planning under uncertainty:\nstochastic programming versus robust optimization approaches",
        "abstract": "In this paper we analyze the effect of two modelling approaches for supply\n planning problems under uncertainty: two-stage stochastic programming (SP) and\n robust optimization (RO). The comparison between the two approaches is\n performed through a scenario-based framework methodology, which can be applied\n to any optimization problem affected by uncertainty. For SP we compute the\n minimum expected cost based on the specific probability distribution of the\n uncertain parameters related to a set of scenarios. For RO we consider static\n approaches where random parameters belong to box or ellipsoidal uncertainty\n sets in compliance with the data used to generate SP scenarios. Dynamic\n approaches for RO, via the concept of adjustable robust counterpart, are also\n considered. The efficiency of the methodology has been illustrated for a supply\n planning problem to optimize vehicle-renting and procurement transportation\n activities involving uncertainty on demands and on buying costs for\n extra-vehicles. Numerical experiments through the scenario-based framework\n allow a fair comparison in real case instances. Advantages and disadvantages of\n RO and SP are discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1501.03808",
        "title": "A Probabilistic Approach to Problems on Distance Graphs and Graphs of\nDiameters (Candidate-Degree Dissertation Author's Review, in Russian)",
        "abstract": "The dissertation is related to combinatorial geometry with a strong\nprobabilistic flavor. The main results can be split into three parts. The\nresults of the first part guarantee that each \"unit distance graph\" in the\nplane has an induced subgraph with chromatic number at most 4 that covers at\nleast 91.7 percent of the vertices of the whole graph.\nThe results of the second and third parts are related to the standard model\nof a random graph with n labeled vertices in which the edges occur\nindependently with probability p, where p is a function of n. This is known as\nthe Erdos--Renyi model G(n,p). Given a monotone property of a graph, Erdos and\nRenyi's theorem (1960) states that there exists a critical threshold value of\np(n) below which the probability that a random graph has that property tends to\none (as n tends to infinity) and above which the probability tends to zero.\nThe main results of the second part are concerned with the (monotone)\nproperty of a graph to be isomorphic to some unit-distance graph in Euclidean\nd-space with fixed dimension d. The results of this part guarantee that for d\nin {2, 3, 4, 5, 6, 7, 8}, the threshold value of p(n) is big-Theta of 1/n.\nFurthermore, the case d = 1 stands apart from the case of higher dimensions;\nhere the threshold probability is big-Theta of 1/(n^(4/3)).\nThe results of the third part are devoted to studying \"graphs of diameters\"\nfrom the probabilistic standpoint. In particular, it is shown that under some\nconditions, almost all graphs of diameters in the plane have chromatic number\nless than 3. More generally, it is shown for G(n,p) that graphs of diameters\nhave a tendency to chromatic degeneration (for large n) when p is close to 0,\nbut have a tendency to completeness when p is close to 1.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2003.04546",
        "title": "First-Order Methods for Nonconvex Quadratic Minimization",
        "abstract": "We consider minimization of indefinite quadratics with either trust-region\n (norm) constraints or cubic regularization. Despite the nonconvexity of these\n problems we prove that, under mild assumptions, gradient descent converges to\n their global solutions, and give a non-asymptotic rate of convergence for the\n cubic variant. We also consider Krylov subspace solutions and establish sharp\n convergence guarantees to the solutions of both trust-region and\n cubic-regularized problems. Our rates mirror the behavior of these methods on\n convex quadratics and eigenvector problems, highlighting their scalability.\n When we use Krylov subspace solutions to approximate the cubic-regularized\n Newton step, our results recover the strongest known convergence guarantees to\n approximate second-order stationary points of general smooth nonconvex\n functions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0701718",
        "title": "A Numerical Treatment of the Rf SQUID: I. General Properties and Noise\nEnergy",
        "abstract": "We investigate the characteristics and noise performance of rf\n Superconducting Quantum Interference Devices (SQUIDs) by solving the\n corresponding Langevin equations numerically and optimizing the model\n parameters with respect to noise energy. After introducing the basic concepts\n of the numerical simulations, we give a detailed discussion of the performance\n of the SQUID as a function of all relevant parameters. The best performance is\n obtained in the crossover region between the dispersive and dissipative\n regimes, characterized by an inductance parameter \\beta_L'\\equiv 2\\pi L\n I_0/\\Phi_0\\approx 1; L is the loop inductance, I_0 the critical current of the\n Josephson junction, and \\Phi_0 the flux quantum. In this regime, which is not\n well explored by previous analytical approaches, the lowest (intrinsic) values\n of noise energy are a factor of about 2 above previous estimates based on\n analytical approaches. However, several other analytical predictions, such as\n the inverse proportionality of the noise energy on the tank circuit quality\n factor and the square of the coupling coefficient between the tank circuit and\n the SQUID loop, could not be well reproduced. The optimized intrinsic noise\n energy of the rf SQUID is superior to that of the dc SQUID at all temperatures.\n Although for technologically achievable parameters this advantage shrinks,\n particularly at low thermal fluctuation levels, we give an example for\n realistic parameters that leads to a noise energy comparable to that of the dc\n SQUID even in this regime.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1907.10685",
        "title": "Angles between Haagerup--Schultz projections and spectrality of\noperators",
        "abstract": "We investigate angles between Haagerup--Schultz projections of operators\n belonging to finite von Neumann algebras, in connection with a property\n analogous to Dunford's notion of spectrality of operators. In particular, we\n show that an operator can be written as the sum of a normal and an\n s.o.t.-quasinilpotent operator that commute if and only if the angles between\n its Haagerup--Schultz projections are uniformly bounded away from zero (and we\n call this the uniformly nonzero anlges property). Moreover, we show that\n spectrality is equivalent to this uniformly nonzero angles property plus\n decomposability. Finally, using this characterization, we construct an easy\n example of an operator which is decomposable but not spectral, and we show that\n Voiculescu's circular operator is not spectral (nor are any of the circular\n free Poisson operators).",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1405.4655",
        "title": "Squeezing effect induced by minimal length uncertainty",
        "abstract": "In this work, the dynamics of the deformed one-dimensional harmonic\n oscillator with minimal length uncertainty is examined and the analytical\n solutions for time evolution of position and momentum operators are presented\n in which the rough approximation that neglects the higher order terms in\n BakerHausdor lemma is avoided. Based on these analytical solutions the\n uncertainties for position and momentum operators are calculated in a coherent\n state, and an unexpected squeezing effect in both coordinate and momentum\n directions is found in comparison with ordinary harmonic oscillator. Obviously\n such a squeezing effect is induced by the minimal length uncertainty\n (gravitational effects). Our results are applied to the electrons trapped in\n strong magnetic fields to examine the degree of the existing squeezing effect\n in a real system, which shows the squeezing degree induced by minimal length\n uncertainty is very small.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/quant-ph/0302124",
        "title": "Entanglement induced by spontaneous emission in spatially extended\ntwo-atom systems",
        "abstract": "We investigate the role of the collective antisymmetric state in entanglement\n creation by spontaneous emission in a system of two non-overlapping two-level\n atoms. We calculate and illustrate graphically populations of the collective\n atomic states and the Wootters entanglement measure (concurrence) for two sets\n of initial atomic conditions. Our calculations include the dipole-dipole\n interaction and a spatial separation between the atoms that the antisymmetric\n state of the system is included throughout even for small interatomic\n separations. It is shown that spontaneous emission can lead to a transient\n entanglement between the atoms even if the atoms were prepared initially in an\n unentangled state. We find that the ability of spontaneous emission to create\n the transient entanglement relies on the absence of population in the\n collective symmetric state of the system. For the initial state of only one\n atom excited, the entanglement builds up rapidly in time and reaches a maximum\n for the parameter values corresponding roughly to zero population in the\n symmetric state. On the other hand, for the initial condition of both atoms\n excited, the atoms remain unentangled until the symmetric state is depopulated.\n A simple physical interpretation of these results is given in terms of the\n diagonal states of the density matrix of the system. We also study entanglement\n creation in a system of two non-identical atoms of different transition\n frequencies. It is found that the entanglement between the atoms can be\n enhanced compared to that for identical atoms, and can decay with two different\n time scales resulting from the coherent transfer of the population from the\n symmetric to the antisymmetric state. In addition, we find that a decaying\n initial entanglement between the atoms can display a revival behaviour.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2303.16745",
        "title": "Pressure tuning of optical reflectivity in LuH2",
        "abstract": "Recently, the claim of room-temperature superconductivity in nitrogen-doped\n lutetium hydride at near-ambient conditions has attracted tremendous attention.\n Criticism of the work rises shortly, while further explorations are needed to\n settle the dispute. One of the intriguing observations is the pressured-induced\n color change, which has been reproduced in the lutetium dihydride LuH2 while\n its mechanism remains unclear. Through optical reflectivity measurements of\n LuH2 in the visible to near-infrared region, we observe strong light absorption\n next to the sharp plasmon resonance, which continuously shifts to higher\n energies with increasing pressure. It gives rise to the increased reflection of\n red light and suppressed reflection of blue light. Our work sheds light on\n resolving the puzzles regarding the pressure induced color change in LuH2.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1802.01829",
        "title": "Average Case $(s, t)$-weak tractability of non-homogenous tensor product\nproblems",
        "abstract": "We study $d$-variate problem in the average case setting with respect to a\n zero-mean Gaussian measure. The covariance kernel of this Gaussian measure is a\n product of univariate kernels and satisfies some special properties. We study\n $(s, t)$-weak tractability of this multivariate problem, and obtain a necessary\n and sufficient condition for $s>0$ and $t\\in(0,1)$. Our result can apply to the\n problems with covariance kernels corresponding to Euler and Wiener integrated\n processes, Korobov kernels, and analytic Korobov kernels.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1210.7110",
        "title": "Gauss-Bonnet theorem in sub-Riemannian Heisenberg space $H^1$",
        "abstract": "We prove a version of Gauss-Bonnet theorem in sub-Riemannian Heisenberg space\n $H^1$. The sub-Riemannian distance makes $H^1$ a metric space and consenquently\n with a spherical Hausdorff measure. Using this measure, we define a Gaussian\n curvature at points of a surface S where the sub-Riemannian distribution is\n transverse to the tangent space of S. If all points of S have this property, we\n prove a Gauss-Bonnet formula and for compact surfaces (which are topologically\n a torus) we obtain $\\int_S K = 0$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.07157",
        "title": "Machine learning potential for the Cu-W system",
        "abstract": "Combining the excellent thermal and electrical properties of Cu with the high\n abrasion resistance and thermal stability of W, Cu-W nanoparticle-reinforced\n metal matrix composites and nano-multilayers (NMLs) are finding applications as\n brazing fillers and shielding material for plasma and radiation. Due to the\n large lattice mismatch between fcc Cu and bcc W, these systems have complex\n interfaces that are beyond the scales suitable for ab initio methods, thus\n motivating the development of chemically accurate interatomic potentials. Here,\n a neural network potential (NNP) for Cu-W is developed within the\n Behler-Parrinello framework using a curated training dataset that captures\n metallurgically-relevant local atomic environments. The Cu-W NNP accurately\n predicts (i) the metallurgical properties (elasticity, stacking faults,\n dislocations, thermodynamic behavior) in elemental Cu and W, (ii) energies and\n structures of Cu-W intermetallics and solid solutions, and (iii) a range of fcc\n Cu/bcc W interfaces, and exhibits physically-reasonable behavior for solid\n W/liquid Cu systems. As will be demonstrated in forthcoming work, this near-ab\n initio-accurate NNP can be applied to understand complex phenomena involving\n interface-driven processes and properties in Cu-W composites.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1807.03891",
        "title": "Decay of correlations and uniqueness of the infinite-volume Gibbs\nmeasure of the canonical ensemble of 1d-lattice systems",
        "abstract": "We consider a one-dimensional lattice system of unbounded, real-valued spins\n with arbitrary strong, quadratic, finite-range interaction. We show the\n equivalence of correlations of the grand canonical (gce) and the canonical\n ensemble (ce). As a corollary we obtain that the correlations of the ce decay\n exponentially plus a volume correction term. Then, we use the decay of\n correlation to verify a conjecture that the infinite-volume Gibbs measure of\n the ce is unique on a one-dimensional lattice. For the equivalence of\n correlations, we modify a method that was recently used by the authors to show\n the equivalence of the ce and the gce on the level of thermodynamic functions.\n In this article we also show that the equivalence of the ce and the gce holds\n on the level of observables. One should be able to extend the methods and\n results to graphs with bounded degree as long as the gce has a sufficient\n strong decay of correlations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1212.6543",
        "title": "Rethinking set theory",
        "abstract": "Mathematicians manipulate sets with confidence almost every day, rarely\nmaking mistakes. Few of us, however, could accurately quote what are often\nreferred to as \"the\" axioms of set theory. This suggests that we all carry\naround with us, perhaps subconsciously, a reliable body of operating principles\nfor manipulating sets. What if we were to take some of those principles and\nadopt them as our axioms instead? The message of this article is that this can\nbe done, in a simple, practical way (due to Lawvere). The resulting axioms are\nten thoroughly mundane statements about sets.\nThis is an expository article for a general mathematical readership.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1701.02356",
        "title": "Closing the Gap between Teaching and Assessment",
        "abstract": "Evidence-based teaching is based upon a model of learning in which assessment\n plays a central role.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0803.3615",
        "title": "Inversion of marine heat flow measurements by expansion of the\ntemperature decay function",
        "abstract": "Marine heat flow data, obtained with a Lister-type probe, consists of two\n temperature decay curves, frictional and heat pulse decay. Both follow the same\n physical model of a cooling cylinder. The mathematical model describing the\n decays is non-linear as to the thermal sediment parameters thus a direct\n inversion is not possible. To overcome this difficulty, the model equations are\n expanded using a first-orderTaylor series. The linearised model equations are\n used in an iterative scheme to invert the temperature decay for undisturbed\n temperature and thermal conductivity of the sediment. The inversion scheme is\n tested first for its theoretical limitations using synthetic data. Inversion of\n heat flow measurements obtained during a cruise of R/V SONNE in 1996 and needle\n probe measurements in material of known thermal conductivity show that the\n algorithm is robust and gives reliable results. The programme can be obtained\n from the authors.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1506.07403",
        "title": "Three-body quantum Coulomb problem: analytic continuation",
        "abstract": "The second (unphysical) critical charge in the 3-body quantum Coulomb system\n of a nucleus of positive charge $Z$ and mass $m_p$, and two electrons,\n predicted by F~Stillinger has been calculated to be equal to $Z_{B}^{\\infty}\\\n =\\ 0.904854$ and $Z_{B}^{m_p}\\ =\\ 0.905138$ for infinite and finite (proton)\n mass $m_p$, respectively. It is shown that in both cases, the ground state\n energy $E(Z)$ (analytically continued beyond the first critical charge $Z_c$,\n for which the ionization energy vanishes, to $Re Z < Z_c$) has a square-root\n branch point with exponent 3/2 at $Z=Z_B$ in the complex $Z$-plane. Based on\n analytic continuation, the second, excited, spin-singlet bound state of\n negative hydrogen ion H${}^-$ is predicted to be at -0.51554 a.u. (-0.51531\n a.u. for the finite proton mass $m_p$). The first critical charge $Z_c$ is\n found accurately for a finite proton mass $m_p$ in the Lagrange mesh method,\n $Z^{m_p}_{c}\\ =\\ 0.911\\, 069\\, 724\\, 655$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1001.5428",
        "title": "Search for Extra dimensions in a single-jet and missing energy channel\nat CMS experiment",
        "abstract": "A possible solution to the hierarchy problem is the presence of extra spatial\n dimensions beyond the three ones which are known from our everyday experience.\n The phenomenological ADD model of large extra-dimensions predicts a missing\n transverse energy plus a single-jet signature. This contribution addresses the\n sensitivity of the CMS detector at the LHC pp collider to parameters of this\n model, focusing on the conditions expected for second half of 2010 running\n (sqrt(s) = 10 TeV, O(100) pb-1). It is shown that a significant improvement of\n the existing limits can be obtained in such an early stage.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1911.13256",
        "title": "Betti Numbers of Random Hypersurface Arrangements",
        "abstract": "We study the expected behavior of the Betti numbers of arrangements of the\n zeros of random (distributed according to the Kostlan distribution) polynomials\n in $\\mathbb{R}\\mathrm{P}^n$. Using a random spectral sequence, we prove an\n asymptotically exact estimate on the expected number of connected components in\n the complement of $s$ such hypersurfaces in $\\mathbb{R}\\mathrm{P}^n$. We also\n investigate the same problem in the case where the hypersurfaces are defined by\n random quadratic polynomials. In this case, we establish a connection between\n the Betti numbers of such arrangements with the expected behavior of a certain\n model of a randomly defined geometric graph. While our general result implies\n that the average zeroth Betti number of the union of random hypersurface\n arrangements is bounded from above by a function that grows linearly in the\n number of polynomials in the arrangement, using the connection with random\n graphs, we show an upper bound on the expected zeroth Betti number of random\n quadrics arrangements that is sublinear in the number of polynomials in the\n arrangement. This bound is a consequence of a general result on the expected\n number of connected components in our random graph model which could be of\n independent interest.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1904.01565",
        "title": "Wave-like Properties of Phasor Fields: Experimental Demonstrations",
        "abstract": "Recently, an optical meta concept called the Phasor Field (P-Field) was\n proposed that yields great quality in the reconstruction of hidden objects\n imaged by non-line-of-sight (NLOS) imaging. It is based on virtual sinusoidal\n modulation of the light with frequencies in the MHz range. Phasor Field\n propagation was shown to be described by the Rayleigh-Sommerfeld diffraction\n integral. We extend this concept and stress the analogy between electric field\n and Phasor Field. We introduce Phasor Field optical elements and present\n experiments demonstrating the validity of the approach. Straightforward use of\n the Phasor Field concept in real-world applications is also discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1905.10611",
        "title": "Coherent elastic nuclear scattering of $^{51}$Cr neutrinos",
        "abstract": "Searches for new physics in the coherent elastic neutrino-nucleus scattering\n require a precise knowledge of the neutrino flux and energy spectrum. In this\n paper we investigate the feasibility and the performances of an experiment\n based on a $^{51}$Cr source, whose neutrino spectrum is known and whose\n activity can be heat-monitored at few permil level. With a 5 MCi source placed\n at ~ 25 cm from the detector, under an exposure of two $^{51}$Cr half-lives\n (55.4 days), we evaluate 3900 (900) counts on a 2000 cm$^3$ target of germanium\n (sapphire) featuring an energy threshold of 8 (20) eV. To further increase the\n exposure, multiple activations of the same source could be possible.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1609.00508",
        "title": "The solar chromosphere as induction disk and the inverse Joule-Thomson\neffect",
        "abstract": "The connection between nuclear fusion in the Sun's core and solar irradiance\n is obscured among other things by uncertainty over the mechanism of coronal\n heating. Data for solar wind density and velocity, sunspot number, and EUV flux\n suggest that electromagnetic energy from the Sun's convection zone is converted\n by induction through the chromosphere into thermal energy. The helium and\n hydrogen mixture exhaled by the Sun is then heated by the inverse Joule-Thomson\n effect when it expands via the corona into space. The almost complete shutdown\n of the solar wind on 10-11 May 1999 demonstrated that its velocity is a more\n faithful indicator of solar activity than are sunspots as it reflects\n short-term variations in coronal heating rather than quasicyclical fluctuations\n in the Sun's magnetism. Its reconstruction from the cosmic ray flux using\n isotopes spanning over 800,000 yr should therefore benefit the analysis and\n long-term forecasting of Earth and space weather.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/9705112",
        "title": "Intermediate Volumes and the Role of Instantons",
        "abstract": "We review recent results for the low-lying glueball spectrum on the\n three-sphere in intermediate volumes that incorporate instanton effects. The\n latter are implemented through boundary conditions on the fundamental domain\n obtained by minimising the norm of the gauge field along the gauge orbit.\n Non-perturbative corrections due to the boundary conditions in field space are\n seen to be crucial.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0412667",
        "title": "Halo Luminosity Function From Photometric Calibration of the Revised\nNLTT",
        "abstract": "We calibrate the photographic photometry of the revised New Luyten Two-Tenths\n catalog (rNLTT) by matching 3448 rNLTT stars to the Sloan Digital Sky Survey\n (SDSS). The correction is linear in magnitude and goes from zero at V ~ 14 to\n 0.32 mag at V=19, in the sense that rNLTT was previously too bright. The\n correction implies that the underlying USNO-A2.0 photometry, on which rNLTT\n photometry is based, is non-linear. The new calibration somewhat improves the\n appearance of the (V,V-J) reduced proper motion diagram in the sense of better\n separation between disk and halo stars. We repeat Gould's analysis of 5000 halo\n stars in rNLTT. The most important change is to move the peak of the halo\n luminosity function about 0.5 mag dimmer, from M_V=10.5 to M_V=11, putting it\n into good agreement with the parallax-based determination of Dahn et al.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2302.09499",
        "title": "Microscopic Energy Storage Mechanism of Dielectric Polymer-Coated\nSupercapacitors",
        "abstract": "Supercapacitors have been attracting significant attention as promising\n energy storage devices. However, the voltage window limitation associated with\n electrolyte solutions has hindered the improvement of their capacitance. To\n address this issue and enhance the energy storage capabilities of general\n traditional supercapacitors, we put forward the dipole induced effects observed\n in the theoretical framework of the electric double-layer structure. The\n molecular dynamics results demonstrate that, compared to traditional systems,\n an improvement of over 50% in integral capacitance at low voltages is achieved.\n Moreover, a new material-based experimental results obtained from a dielectric\n supercapacitor employing a hydrated electrolyte solution corroborated the\n effectiveness of our proposed model, yielding consistent outcomes. We attribute\n the large capacitance variation to the reorientation of the dipoles, which\n induces the neutral-to-bilayer transition and the overscreening-to-steric\n transition, consistent with the polarization process of the polymer in the\n experiment. We further investigate the capacitance variations under different\n dipole parameters, such as varying the number of layers, different number\n densities and different spacings, thereby enriching the experimental results\n with additional conclusions not previously obtained. This work presents a novel\n approach that exploits dipole-induced capacitance effects, paving the way for\n further advances in the field of energy storage technology.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2010.00318",
        "title": "McStas (i): Introduction, use, and basic principles for ray-tracing\nsimulations",
        "abstract": "We present an overview of, and an introduction to, the general-purpose\n neutron simulation package McStas. We present the basic principles behind Monte\n Carlo ray-tracing simulations of neutrons performed in the package and present\n a few simple examples. We present the implementation of McStas, the status of\n the package and its use in the neutron community. Finally, we briefly discuss\n the planned development of the package.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1410.3933",
        "title": "Weighted Discriminants and Mass Formulas for Number Fields",
        "abstract": "We define the notion of a weighted discriminant and corresponding counting\n function for number fields, and what it means for these counting functions to\n have a mass formula for a set of primes. We extend a result of Kedlaya to show\n that any proper counting function for a finite group $\\Gamma$ has a mass\n formula for the set of primes not dividing $|\\Gamma|$. We also prove that if\n $\\Gamma$ is an $\\ell$-group for some prime $\\ell$, then there are only finitely\n many weighted discriminant counting functions for $\\Gamma$-extensions of $\\Q$\n that have a mass formula for all primes. Finally, we enumerate all such\n counting functions for $\\Gamma=D_4$ and $\\Gamma=Q_8$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0704.2862",
        "title": "Feasibility Study of Logic Circuits with Spin Wave Bus",
        "abstract": "We present a feasibility study of logic circuits utilizing spin waves for\n information transmission and processing. As an alternative approach to the\n transistor-based architecture, logic circuits with spin wave bus do not use\n charge as an information carrier. In this work we describe the general concept\n of logic circuits with spin wave bus and illustrate its performance by\n numerical simulations based on available experimental data. Theoretical\n estimates and results of numerical simulations on signal attenuation, signal\n phase velocity, and the minimum spin wave energy required per bit in the spin\n bus are obtained. The transport parameters are compared with ones for\n conventional electronic transmission lines. Spin Wave Bus is not intended to\n substitute traditional metal interconnects since it has higher signal\n attenuation and lower signal propagation speed. The potential value of spin\n wave bus is, however, an interface between electronic circuits and integrated\n spintronics circuits. The logic circuits with spin wave bus allow us to provide\n wireless read-in and read-out.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1709.01568",
        "title": "Model-Based Control Using Koopman Operators",
        "abstract": "This paper explores the application of Koopman operator theory to the control\n of robotic systems. The operator is introduced as a method to generate\n data-driven models that have utility for model-based control methods. We then\n motivate the use of the Koopman operator towards augmenting model-based\n control. Specifically, we illustrate how the operator can be used to obtain a\n linearizable data-driven model for an unknown dynamical process that is useful\n for model-based control synthesis. Simulated results show that with increasing\n complexity in the choice of the basis functions, a closed-loop controller is\n able to invert and stabilize a cart- and VTOL-pendulum systems. Furthermore,\n the specification of the basis function are shown to be of importance when\n generating a Koopman operator for specific robotic systems. Experimental\n results with the Sphero SPRK robot explore the utility of the Koopman operator\n in a reduced state representation setting where increased complexity in the\n basis function improve open- and closed-loop controller performance in various\n terrains, including sand.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1305.5924",
        "title": "The $L^2$-cohomology of a bounded smooth Stein Domain is not necessarily\nHausdorff",
        "abstract": "We give an example of a pseudoconvex domain in a complex manifold whose\n $L^2$-Dolbeault cohomology is non-Hausdorff, yet the domain is Stein. The\n domain is a smoothly bounded Levi-flat domain in a two complex-dimensional\n compact complex manifold. The domain is biholomorphic to a product domain in\n $\\mathbb{C}^2$, hence Stein. This implies that for $q>0$, the usual Dolbeault\n cohomology with respect to smooth forms vanishes in degree $(p,q)$. But the\n $L^2$-Cauchy-Riemann operator on the domain does not have closed range on\n $(2,1)$-forms and consequently its $L^2$-Dolbeault cohomology is not Hausdorff.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1605.08648",
        "title": "Addendum to `Algebraic equations for the exceptional eigenspectrum of\nthe generalized Rabi model'",
        "abstract": "In our recent paper (Li and Batchelor J. Phys. A: Math. Theor. 48, 454005\n (2015)) we obtained exceptional points in the eigenspectrum of the generalized\n Rabi model in terms of a set of algebraic equations. We also gave a proof for\n the number of roots of the constraint polynomials defining these exceptional\n solutions as a function of the system parameters and discussed the number of\n crossing points in the eigenspectrum. This approach however, only covered a\n subset of all exceptional points in the eigenspectrum. In this addendum, we\n clarify the distinction between the exceptional parts of the eigenspectrum for\n this model and discuss the subset of exceptional points not determined in our\n paper.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1011.5410",
        "title": "Fe$^{\\bf{15+}}$ dielectronic recombination and the effects of\nconfiguration interaction between resonances with different captured electron\nprincipal quantum number",
        "abstract": "Dielectronic recombination (DR) of Na-like Fe$^{15+}$ forming Mg-like\n Fe$^{14+}$ via excitation of a $2l$ core electron has been investigated. We\n find that configuration interaction (CI) between DR resonances with different\n captured electron principal quantum numbers $n$ can lead to a significant\n reduction in resonance strengths for $n \\geq 5$. Previous theoretical work for\n this system has not considered this form of CI. Including it accounts for most\n of the discrepancy between previous theoretical and experimental results.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2310.13949",
        "title": "DER Pricing Power in the Presence of Multi-Location Consumers with Load\nMigration Capabilities",
        "abstract": "Renewable distributed energy resources (DERs) have the potential to provide\n multi-location electricity consumers (MLECs) with electricity at prices lower\n than those offered by the grid using behind-the-meter advantages. This study\n examines the pricing power of such DER owners in a local environment with few\n competitors and how it depends on the MLEC's ability to migrate a portion of\n the load between locations. We simulate a dynamic game between an MLEC and the\n local DER owners, where the MLEC is modeled as a cost-minimizer and the DER\n owners as strategic profit maximizers. We show that, when the MLEC is\n inflexible, the DER owners' optimal behavior is to offer their electricity\n close to maximal prices, that is, at the grid price level. However, when the\n MLEC can migrate a fraction of the load to the other locations, the prices\n offered by the DER owners quickly decrease to the minimum level, that is, the\n DERs' grid feed-in tariffs quickly decrease to a lower level, depending on the\n load migration capability.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2007.10518",
        "title": "The expected number of viable autocatalytic sets in chemical reaction\nsystems",
        "abstract": "The emergence of self-sustaining autocatalytic networks in chemical reaction\n systems has been studied as a possible mechanism for modelling how living\n systems first arose. It has been known for several decades that such networks\n will form within systems of polymers (under cleavage and ligation reactions)\n under a simple process of random catalysis, and this process has since been\n mathematically analysed. In this paper, we provide an exact expression for the\n expected number of self-sustaining autocatalytic networks that will form in a\n general chemical reaction system, and the expected number of these networks\n that will also be uninhibited (by some molecule produced by the system). Using\n these equations, we are able to describe the patterns of catalysis and\n inhibition that maximise or minimise the expected number of such networks. We\n apply our results to derive a general theorem concerning the trade-off between\n catalysis and inhibition, and to provide some insight into the extent to which\n the expected number of self-sustaining autocatalytic networks coincides with\n the probability that at least one such system is present.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/quant-ph/0304143",
        "title": "Duality, Quantum Mechanics and (Almost) Complex Manifolds",
        "abstract": "The classical mechanics of a finite number of degrees of freedom requires a\n symplectic structure on phase space C, but it is independent of any complex\n structure. On the contrary, the quantum theory is intimately linked with the\n choice of a complex structure on C. When the latter is a complex-analytic\n manifold admitting just one complex structure, there is a unique quantisation\n whose classical limit is C. Then the notion of coherence is the same for all\n observers. However, when C admits two or more nonbiholomorphic complex\n structures, there is one different quantisation per different complex structure\n on C. The lack of analyticity in transforming between nonbiholomorphic complex\n structures can be interpreted as the loss of quantum-mechanical coherence under\n the corresponding transformation. Observers using one complex structure\n perceive as coherent the states that other observers, using a different complex\n structure, do not perceive as such. This is the notion of a quantum-mechanical\n duality transformation: the relativity of the notion of a quantum.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1006.5052",
        "title": "Bond-order wave phase, spin solitons and thermodynamics of a frustrated\nlinear spin-1/2 Heisenberg antiferromagnet",
        "abstract": "The linear spin-1/2 Heisenberg antiferromagnet with exchanges $J_1$, $J_2$\nbetween first and second neighbors has a bond-order wave (BOW) phase that\nstarts at the fluid-dimer transition at $J_2/J_1 = 0.2411$ and is particularly\nsimple at $J_2/J_1 = 1/2$. The BOW phase has a doubly degenerate singlet ground\nstate, broken inversion symmetry and a finite energy gap $E_m$ to the lowest\ntriplet state.\nThe interval $0.4<J_2/J_1<1.0$ has large $E_m$ and small finite size\ncorrections. Exact solutions are presented up to $N=28$ spins with either\nperiodic or open boundary conditions and for thermodynamics up to $N=18$. The\nelementary excitations of the BOW phase with large $E_m$ are topological\nspin-1/2 solitons that separate BOWs with opposite phase in a regular array of\nspins. The molar spin susceptibility $\\chi_M(T)$ is exponentially small for $T\n\\ll E_m$ and increases nearly linearly with $T$ to a broad maximum. $J_1$,\n$J_2$ spin chains approximate the magnetic properties of the BOW phase of\nHubbard-type models and provide a starting point for modeling alkali-TCNQ\nsalts.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2012.11588",
        "title": "A strong non-equilibrium bound for sorting of crosslinkers on growing\nbiopolymers",
        "abstract": "Understanding the role of non-equilibrium driving in self-organization is\n crucial for developing a predictive description of biological systems, yet it\n is impeded by their complexity. The actin cytoskeleton serves as a paradigm for\n how equilibrium and non-equilibrium forces combine to give rise to\n self-organization. Motivated by recent experiments that show that actin\n filament growth rates can tune the morphology of a growing actin bundle\n crosslinked by two competing types of actin binding proteins, we construct a\n minimal model for such a system and show that the dynamics are subject to a set\n of thermodynamic constraints that relate the non-equilibrium driving, bundle\n morphology, and molecular fluxes. The thermodynamic constraints reveal the\n importance of correlations between these molecular fluxes, and offer a route to\n estimating microscopic driving forces from microscopy experiments.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0509187",
        "title": "Characterizing a cosmic string with the statistics of string lensing",
        "abstract": "The deep imaging of the field of an observed lensing event by a cosmic string\n reveals many additional lensing events. We study the statistics of such string\n lensing. We derive explicit expressions for the distributions of image\n separations of lensing by a cosmic string and point out that they are quite\n sensitive to parameters which characterize the cosmic string, such as the\n redshift and tension of the cosmic string. Thus the statistics of string\n lensing events add new important information on the cosmic string which cannot\n be obtained from the detailed investigation of one lensing event.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0106228",
        "title": "Compactifications defined by arrangements I: the ball quotient case",
        "abstract": "We define a natural compactification of an arrangement complement in a ball\n quotient. We show that when this complement has a moduli space interpretation,\n then this compactification is often one that appears naturally by means of\n geometric invariant theory. We illustrate this with the moduli spaces of smooth\n quartic curves and rational elliptic surfaces.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1710.03176",
        "title": "Frames of exponentials and sub-multitiles in LCA groups",
        "abstract": "In this note we investigate the existence of frames of exponentials for\n $L^2(\\Omega)$ in the setting of LCA groups. Our main result shows that\n sub-multitiling properties of $\\Omega \\subset \\widehat{G}$ with respect to a\n uniform lattice $\\Gamma$ of $\\widehat{G}$ guarantee the existence of a frame of\n exponentials with frequencies in a finite number of translates of the\n annihilator of $\\Gamma$. We also prove the converse of this result and provide\n conditions for the existence of these frames. These conditions extend recent\n results on Riesz bases of exponentials and multitilings to frames.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2405.18814",
        "title": "Design and Implementation of a New Apparatus for Astrochemistry: Kinetic\nMeasurements of the CH + OCS Reaction and Frequency Comb Spectroscopy in a\nCold Uniform Supersonic Flow",
        "abstract": "We present the development of a new astrochemical research tool HILTRAC, the\n Highly Instrumented Low Temperature ReAction Chamber. The instrument is based\n on a pulsed form of the CRESU (Cin\\'etique de R\\'eaction en \\'Ecoulement\n Supersonique Uniforme, meaning reaction kinetics in a uniform supersonic flow)\n apparatus, with the aim of collecting kinetics and spectroscopic information on\n gas phase chemical reactions important in interstellar space or planetary\n atmospheres. We discuss the apparatus design and its flexibility, the\n implementation of pulsed laser photolysis followed by laser induced\n fluorescence (PLP-LIF), and the first implementation of direct infrared\n frequency comb spectroscopy (DFCS) coupled to the uniform supersonic flow.\n Achievable flow temperatures range from 32(3) - 111(9) K, characterising a\n total of five Laval nozzles for use with N2 and Ar buffer gases by pressure\n impact measurements. These results were further validated using LIF and DFCS\n measurements of the CH radical and OCS, respectively. Spectroscopic constants\n and linelists for OCS are reported for the 1001 band near $2890 - 2940 cm^{-1}$\n for both $OC^{32}S$ and $OC^{34}S$, measured using DFCS. Additional peaks in\n the spectrum are tentatively assigned to the OCS-Ar complex. The first reaction\n rate coefficients for the CH + OCS reaction measured between 32(3) K and 58(5)\n K are reported. The reaction rate coefficient at 32(3) K was measured to be\n $3.9(4) \\times 10^{10} cm^3 molecule^{-1} s^{-1}$ and the reaction was found to\n exhibit no observable temperature dependence over this low temperature range.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2208.10333",
        "title": "Are Dipolarization Fronts a Typical Feature of Magnetotail Plasma Jets\nFronts?",
        "abstract": "Plasma jets are ubiquitous in the Earth's magnetotail. Plasma jet fronts\n (JFs) are the seat of particle acceleration and energy conversion. JFs are\n commonly associated with dipolarization fronts (DFs) representing solitary\n sharp and strong increases in the northward component of the magnetic field.\n However, MHD and kinetic instabilities can develop at JFs and disturb the front\n structure which questions on the occurrence of DFs at the JFs. We investigate\n the structure of JFs using 5 years (2017-2021) of the Magnetospheric Multiscale\n observations in the CPS in the Earth's magnetotail. We compiled a database of\n 2394 CPS jets. We find that about half (42\\%) of the JFs are associated with\n large amplitude changes in $B_z$. DFs constitute a quarter of these\n large-amplitude events, while the rest are associated with more complicated\n magnetic field structures. We conclude that the ``classical\" picture of DFs at\n the JFs is not the most common situation.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2001.00989",
        "title": "Segmentation-Aware and Adaptive Iris Recognition",
        "abstract": "Iris recognition has emerged as one of the most accurate and convenient\n biometric for the human identification and has been increasingly employed in a\n wide range of e-security applications. The quality of iris images acquired\n at-a-distance or under less constrained imaging environments is known to\n degrade the iris matching accuracy. The periocular information is inherently\n embedded in such iris images and can be exploited to assist in the iris\n recognition under such non-ideal scenarios. Our analysis of such iris templates\n also indicates significant degradation and reduction in the region of interest,\n where the iris recognition can benefit from a similarity distance that can\n consider importance of different binary bits, instead of the direct use of\n Hamming distance in the literature. Periocular information can be dynamically\n reinforced, by incorporating the differences in the effective area of available\n iris regions, for more accurate iris recognition. This paper presents such a\n segmentation-assisted adaptive framework for more accurate less-constrained\n iris recognition. The effectiveness of this framework is evaluated on three\n publicly available iris databases using within-dataset and cross-dataset\n performance evaluation and validates the merit of the proposed iris recognition\n framework.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/cs/0610011",
        "title": "Creation and use of Citations in the ADS",
        "abstract": "With over 20 million records, the ADS citation database is regularly used by\nresearchers and librarians to measure the scientific impact of individuals,\ngroups, and institutions. In addition to the traditional sources of citations,\nthe ADS has recently added references extracted from the arXiv e-prints on a\nnightly basis. We review the procedures used to harvest and identify the\nreference data used in the creation of citations, the policies and procedures\nthat we follow to avoid double-counting and to eliminate contributions which\nmay not be scholarly in nature. Finally, we describe how users and institutions\ncan easily obtain quantitative citation data from the ADS, both interactively\nand via web-based programming tools.\nThe ADS is available at http://ads.harvard.edu.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1502.07467",
        "title": "Reachability is in DynFO",
        "abstract": "Patnaik and Immerman introduced the dynamic complexity class DynFO of\ndatabase queries that can be maintained by first-order dynamic programs with\nthe help of auxiliary relations under insertions and deletions of edges\n(Patnaik and Immerman 1997). This article confirms their conjecture that the\nReachability query is in DynFO.\nAs a byproduct it is shown that the rank of a matrix with small values can be\nmaintained in DynFO(+,x). It is further shown that the (size of the) maximum\nmatching of a graph can be maintained in non-uniform DynFO, another extension\nof DynFO, with non-uniform initialisation of the auxiliary relations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2005.08745",
        "title": "TAO Conceptual Design Report: A Precision Measurement of the Reactor\nAntineutrino Spectrum with Sub-percent Energy Resolution",
        "abstract": "The Taishan Antineutrino Observatory (TAO, also known as JUNO-TAO) is a\n satellite experiment of the Jiangmen Underground Neutrino Observatory (JUNO). A\n ton-level liquid scintillator detector will be placed at about 30 m from a core\n of the Taishan Nuclear Power Plant. The reactor antineutrino spectrum will be\n measured with sub-percent energy resolution, to provide a reference spectrum\n for future reactor neutrino experiments, and to provide a benchmark measurement\n to test nuclear databases. A spherical acrylic vessel containing 2.8 ton\n gadolinium-doped liquid scintillator will be viewed by 10 m^2 Silicon\n Photomultipliers (SiPMs) of >50% photon detection efficiency with almost full\n coverage. The photoelectron yield is about 4500 per MeV, an order higher than\n any existing large-scale liquid scintillator detectors. The detector operates\n at -50 degree C to lower the dark noise of SiPMs to an acceptable level. The\n detector will measure about 2000 reactor antineutrinos per day, and is designed\n to be well shielded from cosmogenic backgrounds and ambient radioactivities to\n have about 10% background-to-signal ratio. The experiment is expected to start\n operation in 2022.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2308.10603",
        "title": "A step towards understanding why classification helps regression",
        "abstract": "A number of computer vision deep regression ap- proaches report improved results when adding a classifica- tion loss to the regression loss. Here, we explore why this is useful in practice and when it is beneficial. To do so, we start from precisely controlled dataset variations and data samplings and find that the effect of adding a classification loss is the most pronounced for regression with imbalanced data. We explain these empirical findings by formalizing the relation between the balanced and imbalanced regression losses. Finally, we show that our findings hold on two real imbalanced image datasets for depth estimation (NYUD2- DIR), and age estimation (IMDB-WIKI-DIR), and on the problem of imbalanced video progress prediction (Break-fast). Our main takeaway is: for a regression task, if the data sampling is imbalanced, then add a classification loss.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.10173",
        "title": "DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering",
        "abstract": "Realistic human-centric rendering plays a key role in both computer vision and computer graphics. Rapid progress has been made in the algorithm aspect over the years, yet existing human-centric rendering datasets and benchmarks are rather impoverished in terms of diversity (e.g., outfit\u2019s fabric/material, body\u2019s interaction with ob- jects, and motion sequences), which are crucial for render- ing effect. Researchers are usually constrained to explore and evaluate a small set of rendering problems on current datasets, while real-world applications require meth- ods to be robust across different scenarios. In this work, we present DNA-Rendering, a large-scale, high-fidelity reposi- tory of human performance data for neural actor rendering. DNA-Rendering presents several alluring attributes. First, our dataset contains over 1500 human subjects, 5000 mo- tion sequences, and 67.5M frames\u2019 data volume. Upon the massive collections, we provide human subjects with grand categories of pose actions, body shapes, clothing, acces- sories, hairdos, and object intersection, which ranges the geometry and appearance variances from everyday life to professional occasions. Second, we provide rich assets for each subject \u2013 2D/3D human body keypoints, foreground masks, SMPLX models, cloth/accessory materials, multi- view images, and videos. These assets boost the current method\u2019s accuracy on downstream rendering tasks. Third, we construct a professional multi-view system to capture data, which contains 60 synchronous cameras with max 4096 \u00d7 3000 resolution, 15 fps speed, and stern camera calibration steps, ensuring high-quality resources for task training and evaluation.\nAlong with the dataset, we provide a large-scale and quantitative benchmark in full-scale, with multiple tasks to evaluate the existing progress of novel view synthesis, novel pose animation synthesis, and novel identity render- ing methods. In this manuscript, we describe our DNA- Rendering effort as a revealing of new observations, chal- lenges, and future directions to human-centric rendering. The dataset, code, and benchmarks will be publicly avail- able at https://dna-rendering.github.io/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.17597",
        "title": "Robo3D: Towards Robust and Reliable 3D Perception against Corruptions",
        "abstract": "The robustness of 3D perception systems under natu- ral corruptions from environments and sensors is pivotal for safety-critical applications. Existing large-scale 3D perception datasets often contain data that are meticu- lously cleaned. Such configurations, however, cannot reflect the reliability of perception models during the deployment stage. In this work, we present Robo3D, the first compre- hensive benchmark heading toward probing the robustness of 3D detectors and segmentors under out-of-distribution scenarios against natural corruptions that occur in real- world environments. Specifically, we consider eight corrup- tion types stemming from severe weather conditions, exter- nal disturbances, and internal sensor failure. We uncover that, although promising results have been progressively achieved on standard benchmarks, state-of-the-art 3D per- ception models are at risk of being vulnerable to corrup- tions. We draw key observations on the use of data represen- tations, augmentation schemes, and training strategies, that could severely affect the model\u2019s performance. To pursue better robustness, we propose a density-insensitive training framework along with a simple flexible voxelization strat- egy to enhance the model resiliency. We hope our bench- mark and approach could inspire future research in design- ing more robust and reliable 3D perception models. Our robustness benchmark suite is publicly available1.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.14753",
        "title": "Efficient Discovery and Effective Evaluation of Visual Perceptual Similarity: A Benchmark and Beyond",
        "abstract": "Visual similarity discovery (VSD) is an important task with broad e-commerce applications. Given an image of a certain object, the goal of VSD is to retrieve images of different objects with high perceptual visual similarity. Al- though being a highly addressed problem, the evaluation of proposed methods for VSD is often based on a proxy of an identification-retrieval task, evaluating the ability of a model to retrieve different images of the same object. We posit that evaluating VSD methods based on identification tasks is limited, and faithful evaluation must rely on expert annotations. In this paper, we introduce the first large-scale fashion visual similarity benchmark dataset, consisting of more than 110K expert-annotated image pairs. Besides this major contribution, we share insight from the challenges we faced while curating this dataset. Based on these insights, we propose a novel and efficient labeling procedure that can be applied to any dataset. Our analysis examines its limita- tions and inductive biases, and based on these findings, we propose metrics to mitigate those limitations. Though our primary focus lies on visual similarity, the methodologies we present have broader applications for discovering and evaluating perceptual similarity across various domains.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.03483",
        "title": "DetermiNet: A Large-Scale Diagnostic Dataset for Complex Visually-Grounded Referencing using Determiners",
        "abstract": "State-of-the-art visual grounding models can achieve high detection accuracy, but they are not designed to dis- tinguish between all objects versus only certain objects of interest. In natural language, in order to specify a partic- ular object or set of objects of interest, humans use deter- miners such as \u201cmy\u201d, \u201ceither\u201d and \u201cthose\u201d. Determiners, as an important word class, are a type of schema in nat- ural language about the reference or quantity of the noun. Existing grounded referencing datasets place much less em- phasis on determiners, compared to other word classes such as nouns, verbs and adjectives. This makes it difficult to de- velop models that understand the full variety and complex-ity of object referencing. Thus, we have developed and re- leased the DetermiNet dataset 1, which comprises 250,000 synthetically generated images and captions based on 25 determiners. The task is to predict bounding boxes to iden- tify objects of interest, constrained by the semantics of the given determiner. We find that current state-of-the-art vi- sual grounding models do not perform well on the dataset, highlighting the limitations of existing models on reference and quantification tasks.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2212.02710",
        "title": "Beyond Object Recognition: A New Benchmark towards Object Concept Learning",
        "abstract": "Understanding objects is a central building block of AI,especially for embodied AI. Even though object recogni- tion excels with deep learning, current machines struggle to learn higher-level knowledge, e.g., what attributes an object has, and what we can do with it. Here, we propose a chal- lenging Object Concept Learning (OCL) task to push the envelope of object understanding. It requires machines to reason out affordances and simultaneously give the reason: what attributes make an object possess these affordances. To support OCL, we build a densely annotated knowledge base including extensive annotations for three levels of ob- ject concept (category, attribute, affordance), and the clear causal relations of three levels. By analyzing the causal structure of OCL, we present a baseline, Object Concept Reasoning Network (OCRN). It leverages concept instanti- ation and causal intervention to infer the three levels. In experiments, OCRN effectively infers the object knowledge while following the causalities well. Our data and code are available at https://mvig-rhos.com/ocl.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.05390",
        "title": "HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models",
        "abstract": "In recent years, Text-to-Image (T2I) models have been ex- tensively studied, especially with the emergence of diffusion models that achieve state-of-the-art results on T2I synthesis tasks. However, existing benchmarks heavily rely on subjec- tive human evaluation, limiting their ability to holistically as- sess the model\u2019s capabilities. Furthermore, there is a signifi- cant gap between efforts in developing new T2I architectures and those in evaluation. To address this, we introduce HRS-Bench, a concrete evaluation benchmark for T2I models that is Holistic, Reliable, and Scalable. Unlike existing bench- marks that focus on limited aspects, HRS-Bench measures 13 skills that can be categorized into five major categories: accuracy, robustness, generalization, fairness, and bias. In addition, HRS-Bench covers 50 scenarios, including fash- ion, animals, transportation, food, and clothes. We evaluate nine recent large-scale T2I models using metrics that cover a wide range of skills. A human evaluation aligned with 95% of our evaluations on average was conducted to probe the effectiveness of HRS-Bench. Our experiments demon- strate that existing models often struggle to generate images with the desired count of objects, visual text, or grounded emotions. We hope that our benchmark help ease future text-to-image generation research. The code and data are available at https://eslambakr.github.io/hrsbench.github.io/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.11914",
        "title": "Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds",
        "abstract": "Urban modeling from LiDAR point clouds is an im- portant topic in computer vision, computer graphics, pho- togrammetry and remote sensing. 3D city models have found a wide range of applications in smart cities, au- tonomous navigation, urban planning and mapping etc. However, existing datasets for 3D modeling mainly focus on common objects such as furniture or cars. Lack of build- ing datasets has become a major obstacle for applying deep learning technology to specific domains such as urban mod- eling. In this paper, we present an urban-scale dataset con- sisting of more than 160 thousands buildings along with corresponding point clouds, mesh and wireframe models, covering 16 cities in Estonia about 998 K m2 . We exten- sively evaluate performance of state-of-the-art algorithms including handcrafted and deep feature based methods. Ex- perimental results indicate that Building3D has challenges\nof high intra-class variance, data imbalance and large- scale noises. The Building3D is the first and largest urban- scale building modeling benchmark, allowing a comparison of supervised and self-supervised learning methods. We be- lieve that our Building3D will facilitate future research on urban modeling, aerial path planning, mesh simplification, and semantic/part segmentation etc.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2208.08080",
        "title": "Multimodal Lecture Presentations Dataset: Understanding Multimodality in Educational Slides",
        "abstract": "Lecture slide presentations, a sequence of pages that contain text and figures ac- companied by speech, are constructed and presented carefully in order to optimally transfer knowledge to students. Previous studies in multimedia and psychology attribute the effectiveness of lecture presentations to their multimodal nature. As a step toward developing AI to aid in student learning as intelligent teacher assis- tants, we introduce the Multimodal Lecture Presentations dataset as a large-scale benchmark testing the capabilities of machine learning models in multimodal un- derstanding of educational content. Our dataset contains aligned slides and spoken language, for 180+ hours of video and 9000+ slides, with 10 lecturers from various subjects (e.g., computer science, dentistry, biology). We introduce two research tasks which are designed as stepping stones towards AI agents that can explain (automatically captioning a lecture presentation) and illustrate (synthesizing vi- sual figures to accompany spoken explanations) educational content. We provide manual annotations to help implement these two research tasks and evaluate state- of-the-art models on them. Comparing baselines and human student performances,\nwe find that current models struggle in (1) weak crossmodal alignment between slides and spoken text, (2) learning novel visual mediums, (3) technical language, and (4) long-range sequences. Towards addressing this issue, we also introduce PolyViLT, a multimodal transformer trained with a multi-instance learning loss that is more effective than current approaches. We conclude by shedding light on the challenges and opportunities in multimodal understanding of educational presentations.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.01590",
        "title": "Probabilistic Precision and Recall Towards Reliable Evaluation of Generative Models",
        "abstract": "Assessing the fidelity and diversity of the generative model is a difficult but important issue for technological advance- ment. So, recent papers have introduced k-Nearest Neighbor (kNN) based precision-recall metrics to break down the sta- tistical distance into fidelity and diversity. While they provide an intuitive method, we thoroughly analyze these metrics and identify oversimplified assumptions and undesirable prop- erties of kNN that result in unreliable evaluation, such as susceptibility to outliers and insensitivity to distributional changes. Thus, we propose novel metrics, P-precision and P- recall (PP&PR), based on a probabilistic approach that ad-\ndress the problems. Through extensive investigations on toy experiments and state-of-the-art generative models, we show that our PP&PR provide more reliable estimates for compar- ing fidelity and diversity than the existing metrics. The codes are available at https://github.com/kdst-team/ Probablistic_precision_recall.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.00146",
        "title": "The Qiyas Benchmark: Measuring ChatGPT Mathematical and Language Understanding in Arabic",
        "abstract": "Despite the growing importance of Arabic as a global language, there is a notable lack of language models pre- trained exclusively on Arabic data. This shortage has led to limited benchmarks available for assessing language model performance in Arabic. To address this gap, we introduce two novel benchmarks designed to evaluate models' mathematical reasoning and language understanding abilities in Arabic. These benchmarks are derived from a General Aptitude Test (GAT) called Qiyas exam, a standardized test widely used for university admissions in Saudi Arabia. For validation purposes, we assess the performance of ChatGPT-3.5-trubo and ChatGPT-4 on our benchmarks. Our findings reveal that these benchmarks pose a significant challenge, with ChatGPT-4 achieving an overall average accuracy of 64%, while ChatGPT-3.5-trubo achieved an overall accuracy of 49% across the various question types in the Qiyas benchmark. We believe the release of these benchmarks will pave the way for enhancing the mathematical reasoning and language understanding capabilities of future models tailored for the low-resource Arabic language.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.00466",
        "title": "BioKGBench: A Knowledge Graph Checking\nBenchmark of AI Agent for Biomedical Science",
        "abstract": "Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist, draws\nincreasing attention, where one common approach is to build a copilot agent driven\nby Large Language Models (LLMs). However, to evaluate such systems, people\neither rely on direct Question-Answering (QA) to the LLM itself, or in a biomedical\nexperimental manner. How to precisely benchmark biomedical agents from an AI\nScientist perspective remains largely unexplored. To this end, we draw inspiration\nfrom one most important abilities of scientists, understanding the literature, and\nintroduce BioKGBench. In contrast to traditional evaluation benchmark that\nonly focuses on factual QA, where the LLMs are known to have hallucination\nissues, we first disentangle \u201cUnderstanding Literature\u201d into two atomic abilities, i)\n\u201cUnderstanding\u201d the unstructured text from research papers by performing scientific\nclaim verification, and ii) Ability to interact with structured Knowledge-Graph\nQuestion-Answering (KGQA) as a form of \u201cLiterature\u201d grounding. We then\nformulate a novel agent task, dubbed KGCheck, using KGQA and domain-based\nRetrieval-Augmented Generation (RAG) to identify the factual errors of existing\nlarge-scale knowledge graph databases. We collect over two thousand data for two\natomic tasks and 225 high-quality annotated data for the agent task. Surprisingly,\nwe discover that state-of-the-art agents, both daily scenarios and biomedical ones,\nhave either failed or inferior performance on our benchmark. We then introduce\na simple yet effective baseline, dubbed BKGAgent. On the widely used popular\nknowledge graph, we discover over 90 factual errors which provide scenarios for\nagents to make discoveries and demonstrate the effectiveness of our approach. The\ncode and data are available at https://github.com/westlake-autolab/BioKGBench.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.00924",
        "title": "EXCGEC: A Benchmark of Edit-wise Explainable Chinese\nGrammatical Error Correction",
        "abstract": "Existing studies explore the explainability of\nGrammatical Error Correction (GEC) in a limited scenario, where they ignore the interaction between corrections and explanations. To\nbridge the gap, this paper introduces the task of\nEXplainable GEC (EXGEC), which focuses\non the integral role of both correction and explanation tasks. To facilitate the task, we propose EXCGEC, a tailored benchmark for Chinese EXGEC consisting of 8,216 explanationaugmented samples featuring the design of hybrid edit-wise explanations. We benchmark\nseveral series of LLMs in multiple settings, covering post-explaining and pre-explaining. To\npromote the development of the task, we introduce a comprehensive suite of automatic metrics and conduct human evaluation experiments\nto demonstrate the human consistency of the\nautomatic metrics for free-text explanations.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01393",
        "title": "POLygraph: Polish Fake News Dataset",
        "abstract": "This paper presents the POLygraph dataset, a\nunique resource for fake news detection in Polish. The dataset, created by an interdisciplinary\nteam, is composed of two parts: the \u201cfake-ornot\u201d dataset with 11,360 pairs of news articles\n(identified by their URLs) and corresponding labels, and the \u201cfake-they-say\u201d dataset with 5,082\nnews articles (identified by their URLs) and\ntweets commenting on them. Unlike existing\ndatasets, POLygraph encompasses a variety of\napproaches from source literature, providing a\ncomprehensive resource for fake news detection. The data was collected through manual\nannotation by expert and non-expert annotators.\nThe project also developed a software tool that\nuses advanced machine learning techniques to\nanalyze the data and determine content authenticity. The tool and dataset are expected to\nbenefit various entities, from public sector institutions to publishers and fact-checking organizations. Further dataset exploration will foster\nfake news detection and potentially stimulate\nthe implementation of similar models in other\nlanguages. The paper focuses on the creation\nand composition of the dataset, so it does not\ninclude a detailed evaluation of the software\ntool for content authenticity analysis, which is\nplanned at a later stage of the project.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.00379",
        "title": "GraphArena: Benchmarking Large Language Models\non Graph Computational Problems",
        "abstract": "The \u201carms race\u201d of Large Language Models (LLMs) demands novel, challenging, and diverse benchmarks to faithfully examine their progresses. We introduce\nGraphArena, a benchmarking tool designed to evaluate LLMs on graph computational problems using million-scale real-world graphs from diverse scenarios\nsuch as knowledge graphs, social networks, and molecular structures. GraphArena\noffers a suite of 10 computational tasks, encompassing four polynomial-time (e.g.,\nShortest Distance) and six NP-complete challenges (e.g., Travelling Salesman Problem). It features a rigorous evaluation framework that classifies LLM outputs as\ncorrect, suboptimal (feasible but not optimal), or hallucinatory (properly formatted\nbut infeasible). Evaluation of 10 leading LLMs, including GPT-4o and LLaMA3-\n70B-Instruct, reveals that even top-performing models struggle with larger, more\ncomplex graph problems and exhibit hallucination issues. Despite the application\nof strategies such as chain-of-thought prompting, these issues remain unresolved.\nGraphArena contributes a valuable supplement to the existing LLM benchmarks\nand is open-sourced at https://github.com/squareRoot3/GraphArena.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.00981",
        "title": "VisEval: A Benchmark for Data Visualization in the Era of Large\nLanguage Models",
        "abstract": "Translating natural language to visualization (NL2VIS) has shown great promise for visual data analysis, but it remains\na challenging task that requires multiple low-level implementations, such as natural language processing and visualization design.\nRecent advancements in pre-trained large language models (LLMs) are opening new avenues for generating visualizations from\nnatural language. However, the lack of a comprehensive and reliable benchmark hinders our understanding of LLMs\u2019 capabilities\nin visualization generation. In this paper, we address this gap by proposing a new NL2VIS benchmark called VisEval. Firstly, we\nintroduce a high-quality and large-scale dataset. This dataset includes 2,524 representative queries covering 146 databases, paired\nwith accurately labeled ground truths. Secondly, we advocate for a comprehensive automated evaluation methodology covering multiple\ndimensions, including validity, legality, and readability. By systematically scanning for potential issues with a number of heterogeneous\ncheckers, VisEval provides reliable and trustworthy evaluation outcomes. We run VisEval on a series of state-of-the-art LLMs. Our\nevaluation reveals prevalent challenges and delivers essential insights for future advancements.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.00993",
        "title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents",
        "abstract": "With the remarkable advancements of large language models (LLMs), LLM-based agents have\nbecome a research hotspot in human-computer\ninteraction. However, there is a scarcity of\nbenchmarks available for LLM-based mobile\nagents. Benchmarking these agents generally\nfaces three main challenges: (1) The inefficiency of UI-only operations imposes limitations to task evaluation. (2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM\nmobile agents. (3) Current evaluation metrics\nare insufficient to accurately assess the process of sequential actions. To this end, we\npropose Mobile-Bench, a novel benchmark for\nevaluating the capabilities of LLM-based mobile agents. First, we expand conventional\nUI operations by incorporating 103 collected\nAPIs to accelerate the efficiency of task completion. Subsequently, we collect evaluation\ndata by combining real user queries with augmentation from LLMs. To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three\ndistinct groups: SAST, SAMT, and MAMT,\nreflecting varying levels of task complexity.\nMobile-Bench comprises 832 data entries, with\nmore than 200 tasks specifically designed to\nevaluate multi-APP collaboration scenarios.\nFurthermore, we introduce a more accurate\nevaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach\nessential points during their planning and reasoning steps. Dataset and platform are available\nat https://github.com/XiaoMi/MobileBench.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01081",
        "title": "CVLUE: A New Benchmark Dataset\nfor Chinese Vision-Language Understanding Evaluation",
        "abstract": "Despite the rapid development of Chinese\nvision-language models (VLMs), most existing Chinese vision-language (VL) datasets are\nconstructed on Western-centric images from existing English VL datasets. The cultural bias in\nthe images makes these datasets unsuitable for\nevaluating VLMs in Chinese culture. To remedy this issue, we present a new Chinese VisionLanguage Understanding Evaluation (CVLUE)\nbenchmark dataset, where the selection of object categories and images is entirely driven\nby Chinese native speakers, ensuring that the\nsource images are representative of Chinese culture. The benchmark contains four distinct VL\ntasks ranging from image-text retrieval to visual question answering, visual grounding and\nvisual dialogue. We present a detailed statistical analysis of CVLUE and provide a baseline\nperformance analysis with several open-source\nmultilingual VLMs on CVLUE and its English\ncounterparts to reveal their performance gap\nbetween English and Chinese. Our in-depth\ncategory-level analysis reveals a lack of Chinese cultural knowledge in existing VLMs. We\nalso find that fine-tuning on Chinese culturerelated VL datasets effectively enhances VLMs\u2019\nunderstanding of Chinese culture. 1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01284",
        "title": "WE-MATH: Does Your Large Multimodal Model\nAchieve Human-like Mathematical Reasoning?",
        "abstract": "Visual mathematical reasoning, as a fundamental visual reasoning ability, has\nreceived widespread attention from the Large Multimodal Models (LMMs) community. Existing benchmarks focus more on the result-oriented performance,\nbut neglecting the underlying principles in knowledge acquisition and generalization. Inspired by human-like mathematical reasoning, we introduce WE-MATH,\nthe first benchmark specifically designed to explore the problem-solving principles beyond the end-to-end performance. We meticulously collect and categorize\n6.5K visual math problems, spanning 67 hierarchical knowledge concepts and\n5 layers of knowledge granularity. We firstly decompose composite problems\ninto sub-problems according to the required knowledge concepts and introduce a\nnovel four-dimensional metric, namely Insufficient Knowledge (IK), Inadequate\nGeneralization (IG), Complete Mastery (CM), and Rote Memorization (RM) to hierarchically assess inherent issues in LMMs\u2019 reasoning process. With WE-MATH,\nwe conduct a thorough evaluation of existing LMMs in visual mathematical reasoning and reveal a negative correlation between solving step and problem-specific\nperformance. We confirm the IK issue of LMMs can be effectively improved\nvia knowledge augmentation strategy. More notably, the primary challenge of\nGPT-4o has significantly transitioned from IK to IG, establishing it as the first\nLMM advancing towards the knowledge generalization stage. In contrast, other\nLMMs exhibit a marked inclination towards Rote Memorization \u2013 they correctly\nsolve composite problems involving multiple knowledge concepts, yet fail in answering sub-problems. We anticipate that WE-MATH will open new pathways for\nadvancements in visual mathematical reasoning for LMMs. The WE-MATH data\nand evaluation code are available at https://github.com/We-Math/We-Math.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01509",
        "title": "MIA-Bench: Towards Better Instruction Following\nEvaluation of Multimodal LLMs",
        "abstract": "We introduce MIA-Bench, a new benchmark designed to evaluate multimodal large\nlanguage models (MLLMs) on their ability to strictly adhere to complex instructions. Our benchmark comprises a diverse set of 400 image-prompt pairs, each\ncrafted to challenge the models\u2019 compliance with layered instructions in generating\naccurate responses that satisfy specific requested patterns. Evaluation results from a\nwide array of state-of-the-art MLLMs reveal significant variations in performance,\nhighlighting areas for improvement in instruction fidelity. Additionally, we create\nextra training data and explore supervised fine-tuning to enhance the models\u2019 ability\nto strictly follow instructions without compromising performance on other tasks.\nWe hope this benchmark not only serves as a tool for measuring MLLM adherence\nto instructions, but also guides future developments in MLLM training methods.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01523",
        "title": "MMLONGBENCH-DOC: Benchmarking Long-context\nDocument Understanding with Visualizations",
        "abstract": "Understanding documents with rich layouts and multi-modal components is a\nlong-standing and practical task. Recent Large Vision-Language Models (LVLMs)\nhave made remarkable strides in various tasks, particularly in single-page document\nunderstanding (DU). However, their abilities on long-context DU remain an open\nproblem. This work presents MMLONGBENCH-DOC, a long-context, multimodal benchmark comprising 1,062 expert-annotated questions. Distinct from\nprevious datasets, it is constructed upon 130 lengthy PDF-formatted documents\nwith an average of 49.4 pages and 20,971 textual tokens. Towards comprehensive\nevaluation, answers to these questions rely on pieces of evidence from (1) different\nsources (text, image, chart, table, and layout structure) and (2) various locations\n(i.e., page number). Moreover, 33.2% of the questions are cross-page questions\nrequiring evidence across multiple pages. 22.8% of the questions are designed to\nbe unanswerable for detecting potential hallucinations. Experiments on 14 LVLMs\ndemonstrate that long-context DU greatly challenges current models. Notably, the\nbest-performing model, GPT-4o, achieves an F1 score of only 42.7%, while the\nsecond-best, GPT-4V, scores 31.4%. Furthermore, 12 LVLMs (all except GPT-4o\nand GPT-4V) even present worse performance than their LLM counterparts which\nare fed with lossy-parsed OCR documents. These results validate the necessity of\nfuture research toward more capable long-context LVLMs.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01725",
        "title": "DISCOVERYBENCH: Towards Data-Driven Discovery\nwith Large Language Models",
        "abstract": "Can the rapid advances in code generation, function calling, and data analysis\nusing large language models (LLMs) help automate the search and verification of\nhypotheses purely from a set of provided datasets? To evaluate this question, we\npresent DISCOVERYBENCH, the first comprehensive benchmark that formalizes\nthe multi-step process of data-driven discovery. The benchmark is designed to\nsystematically assess current model capabilities in discovery tasks and provide a\nuseful resource for improving them. Our benchmark contains 264 tasks collected\nacross 6 diverse domains, such as sociology and engineering, by manually deriving\ndiscovery workflows from published papers to approximate the real-world challenges faced by researchers, where each task is defined by a dataset, its metadata,\nand a discovery goal in natural language. We additionally provide 903 synthetic\ntasks to conduct controlled evaluations across task complexity. Furthermore, our\nstructured formalism of data-driven discovery enables a facet-based evaluation that\nprovides useful insights into different failure modes. We evaluate several popular\nLLM-based reasoning frameworks using both open and closed LLMs as baselines\non DISCOVERYBENCH and find that even the best system scores only 25%. Our\nbenchmark, thus, illustrates the challenges in autonomous data-driven discovery\nand serves as a valuable resource for the community to make progress.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01896",
        "title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log\nAnalysis",
        "abstract": "Log analysis is crucial for ensuring the orderly and stable operation of information systems, particularly in the field of Artificial\nIntelligence for IT Operations (AIOps). Large Language Models (LLMs) have demonstrated significant potential in natural language\nprocessing tasks. In the AIOps domain, they excel in tasks such as anomaly detection, root cause analysis of faults, operations and\nmaintenance script generation, and alert information summarization. However, the performance of current LLMs in log analysis tasks\nremains inadequately validated. To address this gap, we introduce LogEval, a comprehensive benchmark suite designed to evaluate\nthe capabilities of LLMs in various log analysis tasks for the first time. This benchmark covers tasks such as log parsing, log anomaly\ndetection, log fault diagnosis, and log summarization. LogEval evaluates each task using 4,000 publicly available log data entries and\nemploys 15 different prompts for each task to ensure a thorough and fair assessment. By rigorously evaluating leading LLMs, we\ndemonstrate the impact of various LLM technologies on log analysis performance, focusing on aspects such as self-consistency and\nfew-shot contextual learning. We also discuss findings related to model quantification, Chinese-English question-answering evaluation,\nand prompt engineering. These findings provide insights into the strengths and weaknesses of LLMs in multilingual environments and the effectiveness of different prompt strategies. Various evaluation methods are employed for different tasks to accurately measure the\nperformance of LLMs in log analysis, ensuring a comprehensive assessment. The insights gained from LogEval\u2019s evaluation reveal the\nstrengths and limitations of LLMs in log analysis tasks, providing valuable guidance for researchers and practitioners. Key findings\nindicate that while LLMs show promise in certain areas, there are notable challenges in handling complex log data and maintaining\nhigh accuracy across diverse tasks. LogEval is poised to significantly advance the application and development of LLMs in log analysis,\noffering effective solutions for practical log analysis challenges. The data and code are publicly available at https URL to facilitate\nfurther research and development in this domain.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.02028",
        "title": "Why does in-context learning fail sometimes?\nEvaluating in-context learning on open and closed\nquestions.",
        "abstract": "We measure the performance of in-context learning as a function of task novelty\nand difficulty for open and closed questions. For that purpose, we created a novel\nbenchmark consisting of hard scientific questions, each paired with a context\nof various relevancy. We show that counter-intuitively, a context that is more\naligned with the topic does not always help more than a less relevant context. This\neffect is especially visible for open questions and questions of high difficulty or\nnovelty. This result reveals a fundamental difference between the treatment of closeform and open-form questions by large-language models and shows a need for a\nmore robust evaluation of in-context learning on the variety of different types of\nquestions. It also poses a new question of how to optimally select a context for large\nlanguage models, especially in the context of Retrieval Augmented Generation\n(RAG) systems. Our results suggest that the answer to this question can be highly\napplication-dependent and might be contingent on factors including the format\nof the question, the perceived difficulty level of the questions, and the novelty or\npopularity of the information we seek.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.08816",
        "title": "EgoObjects: A Large-Scale Egocentric Dataset for Fine-Grained Object Understanding",
        "abstract": "Object understanding in egocentric visual data is ar- guably a fundamental research topic in egocentric vision. However, existing object datasets are either non-egocentric or have limitations in object categories, visual content, and annotation granularities. In this work, we intro- duce EgoObjects, a large-scale egocentric dataset for fine- grained object understanding. Its Pilot version contains over 9K videos collected by 250 participants from 50+ countries using 4 wearable devices, and over 650K ob- ject annotations from 368 object categories. Unlike prior datasets containing only object category labels, EgoObjects also annotates each object with an instance-level identifier, and includes over 14K unique object instances. EgoOb- jects was designed to capture the same object under diverse background complexities, surrounding objects, distance, lighting and camera motion. In parallel to the data collec- tion, we conducted data annotation by developing a multi- stage federated annotation process to accommodate the growing nature of the dataset. To bootstrap the research on EgoObjects, we present a suite of 4 benchmark tasks around the egocentric object understanding, including a novel in- stance level- and the classical category level object detec- tion. Moreover, we also introduce 2 novel continual learn- ing object detection tasks. The dataset and API are avail- able at https://github.com/facebookresearch/EgoObjects.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.11111",
        "title": "CAME: Contrastive Automated Model Evaluation",
        "abstract": "The Automated Model Evaluation (AutoEval) framework entertains the possibility of evaluating a trained machine learning model without resorting to a labeled testing set. Despite the promise and some decent results, the existing AutoEval methods heavily rely on computing distribution shifts between the unlabelled testing set and the training set. We believe this reliance on the training set becomes another obstacle in shipping this technology to real-world ML de- velopment. In this work, we propose Contrastive Automatic Model Evaluation (CAME), a novel AutoEval framework that is rid of involving training set in the loop. The core idea of CAME bases on a theoretical analysis which bonds the\nmodel performance with a contrastive loss. Further, with extensive empirical validation, we manage to set up a pre- dictable relationship between the two, simply by deducing on the unlabeled/unseen testing set. The resulting frame- work CAME establishes a new SOTA results for AutoEval by surpassing prior work significantly. 1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.06362",
        "title": "Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception",
        "abstract": "We introduce the Aria Digital Twin (ADT) - an egocen- tric dataset captured using Aria glasses with extensive ob- ject, environment, and human level ground truth. This ADT release contains 200 sequences of real-world activities con- ducted by Aria wearers in two real indoor scenes with 398 object instances (324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two monochrome cam- era streams, one RGB camera stream, two IMU streams; b)\ncomplete sensor calibration; c) ground truth data including continuous 6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye gaze vectors, 3D hu- man poses, 2D image segmentations, image depth maps; and d) photo-realistic synthetic renderings. To the best of our knowledge, there is no existing egocentric dataset with a level of accuracy, photo-realism and comprehensiveness comparable to ADT. By contributing ADT to the research community, our mission is to set a new standard for evalu- ation in the egocentric machine perception domain, which includes very challenging research problems such as 3D ob- ject detection and tracking, scene reconstruction and un- derstanding, sim-to-real learning, human pose prediction - while also inspiring new machine perception tasks for aug- mented reality (AR) applications. To kick start exploration of the ADT research use cases, we evaluated several existing state-of-the-art methods for object detection, segmentation and image translation tasks that demonstrate the usefulness of ADT as a benchmarking dataset.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2211.04894",
        "title": "Exploring Video Quality Assessment on User Generated Contents from Aesthetic and Technical Perspectives",
        "abstract": "The rapid increase in user-generated content (UGC) videos calls for the development of effective video qual- ity assessment (VQA) algorithms. However, the objective of the UGC-VQA problem is still ambiguous and can be viewed from two perspectives: the technical perspective, measuring the perception of distortions; and the aesthetic perspective, which relates to preference and recommenda- tion on contents. To understand how these two perspectives affect overall subjective opinions in UGC-VQA, we con- duct a large-scale subjective study to collect human quality opinions on the overall quality of videos as well as percep- tions from aesthetic and technical perspectives. The col- lected Disentangled Video Quality Database (DIVIDE-3k)\nconfirms that human quality opinions on UGC videos are universally and inevitably affected by both aesthetic and technical perspectives. In light of this, we propose the Disentangled Objective Video Quality Evaluator (DOVER) to learn the quality of UGC videos based on the two per- spectives. The DOVER proves state-of-the-art performance in UGC-VQA under very high efficiency. With perspective opinions in DIVIDE-3k, we further propose DOVER++, the first approach to provide reliable clear-cut quality evalua- tions from a single aesthetic or technical perspective. Code at https://github.com/VQAssessment/DOVER.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.17590",
        "title": "Going Beyond Nouns With Vision & Language Models Using Synthetic Data",
        "abstract": "Large-scale pre-trained Vision & Language (VL) mod-\nels have shown remarkable performance in many applica-\ntions, enabling replacing a fixed set of supported classes\nwith zero-shot open vocabulary reasoning over (almost ar-\nbitrary) natural language prompts. However, recent works\nhave uncovered a fundamental weakness of these models.\nFor example, their difficulty to understand Visual Language Concepts (VLC) that go \u2018beyond nouns\u2019 such as the meaning of non-object words (e.g., attributes, actions, relations,\nstates, etc.), or difficulty in performing compositional rea-\nsoning such as understanding the significance of the or-der of the words in a sentence. In this work, we investi-gate to which extent purely synthetic data could be lever-\naged to teach these models to overcome such shortcomings\nwithout compromising their zero-shot capabilities. We con-\ntribute Synthetic Visual Concepts (SyViC) - a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable data to improve VLC understanding and compositional reasoning of VL models. Additionally, we propose a general VL finetuning strategy for effectively leveraging SyViC towards achieving these im-\nprovements. Our extensive experiments and ablations on VL-Checklist, Winoground, and ARO benchmarks demon-strate that it is possible to adapt strong pre-trained VL mod-els with synthetic data significantly enhancing their VLC understanding (e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their zero-shot accuracy.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2211.15692",
        "title": "H3WB: Human3.6M 3D WholeBody Dataset and Benchmark",
        "abstract": "We present a benchmark for 3D human whole-body pose estimation, which involves identifying accurate 3D key- points on the entire human body, including face, hands, body, and feet. Currently, the lack of a fully annotated and accurate 3D whole-body dataset results in deep networks being trained separately on specific body parts, which are combined during inference. Or they rely on pseudo- groundtruth provided by parametric body models which are not as accurate as detection based methods. To overcome these issues, we introduce the Human3.6M 3D WholeBody (H3WB) dataset, which provides whole-body annotations for the Human3.6M dataset using the COCO Wholebody layout. H3WB comprises 133 whole-body keypoint anno-\ntations on 100K images, made possible by our new multi- view pipeline. We also propose three tasks: i) 3D whole- body pose lifting from 2D complete whole-body pose, ii) 3D whole-body pose lifting from 2D incomplete whole-body pose, and iii) 3D whole-body pose estimation from a single RGB image. Additionally, we report several baselines from popular methods for these tasks. Furthermore, we also pro- vide automated 3D whole-body annotations of TotalCap- ture and experimentally show that when used with H3WB it helps to improve the performance.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.02008",
        "title": "Zenseact Open Dataset: A large-scale and diverse multimodal dataset for autonomous driving",
        "abstract": "Existing datasets for autonomous driving (AD) often lack diversity and long-range capabilities, focusing instead on 360\u00b0 perception and temporal reasoning. To address this gap, we introduce Zenseact Open Dataset (ZOD), a large- scale and diverse multimodal dataset collected over two years in various European countries, covering an area 9\u00d7 that of existing datasets. ZOD boasts the highest range and resolution sensors among comparable datasets, cou- pled with detailed keyframe annotations for 2D and 3D ob-\njects (up to 245m), road instance/semantic segmentation, traffic sign recognition, and road classification. We believe that this unique combination will facilitate breakthroughs in long-range perception and multi-task learning. The dataset is composed of Frames, Sequences, and Drives, designed to encompass both data diversity and support for spatio- temporal learning, sensor fusion, localization, and map- ping. Frames consist of 100k curated camera images with two seconds of other supporting sensor data, while the 1473 Sequences and 29 Drives include the entire sensor suite for 20 seconds and a few minutes, respectively. ZOD is the only large-scale AD dataset released under a permissive license, allowing for both research and commercial use. More information, and an extensive devkit, can be found at zod.zenseact.com.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09011",
        "title": "CAD-Estate: Large-scale CAD Model Annotation in RGB Videos",
        "abstract": "We propose a method for annotating videos of com- plex multi-object scenes with a globally-consistent 3D rep- resentation of the objects. We annotate each object with a CAD model from a database, and place it in the 3D coordinate frame of the scene with a 9-DoF pose trans- formation. Our method is semi-automatic and works on commonly-available RGB videos, without requiring a depth sensor. Many steps are performed automatically, and the tasks performed by humans are simple, well-specified, and require only limited reasoning in 3D. This makes them fea- sible for crowd-sourcing and has allowed us to construct a large-scale dataset by annotating real-estate videos from YouTube. Our dataset CAD-Estate offers 101k instances of 12k unique CAD models placed in the 3D representations of 20k videos. In comparison to Scan2CAD, the largest ex- isting dataset with CAD model annotations on real scenes, CAD-Estate has 7\u00d7 more instances and 4\u00d7 more unique CAD models. We showcase the benefits of pre-training a Mask2CAD model on CAD-Estate for the task of automatic 3D object reconstruction and pose estimation, demonstrat- ing that it leads to performance improvements on the pop-\nular Scan2CAD benchmark. The dataset is available at https://github.com/google-research/cad-estate .",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.17595",
        "title": "Neglected Free Lunch \u2013\nLearning Image Classifiers Using Annotation Byproducts",
        "abstract": "Supervised learning of image classifiers distills human knowledge into a parametric model f\u03b8 through pairs of im- ages and corresponding labels {(Xi, Yi)}Ni=1. We argue that this simple and widely used representation of human knowl- edge neglects rich auxiliary information from the annotation procedure, such as the time-series of mouse traces and clicks left after image selection. Our insight is that such anno- tation byproducts Z provide approximate human attention that weakly guides the model to focus on the foreground cues, reducing spurious correlations and discouraging short- cut learning. To verify this, we create ImageNet-AB and COCO-AB. They are ImageNet and COCO training sets en-\nriched with sample-wise annotation byproducts, collected by replicating the respective original annotation tasks. We refer to the new paradigm of training models with annota- tion byproducts as learning using annotation byproducts (LUAB). We show that a simple multitask loss for regressing Z together with Y already improves the generalisability and robustness of the learned models. Compared to the original supervised learning, LUAB does not require extra annotation costs. ImageNet-AB and COCO-AB are at github.com/naver- ai/NeglectedFreeLunch.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2302.01872",
        "title": "MOSE: A New Dataset for Video Object Segmentation in Complex Scenes",
        "abstract": "Video object segmentation (VOS) aims at segmenting a particular object throughout the entire video clip sequence. The state-of-the-art VOS methods have achieved excellent performance (e.g., 90+% \ue236 &\ue232 ) on existing datasets. How- ever, since the target objects in these existing datasets are usually relatively salient, dominant, and isolated, VOS under complex scenes has rarely been studied. To revisit VOS and make it more applicable in the real world, we collect a new VOS dataset called coMplex video Object SEgmentation (MOSE) to study the tracking and segmenting objects in complex environments. MOSE contains 2,149 video clips and 5,200 objects from 36 categories, with 431,725 high-quality object segmentation masks. The most notable feature of MOSE dataset is complex scenes with crowded and occluded objects. The target objects in the videos are commonly occluded by others and disappear in some frames. To analyze the proposed MOSE dataset, we benchmark 18 existing VOS methods under 4 differ- ent settings on the proposed MOSE dataset and conduct comprehensive comparisons. The experiments show that current VOS algorithms cannot well perceive objects in complex scenes. For example, under the semi-supervised VOS setting, the highest \ue236 &\ue232 by existing state-of-the-art VOS methods is only 59.4% on MOSE, much lower than their \u223c90% \ue236 &\ue232 performance on DAVIS. The results reveal that although excellent performance has been achieved on existing benchmarks, there are unresolved challenges under complex scenes and more efforts are desired to explore these challenges in the future. The proposed MOSE dataset has been released at https://henghuiding.github.io/MOSE.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.14339",
        "title": "Chop & Learn: Recognizing and Generating Object-State Compositions",
        "abstract": "Recognizing and generating object-state compositions has been a challenging task, especially when generalizing to unseen compositions. In this paper, we study the task of cutting objects in different styles and the resulting object state changes. We propose a new benchmark suite Chop & Learn, to accommodate the needs of learning objects and different cut styles using multiple viewpoints. We also propose a new task of Compositional Image Generation, which can transfer learned cut styles to different objects, by generating novel object-state images. Moreover, we also use the videos for Compositional Action Recognition, and show valuable uses of this dataset for multiple video tasks. Project website: https://chopnlearn.github.io.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.17024",
        "title": "HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World",
        "abstract": "Building an interactive AI assistant that can perceive, reason, and collaborate with humans in the real world has been a long-standing pursuit in the AI community. This work is part of a broader research effort to develop intel- ligent agents that can interactively guide humans through performing tasks in the physical world. As a first step in this direction, we introduce HoloAssist, a large-scale egocentric human interaction dataset, where two people collaboratively complete physical manipulation tasks. The task performer executes the task while wearing a mixed-reality headset that captures seven synchronized data streams. The task instruc- tor watches the performer\u2019s egocentric video in real time and guides them verbally. By augmenting the data with action\nand conversational annotations and observing the rich be- haviors of various participants, we present key insights into how human assistants correct mistakes, intervene in the task completion procedure, and ground their instructions to the environment. HoloAssist spans 166 hours of data captured by 350 unique instructor-performer pairs. Furthermore, we construct and present benchmarks on mistake detection, in- tervention type prediction, and hand forecasting, along with detailed analysis. We expect HoloAssist will provide an im- portant resource for building AI assistants that can fluidly collaborate with humans in the real world. Data can be downloaded at https://holoassist.github.io/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.17368",
        "title": "SynBody: Synthetic Dataset with Layered Human Models for 3D Human Perception and Modeling",
        "abstract": "Synthetic data has emerged as a promising source for 3D human research as it offers low-cost access to large- scale human datasets. To advance the diversity and anno- tation quality of human models, we introduce a new syn- thetic dataset, SynBody, with three appealing features: 1) a clothed parametric human model that can generate a di- verse range of subjects; 2) the layered human represen- tation that naturally offers high-quality 3D annotations to support multiple tasks; 3) a scalable system for produc-\ning realistic data to facilitate real-world tasks. The dataset comprises 1.2M images with corresponding accurate 3D annotations, covering 10,000 human body models, 1,187 actions, and various viewpoints. The dataset includes two subsets for human pose and shape estimation as well as human neural rendering. Extensive experiments on Syn- Body indicate that it substantially enhances both SMPL and SMPL-X estimation. Furthermore, the incorporation of lay- ered annotations offers a valuable training resource for in- vestigating the Human Neural Radiance Fields(NeRF).",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.11636",
        "title": "OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?",
        "abstract": "This paper presents OxfordTVG-HIC (Humorous Im- age Captions), a large-scale dataset for humour genera- tion and understanding. Humour is an abstract, subjec- tive, and context-dependent cognitive construct involving several cognitive factors, making it a challenging task to generate and interpret. Hence, humour generation and un- derstanding can serve as a new task for evaluating the abil- ity of deep-learning methods to process abstract and sub- jective information. Due to the scarcity of data, humour- related generation tasks such as captioning remain under- explored. To address this gap, OxfordTVG-HIC offers ap- proximately 2.9M image-text pairs with humour scores to train a generalizable humour captioning model. Contrary to existing captioning datasets, OxfordTVG-HIC features a\nwide range of emotional and semantic diversity resulting in out-of-context examples that are particularly conducive to generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensive content. We also show how OxfordTVG- HIC can be leveraged for evaluating the humour of a gen- erated text. Through explainability analysis of the trained models, we identify the visual and linguistic cues influen- tial for evoking humour prediction (and generation). We observe qualitatively that these cues are aligned with the benign violation theory of humour in cognitive psychology.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.09618",
        "title": "LaRS: A Diverse Panoptic Maritime Obstacle Detection Dataset and Benchmark",
        "abstract": "The progress in maritime obstacle detection is hindered by the lack of a diverse dataset that adequately captures the complexity of general maritime environments. We present\nthe first maritime panoptic obstacle detection benchmark LaRS, featuring scenes from Lakes, Rivers and Seas. Our major contribution is the new dataset, which boasts the largest diversity in recording locations, scene types, obsta- cle classes, and acquisition conditions among the related datasets. LaRS is composed of over 4000 per-pixel labeled key frames with nine preceding frames to allow utilization of the temporal texture, amounting to over 40k frames. Each key frame is annotated with 8 thing, 3 stuff classes and 19 global scene attributes. We report the results of 27 se- mantic and panoptic segmentation methods, along with sev- eral performance insights and future research directions. To enable objective evaluation, we have implemented an online evaluation server. The LaRS dataset, evaluation toolkit and benchmark are publicly available at: https: //lojzezust.github.io/lars-dataset",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.06292",
        "title": "Joint Metrics Matter: A Better Standard for Trajectory Forecasting",
        "abstract": "Multi-modal trajectory forecasting methods commonly evaluate using single-agent metrics (marginal metrics), such as minimum Average Displacement Error (ADE) and Final Displacement Error (FDE), which fail to capture joint performance of multiple interacting agents. Only focusing on marginal metrics can lead to unnatural predictions, such as colliding trajectories or diverging trajectories for people who are clearly walking together as a group. Consequently, methods optimized for marginal metrics lead to overly-optimistic estimations of performance, which is detrimental to progress in trajectory forecasting research. In response to the limitations of marginal metrics, we present the first comprehensive evaluation of state-of-the- art (SOTA) trajectory forecasting methods with respect\nto multi-agent metrics (joint metrics): JADE, JFDE, and collision rate. We demonstrate the importance of joint metrics as opposed to marginal metrics with quantitative evidence and qualitative examples drawn from the ETH / UCY and Stanford Drone datasets. We introduce a new loss function incorporating joint metrics that, when applied\nto a SOTA trajectory forecasting method, achieves a 7% improvement in JADE / JFDE on the ETH / UCY datasets with respect to the previous SOTA. Our results also indicate that optimizing for joint metrics naturally leads to an im- provement in interaction modeling, as evidenced by a 16% decrease in mean collision rate on the ETH / UCY datasets with respect to the previous SOTA. Code is available at github.com/ericaweng/joint-metrics-matter.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.14407",
        "title": "LPFF: A Portrait Dataset for Face Generators Across Large Poses",
        "abstract": "The creation of 2D realistic facial images and 3D face shapes using generative networks has been a hot topic in re- cent years. Existing face generators exhibit exceptional per- formance on faces in small to medium poses (with respect to frontal faces) but struggle to produce realistic results for large poses. The distorted rendering results on large poses in 3D-aware generators further show that the generated 3D face shapes are far from the distribution of 3D faces in real- ity. We find that the above issues are caused by the training dataset\u2019s pose imbalance.\nIn this paper, we present LPFF, a large-pose Flickr face dataset comprised of 19,590 high-quality real large-pose\nportrait images. We utilize our dataset to train a 2D face generator that can process large-pose face images, as well as a 3D-aware generator that can generate realistic human face geometry. To better validate our pose-conditional 3D- aware generators, we develop a new FID measure to evalu- ate the 3D-level performance. Through this novel FID mea- sure and other experiments, we show that LPFF can help 2D face generators extend their latent space and better ma- nipulate the large-pose data, and help 3D-aware face gen- erators achieve better view consistency and more realistic 3D reconstruction results.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.12067",
        "title": "Replay:\nMulti-modal Multi-view Acted Videos for Casual Holography",
        "abstract": "We introduce Replay, a collection of multi-view, multi- modal videos of humans interacting socially. Each scene is filmed in high production quality, from different view- points with several static cameras, as well as wearable action cameras, and recorded with a large array of mi- crophones at different positions in the room. Overall, the dataset contains over 4000 minutes of footage and over 7 million timestamped high-resolution frames annotated with camera poses and partially with foreground masks. The Re- play dataset has many potential applications, such as novel- view synthesis, 3D reconstruction, novel-view acoustic syn- thesis, human body and face analysis, and training genera- tive models. We provide a benchmark for training and eval-\nuating novel-view synthesis, with two scenarios of different difficulty. Finally, we evaluate several baseline state-of-the- art methods on the new benchmark.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.14392",
        "title": "Human-centric Scene Understanding for 3D Large-scale Scenarios",
        "abstract": "Human-centric scene understanding is significant for real-world applications, but it is extremely challenging due to the existence of diverse human poses and ac- tions, complex human-environment interactions, severe oc- clusions in crowds, etc. In this paper, we present a large- scale multi-modal dataset for human-centric scene under- standing, dubbed HuCenLife, which is collected in diverse daily-life scenarios with rich and fine-grained annotations. Our HuCenLife can benefit many 3D perception tasks, such as segmentation, detection, action recognition, etc., and we also provide benchmarks for these tasks to facili- tate related research. In addition, we design novel mod- ules for LiDAR-based segmentation and action recognition, which are more applicable for large-scale human-centric scenarios and achieve state-of-the-art performance. The dataset and code can be found at https://github. com/4DVLab/HuCenLife.git.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.14710",
        "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
        "abstract": "Formula-driven supervised learning (FDSL) is a pre- training method that relies on synthetic images generated from mathematical formulae such as fractals. Prior work on FDSL has shown that pre-training vision transformers on such synthetic datasets can yield competitive accuracy on a wide range of downstream tasks. These synthetic im- ages are categorized according to the parameters in the mathematical formula that generate them. In the present work, we hypothesize that the process for generating dif- ferent instances for the same category in FDSL, can be viewed as a form of data augmentation. We validate this hypothesis by replacing the instances with data augmenta- tion, which means we only need a single image per category. Our experiments shows that this one-instance fractal database (OFDB) performs better than the original dataset where instances were explicitly generated. We further scale up OFDB to 21,000 categories and show that it matches, or even surpasses, the model pre-trained on ImageNet-21k in ImageNet-1k fine-tuning. The number of images in OFDB is 21k, whereas ImageNet-21k has 14M. This opens new possibilities for pre-training vision transformers with much smaller datasets.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.00035",
        "title": "FACET: Fairness in Computer Vision Evaluation Benchmark",
        "abstract": "Computer vision models have known performance dis- parities across attributes such as gender and skin tone. This means during tasks such as classification and detection, model performance differs for certain classes based on the demographics of the people in the image. These dispari- ties have been shown to exist, but until now there has not been a unified approach to measure these differences for common use-cases of computer vision models. We present a new benchmark named FACET (FAirness in Computer Vi- sion EvaluaTion), a large, publicly available evaluation set of 32k images for some of the most common vision tasks - image classification, object detection and segmentation. For every image in FACET, we hired expert reviewers to arXiv:2309.00035v1 [cs.CV] 31 Aug 2023 manually annotate person-related attributes such as per- ceived skin tone and hair type, manually draw bounding boxes and label fine-grained person-related classes such as disk jockey or guitarist. In addition, we use FACET to benchmark state-of-the-art vision models and present a deeper understanding of potential performance dispari- ties and challenges across sensitive demographic attributes. With the exhaustive annotations collected, we probe models using single demographics attributes as well as multiple at- tributes using an intersectional approach (e.g. hair color and perceived skin tone). Our results show that classifica- tion, detection, segmentation, and visual grounding mod- els exhibit performance disparities across demographic at- tributes and intersections of attributes. These harms sug- gest that not all people represented in datasets receive fair\nand equitable treatment in these vision tasks. We hope cur- rent and future results using our benchmark will contribute to fairer, more robust vision models. FACET is available publicly at https://facet.metademolab.com.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.07961",
        "title": "EmoSet: A Large-scale Visual Emotion Dataset with Rich Attributes",
        "abstract": "Visual Emotion Analysis (VEA) aims at predicting peo-\nple\u2019s emotional responses to visual stimuli. This is a promising, yet challenging, task in affective computing, which has drawn increasing attention in recent years. Most of the existing work in this area focuses on feature design, while little attention has been paid to dataset construction. In this work, we introduce EmoSet, the first large-scale vi- sual emotion dataset annotated with rich attributes, which is superior to existing datasets in four aspects: scale, anno- tation richness, diversity, and data balance. EmoSet com- prises 3.3 million images in total, with 118,102 of these im- ages carefully labeled by human annotators, making it five times larger than the largest existing dataset. EmoSet in- cludes images from social networks, as well as artistic im- ages, and it is well balanced between different emotion cat- egories. Motivated by psychological studies, in addition to emotion category, each image is also annotated with a set of describable emotion attributes: brightness, colorfulness, scene type, object class, facial expression, and human ac- tion, which can help understand visual emotions in a precise and interpretable way. The relevance of these emotion at- tributes is validated by analyzing the correlations between them and visual emotion, as well as by designing an at- tribute module to help visual emotion recognition. We believe EmoSet will bring some key insights and encourage further research in visual emotion analysis and understand-\ning. Project page: https://vcc.tech/EmoSet.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.09301",
        "title": "RenderIH: A Large-scale Synthetic Dataset for 3D Interacting Hand Pose Estimation",
        "abstract": "The current interacting hand (IH) datasets are relatively simplistic in terms of background and texture, with hand joints being annotated by a machine annotator, which may result in inaccuracies, and the diversity of pose distribution is limited. However, the variability of background, pose distribution, and texture can greatly influence the gener- alization ability. Therefore, we present a large-scale syn- thetic dataset \u2013RenderIH\u2013 for interacting hands with accu- rate and diverse pose annotations. The dataset contains\n1M photo-realistic images with varied backgrounds, per- spectives, and hand textures. To generate natural and di- verse interacting poses, we propose a new pose optimiza- tion algorithm. Additionally, for better pose estimation accuracy, we introduce a transformer-based pose estima- tion network, TransHand, to leverage the correlation be- tween interacting hands and verify the effectiveness of Ren- derIH in improving results. Our dataset is model-agnostic and can improve more accuracy of any hand pose esti- mation method in comparison to other real or synthetic datasets. Experiments have shown that pretraining on our synthetic data can significantly decrease the error from 6.76mm to 5.79mm, and our Transhand surpasses contem- porary methods. Our dataset and code are available at https://github.com/adwardlee/RenderIH.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.11897",
        "title": "TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering",
        "abstract": "Despite thousands of researchers, engineers, and artists\nactively working on improving text-to-image generation mod-\nels, systems often fail to produce images that accurately\nalign with the text inputs. We introduce TIFA (Text-to-Image\nFaithfulness evaluation with question Answering), an auto-\nmatic evaluation metric that measures the faithfulness of a\ngenerated image to its text input via visual question answer-\ning (VQA). Specifically, given a text input, we automatically\ngenerate several question-answer pairs using a language\nmodel. We calculate image faithfulness by checking whether\nexisting VQA models can answer these questions using the generated image. TIFA is a reference-free metric that allows\nfor fine-grained and interpretable evaluations of generated\nimages. TIFA also has better correlations with human judg-\nments than existing metrics. Based on this approach, we\nintroduce TIFA v1.0, a benchmark consisting of 4K diverse\ntext inputs and 25K questions across 12 categories (object,\ncounting, etc.). We present a comprehensive evaluation of ex-\nisting text-to-image models using TIFA v1.0 and highlight the\nlimitations and challenges of current models. For instance,\nwe find that current text-to-image models, despite doing well\non color and material, still struggle in counting, spatial\nrelations, and composing multiple objects. We hope our\nbenchmark will help carefully measure the research progress\nin text-to-image synthesis and provide valuable insights for further research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.09987",
        "title": "ClothesNet: An Information-Rich 3D Garment Model Repository with Simulated Clothes Environment",
        "abstract": "We present ClothesNet: a large-scale dataset of 3D clothes objects with information-rich annotations. Our dataset consists of around 4400 models covering 11 cat- egories annotated with clothes features, boundary lines, and keypoints. ClothesNet can be used to facili- tate a variety of computer vision and robot interaction tasks. Using our dataset, we establish benchmark tasks for clothes perception, including classification, bound- ary line segmentation, and keypoint detection, and de- velop simulated clothes environments for robotic interac- tion tasks, including rearranging, folding, hanging, and dressing. We also demonstrate the efficacy of our Clothes-\nNet in real-world experiments. Supplemental materi- als and dataset are available on our project webpage at https://sites.google.com/view/clothesnet.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.13933",
        "title": "AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive Driving Perception",
        "abstract": "Driver distraction has become a significant cause of se- vere traffic accidents over the past decade. Despite the growing development of vision-driven driver monitoring systems, the lack of comprehensive perception datasets re- stricts road safety and traffic security. In this paper, we present an AssIstive Driving pErception dataset (AIDE) that considers context information both inside and outside the vehicle in naturalistic scenarios. AIDE facilitates holis- tic driver monitoring through three distinctive character- istics, including multi-view settings of driver and scene, multi-modal annotations of face, body, posture, and ges- ture, and four pragmatic task designs for driving under- standing. To thoroughly explore AIDE, we provide exper- imental benchmarks on three kinds of baseline frameworks via extensive methods. Moreover, two fusion strategies are introduced to give new insights into learning effective multi- stream/modal representations. We also systematically in- vestigate the importance and rationality of the key com- ponents in AIDE and benchmarks. The project link is https://github.com/ydk122024/AIDE.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.07625",
        "title": "PlanarTrack: A Large-scale Challenging Benchmark for Planar Object Tracking",
        "abstract": "Planar object tracking is a critical computer vision prob- lem and has drawn increasing interest owing to its key roles in robotics, augmented reality, etc. Despite rapid progress, its further development, especially in the deep learning era, is largely hindered due to the lack of large-scale challenging benchmarks. Addressing this, we introduce PlanarTrack, a large-scale challenging planar tracking benchmark. Specif- ically, PlanarTrack consists of 1,000 videos with more than 490K images. All these videos are collected in complex un- constrained scenarios from the wild, which makes Planar- Track, compared with existing benchmarks, more challeng- ing but realistic for real-world applications. To ensure the high-quality annotation, each frame in PlanarTrack is man- ually labeled using four corners with multiple-round careful inspection and refinement. To our best knowledge, Planar- Track, to date, is the largest and most challenging dataset dedicated to planar object tracking. In order to analyze the proposed PlanarTrack, we evaluate 10 planar trackers and conduct comprehensive comparisons and in-depth analysis. Our results, not surprisingly, demonstrate that current top- performing planar trackers degenerate significantly on the challenging PlanarTrack and more efforts are needed to im- prove planar tracking in the future. In addition, we further derive a variant named PlanarTrackBB for generic object tracking from PlanarTrack. Our evaluation of 10 excellent generic trackers on PlanarTrackBB manifests that, surpris- ingly, PlanarTrackBB is even more challenging than several popular generic tracking benchmarks and more attention should be paid to handle such planar objects, though they\nare rigid. All benchmarks and evaluations will be released at the project webpage.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.13411",
        "title": "Harvard Glaucoma Detection and Progression: A Multimodal Multitask Dataset and Generalization-Reinforced Semi-Supervised Learning",
        "abstract": "Glaucoma is the number one cause of irreversible blind- ness globally. A major challenge for accurate glaucoma detection and progression forecasting is the bottleneck of limited labeled patients with the state-of-the-art (SOTA) 3D retinal imaging data of optical coherence tomogra- phy (OCT). To address the data scarcity issue, this pa- per proposes two solutions. First, we develop a novel generalization-reinforced semi-supervised learning (SSL) model called pseudo supervisor to optimally utilize unla- beled data. Compared with SOTA models, the proposed pseudo supervisor optimizes the policy of predicting pseudo labels with unlabeled samples to improve empirical gener- alization. Our pseudo supervisor model is evaluated with\ntwo clinical tasks consisting of glaucoma detection and pro- gression forecasting. The progression forecasting task is evaluated both unimodally and multimodally. Our pseudo supervisor model demonstrates superior performance than SOTA SSL comparison models. Moreover, our model also achieves the best results on the publicly available LAG fun- dus dataset. Second, we introduce the Harvard Glaucoma Detection and Progression (Harvard-GDP) Dataset, a mul- timodal multitask dataset that includes data from 1,000 pa- tients with OCT imaging data, as well as labels for glau- coma detection and progression. This is the largest glau- coma detection dataset with 3D OCT imaging data and the first glaucoma progression forecasting dataset that is pub- licly available. Detailed sex and racial analysis are pro- vided, which can be used by interested researchers for fair- ness learning studies. Our released dataset is benchmarked with several SOTA supervised CNN and transformer deep learning models. The dataset and code are made publicly available via https://ophai.hms.harvard.edu/ datasets/harvard-gdp1000.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.04321",
        "title": "ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes",
        "abstract": "Understanding the continuous states of objects is essen- tial for task learning and planning in the real world. How- ever, most existing task learning benchmarks assume discrete (e.g., binary) object goal states, which poses challenges for the learning of complex tasks and transferring learned policy from simulated environments to the real world. Furthermore, state discretization limits a robot\u2019s ability to follow human instructions based on the grounding of actions and states. To tackle these challenges, we present ARNOLD, a benchmark that evaluates language-grounded task learning with con- tinuous states in realistic 3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve understanding object states and learning policies for continuous goals. To promote language-instructed learning, we provide expert demonstrations with template-generated language descrip- tions. We assess task performance by utilizing the latest language-conditioned policy learning models. Our results indicate that current models for language-conditioned ma- nipulations continue to experience significant challenges in novel goal-state generalizations, scene generalizations, and object generalizations. These findings highlight the need to develop new algorithms that address this gap and under- score the potential for further research in this area. Project website: https://arnold-benchmark.github.io.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.05438",
        "title": "Towards Content-based Pixel Retrieval in Revisited Oxford and Paris",
        "abstract": "This paper introduces the first two pixel retrieval bench- marks. Pixel retrieval is segmented instance retrieval. Like semantic segmentation extends classification to the pixel level, pixel retrieval is an extension of image retrieval and offers information about which pixels are related to the query object. In addition to retrieving images for the given query, it helps users quickly identify the query object in true positive images and exclude false positive images by de- noting the correlated pixels. Our user study results show pixel-level annotation can significantly improve the user ex-\nperience. Compared with semantic and instance segmenta- tion, pixel retrieval requires a fine-grained recognition ca- pability for variable-granularity targets. To this end, we propose pixel retrieval benchmarks named PROxford and PRParis, which are based on the widely used image re- trieval datasets, ROxford and RParis. Three professional annotators label 5,942 images with two rounds of double- checking and refinement. Furthermore, we conduct exten- sive experiments and analysis on the SOTA methods in im- age search, image matching, detection, segmentation, and dense matching using our pixel retrieval benchmarks. Re- sults show that the pixel retrieval task is challenging to these approaches and distinctive from existing problems, suggesting that further research can advance the content- based pixel-retrieval and thus user search experience. The datasets can be downloaded from this link.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.13505",
        "title": "A Large-scale Study of Spatiotemporal Representation Learning with a New Benchmark on Action Recognition",
        "abstract": "The goal of building a benchmark (suite of datasets) is to provide a unified protocol for fair evaluation and thus facilitate the evolution of a specific area. Nonethe- less, we point out that existing protocols of action recog- nition could yield partial evaluations due to several limita- tions. To comprehensively probe the effectiveness of spa- tiotemporal representation learning, we introduce BEAR, a new BEnchmark on video Action Recognition. BEAR is a collection of 18 video datasets grouped into 5 cate- gories (anomaly, gesture, daily, sports, and instructional), which covers a diverse set of real-world applications. With\nBEAR, we thoroughly evaluate 6 common spatiotemporal models pre-trained by both supervised and self-supervised learning. We also report transfer performance via stan- dard finetuning, few-shot finetuning, and unsupervised do- main adaptation. Our observation suggests that the cur- rent state-of-the-art cannot solidly guarantee high perfor- mance on datasets close to real-world applications, and we hope BEAR can serve as a fair and challenging evalua- tion benchmark to gain insights on building next-generation spatiotemporal learners. Our dataset, code, and models are released at: https://github.com/AndongDeng/BEAR",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.08723",
        "title": "Revisiting Scene Text Recognition: A Data Perspective",
        "abstract": "This paper aims to re-assess scene text recognition (STR) from a data-oriented perspective. We begin by revisiting the six commonly used benchmarks in STR and observe a trend of performance saturation, whereby only 2.91% of the benchmark images cannot be accurately recognized by an ensemble of 13 representative models. While these re- sults are impressive and suggest that STR could be consid- ered solved, however, we argue that this is primarily due to the less challenging nature of the common benchmarks, thus concealing the underlying issues that STR faces. To this end, we consolidate a large-scale real STR dataset, namely Union14M, which comprises 4 million labeled im-\nages and 10 million unlabeled images, to assess the per- formance of STR models in more complex real-world sce- narios. Our experiments demonstrate that the 13 models can only achieve an average accuracy of 66.53% on the 4 million labeled images, indicating that STR still faces numerous challenges in the real world. By analyzing the error patterns of the 13 models, we identify seven open challenges in STR and develop a challenge-driven bench- mark consisting of eight distinct subsets to facilitate fur- ther progress in the field. Our exploration demonstrates that STR is far from being solved and leveraging data may be a promising solution. In this regard, we find that utiliz- ing the 10 million unlabeled images through self-supervised pre-training can significantly improve the robustness of STR model in real-world scenarios and leads to state-of-the-art performance. Code and dataset is available at https: //github.com/Mountchicken/Union14M .",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2211.08095",
        "title": "Will Large-scale Generative Models Corrupt Future Datasets?",
        "abstract": "Recently proposed large-scale text-to-image generative models such as DALL\u00b7E 2 [47], Midjourney [42], and Sta- bleDiffusion [51] can generate high-quality and realistic images from users\u2019 prompts. Not limited to the research community, ordinary Internet users enjoy these generative models, and consequently, a tremendous amount of gen- erated images have been shared on the Internet. Mean- while, today\u2019s success of deep learning in the computer vi- sion field owes a lot to images collected from the Internet. These trends lead us to a research question: \u201cwill such gen- erated images impact the quality of future datasets and the performance of computer vision models positively or negatively?\u201d This paper empirically answers this ques- tion by simulating contamination. Namely, we generate\nImageNet-scale and COCO-scale datasets using a state-of- the-art generative model and evaluate models trained with \u201ccontaminated\u201d datasets on various tasks, including im- age classification and image generation. Throughout ex- periments, we conclude that generated images negatively affect downstream performance, while the significance de- pends on tasks and the amount of generated images. The generated datasets and the codes for experiments will be publicly released for future research. Generated datasets and source codes are available from https://github. com/moskomule/dataset-contamination.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.14630",
        "title": "360VOT: A New Benchmark Dataset for Omnidirectional Visual Object Tracking",
        "abstract": "360\u25e6 images can provide an omnidirectional field of view which is important for stable and long-term scene percep- tion. In this paper, we explore 360\u25e6 images for visual ob- ject tracking and perceive new challenges caused by large distortion, stitching artifacts, and other unique attributes of 360\u25e6 images. To alleviate these problems, we take ad- vantage of novel representations of target localization, i.e., bounding field-of-view, and then introduce a general 360 tracking framework that can adopt typical trackers for om- nidirectional tracking. More importantly, we propose a new large-scale omnidirectional tracking benchmark dataset, 360VOT, in order to facilitate future research. 360VOT con- tains 120 sequences with up to 113K high-resolution frames in equirectangular projection. The tracking targets cover 32 categories in diverse scenarios. Moreover, we provide 4 types of unbiased ground truth, including (rotated) bound- ing boxes and (rotated) bounding field-of-views, as well as new metrics tailored for 360\u25e6 images which allow for the accurate evaluation of omnidirectional tracking perfor- mance. Finally, we extensively evaluated 20 state-of-the-art visual trackers and provided a new baseline for future com- parisons. Homepage: https://360vot.hkustvgd.com",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2402.19479",
        "title": "Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers",
        "abstract": "The quality of the data and annotation upper-bounds the\nquality of a downstream model. While there exist large text\ncorpora and image-text pairs, high-quality video-text data\nis much harder to collect. First of all, manual labeling is\nmore time-consuming, as it requires an annotator to watch\nan entire video. Second, videos have a temporal dimension,\nconsisting of several scenes stacked together, and showing\nmultiple actions. Accordingly, to establish a video dataset\nwith high-quality captions, we propose an automatic approach leveraging multimodal inputs, such as textual video\ndescription, subtitles, and individual video frames. Specifically, we curate 3.8M high-resolution videos from the publicly available HD-VILA-100M dataset. We then split them\ninto semantically consistent video clips, and apply multiple\ncross-modality teacher models to obtain captions for each\nvideo. Next, we finetune a retrieval model on a small subset\nwhere the best caption of each video is manually selected\nand then employ the model in the whole dataset to select\nthe best caption as the annotation. In this way, we get 70M\nvideos paired with high-quality text captions. We dub the\ndataset as Panda-70M. We show the value of the proposed\ndataset on three downstream tasks: video captioning, video\nand text retrieval, and text-driven video generation. The\nmodels trained on the proposed data score substantially better on the majority of metrics across all the tasks.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2404.00989",
        "title": "360+x: A Panoptic Multi-modal Scene Understanding Dataset",
        "abstract": "Human perception of the world is shaped by a multitude\nof viewpoints and modalities. While many existing datasets\nfocus on scene understanding from a certain perspective\n(e.g. egocentric or third-person views), our dataset offers a\npanoptic perspective (i.e. multiple viewpoints with multiple\ndata modalities). Specifically, we encapsulate third-person\npanoramic and front views, as well as egocentric monocular/binocular views with rich modalities including video,\nmulti-channel audio, directional binaural delay, location\ndata and textual scene descriptions within each scene captured, presenting comprehensive observation of the world.\nFigure 1 offers a glimpse of all 28 scene categories of our\n360+x dataset. To the best of our knowledge, this is the\nfirst database that covers multiple viewpoints with multiple data modalities to mimic how daily information is accessed in the real world. Through our benchmark analysis,\nwe presented 5 different scene understanding tasks on the\nproposed 360+x dataset to evaluate the impact and benefit\nof each data modality and perspective in panoptic scene understanding. We hope this unique dataset could broaden the\nscope of comprehensive scene understanding and encourage the community to approach these problems from more\ndiverse perspectives.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.02835",
        "title": "Traffic Scene Parsing through the TSP6K Dataset",
        "abstract": "Traffic scene perception in computer vision is a critically important task to achieve intelligent cities. To date, most existing datasets focus on autonomous driving scenes. We observe that the models trained on those driving datasets often yield unsatisfactory results on traffic monitoring scenes. However, little effort has been put into improving the traffic monitoring scene understanding, mainly due to the lack of specific datasets. To fill this gap, we introduce a specialized traffic monitoring dataset, termed TSP6K, containing images from the traffic monitoring scenario, with high-quality pixellevel and instance-level annotations. The TSP6K dataset captures more crowded traffic scenes with several times more traffic participants than the existing driving scenes. We perform a detailed analysis of the dataset and comprehensively evaluate previous popular scene parsing methods, instance segmentation methods and unsupervised domain adaption methods. Furthermore, considering the vast difference in instance sizes, we propose a detail refining decoder for scene parsing, which recovers the details of different semantic regions in traffic scenes owing to the proposed TSP6K dataset. Experiments show its effectiveness in parsing the traffic monitoring scenes. Code and dataset are available at https://github.com/PengtaoJiang/TSP6K.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2403.18775",
        "title": "ImageNet-D: Benchmarking Neural Network Robustness on\nDiffusion Synthetic Object",
        "abstract": "We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C,\nImageNet-9, and Stylized ImageNet provide specific type\nof evaluation over synthetic corruptions, backgrounds, and\ntextures, yet those robustness benchmarks are restricted\nin specified variations and have low synthetic quality. In\nthis work, we introduce generative model as a data source\nfor synthesizing hard images that benchmark deep models\u2019 robustness. Leveraging diffusion models, we are able\nto generate images with more diversified backgrounds, textures, and materials than any prior work, where we term\nthis benchmark as ImageNet-D. Experimental results show\nthat ImageNet-D results in a significant accuracy drop to\na range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and\nMiniGPT-4, significantly reducing their accuracy by up to\n60%. Our work suggests that diffusion models can be an\neffective source to test vision models. The code and dataset\nare available at https://github.com/chenshuangzhang/imagenet_d.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.17982",
        "title": "VBench: Comprehensive Benchmark Suite for Video Generative Models",
        "abstract": "Video generation has witnessed significant advancements, yet evaluating these models remains a challenge. A\ncomprehensive evaluation benchmark for video generation\nis indispensable for two reasons: 1) Existing metrics do\nnot fully align with human perceptions; 2) An ideal evaluation system should provide insights to inform future developments of video generation. To this end, we present\nVBench, a comprehensive benchmark suite that dissects\n\u201cvideo generation quality\u201d into specific, hierarchical, and\ndisentangled dimensions, each with tailored prompts and\nevaluation methods. VBench has three appealing properties: 1) Comprehensive Dimensions: VBench comprises 16 dimensions in video generation (e.g., subject identity inconsistency, motion smoothness, temporal flickering, and\nspatial relationship, etc.). The evaluation metrics with\nfine-grained levels reveal individual models\u2019 strengths and\nweaknesses. 2) Human Alignment: We also provide a\ndataset of human preference annotations to validate our\nbenchmarks\u2019 alignment with human perception, for each\nevaluation dimension respectively. 3) Valuable Insights:\nWe look into current models\u2019 ability across various evaluation dimensions, and various content types. We also investigate the gaps between video and image generation models.\nWe will open-source VBench, including all prompts, evaluation methods, generated videos, and human preference\nannotations, and also include more video generation models in VBench to drive forward the field of video generation.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.03321",
        "title": "Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages",
        "abstract": "Many recent works have explored using language models for planning problems.\nOne line of research focuses on translating natural language descriptions of planning tasks into structured planning languages, such as the planning domain definition language (PDDL). While this approach is promising, accurately measuring\nthe quality of generated PDDL code continues to pose significant challenges. First,\ngenerated PDDL code is typically evaluated using planning validators that check\nwhether the problem can be solved with a planner. This method is insufficient\nbecause a language model might generate valid PDDL code that does not align\nwith the natural language description of the task. Second, existing evaluation sets\noften have natural language descriptions of the planning task that closely resemble\nthe ground truth PDDL, reducing the challenge of the task. To bridge this gap, we\nintroduce Planetarium, a benchmark designed to evaluate language models\u2019 ability\nto generate PDDL code from natural language descriptions of planning tasks. We\nbegin by creating a PDDL equivalence algorithm that rigorously evaluates the\ncorrectness of PDDL code generated by language models by flexibly comparing it\nagainst a ground truth PDDL. Then, we present a dataset of 132, 037 text-to-PDDL\npairs across 13 different tasks, with varying levels of difficulty. Finally, we evaluate several API-access and open-weight language models that reveal this task\u2019s\ncomplexity. For example, 87.6% of the PDDL problem descriptions generated\nby GPT-4o are syntactically parseable, 82.2% are valid, solve-able problems, but\nonly 35.1% are semantically correct, highlighting the need for a more rigorous\nbenchmark for this problem.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2407.03314",
        "title": "BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations",
        "abstract": "This paper presents Bag-of-Concept Graph\n(BACON) to gift models with limited linguistic\nabilities to taste the privilege of Vision Language Models (VLMs) and reduce hallucinations in the downstream tasks such as detection,\nvisual question answering (VQA), and image\ngeneration. Since the visual scenes in physical worlds are structured with complex relations between objects, BACON breaks down\nannotations into basic minimum elements and\npresents them in a graph structure. Elementwise style enables easy understanding, and\nstructural composition liberates difficult locating. Careful prompt design births the BACON captions with the help of public-available\nVLMs and segmentation methods. In this way,\nwe gather a dataset with 100K annotated images, which endow VLMs with remarkable capabilities, such as accurately generating BACON, transforming prompts into BACON format, envisioning scenarios in the style of BACON, and dynamically modifying elements\nwithin BACON through interactive dialogue and\nmore. Wide representative experiments, including detection, VQA, and image generation\ntasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel in their current\ncutting-edge solutions.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.03311",
        "title": "Value-Penalized Auxiliary Control from Examples for Learning without Rewards or Demonstrations",
        "abstract": "Learning from examples of success is an appealing approach to reinforcement learning that eliminates many of the disadvantages of using handcrafted reward functions or full expert-demonstration trajectories, both of which can be difficult to acquire, biased, or suboptimal. However, learning from examples alone dramatically increases the exploration challenge, especially for complex tasks. This work introduces value-penalized auxiliary control from examples (VPACE); we significantly improve exploration in example-based control by adding scheduled auxiliary control and examples of auxiliary tasks. Furthermore, we identify a value-calibration problem, where policy value estimates can exceed their theoretical limits based on successful data. We resolve this problem, which is exacerbated by learning auxiliary tasks, through the addition of an above-successlevel value penalty. Across three simulated and one real robotic manipulation environment, and 21 different main tasks, we show that our approach substantially improves learning efficiency. Videos, code, and datasets are available at https://papers.starslab.ca/vpace.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.03308",
        "title": "ACCELERATED PROTON RESONANCE FREQUENCY-BASED\nMAGNETIC RESONANCE THERMOMETRY BY OPTIMIZED DEEP\nLEARNING METHOD",
        "abstract": "Background: Proton resonance frequency (PRF)\u2013based magnetic resonance (MR) thermometry is\nessential in thermal ablation therapies through focused ultrasound (FUS). The clinical treatments\nrequire temperature feedback must be rapid and accurate. Purpose: This work aims to enhance\ntemporal resolution in dynamic MR temperature map reconstruction with an improved deep learning\nmethod, to ensure the safety and effectiveness of FUS treatments.\nMethods: The training-optimized methods and five classical neural networks were applied on the\n2-fold and 4-fold under-sampling k-space data to reconstruct the temperature maps. The used neural\nnetworks were cascade net, complex valued U-Net, shift window transformer for MRI, real valued\nU-Net and U-Net with residual block. The enhanced training modules included offline/online data\naugmentations, knowledge distillation, and the amplitude-phase decoupling loss function. The heating\nexperiments were performed by a FUS transducer on phantom and ex vivo tissues, respectively. In\ndatasets, the ground-truth was the complex MR images with accurate temperature increases. These\ndata were also manually under-sampled to imitate acceleration procedures and trained in our method\nto get the reconstruction model. The additional dozen or so testing datasets were separately obtained\nfor evaluating the real-time performance and temperature accuracy.\nResults: Acceleration factors of 1.9 and 3.7 were found for 2\u00d7 and 4\u00d7 k-space under-sampling\nstrategies and the ResUNet-based deep learning reconstruction performed exceptionally well. In\n2-fold acceleration scenario, the RMSE of temperature map patches provided the values of 0.888 \u00b0C\nand 1.145 \u00b0C on phantom and ex vivo testing datasets. The DICE value of temperature areas enclosed\nby 43 \u00b0C isotherm was 0.809, and the Bland-Altman analysis showed a bias of \u22120.253 \u00b0C with the\napart of \u00b12.16 \u00b0C. In 4\u00d7 under-sampling case, these evaluating values decreased by approximately\n10%.\nConclusion: This study demonstrates that the application of deep learning-based reconstruction\nsignificantly enhances the accuracy and efficiency of MR thermometry, particularly benefiting the\nclinical thermal therapies for uterine fibroid, essential tremor, and prostate cancer by FUS.\nThe source code for our optimizing methods and neural networks is available at: https://github.\ncom/minipuding/FastMRT.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.03307",
        "title": "HoloHisto: End-to-end Gigapixel WSI Segmentation with 4K Resolution Sequential Tokenization",
        "abstract": "In digital pathology, the traditional method for deep learning-based\nimage segmentation typically involves a two-stage process: initially segmenting\nhigh-resolution whole slide images (WSI) into smaller patches (e.g., 256\n\u00d7 256\n,\n512\n\u00d7 512\n, 1024\n\u00d7 1024) and subsequently reconstructing them to their original\nscale. This method often struggles to capture the complex details and vast scope\nof WSIs. In this paper, we propose the holistic histopathology (HoloHisto) segmentation method to achieve end-to-end segmentation on gigapixel WSIs, whose\nmaximum resolution is above 80,000\n\u00d770,000 pixels. HoloHisto fundamentally\nshifts the paradigm of WSI segmentation to an end-to-end learning fashion with\n1) a large (4K) resolution base patch for elevated visual information inclusion\nand efficient processing, and 2) a novel sequential tokenization mechanism to\nproperly model the contextual relationships and efficiently model the rich information from the 4K input. To our best knowledge, HoloHisto presents the first\nholistic approach for gigapixel resolution WSI segmentation, supporting direct\nI/O of complete WSI and their corresponding gigapixel masks. Under the HoloHisto platform, we unveil a random 4K sampler that transcends ultra-high resolution, delivering 31 and 10 times more pixels than standard 2D and 3D patches,\nrespectively, for advancing computational capabilities. To facilitate efficient 4K\nresolution dense prediction, we leverage sequential tokenization, utilizing a pretrained image tokenizer to group image features into a discrete token grid. To\nassess the performance, our team curated a new kidney pathology image segmentation (KPIs) dataset with WSI-level glomeruli segmentation from whole mouse\nkidneys. From the results, HoloHisto-4K delivers remarkable performance gains\nover previous state-of-the-art models.\"",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2407.03300",
        "title": "DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents",
        "abstract": "Diffusion models (DMs) have revolutionized generative learning. They utilize a diffusion process\nto encode data into a simple Gaussian distribution. However, encoding a complex, potentially\nmultimodal data distribution into a single continuous Gaussian distribution arguably represents\nan unnecessarily challenging learning problem.\nWe propose Discrete-Continuous Latent Variable\nDiffusion Models (DisCo-Diff) to simplify this\ntask by introducing complementary discrete latent variables. We augment DMs with learnable\ndiscrete latents, inferred with an encoder, and\ntrain DM and encoder end-to-end. DisCo-Diff\ndoes not rely on pre-trained networks, making the\nframework universally applicable. The discrete\nlatents significantly simplify learning the DM\u2019s\ncomplex noise-to-data mapping by reducing the\ncurvature of the DM\u2019s generative ODE. An additional autoregressive transformer models the distribution of the discrete latents, a simple step because DisCo-Diff requires only few discrete variables with small codebooks. We validate DisCoDiff on toy data, several image synthesis tasks as\nwell as molecular docking, and find that introducing discrete latents consistently improves model\nperformance. For example, DisCo-Diff achieves\nstate-of-the-art FID scores on class-conditioned\nImageNet-64/128 datasets with ODE sampler.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2407.03298",
        "title": "Eyes on the Game: Deciphering Implicit Human Signals to Infer Human Proficiency, Trust, and Intent",
        "abstract": "Effective collaboration between humans and AIs\nhinges on transparent communication and alignment of mental\nmodels. However, explicit, verbal communication is not always\nfeasible. Under such circumstances, human-human teams often\ndepend on implicit, nonverbal cues to glean important information about their teammates such as intent and expertise,\nthereby bolstering team alignment and adaptability. Among\nthese implicit cues, two of the most salient and fundamental\nare a human\u2019s actions in the environment and their visual\nattention. In this paper, we present a novel method to combine\neye gaze data and behavioral data, and evaluate their respective\npredictive power for human proficiency, trust, and intent. We\nfirst collect a dataset of paired eye gaze and gameplay data\nin the fast-paced collaborative \u201cOvercooked\u201d environment. We\nthen train models on this dataset to compare how the predictive\npowers differ between gaze data, gameplay data, and their\ncombination. We additionally compare our method to prior\nworks that aggregate eye gaze data and demonstrate how these\naggregation methods can substantially reduce the predictive\nability of eye gaze. Our results indicate that, while eye gaze\ndata and gameplay data excel in different situations, a model\nthat integrates both types consistently outperforms all baselines.\nThis work paves the way for developing intuitive and responsive\nagents that can efficiently adapt to new teammates.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.03291",
        "title": "VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation",
        "abstract": "Complex human activity recognition (CHAR) remains a pivotal challenge within ubiquitous computing, especially in the\ncontext of smart environments. Existing studies typically require meticulous labeling of both atomic and complex activities,\na task that is labor-intensive and prone to errors due to the scarcity and inaccuracies of available datasets. Most prior\nresearch has focused on datasets that either precisely label atomic activities or, at minimum, their sequence\u2014approaches\nthat are often impractical in real-world settings. In response, we introduce VCHAR (Variance-Driven Complex Human\nActivity Recognition), a novel framework that treats the outputs of atomic activities as a distribution over specified intervals.\nLeveraging generative methodologies, VCHAR elucidates the reasoning behind complex activity classifications through\nvideo-based explanations, accessible to users without prior machine learning expertise. Our evaluation across three publicly\navailable datasets demonstrates that VCHAR enhances the accuracy of complex activity recognition without necessitating\nprecise temporal or sequential labeling of atomic activities. Furthermore, user studies confirm that VCHAR\u2019s explanations are\nmore intelligible compared to existing methods, facilitating a broader understanding of complex activity recognition among\nnon-experts.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2407.03282",
        "title": "LLM Internal States Reveal Hallucination Risk Faced With a Query",
        "abstract": "The hallucination problem of Large Language\nModels (LLMs) significantly limits their reliability and trustworthiness. Humans have a selfawareness process that allows us to recognize\nwhat we don\u2019t know when faced with queries.\nInspired by this, our paper investigates whether\nLLMs can estimate their own hallucination risk\nbefore response generation. We analyze the\ninternal mechanisms of LLMs broadly both in\nterms of training data sources and across 15\ndiverse Natural Language Generation (NLG)\ntasks, spanning over 700 datasets. Our empirical analysis reveals two key insights: (1) LLM\ninternal states indicate whether they have seen\nthe query in training data or not; and (2) LLM\ninternal states show they are likely to hallucinate or not regarding the query. Our study\nexplores particular neurons, activation layers,\nand tokens that play a crucial role in the LLM\nperception of uncertainty and hallucination risk.\nBy a probing estimator, we leverage LLM selfassessment, achieving an average hallucination\nestimation accuracy of 84.32% at run time.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2407.03277",
        "title": "Evaluating Automatic Metrics with Incremental Machine Translation Systems",
        "abstract": "We introduce a dataset comprising commercial\nmachine translations, gathered weekly over six\nyears across 12 translation directions. Since human A/B testing is commonly used, we assume\ncommercial systems improve over time, which\nenables us to evaluate machine translation (MT)\nmetrics based on their preference for more recent translations. Our study confirms several\nprevious findings in MT metrics research and\ndemonstrates the dataset\u2019s value as a testbed\nfor metric evaluation. We release our code.1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.03236",
        "title": "CATT: Character-based Arabic Tashkeel Transformer",
        "abstract": "Tashkeel, or Arabic Text Diacritization (ATD),\ngreatly enhances the comprehension of Arabic text by removing ambiguity and minimizing the risk of misinterpretations caused by\nits absence. It plays a crucial role in improving Arabic text processing, particularly in applications such as text-to-speech and machine\ntranslation. This paper introduces a new approach to training ATD models. First, we\nfinetuned two transformers, encoder-only and\nencoder-decoder, that were initialized from a\npretrained character-based BERT. Then, we\napplied the Noisy-Student approach to boost\nthe performance of the best model. We evaluated our models alongside 11 commercial and\nopen-source models using two manually labeled benchmark datasets: WikiNews and our\nCATT dataset. Our findings show that our top\nmodel surpasses all evaluated models by relative Diacritic Error Rates (DERs) of 30.83%\nand 35.21% on WikiNews and CATT, respectively, achieving state-of-the-art in ATD. In addition, we show that our model outperforms\nGPT-4-turbo on CATT dataset by a relative\nDER of 9.36%. We open-source our CATT\nmodels and benchmark dataset for the research\ncommunity1\n.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.03216",
        "title": "Learning Disentangled Representation in\nObject-Centric Models for Visual Dynamics Prediction\nvia Transformers",
        "abstract": "Recent work has shown that object-centric representations can greatly help improve the accuracy of learning dynamics while also bringing interpretability. In\nthis work, we take this idea one step further, ask the following question: \"can learning disentangled representation further improve the accuracy of visual dynamics\nprediction in object-centric models?\" While there has been some attempt to learn\nsuch disentangled representations for the case of static images [26], to the best of\nour knowledge, ours is the first work which tries to do this in a general setting for\nvideo, without making any specific assumptions about the kind of attributes that\nan object might have. The key building block of our architecture is the notion of a\nblock, where several blocks together constitute an object. Each block is represented\nas a linear combination of a given number of learnable concept vectors, which\nis iteratively refined during the learning process. The blocks in our model are\ndiscovered in an unsupervised manner, by attending over object masks, in a style\nsimilar to discovery of slots [21], for learning a dense object-centric representation.\nWe employ self-attention via transformers over the discovered blocks to predict\nthe next state resulting in discovery of visual dynamics. We perform a series of\nexperiments on several benchmark 2-D, and 3-D datasets demonstrating that our\narchitecture (1) can discover semantically meaningful blocks (2) help improve\naccuracy of dynamics prediction compared to SOTA object-centric models (3) perform significantly better in OOD setting where the specific attribute combinations\nare not seen earlier during training. Our experiments highlight the importance\ndiscovery of disentangled representation for visual dynamics prediction.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2407.03115",
        "title": "Lp-norm Distortion-Efficient Adversarial Attack",
        "abstract": "Adversarial examples have shown a powerful ability to make a well-trained model\nmisclassified. Current mainstream adversarial attack methods only consider one of\nthe distortions among L0-norm, L2-norm, and L\u221e-norm. L0-norm based methods\ncause large modification on a single pixel, resulting in naked-eye visible detection,\nwhile L2-norm and L\u221e-norm based methods suffer from weak robustness against\nadversarial defense since they always diffuse tiny perturbations to all pixels. A\nmore realistic adversarial perturbation should be sparse and imperceptible. In this\npaper, we propose a novel Lp-norm distortion-efficient adversarial attack, which\nnot only owns the least L2-norm (or L\u221e-norm) loss but also significantly reduces\nthe L0-norm distortion. To this aim, we design a new optimization scheme, which\nfirst optimizes an initial adversarial perturbation under L2-norm (or L\u221e-norm)\nconstraint, and then constructs a dimension unimportance matrix for the initial\nperturbation. Such a dimension unimportance matrix can indicate the adversarial unimportance of each dimension of the initial perturbation. Furthermore, we\nintroduce a new concept of adversarial threshold for the dimension unimportance\nmatrix. The dimensions of the initial perturbation whose unimportance is higher\nthan the threshold will be all set to zero, greatly decreasing the L0-norm distortion.\nExperimental results on three benchmark datasets (MNIST, CIFAR10, and ImageNet) show that under the same query budget, the adversarial examples generated\nby our method have lower L0-norm and L2-norm (or L\u221e-norm) distortion than the\nstate-of-the-art. Especially for the MNIST dataset, our attack reduces 8.1% L2-\nnorm distortion meanwhile remaining 47% pixels unattacked. This demonstrates\nthe superiority of the proposed method over its competitors in terms of adversarial\nrobustness and visual imperceptibility.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.03277",
        "title": "INSTRUCTION TUNING WITH GPT-4",
        "abstract": "Prior work has shown that finetuning large language models (LLMs) using machinegenerated instruction-following data enables such models to achieve remarkable\nzero-shot capabilities on new tasks, and no human-written instructions are needed.\nIn this paper, we present the first attempt to use GPT-4 to generate instructionfollowing data for LLM finetuning. Our early experiments on instruction-tuned\nLLaMA models show that the 52K English and Chinese instruction-following\ndata generated by GPT-4 leads to superior zero-shot performance on new tasks to\nthe instruction-following data generated by previous state-of-the-art models. We\nalso collect feedback and comparison data from GPT-4 to enable a comprehensive\nevaluation and reward model training. We make our data generated using GPT-4 as\nwell as our codebase publicly available. 1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.13245",
        "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
        "abstract": "Multi-query attention (MQA), which only uses\na single key-value head, drastically speeds up\ndecoder inference. However, MQA can lead to\nquality degradation, and moreover it may not\nbe desirable to train a separate model just for\nfaster inference. We (1) propose a recipe for\nuptraining existing multi-head language model\ncheckpoints into models with MQA using 5%\nof original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses\nan intermediate (more than one, less than number of query heads) number of key-value heads.\nWe show that uptrained GQA achieves quality\nclose to multi-head attention with comparable\nspeed to MQA.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2306.13394",
        "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models",
        "abstract": "Multimodal Large Language Model (MLLM) relies on\nthe powerful LLM to perform multimodal tasks, showing\namazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for\nthese case studies to fully reflect the performance of MLLM,\nlacking a comprehensive evaluation. In this paper, we fill\nin this blank, presenting the first comprehensive MLLM\nEvaluation benchmark MME1\n. It measures both perception\nand cognition abilities on a total of 14 subtasks. In order\nto avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instructionanswer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with\nsuch an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the\nsubsequent model optimization. The data application manner and online leaderboards are released at https://\ngithub.com/BradyFU/Awesome- MultimodalLarge-Language-Models/tree/Evaluation.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.16154",
        "title": "MMVP: Motion-Matrix-based Video Prediction",
        "abstract": "A central challenge of video prediction lies where the\nsystem has to reason the objects\u2019 future motions from image\nframes while simultaneously maintaining the consistency\nof their appearances across frames. This work introduces\nan end-to-end trainable two-stream video prediction framework, Motion-Matrix-based Video Prediction (MMVP), to\ntackle this challenge. Unlike previous methods that usually handle motion prediction and appearance maintenance\nwithin the same set of modules, MMVP decouples motion\nand appearance information by constructing appearanceagnostic motion matrices. The motion matrices represent\nthe temporal similarity of each and every pair of feature\npatches in the input frames, and are the sole input of the\nmotion prediction module in MMVP. This design improves\nvideo prediction in both accuracy and efficiency, and reduces the model size. Results of extensive experiments\ndemonstrate that MMVP outperforms state-of-the-art systems on public data sets by non-negligible large margins (\u2248\n1 db in PSNR, UCF Sports) in significantly smaller model\nsizes (84% the size or smaller). Please refer to this link for\nthe official code and the datasets used in this paper",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.14596",
        "title": "ICAL: Continual Learning of Multimodal Agents by Transforming Trajectories into Actionable Insights",
        "abstract": "Large-scale generative language and vision-language models (LLMs and VLMs) excel in few-shot in-context learning for decision making and instruction following. However, they require high-quality exemplar demonstrations to be included in their context window. In this work, we ask: Can LLMs and VLMs generate their own prompt examples from generic, sub-optimal demonstrations? We propose In-Context Abstraction Learning (ICAL), a method that builds a memory of multimodal experience insights from sub-optimal demonstrations and human feedback. Given a noisy demonstration in a new domain, VLMs abstract the trajectory into a general program by fixing inefficient actions and annotating cognitive abstractions: task relationships, object state changes, temporal subgoals, and task construals. These abstractions are refined and adapted interactively through human feedback while the agent attempts to execute the trajectory in a similar environment. The resulting abstractions, when used as exemplars in the prompt, significantly improve decision-making in retrieval-augmented LLM and VLM agents. Our ICAL agent surpasses the state-of-the-art in dialogue-based instruction following in TEACh, multimodal web agents in VisualWebArena, and action anticipation in Ego4D. In TEACh, we achieve a 12.6% improvement in goal-condition success. In VisualWebArena, our task success rate improves over the SOTA from 14.3% to 22.7%. In Ego4D action forecasting, we improve over few-shot GPT-4V and remain competitive with supervised models. We show finetuning our retrieval-augmented in-context agent yields additional improvements. Our approach significantly reduces reliance on expert-crafted examples and consistently outperforms in-context learning from action plans that lack such insights.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.14599",
        "title": "Stylebreeder : Exploring and Democratizing Artistic Styles through Text-to-Image Models",
        "abstract": "Text-to-image models are becoming increasingly popular, revolutionizing the land- scape of digital art creation by enabling highly detailed and creative visual content generation. These models have been widely employed across various domains, particularly in art generation, where they facilitate a broad spectrum of creative expression and democratize access to artistic creation. In this paper, we introduce STYLEBREEDER, a comprehensive dataset of 6.8M images and 1.8M prompts gen- erated by 95K users on Artbreeder, a platform that has emerged as a significant hub for creative exploration with over 13M users. We introduce a series of tasks with this dataset aimed at identifying diverse artistic styles, generating personalized content, and recommending styles based on user interests. By documenting unique, user-generated styles that transcend conventional categories like \u2018cyberpunk\u2019 or \u2018Picasso,\u2019 we explore the potential for unique, crowd-sourced styles that could provide deep insights into the collective creative psyche of users worldwide. We also evaluate different personalization methods to enhance artistic expression and introduce a style atlas, making these models available in LoRA format for public use. Our research demonstrates the potential of text-to-image diffusion models to uncover and promote unique artistic expressions, further democratizing AI in art and fostering a more diverse and inclusive artistic community. The dataset, code and models are available at https://stylebreeder.github.io under a Public Domain (CC0) license.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.14874",
        "title": "TraceNet: Segment one thing efficiently",
        "abstract": "Efficient single instance segmentation is essential for unlocking features in the mobile imaging applications, such as capture or editing. Existing on-the-fly mobile imaging applications scope the segmentation task to portraits or the salient subject due to the computational constraints. Instance segmentation, despite its re- cent developments towards efficient networks, is still heavy due to the cost of computation on the entire image to identify all in- stances. To address this, we propose and formulate a one tap driven single instance segmentation task that segments a single instance selected by a user via a positive tap. This task, in contrast to the broader task of segmenting anything as suggested in the Segment Anything Model [19], focuses on efficient segmentation of a single instance specified by the user. To solve this problem, we present TraceNet, which explicitly locates the selected instance by way of receptive field tracing. TraceNet identifies image regions that are related to the user tap and heavy computations are only performed on selected regions of the image. Therefore overall computation cost and memory consumption are reduced during inference. We evaluate the performance of TraceNet on instance IoU average over taps and the proportion of the region that a user tap can fall into for a high-quality single-instance mask. Experimental results on MS-COCO and LVIS demonstrate the effectiveness and efficiency of the proposed approach. TraceNet can jointly achieve the efficiency and interactivity, filling in the gap between needs for efficient mo- bile inference and recent research trend towards multimodal and interactive segmentation models.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2407.01424",
        "title": "A Global-Local Attention Mechanism for Relation Classification",
        "abstract": "Relation classification, a crucial component of relation extraction, involves identifying connections between two entities. Previous studies have predominantly focused on integrating the attention mechanism into relation classification at a global scale, overlooking the importance of the local context. To address this gap, this paper introduces a novel global-local attention mechanism for relation classification, which enhances global attention with a localized focus. Additionally, we propose innovative hard and soft localization mechanisms to identify potential keywords for local attention. By incorporating both hard and soft localization strategies, our approach offers a more nuanced and comprehensive understanding of the contextual cues that contribute to effective relation classification. Our experimental results on the SemEval-2010 Task 8 dataset highlight the superior performance of our method compared to previous attention-based approaches in relation classification.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2407.01411",
        "title": "HyperLoader: Integrating Hypernetwork-Based LoRA and Adapter Layers into Multi-Task Transformers for Sequence Labelling",
        "abstract": "We present HyperLoader, a simple approach that combines different parameter-efficient fine-tuning methods in a multi-task setting. To achieve this goal, our model uses a hypernetwork to generate the weights of these modules based on the task, the transformer layer, and its position within this layer. Our method combines the benefits of multi-task learning by capturing the structure of all tasks while reducing the task interference problem by encapsulating the task-specific knowledge in the generated weights and the benefits of combining different parameter-efficient methods to outperform full-fine tuning. We provide empirical evidence that HyperLoader outperforms previous approaches in most datasets and obtains the best average performance across tasks in high-resource and low-resource scenarios.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2407.01409",
        "title": "Dynamic Few-Shot Learning for Knowledge Graph Question Answering",
        "abstract": "Large language models present opportunities for innovative Question Answering over Knowledge Graphs (KGQA). However, they are not inherently designed for query generation. To bridge this gap, solutions have been proposed that rely on fine-tuning or ad-hoc architectures, achieving good results but limited out-of-domain distribution generalization. In this study, we introduce a novel approach called Dynamic Few-Shot Learning (DFSL). DFSL integrates the efficiency of in-context learning and semantic similarity and provides a generally applicable solution for KGQA with state-of-the-art performance. We run an extensive evaluation across multiple benchmark datasets and architecture configurations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2407.01393",
        "title": "POLygraph: Polish Fake News Dataset",
        "abstract": "This paper presents the POLygraph dataset, a unique resource for fake news detection in Polish. The dataset, created by an interdisciplinary team, is composed of two parts: the \"fake-or-not\" dataset with 11,360 pairs of news articles (identified by their URLs) and corresponding labels, and the \"fake-they-say\" dataset with 5,082 news articles (identified by their URLs) and tweets commenting on them. Unlike existing datasets, POLygraph encompasses a variety of approaches from source literature, providing a comprehensive resource for fake news detection. The data was collected through manual annotation by expert and non-expert annotators. The project also developed a software tool that uses advanced machine learning techniques to analyze the data and determine content authenticity. The tool and dataset are expected to benefit various entities, from public sector institutions to publishers and fact-checking organizations. Further dataset exploration will foster fake news detection and potentially stimulate the implementation of similar models in other languages. The paper focuses on the creation and composition of the dataset, so it does not include a detailed evaluation of the software tool for content authenticity analysis, which is planned at a later stage of the project.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.01374",
        "title": "Bridging the Gap: Transfer Learning from English PLMs to Malaysian English",
        "abstract": "Malaysian English is a low resource creole language, where it carries the elements of Malay, Chinese, and Tamil languages, in addition to Standard English. Named Entity Recognition (NER) models underperform when capturing entities from Malaysian English text due to its distinctive morphosyntactic adaptations, semantic features and code-switching (mixing English and Malay). Considering these gaps, we introduce MENmBERT and MENBERT, a pre-trained language model with contextual understanding, specifically tailored for Malaysian English. We have fine-tuned MENmBERT and MENBERT using manually annotated entities and relations from the Malaysian English News Article (MEN) Dataset. This fine-tuning process allows the PLM to learn representations that capture the nuances of Malaysian English relevant for NER and RE tasks. MENmBERT achieved a 1.52\\% and 26.27\\% improvement on NER and RE tasks respectively compared to the bert-base-multilingual-cased model. Although the overall performance of NER does not have a significant improvement, our further analysis shows that there is a significant improvement when evaluated by the 12 entity labels. These findings suggest that pre-training language models on language-specific and geographically-focused corpora can be a promising approach for improving NER performance in low-resource settings. The dataset and code published in this paper provide valuable resources for NLP research work focusing on Malaysian English.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.01315",
        "title": "Language Portability Strategies for Open-domain Dialogue with Pre-trained Language Models from High to Low Resource Languages",
        "abstract": "In this paper we propose a study of linguistic portability strategies of large pre-trained language models (PLMs) used for open-domain dialogue systems in a high-resource language for this task. In particular the target low-resource language (L_T) will be simulated with French, as it lacks of task-specific resources and allows our human evaluation, when the source language (L_S) is English. For obvious reasons, recent works using such models for open-domain dialogue are mostly developed in English. Yet building specific PLMs for each possible target language supposes collecting new datasets and is costly. For this reason, trying to leverage all existing resources (PLMs and data) in both L_S and L_T , we wish to assess the performance achievable in L_T with different approaches. The first two approaches evaluate the usage of Neural Machine Translation (NMT) at different levels: TrainOnTarget where a L_S dataset is translated before fine-tuning in L_T and TestOnSource where a L_S model is coupled with NMT modules during inference. Then, the advent of BLOOM [2], the world first open-access multilingual large PLM, allow researchers to develop new approaches aiming to leverage not only the model's full accessibility but also its multilingualism and translation abilities. In this context the task is learned in L_S first and adapted to L_T using the MAD-X Adapter architecture [16]. In the two sets of experiments models are evaluated in spoken dialogue conditions with human and the strategies can be compared in terms of perceived interaction quality.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.01264",
        "title": "SignCLIP: Connecting Text and Sign Language by Contrastive Learning",
        "abstract": "We present SignCLIP, which re-purposes CLIP (Contrastive Language-Image Pretraining) to project spoken language text and sign language videos, two classes of natural languages of distinct modalities, into the same space. SignCLIP is an efficient method of learning useful visual representations for sign language processing from large-scale, multilingual video-text pairs, without directly optimizing for a specific task or sign language which is often of limited size. We pretrain SignCLIP on Spreadthesign, a prominent sign language dictionary consisting of ~500 thousand video clips in up to 44 sign languages, and evaluate it with various downstream datasets. SignCLIP discerns in-domain signing with notable text-to-video/video-to-text retrieval accuracy. It also performs competitively for out-of-domain downstream tasks such as isolated sign language recognition upon essential few-shot prompting or fine-tuning. We analyze the latent space formed by the spoken language text and sign language poses, which provides additional linguistic insights. Our code and models are openly available.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01530",
        "title": "xLSTM-UNet can be an Effective 2D \\& 3D Medical Image Segmentation Backbone with Vision-LSTM (ViL) better than its Mamba Counterpart",
        "abstract": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViT) have been pivotal in biomedical image segmentation, yet their ability to manage long-range dependencies remains constrained by inherent locality and computational overhead. To overcome these challenges, in this technical report, we first propose xLSTM-UNet, a UNet structured deep learning neural network that leverages Vision-LSTM (xLSTM) as its backbone for medical image segmentation. xLSTM is a recently proposed as the successor of Long Short-Term Memory (LSTM) networks and have demonstrated superior performance compared to Transformers and State Space Models (SSMs) like Mamba in Neural Language Processing (NLP) and image classification (as demonstrated in Vision-LSTM, or ViL implementation). Here, xLSTM-UNet we designed extend the success in biomedical image segmentation domain. By integrating the local feature extraction strengths of convolutional layers with the long-range dependency capturing abilities of xLSTM, xLSTM-UNet offers a robust solution for comprehensive image analysis. We validate the efficacy of xLSTM-UNet through experiments. Our findings demonstrate that xLSTM-UNet consistently surpasses the performance of leading CNN-based, Transformer-based, and Mamba-based segmentation networks in multiple datasets in biomedical segmentation including organs in abdomen MRI, instruments in endoscopic images, and cells in microscopic images. With comprehensive experiments performed, this technical report highlights the potential of xLSTM-based architectures in advancing biomedical image analysis in both 2D and 3D. The code, models, and datasets are publicly available at \\href{this http URL}{this http URL}",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2403.12895",
        "title": "mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding",
        "abstract": "Structure information is critical for understanding the semantics of text-rich images, such as documents, tables, and charts. Existing Multimodal Large Language Models (MLLMs) for Visual Document Understanding are equipped with text recognition ability but lack general structure understanding abilities for text-rich document images. In this work, we emphasize the importance of structure information in Visual Document Understanding and propose the Unified Structure Learning to boost the performance of MLLMs. Our Unified Structure Learning comprises structureaware parsing tasks and multi-grained text localization tasks across 5 domains: document, webpage, table, chart, and natural image. To better encode structure information, we design a simple and effective vision-to-text module H-Reducer, which can not only maintain the layout information but also reduce the length of visual features by merging horizontal adjacent patches through convolution, enabling the LLM to understand high-resolution images more efficiently. Furthermore, by constructing structure-aware text sequences and multi-grained pairs of texts and bounding boxes for publicly available text-rich images, we build a comprehensive training set DocStruct4M to support structure learning. Finally, we construct a small but high-quality reasoning tuning dataset DocReason25K to trigger the detailed explanation ability in the document domain. Our model DocOwl 1.5 achieves state-of-the-art performance on 10 visual document understanding benchmarks, improving the SOTA performance of MLLMs with a 7B LLM by more than 10 points in 5/10 benchmarks. Our codes, models, and datasets are publicly available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.01080",
        "title": "Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese",
        "abstract": "The prevailing issue of factual inconsistency errors in conventional Retrieval Augmented Generation (RAG) motivates the study of Factual Consistency Evaluation (FCE). Despite the various FCE methods proposed earlier, these methods are evaluated on datasets generated by specific Large Language Models (LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE methods perform on other LLMs with different error distributions or even unseen error types, as these methods may fail to detect the error types generated by other LLMs. To fill this gap, in this paper, we propose the first comprehensive FCE benchmark \\emph{Face4RAG} for RAG independent of the underlying LLM. Our benchmark consists of a synthetic dataset built upon a carefully designed typology for factuality inconsistency error and a real-world dataset constructed from six commonly used LLMs, enabling evaluation of FCE methods on specific error types or real-world error distributions. On the proposed benchmark, we discover the failure of existing FCE methods to detect the logical fallacy, which refers to a mismatch of logic structures between the answer and the retrieved reference. To fix this issue, we further propose a new method called \\emph{L-Face4RAG} with two novel designs of logic-preserving answer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG substantially outperforms previous methods for factual inconsistency detection on a wide range of tasks, notably beyond the RAG task from which it is originally motivated. Both the benchmark and our proposed method are publicly available.\\footnote{\\url{this https URL}\\label{link_face4rag}}",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.01026",
        "title": "Augmenting Document-level Relation Extraction with Efficient Multi-Supervision",
        "abstract": "Despite its popularity in sentence-level relation extraction, distantly supervised data is rarely utilized by existing work in document-level relation extraction due to its noisy nature and low information density. Among its current applications, distantly supervised data is mostly used as a whole for pertaining, which is of low time efficiency. To fill in the gap of efficient and robust utilization of distantly supervised training data, we propose Efficient Multi-Supervision for document-level relation extraction, in which we first select a subset of informative documents from the massive dataset by combining distant supervision with expert supervision, then train the model with Multi-Supervision Ranking Loss that integrates the knowledge from multiple sources of supervision to alleviate the effects of noise. The experiments demonstrate the effectiveness of our method in improving the model performance with higher time efficiency than existing baselines.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.00938",
        "title": "MalAlgoQA: A Pedagogical Approach for Evaluating Counterfactual Reasoning Abilities",
        "abstract": "This paper introduces MalAlgoQA, a novel dataset designed to evaluate the counterfactual reasoning capabilities of Large Language Models (LLMs) through a pedagogical approach. The dataset comprises mathematics and reading comprehension questions, each accompanied by four answer choices and their corresponding rationales. We focus on the incorrect answer rationales, termed \"malgorithms\", which highlights flawed reasoning steps leading to incorrect answers and offers valuable insights into erroneous thought processes. We also propose the Malgorithm Identification task, where LLMs are assessed based on their ability to identify corresponding malgorithm given an incorrect answer choice. To evaluate the model performance, we introduce two metrics: Algorithm Identification Accuracy (AIA) for correct answer rationale identification, and Malgorithm Identification Accuracy (MIA) for incorrect answer rationale identification. The task is challenging since state-of-the-art LLMs exhibit significant drops in MIA as compared to AIA. Moreover, we find that the chain-of-thought prompting technique not only fails to consistently enhance MIA, but can also lead to underperformance compared to simple prompting. These findings hold significant implications for the development of more cognitively-inspired LLMs to improve their counterfactual reasoning abilities, particularly through a pedagogical perspective where understanding and rectifying student misconceptions are crucial.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.00668",
        "title": "HRDE: Retrieval-Augmented Large Language Models for Chinese Health Rumor Detection and Explainability",
        "abstract": "As people increasingly prioritize their health, the speed and breadth of health information dissemination on the internet have also grown. At the same time, the presence of false health information (health rumors) intermingled with genuine content poses a significant potential threat to public health. However, current research on Chinese health rumors still lacks a large-scale, public, and open-source dataset of health rumor information, as well as effective and reliable rumor detection methods. This paper addresses this gap by constructing a dataset containing 1.12 million health-related rumors (HealthRCN) through web scraping of common health-related questions and a series of data processing steps. HealthRCN is the largest known dataset of Chinese health information rumors to date. Based on this dataset, we propose retrieval-augmented large language models for Chinese health rumor detection and explainability (HRDE). This model leverages retrieved relevant information to accurately determine whether the input health information is a rumor and provides explanatory responses, effectively aiding users in verifying the authenticity of health information. In evaluation experiments, we compared multiple models and found that HRDE outperformed them all, including GPT-4-1106-Preview, in rumor detection accuracy and answer quality. HRDE achieved an average accuracy of 91.04% and an F1 score of 91.58%.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.00653",
        "title": "Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs",
        "abstract": "Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new knowledge from existing one.While it has been widely studied in the context of knowledge graphs (KGs), knowledge reasoning in LLMs remains underexplored. In this paper, we introduce Chain-of-Knowledge, a comprehensive framework for knowledge reasoning, including methodologies for both dataset construction and model learning. For dataset construction, we create KnowReason via rule mining on KGs. For model learning, we observe rule overfitting induced by naive training. Hence, we enhance CoK with a trial-and-error mechanism that simulates the human process of internal knowledge exploration. We conduct extensive experiments with KnowReason. Our results show the effectiveness of CoK in refining LLMs in not only knowledge reasoning, but also general reasoning benchmarkms.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.01212",
        "title": "EconNLI: Evaluating Large Language Models on Economics Reasoning",
        "abstract": "Large Language Models (LLMs) are widely used for writing economic analysis reports or providing financial advice, but their ability to understand economic knowledge and reason about potential results of specific economic events lacks systematic evaluation. To address this gap, we propose a new dataset, natural language inference on economic events (EconNLI), to evaluate LLMs' knowledge and reasoning abilities in the economic domain. We evaluate LLMs on (1) their ability to correctly classify whether a premise event will cause a hypothesis event and (2) their ability to generate reasonable events resulting from a given premise. Our experiments reveal that LLMs are not sophisticated in economic reasoning and may generate wrong or hallucinated answers. Our study raises awareness of the limitations of using LLMs for critical decision-making involving economic reasoning and analysis. The dataset and codes are available at this https URL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.01091",
        "title": "M2QA: Multi-domain Multilingual Question Answering",
        "abstract": "Generalization and robustness to input variation are core desiderata of machine learning research. Language varies along several axes, most importantly, language instance (e.g. French) and domain (e.g. news). While adapting NLP models to new languages within a single domain, or to new domains within a single language, is widely studied, research in joint adaptation is hampered by the lack of evaluation datasets. This prevents the transfer of NLP systems from well-resourced languages and domains to non-dominant language-domain combinations. To address this gap, we introduce M2QA, a multi-domain multilingual question answering benchmark. M2QA includes 13,500 SQuAD 2.0-style question-answer instances in German, Turkish, and Chinese for the domains of product reviews, news, and creative writing. We use M2QA to explore cross-lingual cross-domain performance of fine-tuned models and state-of-the-art LLMs and investigate modular approaches to domain and language adaptation. We witness 1) considerable performance variations across domain-language combinations within model classes and 2) considerable performance drops between source and target language-domain combinations across all model sizes. We demonstrate that M2QA is far from solved, and new methods to effectively transfer both linguistic and domain-specific information are necessary. We make M2QA publicly available at this https URL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.05653",
        "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning",
        "abstract": "We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT4\u2019s CoT result. Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2405.11891",
        "title": "Unveiling and Manipulating Prompt Influence in Large Language Models",
        "abstract": "Prompts play a crucial role in guiding the responses of Large Language Models (LLMs). However, the intricate role of individual tokens in prompts, known as input saliency, in shaping the responses remains largely underexplored. Existing saliency methods either misalign with LLM generation objectives or rely heavily on linearity assumptions, leading to potential inaccuracies. To address this, we propose Token Distribution Dynamics (TDD), a simple yet effective approach to unveil and manipulate the role of prompts in generating LLM outputs. TDD leverages the robust interpreting capabilities of the language model head (LM head) to assess input saliency. It projects input tokens into the embedding space and then estimates their significance based on distribution dynamics over the vocabulary. We introduce three TDD variants: forward, backward, and bidirectional, each offering unique insights into token relevance. Extensive experiments reveal that the TDD surpasses state-of-the-art baselines with a big margin in elucidating the causal relationships between prompts and LLM outputs. Beyond mere interpretation, we apply TDD to two prompt manipulation tasks for controlled text generation: zero-shot toxic language suppression and sentiment steering. Empirical results underscore TDD\u2019s proficiency in identifying both toxic and sentimental cues in prompts, subsequently mitigating toxicity or modulating sentiment in the generated content.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2405.15012",
        "title": "Extracting Prompts by Inverting LLM Outputs",
        "abstract": "We consider the problem of language model inversion: given outputs of a language model, we seek to extract the prompt that generated these outputs. We develop a new black-box method, output2prompt, that extracts prompts without access to the model\u2019s logits and without adversarial or jailbreaking queries. Unlike previous methods, output2prompt only needs outputs of normal user queries. To improve memory efficiency, output2prompt employs a new sparse encoding techique. We measure the efficacy of output2prompt on a variety of user and system prompts and demonstrate zero-shot transferability across different LLMs. 1",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2401.14295",
        "title": "Demystifying Chains, Trees, and Graphs of Thoughts",
        "abstract": "The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models\u2019 (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM\u2019s capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxonomy of structure-enhanced LLM reasoning schemes. We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others. We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context. Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost. We also outline theoretical underpinnings, relationships between prompting and other parts of the LLM ecosystem such as knowledge bases, and the associated research challenges. Our work will help to advance future prompt engineering techniques.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2406.16860",
        "title": "Cambrian-1: A Fully Open, Vision-CentricExploration of Multimodal LLMs",
        "abstract": "We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a visioncentric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in realworld scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures\u2014 self-supervised, strongly supervised, or combinations thereof\u2014based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, addressing the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.Website https://cambrian-mllm.github.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01725",
        "title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
        "abstract": "Can the rapid advances in code generation, function calling, and data analysis using large language models (LLMs) help automate the search and verification of hypotheses purely from a set of provided datasets? To evaluate this question, we present DiscoveryBench, the first comprehensive benchmark that formalizes the multi-step process of data-driven discovery. The benchmark is designed to systematically assess current model capabilities in discovery tasks and provide a useful resource for improving them. Our benchmark contains 264 tasks collected across 6 diverse domains, such as sociology and engineering, by manually deriving discovery workflows from published papers to approximate the real-world challenges faced by researchers, where each task is defined by a dataset, its metadata, and a discovery goal in natural language. We additionally provide 903 synthetic tasks to conduct controlled evaluations across task complexity. Furthermore, our structured formalism of data-driven discovery enables a facet-based evaluation that provides useful insights into different failure modes. We evaluate several popular LLM-based reasoning frameworks using both open and closed LLMs as baselines on DiscoveryBench and find that even the best system scores only 25%. Our benchmark, thus, illustrates the challenges in autonomous data-driven discovery and serves as a valuable resource for the community to make progress.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01863",
        "title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
        "abstract": "Vision language models (VLMs) are an exciting emerging class of language models (LMs) that have merged classic LM capabilities with those of image processing systems. However, the ways that these capabilities combine are not always intuitive and warrant direct investigation. One understudied capability in VLMs is visual spatial planning -- the ability to comprehend the spatial arrangements of objects and devise action plans to achieve desired outcomes in visual scenes. In our study, we introduce VSP, a benchmark that 1) evaluates the spatial planning capability in these models in general, and 2) breaks down the visual planning task into finer-grained sub-tasks, including perception and reasoning, and measure the LMs capabilities in these sub-tasks. Our evaluation shows that both open-source and private VLMs fail to generate effective plans for even simple spatial planning tasks. Evaluations on the fine-grained analytical tasks further reveal fundamental deficiencies in the models' visual perception and bottlenecks in reasoning abilities, explaining their worse performance in the general spatial planning tasks. Our work illuminates future directions for improving VLMs' abilities in spatial planning. Our benchmark is publicly available at https://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01896",
        "title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis",
        "abstract": "Log analysis is crucial for ensuring the orderly and stable operation of information systems, particularly in the field of Artificial Intelligence for IT Operations (AIOps). Large Language Models (LLMs) have demonstrated significant potential in natural language processing tasks. In the AIOps domain, they excel in tasks such as anomaly detection, root cause analysis of faults, operations and maintenance script generation, and alert information summarization. However, the performance of current LLMs in log analysis tasks remains inadequately validated. To address this gap, we introduce LogEval, a comprehensive benchmark suite designed to evaluate the capabilities of LLMs in various log analysis tasks for the first time. This benchmark covers tasks such as log parsing, log anomaly detection, log fault diagnosis, and log summarization. LogEval evaluates each task using 4,000 publicly available log data entries and employs 15 different prompts for each task to ensure a thorough and fair assessment. By rigorously evaluating leading LLMs, we demonstrate the impact of various LLM technologies on log analysis performance, focusing on aspects such as self-consistency and few-shot contextual learning. We also discuss findings related to model quantification, Chinese-English question-answering evaluation, and prompt engineering. These findings provide insights into the strengths and weaknesses of LLMs in multilingual environments and the effectiveness of different prompt strategies. Various evaluation methods are employed for different tasks to accurately measure the performance of LLMs in log analysis, ensuring a comprehensive assessment. The insights gained from LogEvals evaluation reveal the strengths and limitations of LLMs in log analysis tasks, providing valuable guidance for researchers and practitioners.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01909",
        "title": "Pinyin Regularization in Error Correction for Chinese Speech Recognition with Large Language Models",
        "abstract": "Recent studies have demonstrated the efficacy of large language models (LLMs) in error correction for automatic speech recognition (ASR). However, much of the research focuses on the English language. This paper redirects the attention to Chinese. Firstly, we construct a specialized benchmark dataset aimed at error correction for Chinese ASR with 724K hypotheses transcription pairs, named the Chinese Hypotheses Paradise dataset (ChineseHP), which contains a wide range of scenarios and presents significant challenges. Subsequently, we conduct a preliminary evaluation using the dataset for both direct-prompting and fine-tuning pre-trained LLMs. Furthermore, we propose a straightforward method of Pinyin regularization for prompts, which involves the transcription of Pinyin directly from text hypotheses. The experimental results reveal that Pinyin regularization consistently enhances the error-correcting ability of LLMs when compared with those without regularization. The dataset is available on the website.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01920",
        "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
        "abstract": "Large Language Models (LLMs) trained on extensive corpora inevitably retain sensitive data, such as personal privacy information and copyrighted material. Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters. Experimental results show that MemFlex is superior to existing methods in both precise knowledge unlearning and general knowledge retaining of LLMs. Code and dataset will be released at https://github.com/zjunlp/KnowUnDo.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.16962",
        "title": "MetaGreen: Meta-Learning Inspired Transformer Selection for Green Semantic Communication",
        "abstract": "Semantic Communication can transform the way we transmit information, prioritizing meaningful and effective content over individual symbols or bits. This evolution promises significant benefits, including reduced latency, lower bandwidth usage, and higher throughput compared to traditional communication. However, the development of Semantic Communication faces a crucial challenge: the need for universal metrics to benchmark the joint effects of semantic information loss and energy consumption. This research introduces an innovative solution: the ``Energy-Optimized Semantic Loss'' (EOSL) function, a novel multi-objective loss function that effectively balances semantic information loss and energy consumption. Through comprehensive experiments on transformer models, including energy benchmarking, we demonstrate the remarkable effectiveness of EOSL-based model selection. We have established that EOSL-based transformer model selection achieves up to 83\\% better similarity-to-power ratio (SPR) compared to BLEU score-based selection and 67\\% better SPR compared to solely lowest power usage-based selection. Furthermore, we extend the applicability of EOSL to diverse and varying contexts, inspired by the principles of Meta-Learning. By cumulatively applying EOSL, we enable the model selection system to adapt to this change, leveraging historical EOSL values to guide the learning process. This work lays the foundation for energy-efficient model selection and the development of green semantic communication.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2406.16953",
        "title": "Validation of a new, minimally-invasive, software smartphone device to predict sleep apnea and its severity: transversal study",
        "abstract": "Obstructive sleep apnea (OSA) is frequent and responsible for cardiovascular complications and excessive daytime sleepiness. It is underdiagnosed due to the difficulty to access the gold standard for diagnosis, polysomnography (PSG). Alternative methods using smartphone sensors could be useful to increase diagnosis. The objective is to assess the performances of Apneal, an application that records the sound using a smartphone's microphone and movements thanks to a smartphone's accelerometer and gyroscope, to estimate patients' AHI. In this article, we perform a monocentric proof-of-concept study with a first manual scoring step, and then an automatic detection of respiratory events from the recorded signals using a sequential deep-learning model which was released internally at Apneal at the end of 2022 (version 0.1 of Apneal automatic scoring of respiratory events), in adult patients during in-hospital polysomnography.46 patients (women 34 per cent, mean BMI 28.7 kg per m2) were included. For AHI superior to 15, sensitivity of manual scoring was 0.91, and positive predictive value (PPV) 0.89. For AHI superior to 30, sensitivity was 0.85, PPV 0.94. We obtained an AUC-ROC of 0.85 and an AUC-PR of 0.94 for the identification of AHI superior to 15, and AUC-ROC of 0.95 and AUC-PR of 0.93 for AHI superior to 30. Promising results are obtained for the automatic annotations of events.This article shows that manual scoring of smartphone-based signals is possible and accurate compared to PSG-based scorings. Automatic scoring method based on a deep learning model provides promising results. A larger multicentric validation study, involving subjects with different SAHS severity is required to confirm these results.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2406.16903",
        "title": "Towards a copilot in BIM authoring tool using a large language model-based agent for intelligent human-machine interaction",
        "abstract": "Facing increasingly complex BIM authoring software and the accompanying expensive learning costs, designers often seek to interact with the software in a more intelligent and lightweight manner. They aim to automate modeling workflows, avoiding obstacles and difficulties caused by software usage, thereby focusing on the design process itself. To address this issue, we proposed an LLM-based autonomous agent framework that can function as a copilot in the BIM authoring tool, answering software usage questions, understanding the user's design intentions from natural language, and autonomously executing modeling tasks by invoking the appropriate tools. In a case study based on the BIM authoring software Vectorworks, we implemented a software prototype to integrate the proposed framework seamlessly into the BIM authoring scenario. We evaluated the planning and reasoning capabilities of different LLMs within this framework when faced with complex instructions. Our work demonstrates the significant potential of LLM-based agents in design automation and intelligent interaction.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2406.17095",
        "title": "Attention Instruction: Amplifying Attention in the Middle via Prompting",
        "abstract": "The context window of large language models has been extended to 128k tokens or more. However, language models still suffer from position bias and have difficulty in accessing and using the middle part of the context due to the lack of attention. We examine the relative position awareness of LLMs and the feasibility of mitigating disproportional attention through prompting. We augment the original task instruction with attention instructions",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2406.17294",
        "title": "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models",
        "abstract": "Large language models (LLMs) have demonstrated impressive reasoning capabilities, particularly in textual mathematical problem-solving. However, existing open-source image instruction fine-tuning datasets, containing limited question-answer pairs per image, do not fully exploit visual information to enhance the multimodal mathematical reasoning capabilities of Multimodal LLMs (MLLMs). To bridge this gap, we address the lack of high-quality, diverse multimodal mathematical datasets by collecting 40K high-quality images with question-answer pairs from 24 existing datasets and synthesizing 320K new pairs, creating the MathV360K dataset, which enhances both the breadth and depth of multimodal mathematical questions. We introduce Math-LLaVA, a LLaVA-1.5-based model fine-tuned with MathV360K. This novel approach significantly improves the multimodal mathematical reasoning capabilities of LLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V on MathVista's minitest split. Furthermore, Math-LLaVA demonstrates enhanced generalizability, showing substantial improvements on the MMMU benchmark. Our research highlights the importance of dataset diversity and synthesis in advancing MLLMs' mathematical reasoning abilities. The code and data are available at: \\url{this https URL}.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17287",
        "title": "Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models",
        "abstract": "Accurate assessment of personality traits is crucial for effective psycho-counseling, yet traditional methods like self-report questionnaires are time-consuming and biased. This study exams whether Large Language Models (LLMs) can predict the Big Five personality traits directly from counseling dialogues and introduces an innovative framework to perform the task. Our framework applies role-play and questionnaire-based prompting to condition LLMs on counseling sessions, simulating client responses to the Big Five Inventory. We evaluated our framework on 853 real-world counseling sessions, finding a significant correlation between LLM-predicted and actual Big Five traits, proving the validity of framework. Moreover, ablation studies highlight the importance of role-play simulations and task simplification via questionnaires in enhancing prediction accuracy. Meanwhile, our fine-tuned Llama3-8B model, utilizing Direct Preference Optimization with Supervised Fine-Tuning, achieves a 130.95\\% improvement, surpassing the state-of-the-art Qwen1.5-110B by 36.94\\% in personality prediction validity. In conclusion, LLMs can predict personality based on counseling dialogues. Our code and model are publicly available at \\url{this https URL}, providing a valuable tool for future research in computational psychometrics.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17274",
        "title": "Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?",
        "abstract": "Text summarization, a key natural language generation (NLG) task, is vital in various domains. However, the high cost of inaccurate summaries in risk-critical applications, particularly those involving human-in-the-loop decision-making, raises concerns about the reliability of uncertainty estimation on text summarization (UE-TS) evaluation methods. This concern stems from the dependency of uncertainty model metrics on diverse and potentially conflicting NLG metrics. To address this issue, we introduce a comprehensive UE-TS benchmark incorporating 31 NLG metrics across four dimensions. The benchmark evaluates the uncertainty estimation capabilities of two large language models and one pre-trained language model on three datasets, with human-annotation analysis incorporated where applicable. We also assess the performance of 14 common uncertainty estimation methods within this benchmark. Our findings emphasize the importance of considering multiple uncorrelated NLG metrics and diverse uncertainty estimation methods to ensure reliable and efficient evaluation of UE-TS techniques.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17260",
        "title": "Mitigating Hallucination in Fictional Character Role-Play",
        "abstract": "Role-playing has wide-ranging applications in customer support, embodied agents, computational social science, etc. The influence of parametric world knowledge of large language models (LLMs) often causes role-playing characters to act out of character and hallucinate about things outside the scope of their knowledge. In this work, we focus on the evaluation and mitigation of hallucination in fictional character role-play. We introduce a dataset with more than 2,000 characters and 72,000 interviews, including 18,000 adversarial questions. We propose RoleFact, a role-playing method that mitigates hallucination by modulating the influence of parametric knowledge using a pre-calibrated confidence threshold. Experiments show that the proposed method improves the factual precision of generated responses by 18% for adversarial questions with a 44% reduction in temporal hallucination for time-sensitive interviews. The code and the dataset will be available at this https URL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17253",
        "title": "How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?",
        "abstract": "As large language models (LLMs) are widely deployed, targeted editing of their knowledge has become a critical challenge. Recently, advancements in model editing techniques, such as Rank-One Model Editing (ROME), have paved the way for updating LLMs with new knowledge. However, the efficacy of these methods varies across different types of knowledge. This study investigates the capability of knowledge editing methods to incorporate new knowledge with varying degrees of \"perplexingness\", a term we use to describe the initial difficulty LLMs have in understanding new concepts. We begin by quantifying the \"perplexingness\" of target knowledge using pre-edit conditional probabilities, and assess the efficacy of edits through post-edit conditional probabilities. Utilizing the widely-used CounterFact dataset, we find significant negative correlations between the \"perplexingness\" of the new knowledge and the edit efficacy across all 12 scenarios. To dive deeper into this phenomenon, we introduce a novel dataset, HierarchyData, consisting of 99 hyponym-hypernym pairs across diverse categories. Our analysis reveal that more abstract concepts (hypernyms) tend to be more perplexing than their specific counterparts (hyponyms). Further exploration into the influence of knowledge hierarchy on editing outcomes indicates that knowledge positioned at higher hierarchical levels is more challenging to modify in some scenarios. Our research highlights a previously overlooked aspect of LLM editing: the variable efficacy of editing methods in handling perplexing knowledge. By revealing how hierarchical relationships can influence editing outcomes, our findings offer new insights into the challenges of updating LLMs and pave the way for more nuanced approaches to model editing in the future.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17169",
        "title": "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models",
        "abstract": "As Large Language Models (LLMs) continue to exhibit remarkable performance in natural language understanding tasks, there is a crucial need to measure their ability for human-like multi-step logical reasoning. Existing logical reasoning evaluation benchmarks often focus primarily on simplistic single-step or multi-step reasoning with a limited set of inference rules. Furthermore, the lack of datasets for evaluating non-monotonic reasoning represents a crucial gap since it aligns more closely with human-like reasoning. To address these limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset encompassing multi-step logical reasoning with various inference rules and depths. Multi-LogiEval covers three logic types--propositional, first-order, and non-monotonic--consisting of more than 30 inference rules and more than 60 of their combinations with various depths. Leveraging this dataset, we conduct evaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca, and Mistral, employing a zero-shot chain-of-thought. Experimental results show that there is a significant drop in the performance of LLMs as the reasoning steps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5). We further conduct a thorough investigation of reasoning chains generated by LLMs which reveals several important findings. We believe that Multi-LogiEval facilitates future research for evaluating and enhancing the logical reasoning ability of LLMs. Data is available at this https URL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17158",
        "title": "DEXTER: A Benchmark for open-domain Complex Question Answering using LLMs",
        "abstract": "Open-domain complex Question Answering (QA) is a difficult task with challenges in evidence retrieval and reasoning. The complexity of such questions could stem from questions being compositional, hybrid evidence, or ambiguity in questions. While retrieval performance for classical QA tasks is well explored, their capabilities for heterogeneous complex retrieval tasks, especially in an open-domain setting, and the impact on downstream QA performance, are relatively unexplored. To address this, in this work, we propose a benchmark composing diverse complex QA tasks and provide a toolkit to evaluate state-of-the-art pre-trained dense and sparse retrieval models in an open-domain setting. We observe that late interaction models and surprisingly lexical models like BM25 perform well compared to other pre-trained dense retrieval models. In addition, since context-based reasoning is critical for solving complex QA tasks, we also evaluate the reasoning capabilities of LLMs and the impact of retrieval performance on their reasoning capabilities. Through experiments, we observe that much progress is to be made in retrieval for complex QA to improve downstream QA performance. Our software and related data can be accessed at this https URL",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17557",
        "title": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale",
        "abstract": "The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. In this work, we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, we carefully document and ablate all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we publicly release our data curation codebase and all of the models trained during our ablation experiments.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17535",
        "title": "Disce aut Deficere: Evaluating LLMs Proficiency on the INVALSI Italian Benchmark",
        "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to generate and manipulate human language, highlighting their potential across various applications. Evaluating LLMs in languages other than English is crucial for ensuring their linguistic versatility, cultural relevance, and applicability in diverse global contexts, thus broadening their usability and effectiveness. We tackle this challenge by introducing a structured benchmark using the INVALSI tests, a set of well-established assessments designed to measure educational competencies across Italy. Our study makes three primary contributions: Firstly, we adapt the INVALSI benchmark for automated LLM evaluation, which involves rigorous adaptation of the test format to suit automated processing while retaining the essence of the original tests. Secondly, we provide a detailed assessment of current LLMs, offering a crucial reference point for the academic community. Finally, we visually compare the performance of these models against human results. Additionally, researchers are invited to submit their models for ongoing evaluation, ensuring the benchmark remains a current and valuable resource.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17526",
        "title": "LumberChunker: Long-Form Narrative Document Segmentation",
        "abstract": "Modern NLP tasks increasingly rely on dense retrieval methods to access up-to-date and relevant contextual information. We are motivated by the premise that retrieval benefits from segments that can vary in size such that a content's semantic independence is better captured. We propose LumberChunker, a method leveraging an LLM to dynamically segment documents, which iteratively prompts the LLM to identify the point within a group of sequential passages where the content begins to shift. To evaluate our method, we introduce GutenQA, a benchmark with 3000 \"needle in a haystack\" type of question-answer pairs derived from 100 public domain narrative books available on Project Gutenberg. Our experiments show that LumberChunker not only outperforms the most competitive baseline by 7.37% in retrieval performance (DCG@20) but also that, when integrated into a RAG pipeline, LumberChunker proves to be more effective than other chunking methods and competitive baselines, such as the Gemini 1.5M Pro. Our Code and Data are available at this https URL",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17385",
        "title": "Native Design Bias: Studying the Impact of English Nativeness on Language Model Performance",
        "abstract": "Large Language Models (LLMs) excel at providing information acquired during pretraining on large-scale corpora and following instructions through user prompts. This study investigates whether the quality of LLM responses varies depending on the demographic profile of users. Considering English as the global lingua franca, along with the diversity of its dialects among speakers of different native languages, we explore whether non-native English speakers receive lower-quality or even factually incorrect responses from LLMs more frequently. Our results show that performance discrepancies occur when LLMs are prompted by native versus non-native English speakers and persist when comparing native speakers from Western countries with others. Additionally, we find a strong anchoring effect when the model recognizes or is made aware of the user's nativeness, which further degrades the response quality when interacting with non-native speakers. Our analysis is based on a newly collected dataset with over 12,000 unique annotations from 124 annotators, including information on their native language and English proficiency.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17764",
        "title": "BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning",
        "abstract": "Large language models (LLMs) possess extensive parametric knowledge, but this knowledge is difficult to update with new information because retraining is very expensive and infeasible for closed-source models. Knowledge editing (KE) has emerged as a viable solution for updating the knowledge of LLMs without compromising their overall performance. On-the-fly KE methods, inspired by in-context learning (ICL), have shown great promise and allow LLMs to be treated as black boxes. In the past, KE was primarily employed in English contexts, whereas the potential for cross-lingual KE in current English-centric LLMs has not been fully explored. To foster more research in this direction, we introduce the BMIKE-53 benchmark for evaluating cross-lingual KE on 53 diverse languages across three KE task types. We also propose a gradient-free KE method called Multilingual In-context Knowledge Editing (MIKE) and evaluate it on BMIKE-53. Our evaluation focuses on cross-lingual knowledge transfer in terms of reliability, generality, locality, and portability, offering valuable insights and a framework for future research in cross-lingual KE. Our code and data are publicly accessible via the anonymous repository at https://anonymous.4open.science/r/MIKE.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17761",
        "title": "CaLMQA: Exploring culturally specific long-form question answering across 23 languages",
        "abstract": "Large language models (LLMs) are commonly used for long-form question answering, which requires them to generate paragraph-length answers to complex questions. While long-form QA has been well-studied in English via many different datasets and evaluation metrics, this research has not been extended to cover most other languages. To bridge this gap, we introduce CaLMQA, a collection of 2.6K complex questions spanning 23 languages, including under-resourced, rarely-studied languages such as Fijian and Kirundi. Our dataset includes both naturally-occurring questions collected from community web forums as well as questions written by native speakers, whom we hire for this purpose. Our process yields diverse, complex questions that reflect cultural topics (e.g. traditions, laws, news) and the language usage of native speakers. We conduct automatic evaluation across a suite of open- and closed-source models using our novel metric CaLMScore, which detects incorrect language and token repetitions in answers, and observe that the quality of LLM-generated answers degrades significantly for some low-resource languages. We perform human evaluation on a subset of models and see that model performance is significantly worse for culturally specific questions than for culturally agnostic questions. Our findings highlight the need for further research in LLM multilingual capabilities and non-English LFQA evaluation.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17755",
        "title": "Accelerating Clinical Evidence Synthesis with Large Language Models",
        "abstract": "Automatic medical discovery by AI is a dream of many. One step toward that goal is to create an AI model to understand clinical studies and synthesize clinical evidence from the literature. Clinical evidence synthesis currently relies on systematic reviews of clinical trials and retrospective analyses from medical literature. However, the rapid expansion of publications presents challenges in efficiently identifying, summarizing, and updating evidence. We introduce TrialMind, a generative AI-based pipeline for conducting medical systematic reviews, encompassing study search, screening, and data extraction phases. We utilize large language models (LLMs) to drive each pipeline component while incorporating human expert oversight to minimize errors. To facilitate evaluation, we also create a benchmark dataset TrialReviewBench, a custom dataset with 870 annotated clinical studies from 25 meta-analysis papers across various medical treatments. Our results demonstrate that TrialMind significantly improves the literature review process, achieving high recall rates (0.897-1.000) in study searching from over 20 million PubMed studies and outperforming traditional language model embeddings-based methods in screening (Recall@20 of 0.227-0.246 vs. 0.000-0.102). Furthermore, our approach surpasses direct GPT-4 performance in result extraction, with accuracy ranging from 0.65 to 0.84. We also support clinical evidence synthesis in forest plots, as validated by eight human annotators who preferred TrialMind over the GPT-4 baseline with a winning rate of 62.5%-100% across the involved reviews. Our findings suggest that an LLM-based clinical evidence synthesis approach, such as TrialMind, can enable reliable and high-quality clinical evidence synthesis to improve clinical research efficiency.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17716",
        "title": "ViANLI: Adversarial Natural Language Inference for Vietnamese",
        "abstract": "The development of Natural Language Processing (NLI) datasets and models has been inspired by innovations in annotation design. With the rapid development of machine learning models today, the performance of existing machine learning models has quickly reached state-of-the-art results on a variety of tasks related to natural language processing, including natural language inference tasks. By using a pre-trained model during the annotation process, it is possible to challenge current NLI models by having humans produce premise-hypothesis combinations that the machine model cannot correctly predict. To remain attractive and challenging in the research of natural language inference for Vietnamese, in this paper, we introduce the adversarial NLI dataset to the NLP research community with the name ViANLI. This data set contains more than 10K premise-hypothesis pairs and is built by a continuously adjusting process to obtain the most out of the patterns generated by the annotators. ViANLI dataset has brought many difficulties to many current SOTA models when the accuracy of the most powerful model on the test set only reached 48.4%. Additionally, the experimental results show that the models trained on our dataset have significantly improved the results on other Vietnamese NLI datasets.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17692",
        "title": "From Distributional to Overton Pluralism: Investigating Large Language Model Alignment",
        "abstract": "The alignment process changes several properties of a large language model's (LLM's) output distribution. We analyze two aspects of post-alignment distributional shift of LLM responses. First, we re-examine previously reported reductions in response diversity post-alignment. Our analysis suggests that an apparent drop in the diversity of responses is largely explained by quality control and information aggregation. Alignment suppresses irrelevant and unhelpful content while shifting the output distribution toward longer responses that cover information spanning several responses from the base LLM, essentially presenting diverse information in a single response. Finding little evidence that alignment suppresses useful information, it is natural to ask the opposite question: do aligned models surface information that cannot be recovered from base models? Our second investigation shows this is not the case and the behavior of aligned models is recoverable from base models without fine-tuning. A combination of in-context examples and lower-resolution semantic hints about response content can elicit responses from base LLMs that are as similar to alignment-tuned LLM responses as alignment-tuned LLM responses are to each other. Taken together, these results indicate that current alignment techniques capture but do not extend the useful subset of assistant-like base LLM behavior, providing further evidence for the Superficial Alignment Hypothesis. They also show that in-context alignment can go surprisingly far as a strategy for imitating aligned LLMs without fine-tuning. Our code and data is available at this https URL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17681",
        "title": "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation",
        "abstract": "As large language models achieve impressive scores on traditional benchmarks, an increasing number of researchers are becoming concerned about benchmark data leakage during pre-training, commonly known as the data contamination problem. To ensure fair evaluation, recent benchmarks release only the training and validation sets, keeping the test set labels closed-source. They require anyone wishing to evaluate his language model to submit the model's predictions for centralized processing and then publish the model's result on their leaderboard. However, this submission process is inefficient and prevents effective error analysis. To address this issue, we propose to variabilize benchmarks and evaluate language models dynamically. Specifically, we extract variables from each test case and define a value range for each variable. For each evaluation, we sample new values from these value ranges to create unique test cases, thus ensuring a fresh evaluation each time. We applied this variable perturbation method to four datasets: GSM8K, ARC, CommonsenseQA, and TruthfulQA, which cover mathematical generation and multiple-choice tasks. Our experimental results demonstrate that this approach provides a more accurate assessment of the true capabilities of language models, effectively mitigating the contamination problem.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17675",
        "title": "Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models",
        "abstract": "Large Language Models (LLMs) have demonstrated exceptional task-solving capabilities, increasingly adopting roles akin to human-like assistants. The broader integration of LLMs into society has sparked interest in whether they manifest psychological attributes, and whether these attributes are stable-inquiries that could deepen the understanding of their behaviors. Inspired by psychometrics, this paper presents a framework for investigating psychology in LLMs, including psychological dimension identification, assessment dataset curation, and assessment with results validation. Following this framework, we introduce a comprehensive psychometrics benchmark for LLMs that covers six psychological dimensions: personality, values, emotion, theory of mind, motivation, and intelligence. This benchmark includes thirteen datasets featuring diverse scenarios and item types. Our findings indicate that LLMs manifest a broad spectrum of psychological attributes. We also uncover discrepancies between LLMs' self-reported traits and their behaviors in real-world scenarios. This paper demonstrates a thorough psychometric assessment of LLMs, providing insights into reliable evaluation and potential applications in AI and social sciences.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17626",
        "title": "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference",
        "abstract": "As large language models (LLMs) constantly evolve, ensuring their safety remains a critical research problem. Previous red-teaming approaches for LLM safety have primarily focused on single prompt attacks or goal hijacking. To the best of our knowledge, we are the first to study LLM safety in multi-turn dialogue coreference. We created a dataset of 1,400 questions across 14 categories, each featuring multi-turn coreference safety attacks. We then conducted detailed evaluations on five widely used open-source LLMs. The results indicated that under multi-turn coreference safety attacks, the highest attack success rate was 56% with the LLaMA2-Chat-7b model, while the lowest was 13.9% with the Mistral-7B-Instruct model. These findings highlight the safety vulnerabilities in LLMs during dialogue coreference interactions.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.17588",
        "title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs",
        "abstract": "The long-context capabilities of large language models (LLMs) have been a hot topic in recent years. To evaluate the performance of LLMs in different scenarios, various assessment benchmarks have emerged. However, as most of these benchmarks focus on identifying key information to answer questions, which mainly requires the retrieval ability of LLMs, these benchmarks can partially represent the reasoning performance of LLMs from large amounts of information. Meanwhile, although LLMs often claim to have context windows of 32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual supported length of these LLMs. To address these issues, we propose the LongIns benchmark dataset, a challenging long-context instruction-based exam for LLMs, which is built based on the existing instruction datasets. Specifically, in our LongIns, we introduce three evaluation settings: Global Instruction & Single Task (GIST), Local Instruction & Single Task (LIST), and Local Instruction & Multiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations on existing LLMs and have the following important findings: (1). The top-performing GPT-4 with 128k context length performs poorly on the evaluation context window of 16k in our LongIns. (2). For the multi-hop reasoning ability of many existing LLMs, significant efforts are still needed under short context windows (less than 4k).",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.11045",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "abstract": "Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs\u2019 reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). Moreover, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36K unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. We make Orca 2 weights publicly available at aka.ms/orca-lm to support research on the development, evaluation, and alignment of smaller LMs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2402.14830",
        "title": "Orca-Math: Unlocking the potential of SLMs in Grade School Math",
        "abstract": "We show that an SLM can reach \u223c 87% pass@1 on GSM8K while trained on only 200K synthetic math problems. Mathematical word problem-solving has long been recognized as a complex task for small language models (SLMs). A recent study hypothesized that the smallest model size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34 billion parameters. To reach this level of performance with smaller models, researcher often train SLMs to generate Python code or use tools to help avoid calculation errors. Additionally, they employ ensembling, where outputs of up to 100 model runs are combined to arrive at a more accurate result. Result selection is done using consensus, majority vote or a separate a verifier model used in conjunction with the SLM. Ensembling provides a substantial boost in accuracy but at a significant cost increase with multiple calls to the model (e.g., Phi-GSM uses top-48 to boost the performance from 68.2 to 81.5, [38] uses top-100 to boost LLAMA-2\u2019s performance from 38.6% to 71.9%). In this work, we present Orca-Math, a 7-billion-parameter SLM based on the Mistral-7B, which achieves 86.81% on GSM8k without the need for multiple model calls or the use of verifiers, code execution or any other external tools. Our approach has the following key elements: (1) A high quality synthetic dataset of 200K math problems created using a multiagent setup where agents collaborate to create the data, (2) An iterative learning techniques that enables the SLM to practice solving problems, receive feedback on its solutions and learn from preference pairs incorporating the SLM solutions and the feedback. When trained with Supervised Fine-Tuning alone, Orca-Math achieves 81.50% on GSM8k pass@1 metric. With iterative preference learning, Orca-Math achieves 86.81% pass@1. Orca-Math surpasses the performance of significantly larger models such as LLAMA-2-70B, WizardMath-70B, Gemini-Pro, ChatGPT-3.5. It also significantly outperforms other smaller models while using much smaller data (hundreds of thousands vs. millions of problems).",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.00720",
        "title": "COMPLEXITY-BASED PROMPTING FOR MULTI-STEP REASONING",
        "abstract": "We study the task of prompting large-scale language models to perform multistep reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexitybased prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multistep reasoning tasks over strong baselines. We further extend our complexitybased criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2306.06427",
        "title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting",
        "abstract": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like \u201cLet\u2019s think step by step\u201d or multiple incontext exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with hallucinations, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F2 -Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks 1 .",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2308.11462",
        "title": "LEGALBENCH: A COLLABORATIVELY BUILT BENCHMARK FOR MEASURING LEGAL REASONING IN LARGE LANGUAGE MODELS",
        "abstract": "The advent of large language models (LLMs) and their adoption by the legal community has given rise to the question: what types of legal reasoning can LLMs perform? To enable greater study of this question, we present LEGALBENCH: a collaboratively constructed legal reasoning bench- mark consisting of 162 tasks covering six different types of legal reasoning. LEGALBENCH was built through an interdisciplinary process, in which we collected tasks designed and hand-crafted by legal professionals. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting. To enable cross-disciplinary conversations about LLMs in the law, we additionally show how popular legal frameworks for describing legal reasoning\u2014which distinguish between its many forms\u2014correspond to LEGALBENCH tasks, thus giving lawyers and LLM developers a common vocabulary. This paper describes LEGALBENCH, presents an empirical evaluation of 20 open-source and commercial LLMs, and illustrates the types of research explorations LEGALBENCH enables.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.07327",
        "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment",
        "abstract": "Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. Models trained on OpenAssistant Conversations show consistent improvements on standard benchmarks over respective base models. We release our code2 and data3 under a fully permissive licence.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.11248",
        "title": "CROSSCODEEVAL: A Diverse and Multilingual Benchmark for Cross-File Code Completion",
        "abstract": "Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly. To fill in this gap, we propose CROSSCODEEVAL, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CROSSCODEEVAL is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-analysis-based approach to pinpoint the use of cross-file context within the current file.\nExtensive experiments on state-of-the-art code language models like CodeGen and StarCoder demonstrate that CROSSCODEEVAL is extremely challenging when the relevant cross-file context is absent, and we see clear improvements when adding these context into the prompt. However, despite such improvements, the pinnacle of performance remains notably unattained even with the highest-performing model, indicating that CROSSCODEEVAL is also capable of assessing model\u2019s capability in leveraging extensive context to make better code completion. Finally, we benchmarked various methods in retrieving cross-file context, and show that CROSSCODEEVAL can also be used to measure the capability of code retrievers.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.11698",
        "title": "DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models",
        "abstract": "Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applica- tions such as healthcare and finance \u2013 where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives\n\u2013 including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT mod- els can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard bench- marks, GPT-4 is more vulnerable given jailbreaking system or user prompts, po- tentially because GPT-4 follows (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/; our dataset can be previewed at https: //huggingface.co/datasets/AI-Secure/DecodingTrust; a concise ver- sion of this work is at https://openreview.net/pdf?id=kaHpo8OZw2.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.01026",
        "title": "Temporal Graph Benchmark\nfor Machine Learning on Temporal Graphs",
        "abstract": "We present the Temporal Graph Benchmark (TGB), a collection of challenging and diverse benchmark datasets for realistic, reproducible, and robust evalua- tion of machine learning models on temporal graphs. TGB datasets are of large scale, spanning years in duration, incorporate both node and edge-level prediction tasks and cover a diverse set of domains including social, trade, transaction, and transportation networks. For both tasks, we design evaluation protocols based on realistic use-cases. We extensively benchmark each dataset and find that the performance of common models can vary drastically across datasets. In addition, on dynamic node property prediction tasks, we show that simple methods often achieve superior performance compared to existing temporal graph models. We believe that these findings open up opportunities for future research on tempo- ral graphs. Finally, TGB provides an automated machine learning pipeline for reproducible and accessible temporal graph research, including data loading, ex- periment setup and performance evaluation. TGB will be maintained and updated on a regular basis and welcomes community feedback. TGB datasets, data load- ers, example codes, evaluation setup, and leaderboards are publicly available at https://tgb.complexdatalab.com/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.13040",
        "title": "SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents",
        "abstract": "Task-oriented dialogue (TOD) models have made significant progress in recent years. However, previous studies primarily focus on datasets written by annotators, which has resulted in a gap between academic research and real-world spoken con- versation scenarios. While several small-scale spoken TOD datasets are proposed to address robustness issues such as ASR errors, they ignore the unique challenges in spoken conversation. To tackle the limitations, we introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD, containing 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from human-to-human spoken conversations. SpokenWOZ further incorporates common spoken characteristics such as word-by-word processing and reasoning in spoken language. Based on these characteristics, we present cross-turn slot and reasoning slot detection as new challenges. We conduct experiments on various baselines, including text-modal models, newly proposed dual-modal models, and LLMs, e.g., ChatGPT. The results show that the current models still have substantial room for improvement in spoken conversation, where the most advanced dialogue state tracker only achieves 25.65% in joint goal accuracy and the SOTA end-to-end model only correctly completes the user request in 52.1% of dialogues. Our dataset, code, and leaderboard are available at https://spokenwoz.github.io/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.05562",
        "title": "AircraftVerse: A Large-Scale Multimodal Dataset of Aerial Vehicle Designs",
        "abstract": "We present AircraftVerse, a publicly available aerial vehicle design dataset. Air- craft design encompasses different physics domains and, hence, multiple modalities of representation. The evaluation of these cyber-physical system (CPS) designs re- quires the use of scientific analytical and simulation models ranging from computer- aided design tools for structural and manufacturing analysis, computational fluid dynamics tools for drag and lift computation, battery models for energy estimation, and simulation models for flight control and dynamics. AircraftVerse contains 27,714 diverse air vehicle designs - the largest corpus of engineering designs with this level of complexity. Each design comprises the following artifacts: a symbolic design tree describing topology, propulsion subsystem, battery subsystem, and other design details; a STandard for the Exchange of Product (STEP) model data; a 3D CAD design using a stereolithography (STL) file format; a 3D point cloud for the shape of the design; and evaluation results from high fidelity state-of-the-art physics models that characterize performance metrics such as maximum flight distance and hover-time. We also present baseline surrogate models that use dif- ferent modalities of design representation to predict design performance metrics, which we provide as part of our dataset release. Finally, we discuss the potential impact of this dataset on the use of learning in aircraft design and, more generally, in CPS. AircraftVerse is accompanied by a data card, and it is released under Creative Commons Attribution-ShareAlike (CC BY-SA) license. The dataset is hosted at https://zenodo.org/record/6525446, baseline models and code at https://github.com/SRI-CSL/AircraftVerse, and the dataset description at https://aircraftverse.onrender.com/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.14623",
        "title": "BUBBLEML: A MULTI-PHYSICS DATASET AND BENCHMARKS FOR MACHINE LEARNING",
        "abstract": "In the field of phase change phenomena, the lack of accessible and diverse datasets suitable for machine learning (ML) training poses a significant challenge. Existing experimental datasets are often restricted, with limited availability and sparse ground truth data, impeding our understanding of this complex multiphysics phenomena. To bridge this gap, we present the BubbleML Dataset 1 which leverages physics-driven simulations to provide accurate ground truth information for various boiling scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled boiling. This extensive dataset covers a wide range of parameters, including varying gravity conditions, flow rates, sub-cooling levels, and wall superheat, comprising 79 simulations. BubbleML is validated against experimental observations and trends, establishing it as an invaluable resource for ML research. Furthermore, we showcase its potential to facilitate exploration of diverse downstream tasks by introducing two benchmarks: (a) optical flow analysis to capture bubble dynamics, and (b) operator networks for learning temperature dynamics. The BubbleML dataset and its benchmarks serve as a catalyst for advancements in ML-driven research on multiphysics phase change phenomena, enabling the development and comparison of state-of-the-art techniques and models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2207.10062",
        "title": "DataPerf:\nBenchmarks for Data-Centric AI Development",
        "abstract": "Machine learning research has long focused on models rather than datasets, and prominent datasets are used for common ML tasks without regard to the breadth, difficulty, and faithfulness of the underlying problems. Neglecting the fundamen- tal importance of data has given rise to inaccuracy, bias, and fragility in real-world applications, and research is hindered by saturation across existing dataset bench- marks. In response, we present DataPerf, a community-led benchmark suite for evaluating ML datasets and data-centric algorithms. We aim to foster innovation in data-centric AI through competition, comparability, and reproducibility. We enable the ML community to iterate on datasets, instead of just architectures, and we provide an open, online platform with multiple rounds of challenges to support this iterative development. The first iteration of DataPerf contains five benchmarks covering a wide spectrum of data-centric techniques, tasks, and modalities in vi- sion, speech, acquisition, debugging, and diffusion prompting, and we support hosting new contributed benchmarks from the community. The benchmarks, on- line evaluation platform, and baseline implementations are open source, and the MLCommons Association will maintain DataPerf to ensure long-term benefits to academia and industry.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2109.10399",
        "title": "SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and Benchmarking",
        "abstract": "Subseasonal forecasting of the weather two to six weeks in advance is critical for resource allocation and advance disaster notice but poses many challenges for the forecasting community. At this forecast horizon, physics-based dynami- cal models have limited skill, and the targets for prediction depend in a complex manner on both local weather and global climate variables. Recently, machine learning methods have shown promise in advancing the state of the art but only at the cost of complex data curation, integrating expert knowledge with aggrega- tion across multiple relevant data sources, file formats, and temporal and spatial resolutions. To streamline this process and accelerate future development, we introduce SubseasonalClimateUSA, a curated dataset for training and benchmark- ing subseasonal forecasting models in the United States. We use this dataset to benchmark a diverse suite of subseasonal models, including operational dynamical models, classical meteorological baselines, and ten state-of-the-art machine learn- ing and deep learning-based methods from the literature. Overall, our benchmarks suggest simple and effective ways to extend the accuracy of current operational models. SubseasonalClimateUSA is regularly updated and accessible via the https://github.com/microsoft/subseasonal_data/ Python package.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.00750",
        "title": "Are These the Same Apple? Comparing Images Based on Object Intrinsics",
        "abstract": "The human visual system can effortlessly recognize an object under different extrinsic factors such as lighting, object poses, and background, yet current com- puter vision systems often struggle with these variations. An important step to understanding and improving artificial vision systems is to measure image simi- larity purely based on intrinsic object properties that define object identity. This problem has been studied in the computer vision literature as re-identification, though mostly restricted to specific object categories such as people and cars. We propose to extend it to general object categories, exploring an image simi- larity metric based on object intrinsics. To benchmark such measurements, we collect the Common paired objects Under differenT Extrinsics (CUTE) dataset of 18, 000 images of 180 objects under different extrinsic factors such as light- ing, poses, and imaging conditions. While existing methods such as LPIPS and CLIP scores do not measure object intrinsics well, we find that combining deep features learned from contrastive self-supervised learning with foreground filtering is a simple yet effective approach to approximating the similarity. We conduct an extensive survey of pre-trained features and foreground extraction methods to arrive at a strong baseline that best measures intrinsic object-centric image similarity among current methods. Finally, we demonstrate that our approach can aid in downstream applications such as acting as an analog for human subjects and improving generalizable re-identification. Please see our project website at https://s-tian.github.io/projects/cute/ for visualizations of the data and demos of our metric.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.03721",
        "title": "ClimateSet: A Large-Scale\nClimate Model Dataset for Machine Learning",
        "abstract": "Climate models have been key for assessing the impact of climate change and simulating future climate scenarios. The machine learning (ML) community has taken an increased interest in supporting climate scientists\u2019 efforts on various tasks such as climate model emulation, downscaling, and prediction tasks. Many of those tasks have been addressed on datasets created with single climate models. However, both the climate science and ML communities have suggested that to address those tasks at scale, we need large, consistent, and ML-ready climate model datasets. Here, we introduce ClimateSet, a dataset containing the inputs and outputs of 36 climate models from the Input4MIPs and CMIP6 archives. In addition, we provide a modular dataset pipeline for retrieving and preprocessing additional climate models and scenarios. We showcase the potential of our dataset by using it as a benchmark for ML-based climate model emulation. We gain new insights about the performance and generalization capabilities of the different ML models by analyzing their performance across different climate models. Furthermore, the dataset can be used to train an ML emulator on several climate models instead of just one. Such a \u201csuper emulator\u201d can quickly project new climate change scenarios, complementing existing scenarios already provided to policymakers. We believe ClimateSet will create the basis needed for the ML community to tackle climate-related tasks at scale.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.06350",
        "title": "T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation",
        "abstract": "Despite the stunning ability to generate high-quality images by recent text-to- image models, current approaches often struggle to effectively compose objects with different attributes and relationships into a complex and coherent scene. We propose T2I-CompBench, a comprehensive benchmark for open-world composi- tional text-to-image generation, consisting of 6,000 compositional text prompts from 3 categories (attribute binding, object relationships, and complex composi- tions) and 6 sub-categories (color binding, shape binding, texture binding, spatial relationships, non-spatial relationships, and complex compositions). We further propose several evaluation metrics specifically designed to evaluate compositional text-to-image generation and explore the potential and limitations of multimodal LLMs for evaluation. We introduce a new approach, Generative mOdel finetun- ing with Reward-driven Sample selection (GORS), to boost the compositional text-to-image generation abilities of pretrained text-to-image models. Extensive experiments and evaluations are conducted to benchmark previous methods on T2I- CompBench, and to validate the effectiveness of our proposed evaluation metrics and GORS approach. Project page is available at https://karine-h.github.io/T2I- CompBench/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.18652",
        "title": "EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records with Chest X-ray Images",
        "abstract": "Electronic Health Records (EHRs), which contain patients\u2019 medical histories in various multi-modal formats, often overlook the potential for joint reasoning across imaging and table modalities underexplored in current EHR Question Answering (QA) systems. In this paper, we introduce EHRXQA, a novel multi-modal ques- tion answering dataset combining structured EHRs and chest X-ray images. To develop our dataset, we first construct two uni-modal resources: 1) The MIMIC- CXR-VQA dataset, our newly created medical visual question answering (VQA) benchmark, specifically designed to augment the imaging modality in EHR QA, and 2) EHRSQL (MIMIC-IV), a refashioned version of a previously established table-based EHR QA dataset. By integrating these two uni-modal resources, we successfully construct a multi-modal EHR QA dataset that necessitates both uni-modal and cross-modal reasoning. To address the unique challenges of multi- modal questions within EHRs, we propose a NeuralSQL-based strategy equipped with an external VQA API. This pioneering endeavor enhances engagement with multi-modal EHR sources and we believe that our dataset can catalyze advances in real-world medical scenarios such as clinical decision-making and research. EHRXQA is available at https://github.com/baeseongsu/ehrxqa.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.11408",
        "title": "Stable Bias:\nEvaluating Societal Representations in Diffusion Models",
        "abstract": "As machine learning-enabled Text-to-Image (TTI) systems are becoming increasingly prevalent and seeing growing adoption as commercial services, characterizing the social biases they exhibit is a necessary first step to lowering their risk of discriminatory outcomes. This evaluation, however, is made more difficult by the synthetic nature of these systems\u2019 outputs: common definitions of diversity are grounded in social categories of people living in the world, whereas the artificial depictions of fictive humans created by these systems have no inherent gender or ethnicity. To address this need, we propose a new method for exploring the social biases in TTI systems. Our approach relies on characterizing the variation in generated images triggered by enumerating gender and ethnicity markers in the prompts, and comparing it to the variation engendered by spanning different professions. This allows us to (1) identify specific bias trends, (2) provide targeted scores to directly compare models in terms of diversity and representation, and (3) jointly model interdependent social variables to support a multidimensional analysis. We leverage this method to analyze images generated by 3 popular TTI systems (Dall\u00b7E 2, Stable Diffusion v 1.4 and 2) and find that while all of their outputs show correlations with US labor demographics, they also consistently under-represent marginalized identities to different extents. We also release the datasets and low-code interactive bias exploration platforms developed for this work, as well as the necessary tools to similarly evaluate additional TTI systems.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.06595",
        "title": "VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use",
        "abstract": "We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real- world use. Our starting point is curating 70 \u201cinstruction families\u201d that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following cura- tion, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction- specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption de- scribes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evalu- ation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and refer- ences using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model\u2019s response on the project website; Data, code and leaderboard is available at https://visit-bench.github.io/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.05391",
        "title": "pinpointing Why Object Recognition Performance Degrades Across Income Levels and Geographies",
        "abstract": "Despite impressive advances in object-recognition, deep learning systems\u2019 performance degrades significantly across geographies and lower income levels\u2014raising press- ing concerns of inequity. Addressing such performance gaps remains a challenge, as little is understood about why performance degrades across incomes or geographies. We take a step in this direction by annotating images from Dollar Street, a popular benchmark of geographically and economically diverse images, labeling each image with factors such as color, shape, and background. These anno- tations unlock a new granular view into how objects differ across incomes/regions. We then use these object differ- ences to pinpoint model vulnerabilities across incomes and regions. We study a range of modern vision models, find- ing that performance disparities are most associated with differences in texture, occlusion, and images with darker lighting. We illustrate how insights from our factor labels can surface mitigations to improve models\u2019 performance disparities. As an example, we show that mitigating a model\u2019s vulnerability to texture can improve performance on the lower income level. We release all the factor an- notations along with an interactive dashboard to facilitate research into more equitable vision systems.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09424",
        "title": "SSL4EO-L:\nDatasets and Foundation Models for Landsat Imagery",
        "abstract": "The Landsat program is the longest-running Earth observation program in history, with 50+ years of data acquisition by 8 satellites. The multispectral imagery cap- tured by sensors onboard these satellites is critical for a wide range of scientific fields. Despite the increasing popularity of deep learning and remote sensing, the majority of researchers still use decision trees and random forests for Land- sat image analysis due to the prevalence of small labeled datasets and lack of foundation models. In this paper, we introduce SSL4EO-L, the first ever dataset designed for Self-Supervised Learning for Earth Observation for the Landsat fam- ily of satellites (including 3 sensors and 2 product levels) and the largest Landsat dataset in history (5M image patches). Additionally, we modernize and re-release the L7 Irish and L8 Biome cloud detection datasets, and introduce the first ML benchmark datasets for Landsats 4\u20135 TM and Landsat 7 ETM+ SR. Finally, we pre-train the first foundation models for Landsat imagery using SSL4EO-L and evaluate their performance on multiple semantic segmentation tasks. All datasets and model weights are available via the TorchGeo1 library, making reproducibility and experimentation easy, and enabling scientific advancements in the burgeoning field of remote sensing for a multitude of downstream applications.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.15521",
        "title": "What a MESS: Multi-Domain Evaluation of Zero-Shot Semantic Segmentation",
        "abstract": "While semantic segmentation has seen tremendous improvements in the past, there are still significant labeling efforts necessary and the problem of limited gener- alization to classes that have not been present during training. To address this problem, zero-shot semantic segmentation makes use of large self-supervised vision-language models, allowing zero-shot transfer to unseen classes. In this work, we build a benchmark for Multi-domain Evaluation of Semantic Segmentation (MESS), which allows a holistic analysis of performance across a wide range of domain-specific datasets such as medicine, engineering, earth monitoring, biology, and agriculture. To do this, we reviewed 120 datasets, developed a taxonomy, and classified the datasets according to the developed taxonomy. We select a repre- sentative subset consisting of 22 datasets and propose it as the MESS benchmark. We evaluate eight recently published models on the proposed MESS benchmark and analyze characteristics for the performance of zero-shot transfer models. The toolkit is available at https://github.com/blumenstiel/MESS.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1805.04687",
        "title": "BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning",
        "abstract": "Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world com- puter vision applications require performing tasks of var- ious complexities. We construct BDD100K 1, the largest driving video dataset with 100K videos and 10 tasks to eval- uate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a bench- mark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to per- form such heterogeneous tasks. BDD100K opens the door for future studies in this important venue.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1901.05946",
        "title": "uided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation",
        "abstract": "Most progress in semantic segmentation reports on day- time images taken under favorable illumination conditions. We instead address the problem of semantic segmentation of nighttime images and improve the state-of-the-art, by adapting daytime models to nighttime without using night- time annotations. Moreover, we design a new evaluation framework to address the substantial uncertainty of seman- tics in nighttime images. Our central contributions are: 1) a curriculum framework to gradually adapt semantic segmen- tation models from day to night via labeled synthetic images and unlabeled real images, both for progressively darker times of day, which exploits cross-time-of-day correspon- dences for the real images to guide the inference of their labels; 2) a novel uncertainty-aware annotation and evalu- ation framework and metric for semantic segmentation, de- signed for adverse conditions and including image regions beyond human recognition capability in the evaluation in a principled fashion; 3) the Dark Zurich dataset, which com- prises 2416 unlabeled nighttime and 2920 unlabeled twi- light images with correspondences to their daytime coun- terparts plus a set of 151 nighttime images with fine pixel- level annotations created with our protocol, which serves as a first benchmark to perform our novel evaluation. Exper- iments show that our guided curriculum adaptation signifi- cantly outperforms state-of-the-art methods on real night- time sets both for standard metrics and our uncertainty- aware metric. Furthermore, our uncertainty-aware eval- uation reveals that selective invalidation of predictions can lead to better results on data with ambiguous content such as our nighttime benchmark and profit safety-oriented ap- plications which involve invalid inputs.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1705.07206",
        "title": "Multi-Human Parsing in the Wild",
        "abstract": "Human parsing is attracting increasing research attention. In this work, we aim to push the frontier of human parsing by introducing the problem of multi-human parsing in the wild. Existing works on human parsing mainly tackle single-person scenarios, which deviates from real-world applications where multiple persons are present simultaneously with interaction and occlusion. To address the multi-human parsing problem, we introduce a new multi-human parsing (MHP) dataset and a novel multi-human parsing model named MH-Parser. The MHP dataset contains multiple persons captured in real-world scenes with pixel-level fine-grained se- mantic annotations in an instance-aware setting. The MH-Parser generates global parsing maps and person instance masks simultaneously in a bottom-up fashion with the help of a new Graph-GAN model. We envision that the MHP dataset will serve as a valuable data resource to develop new multi-human parsing models, and the MH-Parser offers a strong baseline to drive future research for multi-human parsing in the wild.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2105.05409",
        "title": "a Large-Scale Benchmark for Food Image Segmentation",
        "abstract": "Food image segmentation is a critical and indispensible task for developing health-related applications such as estimating food calo- ries and nutrients. Existing food image segmentation models are underperforming due to two reasons: (1) there is a lack of high quality food image datasets with fine-grained ingredient labels and pixel-wise location masks\u2014the existing datasets either carry coarse ingredient labels or are small in size; and (2) the complex appear- ance of food makes it difficult to localize and recognize ingredients in food images, e.g., the ingredients may overlap one another in the same image, and the identical ingredient may appear distinctly in different food images.\nIn this work, we build a new food image dataset FoodSeg103 (and its extension FoodSeg154) containing 9,490 images. We annotate these images with 154 ingredient classes and each image has an average of 6 ingredient labels and pixel-wise masks. In addition, we propose a multi-modality pre-training approach called ReLeM that explicitly equips a segmentation model with rich and semantic food knowledge. In experiments, we use three popular semantic segmentation methods (i.e., Dilated Convolution based [17], Fea- ture Pyramid based [22], and Vision Transformer based [54]) as baselines, and evaluate them as well as ReLeM on our new datasets. We believe that the FoodSeg103 (and its extension FoodSeg154) and the pre-trained models using ReLeM can serve as a bench- mark to facilitate future works on fine-grained food image un- derstanding. We make all these datasets and methods public at https://xiongweiwu.github.io/foodseg103.html.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2111.11567",
        "title": "ATLANTIS: A BENCHMARK FOR SEMANTIC SEGMENTATION OF WATERBODY IMAGES",
        "abstract": "Vision-based semantic segmentation of waterbodies and nearby related objects provides important information for managing water resources and handling flooding emergency. However, the lack of large-scale labeled training and testing datasets for water-related categories prevents researchers from studying water-related issues in the computer vision field. To tackle this problem, we present AT- LANTIS, a new benchmark for semantic segmentation of waterbodies and related objects. ATLANTIS consists of 5,195 images of waterbodies, as well as high quality pixel-level manual annotations of 56 classes of objects, including 17 classes of man-made objects, 18 classes of natural objects and 21 general classes. We analyze ATLANTIS in detail and evaluate several state-of-the-art semantic segmentation networks on our benchmark. In addition, a novel deep neural network, AQUANet, is developed for waterbody semantic segmentation by processing the aquatic and non-aquatic regions in two different paths. AQUANet also incorporates low-level feature modulation and cross-path modula- tion for enhancing feature representation. Experimental results show that the proposed AQUANet outperforms other state-of-the-art semantic segmentation networks on ATLANTIS. We claim that ATLANTIS is the largest waterbody image dataset for semantic segmentation providing a wide range of water and water-related classes and it will benefit researchers of both computer vision and water resources engineering.Monocular depth estimation has experienced significant progress on terrestrial images in recent years, largely due to deep learning advancements. However, it remains in- adequate for underwater scenes, primarily because of data scarcity. Given the inherent challenges of light attenuation and backscattering in water, acquiring clear underwater images or precise depth information is notably difficult and costly. Consequently, learning-based approaches often rely on synthetic data or turn to unsupervised or self-supervised methods to mitigate this lack of data. Nonetheless, the per- formance of these methods is often constrained by the do- main gap and looser constraints. In this paper, we propose a novel pipeline for generating photorealistic underwater images using accurate terrestrial depth data. This approach facilitates the training of supervised models for underwa- ter depth estimation, effectively reducing the performance disparity between terrestrial and underwater environments. Contrary to prior synthetic datasets that merely apply style transfer to terrestrial images without altering the scene con- tent, our approach uniquely creates vibrant, non-existent underwater scenes by leveraging terrestrial depth data\nthrough the innovative Stable Diffusion model. Specifi- cally, we introduce a unique Depth2Underwater Control- Net, trained on specially prepared {Underwater, Depth, Text} data triplets, for this generation task. Our newly de- veloped dataset enables terrestrial depth estimation mod- els to achieve considerable improvements, both quantita- tively and qualitatively, on unseen underwater images, sur- passing their terrestrial pre-trained counterparts. More- over, the enhanced depth accuracy for underwater scenes also aids underwater image restoration techniques that rely on depth maps, further demonstrating our dataset\u2019s util- ity. The dataset will be publicly available at https: //github.com/zkawfanx/Atlantis.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.12471",
        "title": "Atlantis: Enabling Underwater Depth Estimation with Stable Diffusion",
        "abstract": "Monocular depth estimation has experienced significant progress on terrestrial images in recent years, largely due to deep learning advancements. However, it remains in- adequate for underwater scenes, primarily because of data scarcity. Given the inherent challenges of light attenuation and backscattering in water, acquiring clear underwater images or precise depth information is notably difficult and costly. Consequently, learning-based approaches often rely on synthetic data or turn to unsupervised or self-supervised methods to mitigate this lack of data. Nonetheless, the per- formance of these methods is often constrained by the do- main gap and looser constraints. In this paper, we propose a novel pipeline for generating photorealistic underwater images using accurate terrestrial depth data. This approach facilitates the training of supervised models for underwa- ter depth estimation, effectively reducing the performance disparity between terrestrial and underwater environments. Contrary to prior synthetic datasets that merely apply style transfer to terrestrial images without altering the scene con- tent, our approach uniquely creates vibrant, non-existent underwater scenes by leveraging terrestrial depth data\nthrough the innovative Stable Diffusion model. Specifi- cally, we introduce a unique Depth2Underwater Control- Net, trained on specially prepared {Underwater, Depth, Text} data triplets, for this generation task. Our newly de- veloped dataset enables terrestrial depth estimation mod- els to achieve considerable improvements, both quantita- tively and qualitatively, on unseen underwater images, sur- passing their terrestrial pre-trained counterparts. More- over, the enhanced depth accuracy for underwater scenes also aids underwater image restoration techniques that rely on depth maps, further demonstrating our dataset\u2019s util- ity. The dataset will be publicly available at https: //github.com/zkawfanx/Atlantis.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1905.12886",
        "title": "iSAID: A Large-scale Dataset for Instance Segmentation in Aerial Images",
        "abstract": "Existing Earth Vision datasets are either suitable for se- mantic segmentation or object detection. In this work, we introduce the first benchmark dataset for instance segmen- tation in aerial imagery that combines instance-level object detection and pixel-level segmentation tasks. In comparison to instance segmentation in natural scenes, aerial images present unique challenges e.g., a huge number of instances per image, large object-scale variations and abundant tiny objects. Our large-scale and densely annotated Instance Segmentation in Aerial Images Dataset (iSAID) comes with 655,451 object instances for 15 categories across 2,806 high-resolution images. Such precise per-pixel annotations for each instance ensure accurate localization that is essen- tial for detailed scene analysis. Compared to existing small- scale aerial image based instance segmentation datasets, iSAID contains 15\u00d7 the number of object categories and 5\u00d7 the number of instances. We benchmark our dataset us- ing two popular instance segmentation approaches for nat- ural images, namely Mask R-CNN and PANet. In our ex- periments we show that direct application of off-the-shelf Mask R-CNN and PANet on aerial images provide subop- timal instance segmentation results, thus requiring special- ized solutions from the research community. The dataset is publicly available at: https://captain-whu.github. io/iSAID/index.html",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2012.02951",
        "title": "FloodNet: A High Resolution Aerial Imagery Dataset for Post Flood Scene Understanding",
        "abstract": "isual scene understanding is the core task in making any crucial decision in any computer vision system. Al- though popular computer vision datasets like Cityscapes, MS-COCO, PASCAL provide good benchmarks for several tasks (e.g. image classification, segmentation, object de- tection), these datasets are hardly suitable for post disas- ter damage assessments. On the other hand, existing natu- ral disaster datasets include mainly satellite imagery which have low spatial resolution and a high revisit period. There- fore, they do not have a scope to provide quick and effi- cient damage assessment tasks. Unmanned Aerial Vehicle (UAV) can effortlessly access difficult places during any dis- aster and collect high resolution imagery that is required for aforementioned tasks of computer vision. To address these issues we present a high resolution UAV imagery, FloodNet, captured after the hurricane Harvey. This dataset demon- strates the post flooded damages of the affected areas. The images are labeled pixel-wise for semantic segmentation task and questions are produced for the task of visual ques- tion answering. FloodNet poses several challenges includ- ing detection of flooded roads and buildings and distin- guishing between natural water and flooded water. With the advancement of deep learning algorithms, we can analyze the impact of any disaster which can make a precise under- standing of the affected areas. In this paper, we compare and contrast the performances of baseline methods for im- age classification, semantic segmentation, and visual ques- tion answering on our dataset.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2111.02995",
        "title": "Unsupervised Change Detection of Extreme Events Using ML On-Board",
        "abstract": "In this paper, we introduce RaV\u00c6n, a lightweight, unsupervised approach for change detection in satellite data based on Variational Auto-Encoders (VAEs) with the specific purpose of on-board deployment. Applications such as disaster man- agement enormously benefit from the rapid availability of satellite observations. Traditionally, data analysis is performed on the ground after all data is transferred \u2013 downlinked \u2013 to a ground station. Constraint on the downlink capabilities there- fore affects any downstream application. In contrast, RaV\u00c6n pre-processes the sampled data directly on the satellite and flags changed areas to prioritise for downlink, shortening the response time. We verified the efficacy of our system on a dataset composed of time series of catastrophic events \u2013 which we plan to release alongside this publication \u2013 demonstrating that RaV\u00c6n outperforms pixel- wise baselines. Finally we tested our approach on resource-limited hardware for assessing computational and memory limitations.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1810.10438",
        "title": "UAVid: A Semantic Segmentation Dataset for UAV Imagery",
        "abstract": "Semantic segmentation has been one of the leading re- search interests in computer vision recently. It serves as a perception foundation for many fields, such as robotics and autonomous driving. The fast development of seman- tic segmentation attributes enormously to the large scale datasets, especially for the deep learning related methods. There already exist several semantic segmentation datasets for comparison among semantic segmentation methods in complex urban scenes, such as the Cityscapes and CamVid datasets, where the side views of the objects are captured with a camera mounted on the driving car. There also exist semantic labeling datasets for the airborne images and the satellite images, where the top views of the ob- jects are captured. However, only a few datasets capture urban scenes from an oblique Unmanned Aerial Vehicle (UAV) perspective, where both of the top view and the side view of the objects can be observed, providing more in- formation for object recognition. In this paper, we intro- duce our UAVid dataset, a new high-resolution UAV seman- tic segmentation dataset as a complement, which brings new challenges, including large scale variation, moving object recognition and temporal consistency preservation. Our UAV dataset consists of 30 video sequences captur- ing 4K high-resolution images in slanted views. In to- tal, 300 images have been densely labeled with 8 classes for the semantic labeling task. We have provided several deep learning baseline methods with pre-training, among which the proposed Multi-Scale-Dilation net performs the best via multi-scale feature extraction, reaching a mean intersection-over-union (IoU) score around 50% and out- performing the others by more than 1.6%. We have also ex- plored the influence of spatial-temporal regularization for sequence data by leveraging on feature space optimization (FSO) and 3D conditional random field (CRF), which im- proves the mean IoU scores by around another 0.5%. Our UAVid website and the labeling tool have been published",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2101.00442",
        "title": "CRYONUSEG: A DATASET FOR NUCLEI INSTANCE SEGMENTATION OF CRYOSECTIONED H&E-STAINED HISTOLOGICAL IMAGES",
        "abstract": "Nuclei instance segmentation plays an important role in the analysis of Hematoxylin and Eosin (H&E)-stained images. While supervised deep learning (DL)-based approaches rep- resent the state-of-the-art in automatic nuclei instance seg- mentation, annotated datasets are required to train these mod- els. There are two main types of tissue processing protocols, namely formalin-fixed paraffin-embedded samples (FFPE) and frozen tissue samples (FS). Although FFPE-derived H&E stained tissue sections are the most widely used samples, H&E staining on frozen sections derived from FS samples is a relevant method in intra-operative surgical sessions as it can be performed fast. Due to differences in the protocols of these two types of samples, the derived images and in particular the nuclei appearance may be different in the acquired whole slide images. Analysis of FS-derived H&E stained images can be more challenging as rapid preparation, staining, and scanning of FS sections may lead to deterioration in image quality.\nIn this paper, we introduce CryoNuSeg, the first fully annotated FS-derived cryosectioned and H&E-stained nu- clei instance segmentation dataset. The dataset contains images from 10 human organs that were not exploited in other publicly available datasets, and is provided with three manual mark-ups to allow measuring intra-observer and inter- observer variability. Moreover, we investigate the effects of tissue fixation/embedding protocol (i.e., FS or FFPE) on the automatic nuclei instance segmentation performance of one of the state-of-the-art DL approaches. We also create a base- line segmentation benchmark for the dataset that can be used in future research.\nA step-by-step guide to generate the dataset as well as the full dataset and other detailed information are made avail- able to fellow researchers at https://github.com/ masih4/CryoNuSeg.\nIndex Terms\u2014 Medical Image Analysis, Computational Pathology, Nuclei Segmentation, H&E Staining, Frozen Tis-\nsue Samples, Deep Learning, Intra-Observer Variability, Inter-Observer Variability",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.03416",
        "title": "Detailed Annotations of Chest X-Rays via CT Projection for Report Understanding",
        "abstract": "In clinical radiology reports, doctors capture important information about the patient\u2019s health status. They convey their observations from raw medical imaging data about the inner structures of a patient. As such, formulating reports requires medical experts to possess wide-ranging knowledge about anatomical regions with their normal, healthy ap- pearance as well as the ability to recognize abnormalities. This explicit grasp on both the patient\u2019s anatomy and their appearance is missing in current medical image-processing systems as annotations are especially difficult to gather. This renders the models to be narrow experts e.g. for identifying specific diseases. In this work, we recover this miss- ing link by adding human anatomy into the mix and enable the association of content in medical reports to their occurrence in associated imagery (medical phrase grounding). To exploit anatomical structures in this scenario, we present a sophisticated automatic pipeline to gather and integrate human bodily structures from computed tomography datasets, which we incorporate in our PAXRAY: A Projected dataset for the segmenta- tion of Anatomical structures in X-RAY data. Our evaluation shows that methods that take advantage of anatomical information benefit heavily in visually grounding radiol- ogists\u2019 findings, as our anatomical segmentations allow for up to absolute 50% better grounding results on the OpenI dataset as compared to commonly used region proposals.\n2 SEIBOLD ET AL.: DETAILED X-RAY ANNOTATIONS VIA CT PROJECTION\nFigure 1: Overlap between the segmentation of anatomies and expert annotations on a sample of OpenI [16] indicating the necessity of anatomical understanding. Boxes are radiologists\u2019 annotation of findings. Masks show predictions for \u20186th right rib\u2019, \u2018spine\u2019 and \u2018heart\u2019\ntake advantage of anatomical information benefit heavily in visually grounding radiol- ogists\u2019 findings, as our anatomical segmentations allow for up to absolute 50% better grounding results on the OpenI dataset as compared to commonly used region proposals.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2208.13054",
        "title": "CrackSeg9k: A Collection and Benchmark for Crack Segmentation Datasets and Frameworks",
        "abstract": "The detection of cracks is a crucial task in monitoring struc- tural health and ensuring structural safety. The manual process of crack detection is time-consuming and subjective to the inspectors. Several researchers have tried tackling this problem using traditional Image Pro- cessing or learning-based techniques. However, their scope of work is limited to detecting cracks on a single type of surface (walls, pavements, glass, etc.). The metrics used to evaluate these methods are also var- ied across the literature, making it challenging to compare techniques. This paper addresses these problems by combining previously available datasets and unifying the annotations by tackling the inherent problems within each dataset, such as noise and distortions. We also present a pipeline that combines Image Processing and Deep Learning models. Fi- nally, we benchmark the results of proposed models on these metrics on our new dataset and compare them with state-of-the-art models in the literature.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1909.10980",
        "title": "PST900: RGB-Thermal Calibration, Dataset and Segmentation Network",
        "abstract": "Abstract\u2014In this work we propose long wave infrared (LWIR) imagery as a viable supporting modality for semantic segmentation using learning-based techniques. We first address the problem of RGB-thermal camera calibration by proposing a passive calibration target and procedure that is both portable and easy to use. Second, we present PST900, a dataset of 894 synchronized and calibrated RGB and Thermal image pairs with per pixel human annotations across four distinct classes from the DARPA Subterranean Challenge. Lastly, we propose a CNN architecture for fast semantic segmentation that combines both RGB and Thermal imagery in a way that leverages RGB imagery independently. We compare our method against the state-of-the-art and show that our method outperforms them in our dataset.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2106.02740",
        "title": "ZeroWaste Dataset:\nTowards Deformable Object Segmentation in Cluttered Scenes",
        "abstract": "Less than 35% of recyclable waste is being actually re- cycled in the US [2], which leads to increased soil and sea pollution and is one of the major concerns of envi- ronmental researchers as well as the common public. At the heart of the problem are the inefficiencies of the waste sorting process (separating paper, plastic, metal, glass, etc.) due to the extremely complex and cluttered nature of the waste stream. Recyclable waste detection poses a unique computer vision challenge as it requires detection of highly deformable and often translucent objects in clut- tered scenes without the kind of context information usually present in human-centric datasets. This challenging com- puter vision task currently lacks suitable datasets or meth- ods in the available literature. In this paper, we take a step towards computer-aided waste detection and present the first in-the-wild industrial-grade waste detection and segmentation dataset, ZeroWaste. We believe that Ze- roWaste will catalyze research in object detection and semantic segmentation in extreme clutter as well as appli- cations in the recycling domain. Our project page can be found at http://ai.bu.edu/zerowaste/",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2004.01241",
        "title": "Semantic Segmentation of Underwater Imagery: Dataset and Benchmark",
        "abstract": "In this paper, we present the first large-scale dataset for semantic Segmentation of Underwater IMagery (SUIM). It contains over 1500 images with pixel anno- tations for eight object categories: fish (vertebrates), reefs (invertebrates), aquatic plants, wrecks/ruins, human divers, robots, and sea-floor. The images have been rigorously collected during oceanic explorations and human-robot col- laborative experiments, and annotated by human participants. We also present a benchmark evaluation of state-of-the- art semantic segmentation approaches based on standard performance metrics. In addition, we present SUIM-Net, a fully-convolutional encoder-decoder model that balances the trade-off between performance and computational efficiency. It offers competitive performance while ensuring fast end-to- end inference, which is essential for its use in the autonomy pipeline of visually-guided underwater robots. In particular, we demonstrate its usability benefits for visual servoing, saliency prediction, and detailed scene understanding. With a variety of use cases, the proposed model and benchmark dataset open up promising opportunities for future research in underwater robot vision.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.10350",
        "title": "Improving Multimodal Datasets with Image Captioning",
        "abstract": "Massive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2\u00d7 better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable indicator of the utility of the captions it generates for multimodal training. Finally, our experiments with using generated captions at DataComp\u2019s large scale (1.28B image-text pairs) offer insights into the limitations of synthetic text, as well as the importance of image curation with increasing training data quantity. The generated captions used in our experiments are now available on HuggingFace6 .",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.08259",
        "title": "LargeST: A Benchmark Dataset for Large-Scale Traffic Forecasting",
        "abstract": "Road traffic forecasting plays a critical role in smart city initiatives and has experi- enced significant advancements thanks to the power of deep learning in capturing non-linear patterns of traffic data. However, the promising results achieved on current public datasets may not be applicable to practical scenarios due to lim- itations within these datasets. First, the limited sizes of them may not reflect the real-world scale of traffic networks. Second, the temporal coverage of these datasets is typically short, posing hurdles in studying long-term patterns and ac- quiring sufficient samples for training deep models. Third, these datasets often lack adequate metadata for sensors, which compromises the reliability and inter- pretability of the data. To mitigate these limitations, we introduce the LargeST benchmark dataset. It encompasses a total number of 8,600 sensors in California with a 5-year time coverage and includes comprehensive metadata. Using LargeST, we perform in-depth data analysis to extract data insights, benchmark well-known baselines in terms of their performance and efficiency, and identify challenges as well as opportunities for future research. We release the datasets and baseline implementations at: https://github.com/liuxu77/LargeST.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.18773",
        "title": "CityRefer: Geography-aware 3D Visual Grounding Dataset on City-scale Point Cloud Data",
        "abstract": "City-scale 3D point cloud is a promising way to express detailed and complicated outdoor structures. It encompasses both the appearance and geometry features of segmented city components, including cars, streets, and buildings, that can be uti- lized for attractive applications such as user-interactive navigation of autonomous vehicles and drones. However, compared to the extensive text annotations avail- able for images and indoor scenes, the scarcity of text annotations for outdoor scenes poses a significant challenge for achieving these applications. To tackle this problem, we introduce the CityRefer dataset1 for city-level visual grounding. The dataset consists of 35k natural language descriptions of 3D objects appearing in SensatUrban [19] city scenes and 5k landmarks labels synchronizing with Open- StreetMap. To ensure the quality and accuracy of the dataset, all descriptions and labels in the CityRefer dataset are manually verified. We also have developed a baseline system that can learn encoded language descriptions, 3D object instances, and geographical information about the city\u2019s landmarks to perform visual ground- ing on the CityRefer dataset. To the best of our knowledge, the CityRefer dataset is the largest city-level visual grounding dataset for localizing specific 3D objects.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.09688",
        "title": "Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation",
        "abstract": "Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement. Thus, accurately understanding customer preferences is essential for providing personalized recommendations. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular. However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences. To bridge this gap, we present the Amazon Multilingual Multi- locale Shopping Session Dataset, namely Amazon-M2. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish. Remarkably, the dataset can help us enhance personalization and understanding of user preferences, which can benefit various existing tasks as well as enable new tasks. To test the potential of the dataset, we introduce three tasks in this work: (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) next-product title generation. With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further research and practice. In addition, based on the proposed dataset and tasks, we hosted a competition in the KDD CUP 20232 and have attracted thousands of users and submissions. The winning solutions and the associated workshop can be accessed at our website https://kddcup23.github.io/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.01813",
        "title": "FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation",
        "abstract": "Recently, open-domain text-to-video (T2V) generation models have made remark- able progress. However, the promising results are mainly shown by the qualitative cases of generated videos, while the quantitative evaluation of T2V models still faces two critical problems. Firstly, existing studies lack fine-grained evaluation of T2V models on different categories of text prompts. Although some bench- marks have categorized the prompts, their categorization either only focuses on a single aspect or fails to consider the temporal information in video generation. Secondly, it is unclear whether the automatic evaluation metrics are consistent with human standards. To address these problems, we propose FETV, a benchmark for Fine-grained Evaluation of Text-to-Video generation. FETV is multi-aspect, categorizing the prompts based on three orthogonal aspects: the major content, the attributes to control and the prompt complexity. FETV is also temporal-aware, which introduces several temporal categories tailored for video generation. Based on FETV, we conduct comprehensive manual evaluations of four representative T2V models, revealing their pros and cons on different categories of prompts from different aspects. We also extend FETV as a testbed to evaluate the re- liability of automatic T2V metrics. The multi-aspect categorization of FETV enables fine-grained analysis of the metrics\u2019 reliability in different scenarios. We find that existing automatic metrics (e.g., CLIPScore and FVD) correlate poorly with human evaluation. To address this problem, we explore several solutions to improve CLIPScore and FVD, and develop two automatic metrics that exhibit significant higher correlation with humans than existing metrics. Benchmark page: https://github.com/llyx97/FETV.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.13457",
        "title": "Turbulence in Focus: Benchmarking Scaling Behavior of 3D Volumetric Super-Resolution with BLASTNet 2.0 Data",
        "abstract": "Analysis of compressible turbulent flows is essential for applications related to propulsion, energy generation, and the environment. Here, we present BLASTNet 2.0, a 2.2 TB network-of-datasets containing 744 full-domain samples from 34 high-fidelity direct numerical simulations, which addresses the current limited availability of 3D high-fidelity reacting and non-reacting compressible turbulent flow simulation data. With this data, we benchmark a total of 49 variations of five deep learning approaches for 3D super-resolution \u2013 which can be applied for improving scientific imaging, simulations, turbulence models, as well as in com- puter vision applications. We perform neural scaling analysis on these models to examine the performance of different machine learning (ML) approaches, including two scientific ML techniques. We demonstrate that (i) predictive performance can scale with model size and cost, (ii) architecture matters significantly, especially for smaller models, and (iii) the benefits of physics-based losses can persist with increasing model size. The outcomes of this benchmark study are anticipated to offer insights that can aid the design of 3D super-resolution models, especially for turbulence models, while this data is expected to foster ML methods for a broad range of flow physics applications. This data is publicly available with download links and browsing tools consolidated at https://blastnet.github.io.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09200",
        "title": "ChessGPT: Bridging Policy Learning and Language Modeling",
        "abstract": "When solving decision-making tasks, humans typically depend on information from two key sources: (1) Historical policy data, which provides interaction replay from the environment, and (2) Analytical insights in natural language form, exposing the invaluable thought process or strategic considerations. Despite this, the majority of preceding research focuses on only one source: they either use historical replay exclusively to directly learn policy or value functions, or engaged in language model training utilizing mere language corpus. In this paper, we argue that a powerful autonomous agent should cover both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning and language modeling by integrating data from these two sources in Chess games. Specifically, we build a large-scale game and language dataset related to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and ChessGPT, integrating policy learning and language modeling. Finally, we propose a full evaluation framework for evaluating language model\u2019s chess ability. Experimental results validate our model and dataset\u2019s effectiveness. We open source our code, model, and dataset at https://github.com/waterhorse1/ChessGPT.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.04287",
        "title": "Holistic Evaluation of Text-to-Image Models",
        "abstract": "The stunning qualitative improvement of recent text-to-image models has led to their widespread attention and adoption. However, we lack a comprehensive quantitative understanding of their capabilities and risks. To fill this gap, we introduce a new benchmark, Holistic Evaluation of Text-to-Image Models (HEIM). Whereas previous evaluations focus mostly on text-image alignment and image quality, we identify 12 aspects, including text-image alignment, image quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. We curate 62 scenarios encompassing these aspects and evaluate 26 state-of-the-art text-to-image models on this benchmark. Our results reveal that no single model excels in all aspects, with different models demonstrating different strengths. We release the generated images and human evaluation results for full transparency at https://crfm.stanford.edu/heim/v1.1.0 and the code at https://github.com/stanford-crfm/helm, which is integrated with the HELM codebase [1].",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.13490",
        "title": "TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs",
        "abstract": "Precise hardware performance models play a crucial role in code optimizations. They can assist compilers in making heuristic decisions or aid autotuners in identi- fying the optimal configuration for a given program. For example, the autotuner for XLA, a machine learning compiler, discovered 10\u201320% speedup on state-of-the-art models serving substantial production traffic at Google. Although there exist a few datasets for program performance prediction, they target small sub-programs such as basic blocks or kernels. This paper introduces TPUGRAPHS, a performance prediction dataset on full tensor programs, represented as computational graphs, running on Tensor Processing Units (TPUs). Each graph in the dataset represents the main computation of a machine learning workload, e.g., a training epoch or an inference step. Each data sample contains a computational graph, a compila- tion configuration, and the execution time of the graph when compiled with the configuration. The graphs in the dataset are collected from open-source machine learning programs, featuring popular model architectures, e.g., ResNet, Efficient- Net, Mask R-CNN, and Transformer. TPUGRAPHS provides 25x more graphs than the largest graph property prediction dataset (with comparable graph sizes), and 770x larger graphs on average compared to existing performance prediction datasets on machine learning programs. This graph-level prediction task on large graphs introduces new challenges in learning, ranging from scalability, training efficiency, to model quality.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.05877",
        "title": "A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning",
        "abstract": "Academic tabular benchmarks often contain small sets of curated features. In contrast, data scientists typically collect as many features as possible into their datasets, and even engineer new features from existing ones. To prevent overfit- ting in subsequent downstream modeling, practitioners commonly use automated feature selection methods that identify a reduced subset of informative features. Existing benchmarks for tabular feature selection consider classical downstream models, toy synthetic datasets, or do not evaluate feature selectors on the basis of downstream performance. Motivated by the increasing popularity of tabular deep learning, we construct a challenging feature selection benchmark evaluated on downstream neural networks including transformers, using real datasets and multiple methods for generating extraneous features. We also propose an input- gradient-based analogue of Lasso for neural networks that outperforms classical feature selection methods on challenging problems such as selecting from corrupted or second-order features",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.12645",
        "title": "KuaiSim: A Comprehensive Simulator for Recommender Systems",
        "abstract": "Reinforcement Learning (RL)-based recommender systems (RSs) have garnered considerable attention due to their ability to learn optimal recommendation policies and maximize long-term user rewards. However, deploying RL models directly in online environments and generating authentic data through A/B tests can pose challenges and require substantial resources. Simulators offer an alternative ap- proach by providing training and evaluation environments for RS models, reducing reliance on real-world data. Existing simulators have shown promising results but also have limitations such as simplified user feedback, lack of consistency with real-world data, the challenge of simulator evaluation, and difficulties in migration and expansion across RSs. To address these challenges, we propose KuaiSim, a comprehensive user environment that provides user feedback with multi-behavior and cross-session responses. The resulting simulator can support three levels of recommendation problems: the request level list-wise recommendation task, the whole-session level sequential recommendation task, and the cross-session level retention optimization task. For each task, KuaiSim also provides evaluation pro- tocols and baseline recommendation algorithms that further serve as benchmarks for future research. We also restructure existing competitive simulators on the KuaiRand Dataset and compare them against KuaiSim to further assess their perfor- mance and behavioral differences. Furthermore, to showcase KuaiSim\u2019s flexibility in accommodating different datasets, we demonstrate its versatility and robustness when deploying it on the ML-1m dataset. The implementation code is available online to ease reproducibility 3.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.00556",
        "title": "ProBio: A Protocol-guided Multimodal Dataset for Molecular Biology Lab",
        "abstract": "The challenge of replicating research results has posed a significant impediment to the field of molecular biology. The advent of modern intelligent systems has led to notable progress in various domains. Consequently, we embarked on an investigation of intelligent monitoring systems as a means of tackling the issue of the reproducibility crisis. Specifically, we first curate a comprehensive multimodal dataset, named ProBio, as an initial step towards this objective. This dataset comprises fine-grained hierarchical annotations intended for studying activity un- derstanding in Molecular Biology Lab (BioLab). Next, we devise two challenging benchmarks, transparent solution tracking, and multimodal action recognition, to emphasize the unique characteristics and difficulties associated with activity understanding in BioLab settings. Finally, we provide a thorough experimental evaluation of contemporary video understanding models and highlight their limita- tions in this specialized domain to identify potential avenues for future research. We hope ProBio with associated benchmarks may garner increased focus on modern AI techniques in the realm of molecular biology.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.15171",
        "title": "RoboDepth: Robust Out-of-Distribution Depth Estimation under Corruptions",
        "abstract": "Depth estimation from monocular images is pivotal for real-world visual perception systems. While current learning-based depth estimation models train and test on meticulously curated data, they often overlook out-of-distribution (OoD) situations. Yet, in practical settings \u2013 especially safety-critical ones like autonomous driv- ing \u2013 common corruptions can arise. Addressing this oversight, we introduce a comprehensive robustness test suite, RoboDepth, encompassing 18 corruptions spanning three categories: i) weather and lighting conditions; ii) sensor failures and movement; and iii) data processing anomalies. We subsequently benchmark 42 depth estimation models across indoor and outdoor scenes to assess their resilience to these corruptions. Our findings underscore that, in the absence of a dedicated robustness evaluation framework, many leading depth estimation models may be susceptible to typical corruptions. We delve into design considerations for crafting more robust depth estimation models, touching upon pre-training, augmentation, modality, model capacity, and learning paradigms. We anticipate our benchmark will establish a foundational platform for advancing robust OoD depth estimation.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.12269",
        "title": "The Cambridge Law Corpus: A Dataset for Legal AI Research",
        "abstract": "We introduce the Cambridge Law Corpus (CLC), a dataset for legal AI research. It consists of over 250 000 court cases from the UK. Most cases are from the 21st century, but the corpus includes cases as old as the 16th century. This paper presents the first release of the corpus, containing the raw text and meta-data. Together with the corpus, we provide annotations on case outcomes for 638 cases, done by legal experts. Using our annotated data, we have trained and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to provide benchmarks. We include an extensive legal and ethical discussion to address the potentially sensitive nature of this material. As a consequence, the corpus will only be released for research purposes under certain restrictions.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.14183",
        "title": "Species196: A One-Million Semi-supervised Dataset for Fine-grained Species Recognition",
        "abstract": "The development of foundation vision models has pushed the general visual recog- nition to a high level, but cannot well address the fine-grained recognition in special- ized domain such as invasive species classification. Identifying and managing inva- sive species has strong social and ecological value. Currently, most invasive species datasets are limited in scale and cover a narrow range of species, which restricts the development of deep-learning based invasion biometrics systems. To fill the gap of this area, we introduced Species196, a large-scale semi-supervised dataset of 196-category invasive species. It collects over 19K images with expert-level ac- curate annotations (Species196-L), and 1.2M unlabeled images of invasive species (Species196-U). The dataset provides four experimental settings for benchmarking the existing models and algorithms, namely, supervised learning, semi-supervised learning, self-supervised pretraining and zero-shot inference ability of large multi- modal models. To facilitate future research on these four learning paradigms, we conduct an empirical study of the representative methods on the introduced dataset. The dataset is publicly available at https://species-dataset.github.io/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.10577",
        "title": "OpenDataVal: a Unified Benchmark for Data Valuation",
        "abstract": "Assessing the quality and impact of individual data points is critical for improving model performance and mitigating undesirable biases within the training dataset. Several data valuation algorithms have been proposed to quantify data quality, however, there lacks a systemic and standardized benchmarking system for data valuation. In this paper, we introduce OpenDataVal, an easy-to-use and unified benchmark framework that empowers researchers and practitioners to apply and compare various data valuation algorithms. OpenDataVal provides an integrated environment that includes (i) a diverse collection of image, natural language, and tabular datasets, (ii) implementations of eleven different state-of-the-art data valu- ation algorithms, and (iii) a prediction model API that can import any models in scikit-learn. Furthermore, we propose four downstream machine learning tasks for evaluating the quality of data values. We perform benchmarking analysis using OpenDataVal, quantifying and comparing the efficacy of state-of-the-art data valu- ation approaches. We find that no single algorithm performs uniformly best across all tasks, and an appropriate algorithm should be employed for a user\u2019s downstream task. OpenDataVal is publicly available at https://opendataval.github.io with comprehensive documentation. Furthermore, we provide a leaderboard where researchers can evaluate the effectiveness of their own data valuation algorithms.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.13347",
        "title": "NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding",
        "abstract": "The application of deep learning to nursing procedure activity understanding has the potential to greatly enhance the quality and safety of nurse-patient interactions. By utilizing the technique, we can facilitate training and education, improve quality control, and enable operational compliance monitoring. However, the development of automatic recognition systems in this field is currently hindered by the scarcity of appropriately labeled datasets. The existing video datasets pose several limitations: 1) these datasets are small-scale in size to support comprehensive investigations of nursing activity; 2) they primarily focus on single procedures, lacking expert- level annotations for various nursing procedures and action steps; and 3) they lack temporally localized annotations, which prevents the effective localization of targeted actions within longer video sequences. To mitigate these limitations, we propose NurViD, a large video dataset with expert-level annotation for nursing procedure activity understanding. NurViD consists of over 1.5k videos totaling 144 hours, making it approximately four times longer than the existing largest nursing activity datasets. Notably, it encompasses 51 distinct nursing procedures and 177 action steps, providing a much more comprehensive coverage compared to existing datasets that primarily focus on limited procedures. To evaluate the efficacy of current deep learning methods on nursing activity understanding, we establish three benchmarks on NurViD: procedure recognition on untrimmed videos, procedure and action recognition on trimmed videos, and action detection. Our benchmark and code will be available at https://github.com/minghu0830/NurViD-benchmark.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2206.08005",
        "title": "Evaluating Self-Supervised Learning for Molecular Graph Embeddings",
        "abstract": "Graph Self-Supervised Learning (GSSL) provides a robust pathway for acquiring\nembeddings without expert labelling, a capability that carries profound implica-\ntions for molecular graphs due to the staggering number of potential molecules\nand the high cost of obtaining labels. However, GSSL methods are designed not\nfor optimisation within a specific domain but rather for transferability across a\nvariety of downstream tasks. This broad applicability complicates their evalu-\nation. Addressing this challenge, we present \"Molecular Graph Representation\nEvaluation\" (MOLGRAPHEVAL), generating detailed profiles of molecular graph\nembeddings with interpretable and diversified attributes. MOLGRAPHEVAL of-\nfers a suite of probing tasks grouped into three categories: (i) generic graph,\n(ii) molecular substructure, and (iii) embedding space properties. By leverag-\ning MOLGRAPHEVAL to benchmark existing GSSL methods against both current\ndownstream datasets and our suite of tasks, we uncover significant inconsistencies\nbetween inferences drawn solely from existing datasets and those derived from\nmore nuanced probing. These findings suggest that current evaluation methodolo-\ngies fail to capture the entirety of the landscape.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.01835",
        "title": "EMBERSim: A Large-Scale Databank for Boosting Similarity Search in Malware Analysis",
        "abstract": "In recent years there has been a shift from heuristics-based malware detection towards machine learning, which proves to be more robust in the current heavily adversarial threat landscape. While we acknowledge machine learning to be better equipped to mine for patterns in the increasingly high amounts of similar-looking files, we also note a remarkable scarcity of the data available for similarity-targeted research. Moreover, we observe that the focus in the few related works falls on quantifying similarity in malware, often overlooking the clean data. This one- sided quantification is especially dangerous in the context of detection bypass. We propose to address the deficiencies in the space of similarity research on binary files, starting from EMBER \u2014 one of the largest malware classification data sets. We enhance EMBER with similarity information as well as malware class tags, to enable further research in the similarity space. Our contribution is threefold: (1) we publish EMBERSim, an augmented version of EMBER, that includes similarity-informed tags; (2) we enrich EMBERSim with automatically determined malware class tags using the open-source tool AVClass on VirusTotal data and (3) we describe and share the implementation for our class scoring technique and leaf similarity method.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1804.04637",
        "title": "EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models",
        "abstract": "This paper describes EMBER: a labeled benchmark dataset for training machine learning models to statically detect malicious Windows portable executable files. The dataset includes features extracted from 1.1M binary files: 900K training samples (300K malicious, 300K benign, 300K un- labeled) and 200K test samples (100K malicious, 100K be- nign). To accompany the dataset, we also release open source code for extracting features from additional bina- ries so that additional sample features can be appended to the dataset. This dataset fills a void in the information security machine learning community: a benign/malicious dataset that is large, open and general enough to cover sev- eral interesting use cases. We enumerate several use cases that we considered when structuring the dataset. Addition- ally, we demonstrate one use case wherein we compare a baseline gradient boosted decision tree model trained us- ing LightGBM with default settings to MalConv, a recently published end-to-end (featureless) deep learning model for malware detection. Results show that even without hyper- parameter optimization, the baseline EMBER model outper- forms MalConv. The authors hope that the dataset, code and baseline model provided by EMBER will help invigorate machine learning research for malware detection, in much the same way that benchmark datasets have advanced com- puter vision research",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.08710",
        "title": "Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research",
        "abstract": "Simulation is an essential tool to develop and benchmark autonomous vehicle planning software in a safe and cost-effective manner. However, realistic simula- tion requires accurate modeling of nuanced and complex multi-agent interactive behaviors. To address these challenges, we introduce Waymax, a new data-driven simulator for autonomous driving in multi-agent scenes, designed for large-scale simulation and testing. Waymax uses publicly-released, real-world driving data (e.g., the Waymo Open Motion Dataset [15]) to initialize or play back a diverse set of multi-agent simulated scenarios. It runs entirely on hardware accelerators such as TPUs/GPUs and supports in-graph simulation for training, making it suitable for modern large-scale, distributed machine learning workflows. To support online training and evaluation, Waymax includes several learned and hard-coded behav- ior models that allow for realistic interaction within simulation. To supplement Waymax, we benchmark a suite of popular imitation and reinforcement learning algorithms with ablation studies on different design decisions, where we highlight the effectiveness of routes as guidance for planning agents and the ability of RL to overfit against simulated agents.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.17448",
        "title": "SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation",
        "abstract": "Expressive human pose and shape estimation (EHPS) unifies body, hands, and face motion capture with numerous applications. Despite encouraging progress, current state-of-the-art methods still depend largely on a confined set of training datasets. In this work, we investigate scaling up EHPS towards the first generalist foundation model (dubbed SMPLer-X), with up to ViT-Huge as the backbone and training with up to 4.5M instances from diverse data sources. With big data and the large model, SMPLer-X exhibits strong performance across diverse test benchmarks and excellent transferability to even unseen environments. 1) For the data scaling, we perform a systematic investigation on 32 EHPS datasets, including a wide range of scenarios that a model trained on any single dataset cannot handle. More importantly, capitalizing on insights obtained from the extensive benchmarking process, we optimize our training scheme and select datasets that lead to a significant leap in EHPS capabilities. 2) For the model scaling, we take advantage of vision transformers to study the scaling law of model sizes in EHPS. Moreover, our finetuning strategy turn SMPLer-X into specialist models, allowing them to achieve further performance boosts. Notably, our foundation model SMPLer-X consistently delivers state-of-the-art results on seven benchmarks such as AGORA (107.2 mm NMVE), UBody (57.4 mm PVE), EgoBody (63.6 mm PVE), and EHF (62.3 mm PVE without finetuning).",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.15482",
        "title": "Salient Object Detection in RGB-D Videos",
        "abstract": "Given the widespread adoption of depth-sensing acquisition devices, RGB-D videos and related data/media have gained considerable traction in various aspects of daily life. Consequently, conducting salient object detection (SOD) in RGB- D videos presents a highly promising and evolving avenue. Despite the potential of this area, SOD in RGB-D videos remains somewhat under-explored, with RGB-D SOD and video SOD (VSOD) traditionally studied in isolation. To explore this emerging field, this paper makes two primary contributions: the dataset and the model. On one front, we construct the RDVS dataset, a new RGB-D VSOD dataset with realistic depth and characterized by its diversity of scenes and rigorous frame- by-frame annotations. We validate the dataset through com- prehensive attribute and object-oriented analyses, and provide training and testing splits. Moreover, we introduce DCTNet+, a three-stream network tailored for RGB-D VSOD, with an emphasis on RGB modality and treats depth and optical flow as auxiliary modalities. In pursuit of effective feature enhancement, refinement, and fusion for precise final prediction, we propose two modules: the multi-modal attention module (MAM) and the refinement fusion module (RFM). To enhance interaction and fusion within RFM, we design a universal interaction module (UIM) and then integrate holistic multi-modal attentive paths (HMAPs) for refining multi-modal low-level features be- fore reaching RFMs. Comprehensive experiments, conducted on pseudo RGB-D video datasets alongside our proposed RDVS, highlight the superiority of DCTNet+ over 17 VSOD models and 14 RGB-D SOD models. Additionally, insightful ablation experiments were performed on both pseudo and realistic RGB- D video datasets to demonstrate the advantages of individual modules as well as the necessity of introducing realistic depth into VSOD. Our code together with RDVS dataset will be available at https://github.com/kerenfu/RDVS/.\nIndex Terms\u2014Salient object detection, RGB-D videos, depth, optical flow, multi-modal fusion",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.05721",
        "title": "HA-VID: A HUMAN ASSEMBLY VIDEO DATASET FOR COMPREHENSIVE ASSEMBLY KNOWLEDGE UNDERSTANDING",
        "abstract": "Understanding comprehensive assembly knowledge from videos is critical for futuristic ultra- intelligent industry. To enable technological breakthrough, we present HA-ViD \u2013 the first human assembly video dataset that features representative industrial assembly scenarios, natural procedural knowledge acquisition process, and consistent human-robot shared annotations. Specifically, HA-ViD captures diverse collaboration patterns of real-world assembly, natural human behaviors and learning progression during assembly, and granulate action annotations to subject, action verb, manipulated object, target object, and tool. We provide 3222 multi-view, multi-modality videos (each video contains one assembly task), 1.5M frames, 96K temporal labels and 2M spatial labels. We benchmark four foundational video understanding tasks: action recognition, action segmentation, object detection and multi-object tracking. Importantly, we analyze their performance for comprehending knowledge in assembly progress, process efficiency, task collaboration, skill parameters and human intention. Details of HA-ViD is available at: https://iai-hrc.github.io/ha-vid",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2211.12421",
        "title": "Cola: A Benchmark for Compositional Text-to-image Retrieval",
        "abstract": "Compositional reasoning is a hallmark of human visual intelligence. Yet despite the size of large vision-language models, they struggle to represent simple com- positions by combining objects with their attributes. To measure this lack of compositional capability, we design Cola, a text-to-image retrieval benchmark to Compose Objects Localized with Attributes. To solve Cola, a model must retrieve images with the correct configuration of attributes and objects, and avoid choosing a distractor image with the same objects and attributes but in the wrong configuration. Cola contains about 1.2k composed queries of 168 objects and 197 attributes on around 30K images. Our human evaluation finds that Cola is 83.33% accurate, similar to contemporary compositionality benchmarks. Using Cola as a testbed, we explore empirical modeling designs to adapt pre-trained vision-language models to reason compositionally. We explore 6 adaptation strate- gies on 2 seminal vision-language models, using compositionality-centric test benchmarks - Cola and CREPE. We find the optimal adaptation strategy is to train a multi-modal attention layer that jointly attends over the frozen pre-trained image and language features. Surprisingly, training multimodal layers on CLIP performs better than tuning a larger FLAVA model with already pre-trained multi- modal layers. Furthermore, our adaptation strategy improves CLIP and FLAVA to comparable levels, suggesting that training multimodal layers using contrastive attribute-object data is key, as opposed to using them pre-trained. Lastly, we show that Cola is harder than a closely-related contemporary benchmark, CREPE, since simpler fine-tuning strategies without multimodal layers suffice on CREPE, but not on Cola. However, we still see a significant gap between our best adaptation and human accuracy, suggesting considerable room for further research. Project page: https://cs-people.bu.edu/array/research/cola/",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.12241",
        "title": "ScenarioNet: Open-Source Platform for Large-Scale Traffic Scenario Simulation and Modeling",
        "abstract": "Large-scale driving datasets such as Waymo Open Dataset and nuScenes substan- tially accelerate autonomous driving research, especially for perception tasks such as 3D detection and trajectory forecasting. Since the driving logs in these datasets contain HD maps and detailed object annotations that accurately reflect the real- world complexity of traffic behaviors, we can harvest a massive number of complex traffic scenarios and recreate their digital twins in simulation. Compared to the hand- crafted scenarios often used in existing simulators, data-driven scenarios collected from the real world can facilitate many research opportunities in machine learning and autonomous driving. In this work, we present ScenarioNet, an open-source platform for large-scale traffic scenario modeling and simulation. ScenarioNet defines a unified scenario description format and collects a large-scale repository of real-world traffic scenarios from the heterogeneous data in various driving datasets including Waymo, nuScenes, Lyft L5, Argoverse, and nuPlan datasets. These scenarios can be further replayed and interacted with in multiple views from Bird- Eye-View layout to realistic 3D rendering in MetaDrive simulator. This provides a benchmark for evaluating the safety of autonomous driving stacks in simulation before their real-world deployment. We further demonstrate the strengths of Sce- narioNet on large-scale scenario generation, imitation learning, and reinforcement learning in both single-agent and multi-agent settings. Code, demo videos, and website are available at https://metadriverse.github.io/scenarionet.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1912.04838",
        "title": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
        "abstract": "The research community has increasing interest in au- tonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self- driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the over- all viability of the technology. In an effort to help align the research community\u2019s contributions with real-world self- driving problems, we introduce a new large-scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well syn- chronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban ge- ographies. It is 15x more diverse than the largest cam- era+LiDAR dataset available based on our proposed geo- graphical coverage metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1903.11027",
        "title": "nuScenes: A multimodal dataset for autonomous driving",
        "abstract": "Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in com- puter vision tasks such as object detection, tracking and seg- mentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learn- ing based methods for detection and tracking become more prevalent, there is a need to train and evaluate such meth- ods on datasets containing range sensor data along with im- ages. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 de- gree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for li- dar and image based detection and tracking. Data, devel- opment kit and more information are available online",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.06352",
        "title": "NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving Datasets using Markup Annotations",
        "abstract": "Visual Question Answering (VQA) is one of the most im- portant tasks in autonomous driving, which requires accu- rate recognition and complex situation evaluations. How- ever, datasets annotated in a QA format, which guaran- tees precise language generation and scene recognition from driving scenes, have not been established yet. In this work, we introduce Markup-QA, a novel dataset annota- tion technique in which QAs are enclosed within markups. This approach facilitates the simultaneous evaluation of a model\u2019s capabilities in sentence generation and VQA. Moreover, using this annotation methodology, we designed the NuScenes-MQA dataset. This dataset empowers the de- velopment of vision language models, especially for au- tonomous driving tasks, by focusing on both descriptive capabilities and precise QA. The dataset is available at https://github.com/turingmotors/NuScenes-MQA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2006.14480",
        "title": "One Thousand and One Hours: Self-driving Motion Prediction Dataset",
        "abstract": "Motivated by the impact of large-scale datasets on ML systems we present the largest self-driving dataset for motion prediction to date, containing over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles along a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the percep- tion output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time. On top of this, the dataset contains a high-definition semantic map with 15,242 labelled elements and a high-definition aerial view over the area. We show that using a dataset of this size dramatically improves performance for key self-driving problems. Combined with the provided software kit, this collection forms the largest and most detailed dataset to date for the development of self-driving machine learning tasks, such as motion forecasting, motion planning and simulation.\nKeywords: Dataset, Self-driving, Motion prediction",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.18921",
        "title": "Large Car-following Data Based on Lyft level-5 Open Dataset: Following Autonomous Vehicles vs. Human-driven Vehicles",
        "abstract": "Car-Following (CF), as a fundamental driving behaviour, has significant influences on the safety and efficiency of traffic flow. Investigating how human drivers react differently when following autonomous vs. human-driven vehicles (HV) is thus critical for mixed traffic flow. Research in this field can be expedited with trajectory datasets collected by Autonomous Vehicles (AVs). However, trajectories collected by AVs are noisy and not readily applicable for studying CF behaviour. This paper extracts and enhances two categories of CF data, HV-following-AV (H-A) and HV-following-HV (H-H), from the open Lyft level-5 dataset. First, CF pairs are selected based on specific rules. Next, the quality of raw data is assessed by anomaly analysis. Then, the raw CF data is corrected and enhanced via motion planning, Kalman filtering, and wavelet denoising. As a result, 29k+ H-A and 42k+ H-H car-following segments are obtained, with a total driving distance of 150k+ km. A diversity assessment shows that the processed data cover complete CF regimes for calibrating CF models. This open and ready-to-use dataset provides the opportunity to investigate the CF behaviours of following AVs vs. HVs from real-world data. It can further facilitate studies on exploring the impact of AVs on mixed urban traffic.\nIndex Terms\u2014 Car-following, trajectory dataset, autonomous vehicle, driving behaviour",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1911.02620",
        "title": "Argoverse: 3D Tracking and Forecasting with Rich Maps",
        "abstract": "We present Argoverse \u2013 two datasets designed to sup- port autonomous vehicle machine learning tasks such as 3D tracking and motion forecasting. Argoverse was col- lected by a fleet of autonomous vehicles in Pittsburgh and Miami. The Argoverse 3D Tracking dataset includes 360\u25e6 images from 7 cameras with overlapping fields of view, 3D point clouds from long range LiDAR, 6-DOF pose, and 3D track annotations. Notably, it is the only modern AV dataset that provides forward-facing stereo imagery. The Argoverse Motion Forecasting dataset includes more than 300,000 5 second tracked scenarios with a particular vehicle identi- fied for trajectory forecasting. Argoverse is the first au- tonomous vehicle dataset to include \u201cHD maps\u201d with 290 km of mapped lanes with geometric and semantic metadata. All data is released under a Creative Commons license at www.argoverse.org . In our baseline experiments, we illustrate how detailed map information such as lane direc- tion, driveable area, and ground height improves the ac- curacy of 3D object tracking and motion forecasting. Our tracking and forecasting experiments represent only an ini- tial exploration of the use of rich maps in robotic percep- tion. We hope that Argoverse will enable the research com- munity to explore these problems in greater depth.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2301.00493",
        "title": "Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting",
        "abstract": "We introduce Argoverse 2 (AV2) \u2014 a collection of three datasets for perception and forecasting research in the self-driving domain. The annotated Sensor Dataset con- tains 1,000 sequences of multimodal data, encompassing high-resolution imagery from seven ring cameras, and two stereo cameras in addition to lidar point clouds, and 6-DOF map-aligned pose. Sequences contain 3D cuboid annotations for 26 object categories, all of which are sufficiently-sampled to support training and evaluation of 3D perception models. The Lidar Dataset contains 20,000 sequences of unlabeled lidar point clouds and map-aligned pose. This dataset is the largest ever collection of lidar sensor data and supports self-supervised learning and the emerging task of point cloud forecasting. Finally, the Motion Forecasting Dataset contains 250,000 scenarios mined for interesting and challenging interactions be- tween the autonomous vehicle and other actors in each local scene. Models are tasked with the prediction of future motion for \u201cscored actors\" in each scenario and are provided with track histories that capture object location, heading, velocity, and category. In all three datasets, each scenario contains its own HD Map with 3D lane and crosswalk geometry \u2014 sourced from data captured in six distinct cities. We believe these datasets will support new and existing machine learning research problems in ways that existing datasets do not. All datasets are released under the CC BY-NC-SA 4.0 license.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2106.11810",
        "title": "nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles",
        "abstract": "In this work, we propose the world\u2019s first closed-loop ML-based planning benchmark for autonomous driving. While there is a growing body of ML-based motion plan- ners, the lack of established datasets and metrics has limited the progress in this area. Existing benchmarks for autonomous vehicle motion prediction have focused on short-term motion forecasting, rather than long-term planning. This has led previous works to use open-loop evaluation with L2-based metrics, which are not suitable for fairly evaluating long-term planning. Our bench- mark overcomes these limitations by introducing a large- scale driving dataset, lightweight closed-loop simulator, and motion-planning-specific metrics. We provide a high- quality dataset with 1500h of human driving data from 4 cities across the US and Asia with widely varying traffic pat- terns (Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop simulation framework with re- active agents and provide a large set of both general and scenario-specific planning metrics. We plan to release the dataset at NeurIPS 2021 and organize benchmark chal- lenges starting in early 2022.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2403.04133",
        "title": "Towards learning-based planning:\nThe nuPlan benchmark for real-world autonomous driving",
        "abstract": "Machine Learning (ML) has replaced handcrafted methods for perception and prediction in autonomous vehicles. Yet for the equally important planning task, the adoption of ML-based techniques is slow. We present nuPlan, the world\u2019s first real-world autonomous driving dataset and benchmark. The benchmark is designed to test the ability of ML-based planners to handle diverse driving situations and to make safe and efficient decisions. We introduce a new large-scale dataset that consists of 1282 hours of diverse driving scenarios from 4 cities (Las Vegas, Boston, Pittsburgh, and Singapore) and includes high-quality auto-labeled object tracks and traffic light data. We mine and taxonomize common & rare driving scenarios which are used during evaluation to get fine-grained insights into the performance and characteristics of a planner. Beyond the dataset, we provide a simulation and evaluation framework that enables a planner\u2019s actions to be simulated in closed-loop to account for interactions with other traffic partici- pants. We present a detailed analysis of numerous baselines and investigate gaps between ML-based and traditional methods. Find the nuPlan dataset and code at nuplan.org.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.08348",
        "title": "LightZero: A Unified Benchmark for Monte Carlo Tree Search in General Sequential Decision Scenarios",
        "abstract": "Building agents based on tree-search planning capabilities with learned models has achieved remarkable success in classic decision-making problems, such as Go and Atari. However, it has been deemed challenging or even infeasible to extend Monte Carlo Tree Search (MCTS) based algorithms to diverse real-world applications, especially when these environments involve complex action spaces and significant simulation costs, or inherent stochasticity. In this work, we introduce LightZero, the first unified benchmark for deploying MCTS/MuZero in general sequential decision scenarios. Specificially, we summarize the most critical challenges in designing a general MCTS-style decision-making solver, then decompose the tightly-coupled algorithm and system design of tree-search RL methods into distinct sub-modules. By incorporating more appropriate exploration and optimization strategies, we can significantly enhance these sub-modules and construct powerful LightZero agents to tackle tasks across a wide range of domains, such as board games, Atari, MuJoCo, MiniGrid and GoBigger. Detailed benchmark results reveal the significant potential of such methods in building scalable and efficient decision intelligence. The code is available as part of OpenDILab at https://github.com/opendilab/LightZero.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1708.01641",
        "title": "Localizing Moments in Video with Natural Language",
        "abstract": "We consider retrieving a specific temporal segment, or moment, from a video given a natural language text de- scription. Methods designed to retrieve whole video clips with natural language determine what occurs in a video but not when. To address this issue, we propose the Mo- ment Context Network (MCN) which effectively localizes natural language queries in videos by integrating local and global video features over time. A key obstacle to train- ing our MCN model is that current video datasets do not include pairs of localized video segments and referring ex- pressions, or text descriptions which uniquely identify a corresponding moment. Therefore, we collect the Distinct Describable Moments (DiDeMo) dataset which consists of over 10,000 unedited, personal videos in diverse visual set- tings with pairs of localized video segments and referring expressions. We demonstrate that MCN outperforms sev- eral baseline methods and believe that our initial results together with the release of DiDeMo will inspire further re- search on localizing video moments with natural language.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2107.09609",
        "title": "QVHIGHLIGHTS: Detecting Moments and Highlights in Videos via Natural Language Queries",
        "abstract": "Detecting customized moments and highlights from videos given natural language (NL) user queries is an important but under-studied topic. One of the challenges in pursuing this direction is the lack of annotated data. To address this issue, we present the Query-based Video Highlights (QVHIGHLIGHTS) dataset. It consists of over 10,000 YouTube videos, covering a wide range of topics, from everyday activities and travel in lifestyle vlog videos to social and political activities in news videos. Each video in the dataset is annotated with: (1) a human-written free-form NL query, (2) relevant moments in the video w.r.t. the query, and (3) five-point scale saliency scores for all query-relevant clips. This comprehensive annotation enables us to develop and evaluate systems that detect relevant moments as well as salient highlights for diverse, flexible user queries. We also present a strong baseline for this task, Moment-DETR, a transformer encoder-decoder model that views moment retrieval as a direct set prediction problem, taking extracted video and query representations as inputs and predicting moment coordinates and saliency scores end-to-end. While our model does not utilize any human prior, we show that it performs competitively when compared to well-engineered architectures. With weakly supervised pretraining using ASR captions, Moment- DETR substantially outperforms previous methods. Lastly, we present several ablations and visualizations of Moment-DETR. Data and code is publicly available at https://github.com/jayleicn/moment_detr.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.00168",
        "title": "REASONER: An Explainable Recommendation Dataset with Multi-aspect Real User Labeled Ground Truths",
        "abstract": "Explainable recommendation has attracted much attention from the industry and academic communities. It has shown great potential for improving the recommendation persuasiveness, informative- ness and user satisfaction. Despite a lot of promising explainable recommender models have been proposed in the past few years, the evaluation strategies of these models suffer from several limitations. For example, the explanation ground truths are not labeled by real users, the explanations are mostly evaluated based on only one as- pect and the evaluation strategies can be hard to unify. To alleviate the above problems, we propose to build an explainable recommen- dation dataset with multi-aspect real user labeled ground truths. In specific, we firstly develop a video recommendation platform, where a series of questions around the recommendation explainabil- ity are carefully designed. Then, we recruit about 3000 users with different backgrounds to use the system, and collect their behaviors and feedback to our questions. In this paper, we detail the con- struction process of our dataset and also provide extensive analysis on its characteristics. In addition, we develop a library, where ten well-known explainable recommender models are implemented in a unified framework. Based on this library, we build several bench- marks for different explainable recommendation tasks. At last, we present many new opportunities brought by our dataset, which are expected to shed some new lights to the explainable recommenda- tion field. Our dataset, library and the related documents have been released at https://reasoner2023.github.io/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2202.05012",
        "title": "SUPA: A Lightweight Diagnostic Simulator for Machine Learning in Particle Physics",
        "abstract": "Deep learning methods have gained popularity in high energy physics for fast mod- eling of particle showers in detectors. Detailed simulation frameworks such as the gold standard GEANT4 are computationally intensive, and current deep generative architectures work on discretized, lower resolution versions of the detailed simula- tion. The development of models that work at higher spatial resolutions is currently hindered by the complexity of the full simulation data, and by the lack of simpler, more interpretable benchmarks. Our contribution is SUPA, the SUrrogate PArticle propagation simulator, an algorithm and software package for generating data by simulating simplified particle propagation, scattering and shower development in matter. The generation is extremely fast and easy to use compared to GEANT4, but still exhibits the key characteristics and challenges of the detailed simulation. The proposed simulator generates thousands of particle showers per second on a desktop machine, a speed up of up to 6 orders of magnitudes over GEANT4, and stores detailed geometric information about the shower propagation. SUPA provides much greater flexibility for setting initial conditions and defining multi- ple benchmarks for the development of models. Moreover, interpreting particle showers as point clouds creates a connection to geometric machine learning and provides challenging and fundamentally new datasets for the field.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.04455",
        "title": "RD-Suite: A Benchmark for Ranking Distillation",
        "abstract": "The distillation of ranking models has become an important topic in both academia and industry. In recent years, several advanced methods have been proposed to tackle this problem, often leveraging ranking information from teacher rankers that is absent in traditional classification settings. To date, there is no well- established consensus on how to evaluate this class of models. Moreover, in- consistent benchmarking on a wide range of tasks and datasets make it difficult to assess or invigorate advances in this field. This paper first examines represen- tative prior arts on ranking distillation, and raises three questions to be answered around methodology and reproducibility. To that end, we propose a systematic and unified benchmark, Ranking Distillation Suite (RD-Suite), which is a collection of tasks with 4 large real-world datasets, encompassing two major modalities (textual and numeric) and two applications (standard distillation and distillation transfer). RD-Suite consists of benchmark results that challenge some of the common wis- dom in the field, and the release of datasets with teacher scores and evaluation scripts for future research. RD-Suite paves the way towards better understanding of ranking distillation, facilities more research in this direction, and presents new challenges.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.11443",
        "title": "UUKG: Unified Urban Knowledge Graph Dataset for Urban Spatiotemporal Prediction",
        "abstract": "Accurate Urban SpatioTemporal Prediction (USTP) is of great importance to the development and operation of the smart city. As an emerging building block, multi-sourced urban data are usually integrated as urban knowledge graphs (Ur- banKGs) to provide critical knowledge for urban spatiotemporal prediction models. However, existing UrbanKGs are often tailored for specific downstream prediction tasks and are not publicly available, which limits the potential advancement. This paper presents UUKG, the unified urban knowledge graph dataset for knowledge- enhanced urban spatiotemporal predictions. Specifically, we first construct Ur- banKGs consisting of millions of triplets for two metropolises by connecting heterogeneous urban entities such as administrative boroughs, POIs, and road seg- ments. Moreover, we conduct qualitative and quantitative analysis on constructed UrbanKGs and uncover diverse high-order structural patterns, such as hierarchies and cycles, that can be leveraged to benefit downstream USTP tasks. To validate and facilitate the use of UrbanKGs, we implement and evaluate 15 KG embedding methods on the KG completion task and integrate the learned KG embeddings into 9 spatiotemporal models for five different USTP tasks. The extensive experimental results not only provide benchmarks of knowledge-enhanced USTP models under different task settings but also highlight the potential of state-of-the-art high-order structure-aware UrbanKG embedding methods. We hope the proposed UUKG fosters research on urban knowledge graphs and broad smart city applications. The dataset and source code are available at https://github.com/usail-hkust/UUKG/",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09803",
        "title": "Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization",
        "abstract": "This paper introduces a modular framework for Mixed-variable and Combinatorial Bayesian Optimization (MCBO) to address the lack of systematic benchmarking and standardized evaluation in the field. Current MCBO papers often introduce non-diverse or non-standard benchmarks to evaluate their methods, impeding the proper assessment of different MCBO primitives and their combinations. Addi- tionally, papers introducing a solution for a single MCBO primitive often omit benchmarking against baselines that utilize the same methods for the remaining primitives [1\u20134]. This omission is primarily due to the significant implementation overhead involved, resulting in a lack of controlled assessments and an inability to showcase the merits of a contribution effectively. To overcome these chal- lenges, our proposed framework enables an effortless combination of Bayesian Optimization components, and provides a diverse set of synthetic and real-world benchmarking tasks. Leveraging this flexibility, we implement 47 novel MCBO algorithms and benchmark them against seven existing MCBO solvers and five standard black-box optimization algorithms on ten tasks, conducting over 4000 experiments. Our findings reveal a superior combination of MCBO primitives outperforming existing approaches and illustrate the significance of model fit and the use of a trust region. We make our MCBO library available under the MIT license at https://github.com/huawei-noah/HEBO/tree/master/MCBO.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.00716",
        "title": "rPPG-Toolbox: Deep Remote PPG Toolbox",
        "abstract": "Camera-based physiological measurement is a fast growing field of computer vision. Remote photoplethysmography (rPPG) utilizes imaging devices (e.g., cameras) to measure the peripheral blood volume pulse (BVP), and enables cardiac measurement via webcams and smartphones. However, the task is non-trivial with important pre-processing, modeling, and post-processing steps required to obtain state-of-the-art results. Replication of results and benchmarking of new models is critical for scientific progress; however, as with many other applications of deep learning, reliable codebases are not easy to find or use. We present a comprehensive toolbox, rPPG-Toolbox, that contains unsupervised and supervised rPPG models with support for public benchmark datasets, data augmentation, and systematic evaluation: https://github.com/ubicomplab/rPPG-Toolbox",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.05174",
        "title": "GSLB: The Graph Structure Learning Benchmark",
        "abstract": "Graph Structure Learning (GSL) has recently garnered considerable attention due to its ability to optimize both the parameters of Graph Neural Networks (GNNs) and the computation graph structure simultaneously. Despite the proliferation of GSL methods developed in recent years, there is no standard experimental setting or fair comparison for performance evaluation, which creates a great obstacle to understanding the progress in this field. To fill this gap, we systematically analyze the performance of GSL in different scenarios and develop a comprehensive Graph Structure Learning Benchmark (GSLB) curated from 20 diverse graph datasets and 16 distinct GSL algorithms. Specifically, GSLB systematically investigates the characteristics of GSL in terms of three dimensions: effectiveness, robustness, and complexity. We comprehensively evaluate state-of-the-art GSL algorithms in node- and graph-level tasks, and analyze their performance in robust learning and model complexity. Further, to facilitate reproducible research, we have developed an easy-to-use library for training, evaluating, and visualizing different GSL methods. Empirical results of our extensive experiments demonstrate the ability of GSL and reveal its potential benefits on various downstream tasks, offering insights and opportunities for future research. The code of GSLB is available at: https: //github.com/GSL-Benchmark/GSLB.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09030",
        "title": "DiPlomat:\nA Dialogue Dataset for Situated Pragmatic Reasoning",
        "abstract": "Pragmatic reasoning plays a pivotal role in deciphering implicit meanings that frequently arise in real-life conversations and is essential for the development of communicative social agents. In this paper, we introduce a novel challenge, DiPlomat, aiming at benchmarking machines\u2019 capabilities on pragmatic reason- ing and situated conversational understanding. Compared with previous works that treat different figurative expressions (e.g. metaphor, sarcasm) as individual tasks, DiPlomat provides a cohesive framework towards general pragmatic un- derstanding. Our dataset is created through the utilization of Amazon Mechanical Turk (AMT), resulting in a total of 4, 177 multi-turn dialogues. In conjunction with the dataset, we propose two tasks, Pragmatic Identification and Reasoning (PIR) and Conversational Question Answering (CQA). Experimental results with state-of-the-art (SOTA) neural architectures reveal several significant findings: 1) large language models (LLMs) exhibit poor performance in tackling this subjective domain; 2) comprehensive comprehension of context emerges as a critical factor for establishing benign human-machine interactions; 3) current models defect in the application of pragmatic reasoning. As a result, we call on more attention to improve the ability of context understanding, reasoning, and implied meaning modeling.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.12552",
        "title": "SITUATEDGEN: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning",
        "abstract": "Recently, commonsense reasoning in text generation has attracted much attention. Generative commonsense reasoning is the task that requires machines, given a group of keywords, to compose a single coherent sentence with commonsense plausibility. While existing datasets targeting generative commonsense reason- ing focus on everyday scenarios, it is unclear how well machines reason under specific geographical and temporal contexts. We formalize this challenging task as SITUATEDGEN, where machines with commonsense should generate a pair of contrastive sentences given a group of keywords including geographical or temporal entities. We introduce a corresponding English dataset consisting of 8,268 contrastive sentence pairs, which are built upon several existing common- sense reasoning benchmarks with minimal manual labor. Experiments show that state-of-the-art generative language models struggle to generate sentences with commonsense plausibility and still lag far behind human performance. Our dataset is publicly available at https://github.com/yunx-z/situated_gen",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.05443",
        "title": "PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance",
        "abstract": "Although large language models (LLMs) has shown great performance on natural language processing (NLP) in the financial domain, there are no publicly available financial tailtored LLMs, instruction tuning datasets, and evaluation benchmarks, which is critical for continually pushing forward the open-source development of financial artificial intelligence (AI). This paper introduces PIXIU, a comprehensive framework including the first financial LLM based on fine-tuning LLaMA with instruction data, the first instruction data with 136K data samples to support the fine- tuning, and an evaluation benchmark with 5 tasks and 9 datasets. We first construct the large-scale multi-task instruction data considering a variety of financial tasks, financial document types, and financial data modalities. We then propose a financial LLM called FinMA by fine-tuning LLaMA with the constructed dataset to be able to follow instructions for various financial tasks. To support the evaluation of financial LLMs, we propose a standardized benchmark that covers a set of critical financial tasks, including five financial NLP tasks and one financial prediction task. With this benchmark, we conduct a detailed analysis of FinMA and several existing LLMs, uncovering their strengths and weaknesses in handling critical financial tasks. The model, datasets, benchmark, and experimental results are open-sourced 1 to facilitate future research in financial AI.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.13304",
        "title": "ToolQA: A Dataset for LLM Question Answering with External Tools",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning. To overcome these challenges, external tools can be used to enhance LLMs\u2019 question-answering abilities. However, current evaluation methods do not distinguish between questions that can be answered using LLMs\u2019 internal knowledge and those that require external information through tool use. To address this issue, we introduce a new dataset called ToolQA, which is designed to faithfully evaluate LLMs\u2019 ability to use external tools for question answering. Our development of ToolQA involved a scalable, automated process for dataset curation, along with 13 specialized tools designed for interaction with external knowledge in order to answer questions. Importantly, we strive to minimize the overlap between our benchmark data and LLMs\u2019 pre-training data, enabling a more precise evaluation of LLMs\u2019 tool-use reasoning abilities. We conducted an in-depth diagnosis of existing tool-use LLMs to highlight their strengths, weaknesses, and potential improvements. Our findings set a new benchmark for evaluating LLMs and suggest new directions for future advancements. Our data and code are freely available for the broader scientific community on GitHub 2.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.07934",
        "title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
        "abstract": "Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset called BoardgameQA for measuring the reasoning capacity of LMs in this setting. BoardgameQA also incorporates reasoning with implicit background knowledge, to better reflect reasoning problems in downstream applications. We benchmark various LMs on BoardgameQA and the results reveal a significant gap in the reasoning capacity of state-of-the-art LMs on this problem, showing that reasoning with conflicting information does not surface out-of-the-box in LMs. While performance can be improved with finetuning, it nevertheless remains poor.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.02394",
        "title": "NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications",
        "abstract": "Recently, the Deep Learning community has become interested in evolutionary opti- mization (EO) as a means to address hard optimization problems, e.g. meta-learning through long inner loop unrolls or optimizing non-differentiable operators. One core reason for this trend has been the recent innovation in hardware acceleration and compatible software \u2013 making distributed population evaluations much easier than before. Unlike for gradient descent-based methods though, there is a lack of hyperparameter understanding and best practices for EO \u2013 arguably due to severely less \u201cgraduate student descent\u201d and benchmarking being performed for EO meth- ods. Additionally, classical benchmarks from the evolutionary community provide few practical insights for Deep Learning applications. This poses challenges for newcomers to hardware-accelerated EO and hinders significant adoption. Hence, we establish a new benchmark of EO methods (NeuroEvoBench) tailored toward Deep Learning applications and exhaustively evaluate traditional and meta-learned EO. We investigate core scientific questions including resource allocation, fitness shaping, normalization, regularization & scalability of EO. The benchmark is open-sourced at https://github.com/neuroevobench/neuroevobench un- der Apache-2.0 license.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.19257",
        "title": "A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture",
        "abstract": "Instance detection (InsDet) is a long-lasting problem in robotics and computer vision, aiming to detect object instances (predefined by some visual examples) in a cluttered scene. Despite its practical significance, its advancement is overshadowed by Object Detection, which aims to detect objects belonging to some predefined classes. One major reason is that current InsDet datasets are too small in scale by today\u2019s standards. For example, the popular InsDet dataset GMU (published in 2016) has only 23 instances, far less than COCO (80 classes), a well-known object detection dataset published in 2014. We are motivated to introduce a new InsDet dataset and protocol. First, we define a realistic setup for InsDet: training data consists of multi-view instance captures, along with diverse scene images allowing synthesizing training images by pasting instance images on them with free box annotations. Second, we release a real-world database, which contains multi-view capture of 100 object instances, and high-resolution (6k\u00d78k) testing images. Third, we extensively study baseline methods for InsDet on our dataset, analyze their performance and suggest future work. Somewhat surprisingly, using the off-the-shelf class-agnostic segmentation model (Segment Anything Model, SAM) and the self-supervised feature representation DINOv2 performs the best, achieving >10 AP better than end-to-end trained InsDet models that repurpose object detectors (e.g., FasterRCNN and RetinaNet).",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.15599",
        "title": "OCEANBENCH:\nThe Sea Surface Height Edition",
        "abstract": "The ocean is a crucial component of the Earth\u2019s system. It profoundly influences human activities and plays a critical role in climate regulation. Our understand- ing has significantly improved over the last decades with the advent of satellite remote sensing data, allowing us to capture essential sea surface quantities over the globe, e.g., sea surface height (SSH). Despite their ever-increasing abundance, ocean satellite data presents challenges for information extraction due to their sparsity and irregular sampling, signal complexity, and noise. Machine learning (ML) techniques have demonstrated their capabilities in dealing with large-scale, complex signals. Therefore we see an opportunity for these ML models to har- ness the full extent of the information contained in ocean satellite data. However, data representation and relevant evaluation metrics can be the defining factors when determining the success of applied ML. The processing steps from the raw observation data to a ML-ready state and from model outputs to interpretable quantities require domain expertise, which can be a significant barrier to entry for ML researchers. In addition, imposing fixed processing steps, like committing to specific variables, regions, and geometries, will narrow the scope of ML models and their potential impact on real-world applications. OceanBench is a unifying framework that provides standardized processing steps that comply with domain- expert standards. It is designed with a flexible and pedagogical abstraction: it a) provides plug-and-play data and pre-configured pipelines for ML researchers to benchmark their models w.r.t. ML and domain-related baselines and b) provides a transparent and configurable framework for researchers to customize and extend the pipeline for their tasks. In this work, we demonstrate the OceanBench framework through a first edition dedicated to SSH interpolation challenges. We provide datasets and ML-ready benchmarking pipelines for the long-standing problem of interpolating observations from simulated ocean satellite data, multi-modal and multi-sensor fusion issues, and transfer-learning to real ocean satellite observations. The OceanBench framework is available at github.com/jejjohnson/oceanbench and the dataset registry is available at github.com/quentinf00/oceanbench-data-registry.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.13353",
        "title": "RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars",
        "abstract": "Synthesizing high-fidelity head avatars is a central prob- lem for many applications on AR, VR, and Metaverse. While head avatar synthesis algorithms have advanced rapidly, the best ones still face great obstacles in real-world sce- narios. One of the vital causes is the inadequate datasets \u2013 1) current public datasets can only support researchers to explore high-fidelity head avatars in one or two task direc- tions, such as viewpoint, head pose, hairstyle, or facial ex-\npression; 2) these datasets usually contain digital head as- sets with limited data volume, and narrow distribution over different attributes, such as expressions, ages, and acces- sories. In this paper, we present RenderMe-360, a compre- hensive 4D human head dataset to drive advance in head avatar algorithms across different scenarios. RenderMe- 360 contains massive data assets, with 243+ million com- plete head frames of over 800k video sequences from 500 different identities captured by synchronized HD multi-view cameras at 30 FPS. It is a large-scale digital library for (a) High Fidelity\n(b) High Diversity\n(c) Rich Annotations\narXiv:2305.13353v1 [cs.CV] 22 May 2023\nhead avatars with three key attributes: 1) High Fidelity: all subjects are captured by 60 synchronized, high-resolution 2K cameras to collect their portrait data in 360 degrees. 2) High Diversity: The collected subjects vary from differ- ent ages, eras, ethnicities, and cultures, providing abundant materials with distinctive styles in appearance and geom- etry. Moreover, each subject is asked to perform various dynamic motions, such as expressions and head rotations, which further extend the richness of assets. 3) Rich Annota- tions: the dataset provides annotations with different gran- ularities: cameras\u2019 parameters, background matting, scan, 2D as well as 3D facial landmarks, FLAME fitting, and text description.\nBased on the dataset, we build a comprehensive bench- mark for head avatar research, with 16 state-of-the-art methods performed on five main tasks: novel view synthe- sis, novel expression synthesis, hair rendering, hair edit- ing, and talking head generation. Our experiments un- cover the strengths and weaknesses of state-of-the-art meth- ods, showing that extra efforts are needed for them to per- form in such diverse scenarios. RenderMe-360 opens the door for future exploration in modern head avatars. All of the data, code, and models will be publicly available at https://RenderMe-360.github.io/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.05685",
        "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
        "abstract": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https: //github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.08571",
        "title": "GenImage: A Million-Scale Benchmark for Detecting AI-Generated Image",
        "abstract": "The extraordinary ability of generative models to generate photographic images has intensified concerns about the spread of disinformation, thereby leading to the demand for detectors capable of distinguishing between AI-generated fake images and real images. However, the lack of large datasets containing images from the most advanced image generators poses an obstacle to the development of such detectors. In this paper, we introduce the GenImage dataset, which has the following advantages: 1) Plenty of Images, including over one million pairs of AI-generated fake images and collected real images. 2) Rich Image Content, encompassing a broad range of image classes. 3) State-of-the-art Generators, synthesizing images with advanced diffusion models and GANs. The aforemen- tioned advantages allow the detectors trained on GenImage to undergo a thorough evaluation and demonstrate strong applicability to diverse images. We conduct a comprehensive analysis of the dataset and propose two tasks for evaluating the detection method in resembling real-world scenarios. The cross-generator image classification task measures the performance of a detector trained on one generator when tested on the others. The degraded image classification task assesses the capa- bility of the detectors in handling degraded images such as low-resolution, blurred, and compressed images. With the GenImage dataset, researchers can effectively expedite the development and evaluation of superior AI-generated image detectors in comparison to prevailing methodologies.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.11551",
        "title": "IMP-MARL: a Suite of Environments for Large-scale Infrastructure Management Planning via MARL",
        "abstract": "We introduce IMP-MARL, an open-source suite of multi-agent reinforcement learning (MARL) environments for large-scale Infrastructure Management Plan- ning (IMP), offering a platform for benchmarking the scalability of cooperative MARL methods in real-world engineering applications. In IMP, a multi-component engineering system is subject to a risk of failure due to its components\u2019 damage con- dition. Specifically, each agent plans inspections and repairs for a specific system component, aiming to minimise maintenance costs while cooperating to minimise system failure risk. With IMP-MARL, we release several environments including one related to offshore wind structural systems, in an effort to meet today\u2019s needs to improve management strategies to support sustainable and reliable energy systems. Supported by IMP practical engineering environments featuring up to 100 agents, we conduct a benchmark campaign, where the scalability and performance of state-of-the-art cooperative MARL methods are compared against expert-based heuristic policies. The results reveal that centralised training with decentralised execution methods scale better with the number of agents than fully centralised or decentralised RL approaches, while also outperforming expert-based heuristic policies in most IMP environments. Based on our findings, we additionally out- line remaining cooperation and scalability challenges that future MARL methods should still address. Through IMP-MARL, we encourage the implementation of new environments and the further development of MARL methods.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.06939",
        "title": "Multimodal C4:\nAn Open, Billion-scale Corpus of Images Interleaved with Text",
        "abstract": "In-context vision and language models like Flamingo [2] support arbitrarily in- terleaved sequences of images and text as input. This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g., \u201cWhat do image A and image B have in common?\u201d To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text. To date, however, large-scale data of this form have not been publicly available.\nWe release Multimodal C4 (mmc4), an augmentation of the popular text-only c4 corpus2 with images interleaved. We use a linear assignment algorithm to place images into longer bodies of text using CLIP features [24], a process that we show outperforms alternatives. mmc4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (88%) of images are topically relevant, and that linear assignment frequently selects individual sentences specifically well-aligned with each image (80%). After filtering NSFW images, ads, etc., the resulting mmc4 corpus consists of 101.2M documents with 571M images interleaved in 43B English tokens.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.07921",
        "title": "OpenIllumination: A Multi-Illumination Dataset for Inverse Rendering Evaluation on Real Objects",
        "abstract": "We introduce OpenIllumination, a real-world dataset containing over 108K im- ages of 64 objects with diverse materials, captured under 72 camera views and a large number of different illuminations. For each image in the dataset, we provide accurate camera parameters, illumination ground truth, and foreground segmentation masks. Our dataset enables the quantitative evaluation of most in- verse rendering and material decomposition methods for real objects. We examine several state-of-the-art inverse rendering methods on our dataset and compare their performances. The dataset and code can be found on the project page: https://oppo-us-research.github.io/OpenIllumination.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.10188",
        "title": "WordScape: a Pipeline to extract multilingual, visually rich Documents with Layout Annotations from Web Crawl Data",
        "abstract": "We introduce WordScape, a novel pipeline for the creation of cross-disciplinary, multilingual corpora comprising millions of pages with annotations for document layout detection. Relating visual and textual items on document pages has gained further significance with the advent of multimodal models. Various approaches proved effective for visual question answering or layout segmentation. However, the interplay of text, tables, and visuals remains challenging for a variety of document understanding tasks. In particular, many models fail to generalize well to diverse domains and new languages due to insufficient availability of training data. WordScape addresses these limitations. Our automatic annotation pipeline parses the Open XML structure of Word documents obtained from the web, jointly providing layout-annotated document images and their textual representations. In turn, WordScape offers unique properties as it (1) leverages the ubiquity of the Word file format on the internet, (2) is readily accessible through the Common Crawl web corpus, (3) is adaptive to domain-specific documents, and (4) offers culturally and linguistically diverse document pages with natural semantic structure and high-quality text. Together with the pipeline, we will additionally release 9.5M urls to word documents which can be processed using WordScape to create a dataset of over 40M pages. Finally, we investigate the quality of text and layout annotations extracted by WordScape, assess the impact on document understanding benchmarks, and demonstrate that manual labeling costs can be substantially reduced.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.13384",
        "title": "DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology",
        "abstract": "We present DiffInfinite, a hierarchical diffusion model that generates arbitrarily large histological images while preserving long-range correlation structural infor- mation. Our approach first generates synthetic segmentation masks, subsequently used as conditions for the high-fidelity generative diffusion process. The proposed sampling method can be scaled up to any desired image size while only requiring\nsmall patches for fast training. Moreover, it can be parallelized more efficiently than previous large-content generation methods while avoiding tiling artifacts. The training leverages classifier-free guidance to augment a small, sparsely an- notated dataset with unlabelled data. Our method alleviates unique challenges in histopathological imaging practice: large-scale information, costly manual anno- tation, and protective data handling. The biological plausibility of DiffInfinite data is evaluated in a survey by ten experienced pathologists as well as a downstream classification and segmentation task. Samples from the model score strongly on anti-copying metrics which is relevant for the protection of patient data",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.03310",
        "title": "LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning",
        "abstract": "Lifelong learning offers a promising paradigm of building a generalist agent that learns and adapts over its lifespan. Unlike traditional lifelong learning problems in image and text domains, which primarily involve the transfer of declarative knowl- edge of entities and concepts, lifelong learning in decision-making (LLDM) also necessitates the transfer of procedural knowledge, such as actions and behaviors. To advance research in LLDM, we introduce LIBERO, a novel benchmark of lifelong learning for robot manipulation. Specifically, LIBERO highlights five key research topics in LLDM: 1) how to efficiently transfer declarative knowledge, procedural knowledge, or the mixture of both; 2) how to design effective policy architectures and 3) effective algorithms for LLDM; 4) the robustness of a lifelong learner with respect to task ordering; and 5) the effect of model pretraining for LLDM. We develop an extendible procedural generation pipeline that can in principle generate infinitely many tasks. For benchmarking purpose, we create four task suites (130 tasks in total) that we use to investigate the above-mentioned research topics. To support sample-efficient learning, we provide high-quality human-teleoperated demonstration data for all tasks. Our extensive experiments present several insightful or even unexpected discoveries: sequential finetuning outperforms existing lifelong learning methods in forward transfer, no single visual encoder architecture excels at all types of knowledge transfer, and naive supervised pretraining can hinder agents\u2019 performance in the subsequent LLDM",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2301.13867",
        "title": "Mathematical Capabilities of ChatGPT",
        "abstract": "We investigate the mathematical capabilities of two iterations of ChatGPT (released 9-January- 2023 and 30-January-2023) and of GPT-4 by testing them on publicly available datasets, as well as hand-crafted ones, using a novel methodology. In contrast to formal mathematics, where large databases of formal proofs are available (e.g., the Lean Mathematical Library), current datasets of natural-language mathematics, used to benchmark language models, either cover only elementary mathematics or are very small. We address this by publicly releasing two new datasets: GHOSTS and miniGHOSTS. These are the first natural-language datasets curated by working researchers in mathematics that (1) aim to cover graduate-level mathematics, (2) provide a holistic overview of the mathematical capabilities of language models, and (3) distinguish multiple dimensions of mathematical reasoning. These datasets also test whether ChatGPT and GPT-4 can be helpful assistants to professional mathematicians by emulating use cases that arise in the daily professional activities of mathematicians. We benchmark the models on a range of fine-grained performance metrics. For advanced mathematics, this is the most detailed evaluation effort to date. We find that ChatGPT can be used most successfully as a mathematical assistant for querying facts, acting as a mathematical search engine and knowledge base interface. GPT-4 can additionally be used for undergraduate-level mathematics but fails on graduate-level difficulty. Contrary to many positive reports in the media about GPT-4 and ChatGPT\u2019s exam-solving abilities (a potential case of selection bias), their overall mathematical performance is well below the level of a graduate student. Hence, if your goal is to use ChatGPT to pass a graduate-level math exam, you would be better off copying from your average peer!",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.04751",
        "title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources",
        "abstract": "In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Despite recent claims that open models can be on par with state-of-the-art proprietary models, these claims are often accompanied by limited evaluation, making it difficult to compare models across the board and determine the utility of various resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size, trained on 12 instruc- tion datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning, multilinguality, coding, safety, and open-ended instruction following abilities through a collection of automatic, model-based, and human-based metrics. We further introduce T\u00dcLU , our best performing instruction-tuned model suite finetuned on a combination of high-quality open resources.\nOur experiments show that different instruction-tuning datasets can uncover or enhance specific skills, while no single dataset (or combination) provides the best performance across all evaluations. Interestingly, we find that model and human preference-based evaluations fail to reflect differences in model capabilities exposed by benchmark-based evaluations, suggesting the need for the type of systemic evaluation performed in this work. Our evaluations show that the best model in any given evaluation reaches on average 87% of ChatGPT performance, and 73% of GPT-4 performance, suggesting that further investment in building better base models and instruction-tuning data is required to close the gap. We release our instruction-tuned models, including a fully finetuned 65B T\u00dcLU , along with our code, data, and evaluation framework to facilitate future research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.03517",
        "title": "SOUNDCAM: A Dataset for Finding Humans Using Room Acoustics",
        "abstract": "A room\u2019s acoustic properties are a product of the room\u2019s geometry, the objects within the room, and their specific positions. A room\u2019s acoustic properties can be characterized by its impulse response (RIR) between a source and listener location, or roughly inferred from recordings of natural signals present in the room. Variations in the positions of objects in a room can effect measurable changes in the room\u2019s acoustic properties, as characterized by the RIR. Existing datasets of RIRs either do not systematically vary positions of objects in an environment, or they consist of only simulated RIRs. We present SOUNDCAM, the largest dataset of unique RIRs from in-the-wild rooms publicly released to date.1 It includes 5,000 10-channel real-world measurements of room impulse responses and 2,000 10-channel recordings of music in three different rooms, including a controlled acoustic lab, an in-the-wild living room, and a conference room, with different humans in positions throughout each room. We show that these measurements can be used for interesting tasks, such as detecting and identifying humans, and tracking their positions.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.07494",
        "title": "A Comprehensive Study on Large-Scale Graph Training: Benchmarking and Rethinking",
        "abstract": "Large-scale graph training is a notoriously challenging problem for graph neu- ral networks (GNNs). Due to the nature of evolving graph structures into the training process, vanilla GNNs usually fail to scale up, limited by the GPU memory space. Up to now, though numerous scalable GNN architectures have been proposed, we still lack a comprehensive survey and fair benchmark of this reservoir to find the rationale for designing scalable GNNs. To this end, we first systematically formulate the representative methods of large-scale graph training into several branches and further establish a fair and consistent bench- mark for them by a greedy hyperparameter searching. In addition, regarding efficiency, we theoretically evaluate the time and space complexity of various branches and empirically compare them w.r.t GPU memory usage, through- put, and convergence. Furthermore, We analyze the pros and cons for various branches of scalable GNNs and then present a new ensembling training man- ner, named EnGCN, to address the existing issues. Our code is available at https://github.com/VITA-Group/Large_Scale_GCN_Benchmarking.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.11606",
        "title": "StoryBench: A Multifaceted Benchmark for Continuous Story Visualization",
        "abstract": "Generating video stories from text prompts is a complex task. In addition to having high visual quality, videos need to realistically adhere to a sequence of text prompts whilst being consistent throughout the frames. Creating a benchmark for video gen- eration requires data annotated over time, which contrasts with the single caption used often in video datasets. To fill this gap, we collect comprehensive human an- notations on three existing datasets, and introduce StoryBench: a new, challenging multi-task benchmark to reliably evaluate forthcoming text-to-video models. Our benchmark includes three video generation tasks of increasing difficulty: action execution, where the next action must be generated starting from a conditioning video; story continuation, where a sequence of actions must be executed starting from a conditioning video; and story generation, where a video must be generated from only text prompts. We evaluate small yet strong text-to-video baselines, and show the benefits of training on story-like data algorithmically generated from existing video captions. Finally, we establish guidelines for human evaluation of video stories, and reaffirm the need of better automatic metrics for video generation. StoryBench aims at encouraging future research efforts in this exciting new area.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.09666",
        "title": "AbdomenAtlas-8K: Annotating 8,000 CT Volumes for Multi-Organ Segmentation in Three Weeks",
        "abstract": "Annotating medical images, particularly for organ segmentation, is laborious and time-consuming. For example, annotating an abdominal organ requires an estimated rate of 30\u201360 minutes per CT volume based on the expertise of an annotator and the size, visibility, and complexity of the organ. Therefore, publicly available datasets for multi-organ segmentation are often limited in data size and organ diversity. This paper proposes an active learning procedure to expedite the annotation process for organ segmentation and creates the largest multi-organ dataset (by far) with the spleen, liver, kidneys, stomach, gallbladder, pancreas, aorta, and IVC annotated in 8,448 CT volumes, equating to 3.2 million slices. The conventional annotation methods would take an experienced annotator up to 1,600 weeks (or roughly 30.8 years) to complete this task. In contrast, our annotation procedure has accomplished this task in three weeks (based on an 8-hour workday, five days a week) while maintaining a similar or even better annotation quality. This achievement is attributed to three unique properties of our method: (1) label bias reduction using multiple pre-trained segmentation models, (2) effective error detection in the model predictions, and (3) attention guidance for annotators to make corrections on the most salient errors. Furthermore, we summarize the taxonomy of common errors made by AI algorithms and annotators. This allows for continuous improvement of AI and annotations, significantly reducing the annotation costs required to create large-scale datasets for a wider variety of medical imaging tasks.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.05934",
        "title": "ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign Language Recognition",
        "abstract": "Sign languages are used as a primary language by approximately 70 million D/deaf people world-wide. However, most communication technologies operate in spoken and written languages, creating inequities in access. To help tackle this problem, we release ASL Citizen, the first crowdsourced Isolated Sign Language Recognition (ISLR) dataset, collected with consent and containing 83,399 videos for 2,731 distinct signs filmed by 52 signers in a variety of environments. We propose that this dataset be used for sign language dictionary retrieval for American Sign Language (ASL), where a user demonstrates a sign to their webcam to retrieve matching signs from a dictionary. We show that training supervised machine learning classifiers with our dataset advances the state-of-the-art on metrics relevant for dictionary retrieval, achieving 63% accuracy and a recall-at-10 of 91%, evaluated entirely on videos of users who are not present in the training or validation sets.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.04618",
        "title": "Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations",
        "abstract": "This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies com- monly lack adequate challenges, hindering the accurate evaluation of OOD robust- ness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre- trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic meth- ods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at https://github.com/lifan-yuan/OOD_NLP.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1704.05426",
        "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
        "abstract": "This paper introduces the Multi-Genre Natu- ral Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora avail- able for natural language inference (a.k.a. rec- ognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by of- fering data from ten distinct genres of written and spoken English, making it possible to eval- uate systems on nearly the full complexity of the language, while supplying an explicit set- ting for evaluating cross-genre domain adap- tation. In addition, an evaluation using exist- ing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2105.07464",
        "title": "FEW-NERD: A Few-shot Named Entity Recognition Dataset",
        "abstract": "Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published bench- mark data specifically focused on the practical and challenging task. Current approaches col- lect existing supervised NER datasets and re- organize them into the few-shot setting for em- pirical study. These strategies conventionally aim to recognize coarse-grained entity types with few examples, while in practice, most unseen entity types are fine-grained. In this paper, we present FEW-NERD, a large-scale human-annotated few-shot NER dataset with a hierarchy of 8 coarse-grained and 66 fine- grained entity types. FEW-NERD consists of 188,238 sentences from Wikipedia, 4,601,160 words are included and each is annotated as context or a part of a two-level entity type. To the best of our knowledge, this is the first few-shot NER dataset and the largest human- crafted NER dataset. We construct bench- mark tasks with different emphases to com- prehensively assess the generalization capabil- ity of models. Extensive empirical results and analysis show that FEW-NERD is challeng- ing and the problem requires further research. We make FEW-NERD public at https:// ningding97.github.io/fewnerd/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1606.05250",
        "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
        "abstract": "We present the Stanford Question Answer- ing Dataset (SQuAD), a new reading compre- hension dataset consisting of 100,000+ ques- tions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the cor- responding reading passage. We analyze the dataset to understand the types of reason- ing required to answer the questions, lean- ing heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple base- line (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2012.15349",
        "title": "DynaSent: A Dynamic Benchmark for Sentiment Analysis",
        "abstract": "We introduce DynaSent (\u2018Dynamic Senti- ment\u2019), a new English-language benchmark task for ternary (positive/negative/neutral) sen- timent analysis. DynaSent combines natu- rally occurring sentences with sentences cre- ated using the open-source Dynabench Plat- form, which facilities human-and-model-in- the-loop dataset creation. DynaSent has a total of 121,634 sentences, each validated by five crowdworkers, and its development and test splits are designed to produce chance perfor- mance for even the best models we have been able to develop; when future models solve this task, we will use them to create DynaSent ver- sion 2, continuing the dynamic evolution of this benchmark. Here, we report on the dataset creation effort, focusing on the steps we took to increase quality and reduce artifacts. We also present evidence that DynaSent\u2019s Neutral category is more coherent than the compara- ble category in other benchmarks, and we mo- tivate training models from scratch for each round over successive fine-tuning.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2203.09509",
        "title": "TOXIGEN: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
        "abstract": "Toxic language detection systems often falsely flag text that contains minority group men- tions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic lan- guage. To help mitigate these issues, we cre- ate TOXIGEN, a new large-scale and machine- generated dataset of 274k toxic and benign statements about 13 minority groups. We de- velop a demonstration-based prompting frame- work and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model (Brown et al., 2020). Controlling ma- chine generation in this way allows TOXIGEN to cover implicitly toxic text at a larger scale, and about more demographic groups, than pre- vious resources of human-written text. We conduct a human evaluation on a challeng- ing subset of TOXIGEN and find that annota- tors struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data im- proves its performance on human-written data substantially. We also demonstrate that TOXI- GEN can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset. Our code and data can be found at https:// github.com/microsoft/ToxiGen.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2109.05322",
        "title": "Latent Hatred: A Benchmark for Understanding Implicit Hate Speech",
        "abstract": "Hate speech has grown significantly on social media, causing serious consequences for vic- tims of all demographics. Despite much at- tention being paid to characterize and detect discriminatory speech, most work has focused on explicit or overt hate speech, failing to ad- dress a more pervasive form based on coded or indirect language. To fill this gap, this work introduces a theoretically-justified taxon- omy of implicit hate speech and a benchmark corpus with fine-grained labels for each mes- sage and its implication. We present system- atic analyses of our dataset using contempo- rary baselines to detect and explain implicit hate speech, and we discuss key features that challenge existing models. This dataset will continue to serve as a useful benchmark for un- derstanding this multifaceted issue. To down- load the data, see https://github.com/ GT-SALT/implicit-hate",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1910.14599",
        "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding",
        "abstract": "We introduce a new large-scale NLI bench- mark dataset, collected via an iterative, ad- versarial human-and-model-in-the-loop proce- dure. We show that training models on this new dataset leads to state-of-the-art perfor- mance on a variety of popular NLI bench- marks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the- art models, and shows that non-expert annota- tors are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2110.01799",
        "title": "ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts",
        "abstract": "Reviewing contracts is a time-consuming pro- cedure that incurs large expenses to companies and social inequality to those who cannot af- ford it. In this work, we propose document- level natural language inference (NLI) for con- tracts, a novel, real-world application of NLI that addresses such problems. In this task, a system is given a set of hypotheses (such as \u201cSome obligations of Agreement may survive termination.\u201d) and a contract, and it is asked to classify whether each hypothesis is entailed by, contradicting to or not mentioned by (neu- tral to) the contract as well as identifying ev- idence for the decision as spans in the con- tract. We annotated and release the largest cor- pus to date consisting of 607 annotated con- tracts. We then show that existing models fail badly on our task and introduce a strong baseline, which (1) models evidence identifi- cation as multi-label classification over spans instead of trying to predict start and end to- kens, and (2) employs more sophisticated con- text segmentation for dealing with long docu- ments. We also show that linguistic character- istics of contracts, such as negations by excep- tions, are contributing to the difficulty of this task and that there is much room for improve- ment.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2201.05955",
        "title": "WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation",
        "abstract": "A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting ex- amples, leading to a lack of linguistic diver- sity. We introduce a novel approach for dataset creation based on worker and AI collabo- ration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI for natural language infer- ence (NLI), our approach uses dataset cartog- raphy to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new exam- ples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowd- workers. The resulting dataset, WANLI, con- sists of 107,885 NLI examples and presents unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI improves performance on eight out- of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI, compared to training on the 4\u00d7 larger MultiNLI. Moreover, it continues to be more effective than MultiNLI augmented with other NLI datasets. Our results demonstrate the promise of leveraging natural language gener- ation techniques and re-imagining the role of humans in the dataset creation process.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.16225",
        "title": "CLEANCONLL: A Nearly Noise-Free Named Entity Recognition Dataset",
        "abstract": "The CoNLL-03 corpus is arguably the most well-known and utilized benchmark dataset for named entity recognition (NER). However, prior works found significant numbers of an- notation errors, incompleteness, and inconsis- tencies in the data. This poses challenges to objectively comparing NER approaches and an- alyzing their errors, as current state-of-the-art models achieve F1-scores that are comparable to or even exceed the estimated noise level in CoNLL-03. To address this issue, we present a comprehensive relabeling effort assisted by automatic consistency checking that corrects 7.0% of all labels in the English CoNLL-03. Our effort adds a layer of entity linking annota- tion both for better explainability of NER labels and as additional safeguard of annotation qual- ity. Our experimental evaluation finds not only that state-of-the-art approaches reach signifi- cantly higher F1-scores (97.1%) on our data, but crucially that the share of correct predic- tions falsely counted as errors due to annotation noise drops from 47% to 6%. This indicates that our resource is well suited to analyze the remaining errors made by state-of-the-art mod- els, and that the theoretical upper bound even on high resource, coarse-grained NER is not yet reached. To facilitate such analysis, we make CLEANCONLL publicly available to the research community1.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2405.11865",
        "title": "CoNLL#: Fine-grained Error Analysis and a Corrected Test Set for CoNLL-03 English",
        "abstract": "Modern named entity recognition systems have steadily improved performance in the age of larger and more powerful neural models. However, over the past several years, the state-of-the-art has seemingly hit another plateau on the benchmark CoNLL-03 English dataset. In this paper, we perform a deep dive into the test outputs of the highest-performing NER models, conducting a fine-grained evaluation of their performance by introducing new document-level annotations on the test set. We go beyond F1 scores by categorizing errors in order to interpret the true state of the art for NER and guide future work. We review previous attempts at correcting the various flaws of the test set and introduce CoNLL#, a new corrected version of the test set that addresses its systematic and most prevalent errors, allowing for low-noise, interpretable error analysis.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2212.09306",
        "title": "E-NER \u2014 An Annotated Named Entity Recognition Corpus of Legal Text",
        "abstract": "Identifying named entities such as a person, location or organization, in documents can highlight key information to readers. Training Named Entity Recognition (NER) models re- quires an annotated data set, which can be a time-consuming labour-intensive task. Never- theless, there are publicly available NER data sets for general English. Recently there has been interest in developing NER for legal text. However, prior work and experimental results reported here indicate that there is a signifi- cant degradation in performance when NER methods trained on a general English data set are applied to legal text. We describe a pub- licly available legal NER data set, called E- NER, based on legal company filings available from the US Securities and Exchange Commis- sion\u2019s EDGAR data set. Training a number of different NER algorithms on the general En- glish CoNLL-2003 corpus but testing on our test collection confirmed significant degrada- tions in accuracy, as measured by the F1-score, of between 29.4% and 60.4%, compared to training and testing on the E-NER collection.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.16156",
        "title": "From Text to Multimodal: A Comprehensive Survey of Adversarial Example Generation in Question Answering Systems",
        "abstract": "Integrating adversarial machine learning with Question Answering (QA) systems has emerged as a critical area for understanding the vulnerabilities and robust- ness of these systems. This article aims to comprehensively review adversarial example-generation techniques in the QA field, including textual and multimodal contexts. We examine the techniques employed through systematic categoriza- tion, providing a comprehensive, structured review. Beginning with an overview of traditional QA models, we traverse the adversarial example generation by exploring rule-based perturbations and advanced generative models. We then extend our research to include multimodal QA systems, analyze them across var- ious methods, and examine generative models, seq2seq architectures, and hybrid methodologies. Our research grows to different defense strategies, adversarial datasets, and evaluation metrics and illustrates the comprehensive literature on adversarial QA. Finally, the paper considers the future landscape of adversarial question generation, highlighting potential research directions that can advance textual and multimodal QA systems in the context of adversarial challenges.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1611.09830",
        "title": "NEWSQA: A MACHINE COMPREHENSION DATASET",
        "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and an- swers based on a set of over 10,000 news articles from CNN, with answers consist- ing of spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (0.198 in F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available athttps://datasets.maluuba.com/NewsQA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1704.05179",
        "title": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine",
        "abstract": "We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article and generate a question-answer pair, but start from an ex- isting question-answer pair, crawled from J! Archive, and augment it with text snip- pets retrieved by Google. Following this approach, we built SearchQA, which con- sists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with ad- ditional meta-data such as the snippet\u2019s URL, which we believe will be valuable resources for future research. We conduct human evaluation as well as test two base- line methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a mean- ingful gap between the human and ma- chine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09375",
        "title": "Symmetry-Informed Geometric Representation for Molecules, Proteins, and Crystalline Materials",
        "abstract": "Artificial intelligence for scientific discovery has recently generated significant interest within the machine learning and scientific communities, particularly in the domains of chemistry, biology, and material discovery. For these scientific prob- lems, molecules serve as the fundamental building blocks, and machine learning has emerged as a highly effective and powerful tool for modeling their geometric structures. Nevertheless, due to the rapidly evolving process of the field and the knowledge gap between science (e.g., physics, chemistry, & biology) and machine learning communities, a benchmarking study on geometrical representation for such data has not been conducted. To address such an issue, in this paper, we first provide a unified view of the current symmetry-informed geometric methods, classifying them into three main categories: invariance, equivariance with spherical frame basis, and equivariance with vector frame basis. Then we propose a platform, coined Geom3D, which enables benchmarking the effectiveness of geometric strategies. Geom3D contains 16 advanced symmetry-informed geometric repre- sentation models and 14 geometric pretraining methods over 46 diverse datasets, including small molecules, proteins, and crystalline materials. We hope that Geom3D can, on the one hand, eliminate barriers for machine learning researchers interested in exploring scientific problems; and, on the other hand, provide valuable guidance for researchers in computational chemistry, structural biology, and materials science, aiding in the informed selection of representation techniques for specific applications. The source code is available on the GitHub repository.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.13226",
        "title": "Real3D-AD: A Dataset of Point Cloud Anomaly Detection",
        "abstract": "High-precision point cloud anomaly detection is the gold standard for identify- ing the defects of advancing machining and precision manufacturing. Despite some methodological advances in this area, the scarcity of datasets and the lack of a systematic benchmark hinder its development. We introduce Real3D-AD, a challenging high-precision point cloud anomaly detection dataset, addressing the limitations in the field. With 1,254 high-resolution 3D items (from forty thou- sand to millions of points for each item), Real3D-AD is the largest dataset for high-precision 3D industrial anomaly detection to date. Real3D-AD surpasses existing 3D anomaly detection datasets available regarding point cloud resolution (0.0010mm-0.0015mm), 360 degree coverage and perfect prototype. Additionally, we present a comprehensive benchmark for Real3D-AD, revealing the absence of baseline methods for high-precision point cloud anomaly detection. To ad- dress this, we propose Reg3D-AD, a registration-based 3D anomaly detection method incorporating a novel feature memory bank that preserves local and global representations. Extensive experiments on the Real3D-AD dataset highlight the effectiveness of Reg3D-AD. For reproducibility and accessibility, we provide the Real3D-AD dataset, benchmark source code, and Reg3D-AD on our web- site:https://github.com/M-3LAB/Real3D-AD.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.16044",
        "title": "Stanford-ORB: A Real-World 3D Object Inverse Rendering Benchmark",
        "abstract": "We introduce Stanford-ORB, a new real-world 3D Object inverse Rendering Bench- mark. Recent advances in inverse rendering have enabled a wide range of real-world applications in 3D content generation, moving rapidly from research and commer- cial use cases to consumer devices. While the results continue to improve, there is no real-world benchmark that can quantitatively assess and compare the perfor- mance of various inverse rendering methods. Existing real-world datasets typically consist only of the shape and multi-view images of objects, which are not sufficient for evaluating the quality of material recovery and object relighting. Methods capa- ble of recovering material and lighting often resort to synthetic data for quantitative evaluation, which on the other hand does not guarantee generalization to complex real-world environments. We introduce a new dataset of real-world objects captured under a variety of natural scenes with ground-truth 3D scans, multi-view images, and environment lighting. Using this dataset, we establish the first comprehensive real-world evaluation benchmark for object inverse rendering tasks from in-the- wild scenes and compare the performance of various existing methods. All data, code, and models can be accessed at https://stanfordorb.github.io/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.11709",
        "title": "Live Graph Lab: Towards Open, Dynamic and Real Transaction Graphs with NFT",
        "abstract": "Numerous studies have been conducted to investigate the properties of large-scale temporal graphs. Despite the ubiquity of these graphs in real-world scenarios, it\u2019s usually impractical for us to obtain the whole real-time graphs due to privacy concerns and technical limitations. In this paper, we introduce the concept of Live Graph Lab for temporal graphs, which enables open, dynamic and real transaction graphs from blockchains. Among them, Non-fungible tokens (NFTs) have become one of the most prominent parts of blockchain over the past several years. With more than $40 billion market capitalization, this decentralized ecosystem produces massive, anonymous and real transaction activities, which naturally forms a com- plicated transaction network. However, there is limited understanding about the characteristics of this emerging NFT ecosystem from a temporal graph analysis perspective. To mitigate this gap, we instantiate a live graph with NFT transaction network and investigate its dynamics to provide new observations and insights. Specifically, through downloading and parsing the NFT transaction activities, we obtain a temporal graph with more than 4.5 million nodes and 124 million edges. Then, a series of measurements are presented to understand the properties of the NFT ecosystem. Through comparisons with social, citation, and web networks, our analyses give intriguing findings and point out potential directions for future exploration. Finally, we also study machine learning models in this live graph to enrich the current datasets and provide new opportunities for the graph community. The source codes and dataset are available at https://livegraphlab.github.io.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.04662",
        "title": "MADLAD-400: A Multilingual And Document-Level Large Audited Dataset",
        "abstract": "We introduce MADLAD-400, a manually audited, general domain 3T token mono- lingual dataset based on CommonCrawl, spanning 419 languages. We discuss the limitations revealed by self-auditing MADLAD-400, and the role data auditing had in the dataset creation process. We then train and release a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages using publicly available data, and find that it is competitive with models that are significantly larger, and report the results on different domains. In addi- tion, we train a 8B-parameter language model, and assess the results on few-shot translation. We make the baseline models 1 available to the research community.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.07105",
        "title": "CORL: Research-oriented Deep Offline Reinforcement Learning Library",
        "abstract": "CORL1 is an open-source library that provides thoroughly benchmarked single-file implementations of both deep offline and offline-to-online reinforcement learning algorithms. It emphasizes a simple developing experience with a straightforward codebase and a modern analysis tracking tool. In CORL, we isolate methods implementation into separate single files, making performance-relevant details easier to recognize. Additionally, an experiment tracking feature is available to help log metrics, hyperparameters, dependencies, and more to the cloud. Finally, we have ensured the reliability of the implementations by benchmarking commonly employed D4RL datasets providing a transparent source of results that can be reused for robust evaluation tools such as performance profiles, probability of improvement, or expected online performance.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.13890",
        "title": "Bitstream-corrupted Video Recovery: A Novel Benchmark Dataset and Method",
        "abstract": "The past decade has witnessed great strides in video recovery by specialist tech- nologies, like video inpainting, completion, and error concealment. However, they typically simulate the missing content by manual-designed error masks, thus failing to fill in the realistic video loss in video communication (e.g., telepresence, live streaming, and internet video) and multimedia forensics. To address this, we intro- duce the bitstream-corrupted video (BSCV) benchmark, the first benchmark dataset with more than 28,000 video clips, which can be used for bitstream-corrupted video recovery in the real world. The BSCV is a collection of 1) a proposed three- parameter corruption model for video bitstream, 2) a large-scale dataset containing rich error patterns, multiple corruption levels, and flexible dataset branches, and 3) a plug-and-play module in video recovery framework that serves as a bench- mark. We evaluate state-of-the-art video inpainting methods on the BSCV dataset, demonstrating existing approaches\u2019 limitations and our framework\u2019s advantages in solving the bitstream-corrupted video recovery problem. The benchmark and dataset are released at https://github.com/LIUTIGHE/BSCV-Dataset.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.10280",
        "title": "OpenGSL: A Comprehensive Benchmark for Graph Structure Learning",
        "abstract": "Graph Neural Networks (GNNs) have emerged as the de facto standard for repre- sentation learning on graphs, owing to their ability to effectively integrate graph topology and node attributes. However, the inherent suboptimal nature of node con- nections, resulting from the complex and contingent formation process of graphs, presents significant challenges in modeling them effectively. To tackle this issue, Graph Structure Learning (GSL), a family of data-centric learning approaches, has garnered substantial attention in recent years. The core concept behind GSL is to jointly optimize the graph structure and the corresponding GNN models. Despite the proposal of numerous GSL methods, the progress in this field remains unclear due to inconsistent experimental protocols, including variations in datasets, data processing techniques, and splitting strategies. In this paper, we introduce OpenGSL, the first comprehensive benchmark for GSL, aimed at addressing this gap. OpenGSL enables a fair comparison among state-of-the-art GSL methods by evaluating them across various popular datasets using uniform data processing and splitting strategies. Through extensive experiments, we observe that existing GSL methods do not consistently outperform vanilla GNN counterparts. We also find that there is no significant correlation between the homophily of the learned struc- ture and task performance, challenging the common belief. Moreover, we observe that the learned graph structure demonstrates a strong generalization ability across different GNN models, despite the high computational and space consumption. We hope that our open-sourced library will facilitate rapid and equitable evaluation and inspire further innovative research in this field. The code of the benchmark can be found in https://github.com/OpenGSL/OpenGSL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.15701",
        "title": "HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models",
        "abstract": "Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acous- tic model is sensitive to variations in the speech domain, e.g., background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observa- tion, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses pro- vide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as the output transcription. The proposed benchmark contains a novel dataset, \u201cHyPoradise\u201d (HP), encompassing more than 334,000 pairs of N-best hypotheses and corresponding accurate transcriptions across prevalent speech domains. Given this dataset, we examine three types of error correction techniques based on LLMs with varying amounts of labeled hypotheses-transcription pairs, which gains a significant word error rate (WER) reduction. Experimental evidence demonstrates the proposed technique achieves a breakthrough by surpassing the upper bound of traditional re-ranking based meth- ods. More surprisingly, LLM with reasonable prompt and its generative capability can even correct those tokens that are missing in N-best list. We make our results publicly accessible for reproducible pipelines with released pre-trained models, thus providing a new evaluation paradigm for ASR error correction with LLMs.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.03533",
        "title": "Low-shot Object Learning with Mutual Exclusivity Bias",
        "abstract": "This paper introduces Low-shot Object Learning with Mutual Exclusivity Bias (LSME), the first computational framing of mutual exclusivity bias, a phenomenon commonly observed in infants during word learning. We provide a novel dataset, comprehensive baselines, and a state-of-the-art method to enable the ML com- munity to tackle this challenging learning task. The goal of LSME is to analyze an RGB image of a scene containing multiple objects and correctly associate a previously-unknown object instance with a provided category label. This associa- tion is then used to perform low-shot learning to test category generalization. We provide a data generation pipeline for the LSME problem and conduct a thorough analysis of the factors that contribute to its difficulty. Additionally, we evaluate the performance of multiple baselines, including state-of-the-art foundation models. Finally, we present a baseline approach that outperforms state-of-the-art models in terms of low-shot accuracy. Code and data are available at https://github.com/rehg- lab/LSME.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2206.10498",
        "title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change",
        "abstract": "Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks\u2013where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities\u2013including plan generation\u2013LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.05144",
        "title": "Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean",
        "abstract": "We introduce Mesogeos1, a large-scale multi-purpose dataset for wildfire modeling in the Mediterranean. Mesogeos integrates variables representing wildfire drivers (meteorology, vegetation, human activity) and historical records of wildfire igni- tions and burned areas for 17 years (2006-2022). It is designed as a cloud-friendly spatio-temporal dataset, namely a datacube, harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The datacube structure offers opportunities to assess machine learning (ML) usage in various wildfire modeling tasks. We extract two ML-ready datasets that establish distinct tracks to demonstrate this potential: (1) short-term wildfire danger forecasting and (2) final burned area estimation given the point of ignition. We define appropriate metrics and baselines to evaluate the performance of models in each track. By publishing the datacube, along with the code to create the ML datasets and models, we encourage the community to foster the implementation of additional tracks for mitigating the increasing threat of wildfires in the Mediterranean.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.08754",
        "title": "ClimSim: A large multi-scale dataset for hybrid physics-ML climate emulation",
        "abstract": "Modern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise predictions of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore\u2019s Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset designed for hybrid ML-physics re- search. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics on a host climate simulator\u2019s macro-scale physical state.\nThe dataset is global in coverage, spans multiple years at high sampling frequency, and is designed such that resulting emulators are compatible with downstream cou- pling into operational climate simulators. We implement a range of deterministic and stochastic regression baselines to highlight the ML challenges and their scoring. The data (https://huggingface.co/datasets/LEAP/ClimSim_high-res 2) and code (https://leap-stc.github.io/ClimSim) are released openly to support the development of hybrid ML-physics and high-fidelity climate simula- tions for the benefit of science and society.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.11207",
        "title": "Quilt-1M: One Million Image-Text Pairs for Histopathology",
        "abstract": "Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has slowed comparable progress. To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering 1, 087 hours of valuable educational histopathology videos from expert clinicians. From YouTube, we curate QUILT: a large-scale vision-language dataset consisting of 802,144 image and text pairs. QUILT was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around 200K samples. We combine QUILT with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even larger dataset: QUILT-1M, with 1M paired image- text samples, marking it as the largest vision-language histopathology dataset to date. We demonstrate the value of QUILT-1M by fine-tuning a pre-trained CLIP model. Our model outperforms state-of-the-art models on both zero-shot and linear probing tasks for classifying new histopathology images across 13 diverse patch-level datasets of 8 different sub-pathologies and cross-modal retrieval tasks",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.18724",
        "title": "WCLD: Curated Large Dataset of Criminal Cases from Wisconsin Circuit Courts",
        "abstract": "Machine learning based decision-support tools in criminal justice systems are subjects of intense discussions and academic research. There are important open questions about the utility and fairness of such tools. Academic researchers often rely on a few small datasets that are not sufficient to empirically study various real-world aspects of these questions. In this paper, we contribute WCLD, a curated large dataset of 1.5 million criminal cases from circuit courts in the U.S. state of Wisconsin. We used reliable public data from 1970 to 2020 to curate attributes like prior criminal counts and recidivism outcomes. The dataset contains large number of samples from five racial groups, in addition to information like sex and age (at judgment and first offense). Other attributes in this dataset include neighborhood characteristics obtained from census data, detailed types of offense, charge severity, case decisions, sentence lengths, year of filing etc. We also provide pseudo-identifiers for judge, county and zipcode. The dataset will not only enable researchers to more rigorously study algorithmic fairness in the context of criminal justice, but also relate algorithmic challenges with various systemic issues. We also discuss in detail the process of constructing the dataset and provide a datasheet. The WCLD dataset is available at https://clezdata.github.io/wcld/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.02405",
        "title": "BEDD: The MineRL BASALT Evaluation and Demonstrations Dataset for Training and Benchmarking Agents that Solve Fuzzy Tasks",
        "abstract": "The MineRL BASALT competition has served to catalyze advances in learning from human feedback through four hard-to-specify tasks in Minecraft, such as create and photograph a waterfall. Given the completion of two years of BASALT competitions, we offer to the community a formalized benchmark through the BASALT Evaluation and Demonstrations Dataset (BEDD), which serves as a re- source for algorithm development and performance assessment. BEDD consists of a collection of 26 million image-action pairs from nearly 14,000 videos of human players completing the BASALT tasks in Minecraft. It also includes over 3,000 dense pairwise human evaluations of human and algorithmic agents. These com- parisons serve as a fixed, preliminary leaderboard for evaluating newly-developed algorithms. To enable this comparison, we present a streamlined codebase for benchmarking new algorithms against the leaderboard. In addition to presenting these datasets, we conduct a detailed analysis of the data from both datasets to guide algorithm development and evaluation. The released code and data are available at https://github.com/minerllabs/basalt-benchmark.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.08893",
        "title": "LOVM: Language-Only Vision Model Selection",
        "abstract": "Pre-trained multi-modal vision-language models (VLMs) are becoming increas- ingly popular due to their exceptional performance on downstream vision appli- cations, particularly in the few- and zero-shot settings. However, selecting the best-performing VLM for some downstream applications is non-trivial, as it is dataset and task-dependent. Meanwhile, the exhaustive evaluation of all available VLMs on a novel application is not only time and computationally demanding but also necessitates the collection of a labeled dataset for evaluation. As the number of open-source VLM variants increases, there is a need for an efficient model selection strategy that does not require access to a curated evaluation dataset. This paper proposes a novel task and benchmark for efficiently evaluating VLMs\u2019 zero-shot performance on downstream applications without access to the downstream task dataset. Specifically, we introduce a new task LOVM: Language- Only Vision Model Selection, where methods are expected to perform both model selection and performance prediction based solely on a text description of the desired downstream application. We then introduced an extensive LOVM benchmark consisting of ground-truth evaluations of 35 pre-trained VLMs and 23 datasets, where methods are expected to rank the pre-trained VLMs and predict their zero-shot performance.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.03329",
        "title": "AVIDa-hIL6: A Large-Scale VHH Dataset Produced from an Immunized Alpaca for Predicting Antigen-Antibody Interactions",
        "abstract": "Antibodies have become an important class of therapeutic agents to treat human diseases. To accelerate therapeutic antibody discovery, computational methods, especially machine learning, have attracted considerable interest for predicting specific interactions between antibody candidates and target antigens such as viruses and bacteria. However, the publicly available datasets in existing works have notable limitations, such as small sizes and the lack of non-binding samples and exact amino acid sequences. To overcome these limitations, we have developed AVIDa-hIL6, a large-scale dataset for predicting antigen-antibody interactions in the variable domain of heavy chain of heavy chain antibodies (VHHs), produced from an alpaca immunized with the human interleukin-6 (IL-6) protein, as antigens. By leveraging the simple structure of VHHs, which facilitates identification of full- length amino acid sequences by DNA sequencing technology, AVIDa-hIL6 contains 573,891 antigen-VHH pairs with amino acid sequences. All the antigen-VHH pairs have reliable labels for binding or non-binding, as generated by a novel labeling method. Furthermore, via introduction of artificial mutations, AVIDa-hIL6 contains 30 different mutants in addition to wild-type IL-6 protein. This characteristic provides opportunities to develop machine learning models for predicting changes in antibody binding by antigen mutations. We report experimental benchmark results on AVIDa-hIL6 by using machine learning models. The results indicate that the existing models have potential, but further research is needed to generalize them to predict effective antibodies against unknown mutants. The dataset is available at https://avida-hil6.cognanous.com.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.14108",
        "title": "DATACOMP:\nIn search of the next generation of multimodal datasets",
        "abstract": "Multimodal datasets are a critical component in recent breakthroughs such as CLIP, Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DATACOMP, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets. Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DATACOMP workflow leads to better training sets. Our best baseline, DATACOMP-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI\u2019s CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute. We release DATACOMP and all accompanying code at www.datacomp.ai.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09549",
        "title": "QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules",
        "abstract": "Supervised machine learning approaches have been increasingly used in acceler- ating electronic structure prediction as surrogates of first-principle computational methods, such as density functional theory (DFT). While numerous quantum chem- istry datasets focus on chemical properties and atomic forces, the ability to achieve accurate and efficient prediction of the Hamiltonian matrix is highly desired, as it is the most important and fundamental physical quantity that determines the quantum states of physical systems and chemical properties. In this work, we generate a new Quantum Hamiltonian dataset, named as QH9, to provide pre- cise Hamiltonian matrices for 999 or 2998 molecular dynamics trajectories and 130,831 stable molecular geometries, based on the QM9 dataset. By designing benchmark tasks with various molecules, we show that current machine learning models have the capacity to predict Hamiltonian matrices for arbitrary molecules. Both the QH9 dataset and the baseline models are provided to the community through an open-source benchmark, which can be highly valuable for developing machine learning methods and accelerating molecular and materials design for scientific and technological applications. Our benchmark is publicly available at https://github.com/divelab/AIRS/tree/main/OpenDFT/QHBench.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.03886",
        "title": "FIND: A Function Description Benchmark for Evaluating Interpretability Methods",
        "abstract": "Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of trained neural networks, and accompanying descrip- tions of the kind we seek to generate. The functions are procedurally constructed across textual and numeric domains, and involve a range of real-world complexities, including noise, composition, approximation, and bias. We evaluate methods that use pretrained language models (LMs) to produce code-based and natural language descriptions of function behavior. Additionally, we introduce a new interactive method in which an Automated Interpretability Agent (AIA) generates function descriptions. We find that an AIA, built with an off-the-shelf LM augmented with black-box access to functions, can sometimes infer function structure\u2014acting as a scientist by forming hypotheses, proposing experiments, and updating descriptions in light of new data. However, FIND also reveals that LM-based descriptions capture global function behavior while missing local details. These results suggest that FIND will be useful for characterizing the performance of more sophisticated interpretability methods before they are applied to real-world models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.08322",
        "title": "C-EVAL: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
        "abstract": "New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-EVAL , the first comprehensive Chi- nese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-EVAL comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-EVAL is accompanied by C-EVAL HARD, a subset of very challenging subjects in C-EVAL that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-EVAL, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-EVAL will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.16447",
        "title": "ChimpACT: A Longitudinal Dataset for Understanding Chimpanzee Behaviors",
        "abstract": "Understanding the behavior of non-human primates is crucial for improving animal welfare, modeling social behavior, and gaining insights into distinctively human and phylogenetically shared behaviors. However, the lack of datasets on non-human primate behavior hinders in-depth exploration of primate social interactions, posing challenges to research on our closest living relatives. To address these limitations, we present ChimpACT, a comprehensive dataset for quantifying the longitudinal behavior and social relations of chimpanzees within a social group. Spanning from 2015 to 2018, ChimpACT features videos of a group of over 20 chimpanzees residing at the Leipzig Zoo, Germany, with a particular focus on documenting the developmental trajectory of one young male, Azibo. ChimpACT is both com- prehensive and challenging, consisting of 163 videos with a cumulative 160,500 frames, each richly annotated with detection, identification, pose estimation, and fine-grained spatiotemporal behavior labels. We benchmark representative methods of three tracks on ChimpACT: (i) tracking and identification, (ii) pose estimation, and (iii) spatiotemporal action detection of the chimpanzees. Our experiments reveal that ChimpACT offers ample opportunities for both devising new methods and adapting existing ones to solve fundamental computer vision tasks applied to chimpanzee groups, such as detection, pose estimation, and behavior analy- sis, ultimately deepening our comprehension of communication and sociality in non-human primates",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2207.04043",
        "title": "The Harvard USPTO Patent Dataset:\nA Large-Scale, Well-Structured, and Multi-Purpose Corpus of Patent Applications",
        "abstract": "Innovation is a major driver of economic and social development, and informa- tion about many kinds of innovation is embedded in semi-structured data from patents and patent applications. Although the impact and novelty of innovations expressed in patent data are difficult to measure through traditional means, ML offers a promising set of techniques for evaluating novelty, summarizing contribu- tions, and embedding semantics. In this paper, we introduce the Harvard USPTO Patent Dataset (HUPD), a large-scale, well-structured, and multi-purpose corpus of English-language patent applications filed to the United States Patent and Trade- mark Office (USPTO) between 2004 and 2018. With more than 4.5 million patent documents, HUPD is two to three times larger than comparable corpora. Unlike previously proposed patent datasets in NLP, HUPD contains the inventor-submitted versions of patent applications\u2014not the final versions of granted patents\u2014thereby allowing us to study patentability at the time of filing using NLP methods for the first time. It is also novel in its inclusion of rich structured metadata alongside the text of patent filings: By providing each application\u2019s metadata along with all of its text fields, the dataset enables researchers to perform new sets of NLP tasks that leverage variation in structured covariates. As a case study on the types of research HUPD makes possible, we introduce a new task to the NLP community\u2014namely, binary classification of patent decisions. We additionally show the structured metadata provided in the dataset enables us to conduct explicit studies of concept shifts for this task. Finally, we demonstrate how our dataset can be used for three additional tasks: multi-class classification of patent subject areas, language mod- eling, and summarization. Overall, HUPD is one of the largest multi-purpose NLP datasets containing domain-specific textual data, along with well-structured bibliographic metadata, and aims to advance research extending language and classification models to diverse and dynamic real-world data distributions.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.13117",
        "title": "AVERITEC: A Dataset for Real-world Claim Verification with Evidence from the Web",
        "abstract": "Existing datasets for automated fact-checking have substantial limitations, such as relying on artificial claims, lacking annotations for evidence and intermediate reasoning, or including evidence published after the claim. In this paper, we introduce AVERITEC, a new dataset of 4,568 real-world claims covering fact- checks by 50 different organizations. Each claim is annotated with question- answer pairs supported by evidence available online, as well as textual justifications explaining how the evidence combines to produce a verdict. Through a multi- round annotation process, we avoid common pitfalls including context dependence, evidence insufficiency, and temporal leakage, and reach a substantial inter-annotator agreement of \u03ba = 0.619 on verdicts. We develop a baseline as well as an evaluation scheme for verifying claims through question-answering against the open web.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.13446",
        "title": "Video Timeline Modeling For News Story Understanding",
        "abstract": "In this paper, we present a novel problem, namely video timeline modeling. Our objective is to create a video-associated timeline from a set of videos related to a specific topic, thereby facilitating the content and structure understanding of the story being told. This problem has significant potential in various real-world applications, for instance, news story summarization. To bootstrap research in this area, we curate a realistic benchmark dataset, YouTube-News-Timeline, con- sisting of over 12k timelines and 300k YouTube news videos. Additionally, we propose a set of quantitative metrics to comprehensively evaluate and compare methodologies. With such a testbed, we further develop and benchmark sev- eral deep learning approaches to tackling this problem. We anticipate that this exploratory work will pave the way for further research in video timeline mod- eling. The assets are available via https://github.com/google-research/ google-research/tree/master/video_timeline_modeling.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.08310",
        "title": "TWIGMA: A dataset of AI-Generated Images with Metadata From Twitter",
        "abstract": "Recent progress in generative artificial intelligence (gen-AI) has enabled the gener- ation of photo-realistic and artistically-inspiring photos at a single click, catering to millions of users online. To explore how people use gen-AI models such as DALLE and StableDiffusion, it is critical to understand the themes, contents, and variations present in the AI-generated photos. In this work, we introduce TWIGMA (TWItter Generative-ai images with MetadatA), a comprehensive dataset encompassing over 800,000 gen-AI images collected from Jan 2021 to March 2023 on Twitter, with associated metadata (e.g., tweet text, creation date, number of likes), available at https://zenodo.org/records/8031785. Through a comparative analysis of TWIGMA with natural images and human artwork, we find that gen-AI images possess distinctive characteristics and exhibit, on average, lower variability when compared to their non-gen-AI counterparts. Additionally, we find that the similarity between a gen-AI image and natural images is inversely correlated with the number of likes. Finally, we observe a longitudinal shift in the themes of AI-generated images on Twitter, with users increasingly sharing artistically sophisticated content such as intricate human portraits, whereas their interest in simple subjects such as natural scenes and animals has decreased. Our findings underscore the significance of TWIGMA as a unique data resource for studying AI-generated images.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.08252",
        "title": "MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning",
        "abstract": "Recently, Meta-Black-Box Optimization with Reinforcement Learning (MetaBBO- RL) has showcased the power of leveraging RL at the meta-level to mitigate manual fine-tuning of low-level black-box optimizers. However, this field is hindered by the lack of a unified benchmark. To fill this gap, we introduce MetaBox, the first benchmark platform expressly tailored for developing and evaluating MetaBBO- RL methods. MetaBox offers a flexible algorithmic template that allows users to effortlessly implement their unique designs within the platform. Moreover, it provides a broad spectrum of over 300 problem instances, collected from synthetic to realistic scenarios, and an extensive library of 19 baseline methods, including both traditional black-box optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three standardized performance metrics, enabling a more thorough assessment of the methods. In a bid to illustrate the utility of MetaBox for facilitating rigorous evaluation and in-depth analysis, we carry out a wide- ranging benchmarking study on existing MetaBBO-RL methods. Our MetaBox is open-source and accessible at: https://github.com/GMC-DRL/MetaBox.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.12386",
        "title": "EFWI: Multiparameter Benchmark Datasets for Elastic Full Waveform Inversion of Geophysical Properties",
        "abstract": "Elastic geophysical properties (such as P- and S-wave velocities) are of great importance to various subsurface applications like CO2 sequestration and energy exploration (e.g., hydrogen and geothermal). Elastic full waveform inversion (FWI) is widely applied for characterizing reservoir properties. In this paper, we introduce EFWI, a comprehensive benchmark dataset that is specifically designed for elastic FWI. EFWI encompasses 8 distinct datasets that cover diverse subsurface geologic structures (flat, curve, faults, etc). The benchmark results produced by three differ- ent deep learning methods are provided. In contrast to our previously presented dataset (pressure recordings) for acoustic FWI (referred to as OPENFWI), the seismic dataset in EFWI has both vertical and horizontal components. Moreover, the velocity maps in EFWI incorporate both P- and S-wave velocities. While the multicomponent data and the added S-wave velocity make the data more realistic, more challenges are introduced regarding the convergence and computational cost of the inversion. We conduct comprehensive numerical experiments to explore the relationship between P-wave and S-wave velocities in seismic data. The relation between P- and S-wave velocities provides crucial insights into the subsurface properties such as lithology, porosity, fluid content, etc. We anticipate that EFWI will facilitate future research on multiparameter inversions and stimulate endeavors in several critical research topics of carbon-zero and new energy exploration. All datasets, codes1 and relevant information can be accessed through our website at https://efwi-lanl.github.io/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.18494",
        "title": "Knowledge-based in silico models and dataset for the comparative evaluation of mammography AI for a range of breast characteristics, lesion conspicuities and doses",
        "abstract": "To generate evidence regarding the safety and efficacy of artificial intelligence (AI) enabled medical devices, AI models need to be evaluated on a diverse population of patient cases, some of which may not be readily available. We propose an evaluation approach for testing medical imaging AI models that relies on in silico imaging pipelines in which stochastic digital models of human anatomy (in object space) with and without pathology are imaged using a digital replica imaging acquisition system to generate realistic synthetic image datasets. Here, we release M-SYNTH*, a dataset of cohorts with four breast fibroglandular density distributions imaged at different exposure levels using Monte Carlo x-ray simulations with the publicly available Virtual Imaging Clinical Trial for Regulatory Evaluation (VICTRE) toolkit. We utilize the synthetic dataset to analyze AI model performance and find that model performance decreases with increasing breast density and increases with higher mass density, as expected. As exposure levels decrease, AI model performance drops with the highest performance achieved at exposure levels lower than the nominal recommended dose for the breast type.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.12474",
        "title": "CSMED: Bridging the Dataset Gap in Automated Citation Screening for Systematic Literature Reviews",
        "abstract": "Systematic literature reviews (SLRs) play an essential role in summarising, synthe- sising and validating scientific evidence. In recent years, there has been a growing interest in using machine learning techniques to automate the identification of rele- vant studies for SLRs. However, the lack of standardised evaluation datasets makes comparing the performance of such automated literature screening systems difficult. In this paper, we analyse the citation screening evaluation datasets, revealing that many of the available datasets are either too small, suffer from data leakage or have limited applicability to systems treating automated literature screening as a classifi- cation task, as opposed to, for example, a retrieval or question-answering task. To address these challenges, we introduce CSMED, a meta-dataset consolidating nine publicly released collections, providing unified access to 325 SLRs from the fields of medicine and computer science. CSMED serves as a comprehensive resource for training and evaluating the performance of automated citation screening models. Additionally, we introduce CSMED-FT, a new dataset designed explicitly for evaluating the full text publication screening task. To demonstrate the utility of CSMED, we conduct experiments and establish baselines on new datasets.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.00741",
        "title": "FELM: Benchmarking Factuality Evaluation of Large Language Models",
        "abstract": "Assessing factuality of text generated by large language models (LLMs) is an emerging yet crucial research area, aimed at alerting users to potential errors and guiding the development of more reliable LLMs. Nonetheless, the evaluators assessing factuality necessitate suitable evaluation themselves to gauge progress and foster advancements. This direction remains under-explored, resulting in sub- stantial impediments to the progress of factuality evaluators. To mitigate this issue, we introduce a benchmark for Factuality Evaluation of large Language Models, referred to as FELM. In this benchmark, we collect responses generated from LLMs and annotate factuality labels in a fine-grained manner. Contrary to previous studies that primarily concentrate on the factuality of world knowledge (e.g. in- formation from Wikipedia), FELM focuses on factuality across diverse domains, spanning from world knowledge to math and reasoning. Our annotation is based on text segments, which can help pinpoint specific factual errors. The factuality annotations are further supplemented by predefined error types and reference links that either support or contradict the statement. In our experiments, we investigate the performance of several LLM-based factuality evaluators on FELM, including both vanilla LLMs and those augmented with retrieval mechanisms and chain-of- thought processes. Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory to faithfully detect factual errors.1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.13336",
        "title": "FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery",
        "abstract": "We introduce the French Land cover from Aerospace ImageRy (FLAIR), an ex- tensive dataset from the French National Institute of Geographical and Forest Information (IGN) that provides a unique and rich resource for large-scale geospa- tial analysis. FLAIR contains high-resolution aerial imagery with a ground sample distance of 20 cm and over 20 billion individually labeled pixels for precise land- cover classification. The dataset also integrates temporal and spectral data from optical satellite time series. FLAIR thus combines data with varying spatial, spec- tral, and temporal resolutions across over 817 km2 of acquisitions representing the full landscape diversity of France. This diversity makes FLAIR a valuable resource for the development and evaluation of novel methods for large-scale land-cover semantic segmentation and raises significant challenges in terms of computer vision, data fusion, and geospatial analysis. We also provide power- ful uni- and multi-sensor baseline models that can be employed to assess algo- rithm\u2019s performance and for downstream applications. Through its extent and the quality of its annotation, FLAIR aims to spur improvements in monitoring and understanding key anthropogenic development indicators such as urban growth, deforestation, and soil artificialization. Dataset and codes can be accessed at https://ignf.github.io/FLAIR/",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.11547",
        "title": "Event Stream GPT: A Data Pre-processing and Modeling Library for Generative, Pre-trained Transformers over Continuous-time Sequences of Complex Events",
        "abstract": "Generative, pre-trained transformers (GPTs, a.k.a. \"Foundation Models\") have reshaped natural language processing (NLP) through their versatility in diverse downstream tasks. However, their potential extends far beyond NLP. This paper pro- vides a software utility to help realize this potential, extending the applicability of GPTs to continuous-time sequences of complex events with internal dependencies, such as medical record datasets. Despite their potential, the adoption of foundation models in these domains has been hampered by the lack of suitable tools for model construction and evaluation. To bridge this gap, we introduce Event Stream GPT (ESGPT), an open-source library designed to streamline the end-to-end process for building GPTs for continuous-time event sequences. ESGPT allows users to (1) build flexible, foundation-model scale input datasets by specifying only a minimal configuration file, (2) leverage a Hugging Face compatible modeling API for GPTs over this modality that incorporates intra-event causal dependency structures and autoregressive generation capabilities, and (3) evaluate models via standardized processes that can assess few and even zero-shot performance of pre-trained models on user-specified fine-tuning tasks.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.09619",
        "title": "Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning",
        "abstract": "We introduce Dataset Grouper, a library to create large-scale group-structured (e.g., federated) datasets, enabling federated learning simulation at the scale of foundation models. This library facilitates the creation of group-structured ver- sions of existing datasets based on user-specified partitions, and directly leads to a variety of useful heterogeneous datasets that can be plugged into existing software frameworks. Dataset Grouper offers three key advantages. First, it scales to settings where even a single group\u2019s dataset is too large to fit in mem- ory. Second, it provides flexibility, both in choosing the base (non-partitioned) dataset and in defining partitions. Finally, it is framework-agnostic. We empir- ically demonstrate that Dataset Grouper enables large-scale federated language modeling simulations on datasets that are orders of magnitude larger than in pre- vious work, allowing for federated training of language models with hundreds of millions, and even billions, of parameters. Our experimental results show that algorithms like FedAvg operate more as meta-learning methods than as em- pirical risk minimization methods at this scale, suggesting their utility in down- stream personalization and task-specific adaptation. Dataset Grouper is available at https://github.com/google-research/dataset_grouper.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.05663",
        "title": "Objaverse-XL: A Universe of 10M+ 3D Objects",
        "abstract": "Natural language processing and 2D vision models have attained remarkable profi- ciency on many tasks primarily by escalating the scale of training data. However, 3D vision tasks have not seen the same progress, in part due to the challenges of acquiring high-quality 3D data. In this work, we present Objaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises deduplicated 3D objects from a diverse set of sources, including manually designed objects, photogrammetry scans of landmarks and everyday items, and professional scans of historic and antique artifacts. Representing the largest scale and diversity in the realm of 3D datasets, Objaverse-XL enables significant new possibilities for 3D vision. Our experiments demonstrate the improvements enabled with the scale provided by Objaverse-XL. We show that by training Zero123 on novel view synthesis, utilizing over 100 million multi-view rendered images, we achieve strong zero-shot generalization abilities. We hope that releasing Objaverse-XL will enable further innovations in the field of 3D vision at scale.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.14365",
        "title": "Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving",
        "abstract": "Robotic perception requires the modeling of both 3D geometry and semantics. Existing methods typically focus on estimating 3D bounding boxes, neglecting finer geometric details and struggling to handle general, out-of-vocabulary ob- jects. 3D occupancy prediction, which estimates the detailed occupancy states and semantics of a scene, is an emerging task to overcome these limitations. To support 3D occupancy prediction, we develop a label generation pipeline that produces dense, visibility-aware labels for any given scene. This pipeline com- prises three stages: voxel densification, occlusion reasoning, and image-guided voxel refinement. We establish two benchmarks, derived from the Waymo Open Dataset and the nuScenes Dataset, namely Occ3D-Waymo and Occ3D-nuScenes benchmarks. Furthermore, we provide an extensive analysis of the proposed dataset with various baseline models. Lastly, we propose a new model, dubbed Coarse-to-Fine Occupancy (CTF-Occ) network, which demonstrates superior per- formance on the Occ3D benchmarks. The code, data, and benchmarks are released at https://tsinghua-mars-lab.github.io/Occ3D/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.00142",
        "title": "BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting",
        "abstract": "Short-term forecasting of residential and commercial building energy consumption is widely used in power systems and continues to grow in importance. Data-driven short-term load forecasting (STLF), although promising, has suffered from a lack of open, large-scale datasets with high building diversity. This has hindered exploring the pretrain-then-fine-tune paradigm for STLF. To help address this, we present BuildingsBench, which consists of: 1) Buildings-900K, a large-scale dataset of 900K simulated buildings representing the U.S. building stock; and 2) an evaluation platform with over 1,900 real residential and commercial buildings from 7 open datasets. BuildingsBench benchmarks two under-explored tasks: zero-shot STLF, where a pretrained model is evaluated on unseen buildings without fine-tuning, and transfer learning, where a pretrained model is fine-tuned on a target building. The main finding of our benchmark analysis is that synthetically pretrained models generalize surprisingly well to real commercial buildings. An exploration of the effect of increasing dataset size and diversity on zero-shot commercial building performance reveals a power-law with diminishing returns. We also show that fine- tuning pretrained models on real commercial and residential buildings improves performance for a majority of target buildings. We hope that BuildingsBench encourages and facilitates future research on generalizable STLF. All datasets and code can be accessed from https://github.com/NREL/BuildingsBench.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.00936",
        "title": "SatBird: Bird Species Distribution Modeling with Remote Sensing and Citizen Science Data",
        "abstract": "Biodiversity is declining at an unprecedented rate, impacting ecosystem services necessary to ensure food, water, and human health and well-being. Understanding the distribution of species and their habitats is crucial for conservation policy planning. However, traditional methods in ecology for species distribution models (SDMs) generally focus either on narrow sets of species or narrow geographical areas and there remain significant knowledge gaps about the distribution of species. A major reason for this is the limited availability of data traditionally used, due to the prohibitive amount of effort and expertise required for traditional field monitoring. The wide availability of remote sensing data and the growing adoption of citizen science tools to collect species observations data at low cost offer an opportunity for improving biodiversity monitoring and enabling the modelling of complex ecosystems. We introduce a novel task for mapping bird species to their habitats by predicting species encounter rates from satellite images, and present SatBird1, a satellite dataset of locations in the USA with labels derived from presence-absence observation data from the citizen science database eBird, considering summer (breeding) and winter seasons. We also provide a dataset in Kenya representing low-data regimes. We additionally provide environmental data and species range maps for each location. We benchmark a set of baselines on our dataset, including SOTA models for remote sensing tasks. SatBird opens up possibilities for scalably modelling properties of ecosystems worldwide.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.06202",
        "title": "NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics",
        "abstract": "Machine learning provides a valuable tool for analyzing high-dimensional func- tional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional magnetic resonance imaging (MRI) research, interactions between brain regions are com- monly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a trans- formative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain has been challenging due to the expansive number of potential preprocessing pipelines and the large parameter search space for graph-based dataset construction. In this paper, we introduce NeuroGraph1, a collection of graph-based neuroimaging datasets, and demonstrated its utility for predicting multiple categories of behavioral and cogni- tive traits. We delve deeply into the dataset generation search space by crafting 35 datasets that encompass static and dynamic brain connectivity, running in excess of 15 baseline methods for benchmarking. Additionally, we provide generic frame- works for learning on both static and dynamic graphs. Our extensive experiments lead to several key observations. Notably, using correlation vectors as node features, incorporating larger number of regions of interest, and employing sparser graphs lead to improved performance. To foster further advancements in graph-based data driven neuroimaging analysis, we offer a comprehensive open-source Python package that includes the benchmark datasets, baseline implementations, model training, and standard evaluation.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.03035",
        "title": "SEVA: Leveraging sketches to evaluate alignment between human and machine visual abstraction",
        "abstract": "Sketching is a powerful tool for creating abstract images that are sparse but meaningful. Sketch understanding poses fundamental challenges for general- purpose vision algorithms because it requires robustness to the sparsity of sketches relative to natural visual inputs and because it demands tolerance for semantic ambiguity, as sketches can reliably evoke multiple meanings. While current vision algorithms have achieved high performance on a variety of visual tasks, it remains unclear to what extent they understand sketches in a human-like way. Here we introduce SEVA, a new benchmark dataset containing approximately 90K human- generated sketches of 128 object concepts produced under different time constraints, and thus systematically varying in sparsity. We evaluated a suite of state-of-the-art vision algorithms on their ability to correctly identify the target concept depicted in these sketches and to generate responses that are strongly aligned with human response patterns on the same sketch recognition task. We found that vision algorithms that better predicted human sketch recognition performance also better approximated human uncertainty about sketch meaning, but there remains a sizable gap between model and human response patterns. To explore the potential of models that emulate human visual abstraction in generative tasks, we conducted further evaluations of a recently developed sketch generation algorithm [91] capable of generating sketches that vary in sparsity. We hope that public release of this dataset and evaluation protocol will catalyze progress towards algorithms with enhanced capacities for human-like visual abstraction.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.13786",
        "title": "Perception Test: A Diagnostic Benchmark for Multimodal Video Models",
        "abstract": "We propose a novel multimodal video benchmark \u2013 the Perception Test \u2013 to eval- uate the perception and reasoning skills of pre-trained multimodal models (e.g. Flamingo, SeViLA, or GPT-4). Compared to existing benchmarks that focus on computational tasks (e.g. classification, detection or tracking), the Perception Test focuses on skills (Memory, Abstraction, Physics, Semantics) and types of reasoning (descriptive, explanatory, predictive, counterfactual) across video, audio, and text modalities, to provide a comprehensive and efficient evaluation tool. The benchmark probes pre-trained models for their transfer capabilities, in a zero-shot / few-shot or limited finetuning regime. For these purposes, the Perception Test intro- duces 11.6k real-world videos, 23s average length, designed to show perceptually interesting situations, filmed by around 100 participants worldwide. The videos are densely annotated with six types of labels (multiple-choice and grounded video question-answers, object and point tracks, temporal action and sound segments), enabling both language and non-language evaluations. The fine-tuning and valida- tion splits of the benchmark are publicly available (CC-BY license), in addition to a challenge server with a held-out test split. Human baseline results compared to state-of-the-art video QA models show a substantial gap in performance (91.4% vs 46.2%), suggesting that there is significant room for improvement in multimodal video understanding. Dataset, baseline code, and challenge server are available at https://github.com/deepmind/perception_test",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.11247",
        "title": "DICES Dataset:\nDiversity in Conversational AI Evaluation for Safety",
        "abstract": "Machine learning approaches often require training and evaluation datasets with a clear separation between positive and negative examples. This risks simplifying and even obscuring the inherent subjectivity present in many tasks. Preserving such variance in content and diversity in datasets is often expensive and laborious. This is especially troubling when building safety datasets for conversational AI systems, as safety is both socially and culturally situated. To demonstrate this crucial aspect of conversational AI safety, and to facilitate in-depth model performance analyses, we introduce the DICES (Diversity In Conversational AI Evaluation for Safety) dataset that contains fine-grained demographic information about raters, high replication of ratings per item to ensure statistical power for analyses, and encodes rater votes as distributions across different demographics to allow for in- depth explorations of different aggregation strategies. In short, the DICES dataset enables the observation and measurement of variance, ambiguity, and diversity in the context of conversational AI safety. We also illustrate how the dataset offers a basis for establishing metrics to show how raters\u2019 ratings can intersects with demographic categories such as racial/ethnic groups, age groups, and genders. The goal of DICES is to be used as a shared resource and benchmark that respects diverse perspectives during safety evaluation of conversational AI systems.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.18365",
        "title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
        "abstract": "Large Language Models (LLMs) with strong abilities in natural language process- ing tasks have emerged and have been applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper, rather than pursuing state-of-the-art performance, we aim to evaluate capabilities of LLMs in a wide range of tasks across the chemistry domain. We identify three key chemistry- related capabilities including understanding, reasoning and explaining to explore in LLMs and establish a benchmark containing eight chemistry tasks. Our anal- ysis draws on widely recognized datasets facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts. Our investigation found that GPT-4 outperformed other models and LLMs exhibit different competitive levels in eight chemistry tasks. In addition to the key findings from the comprehen- sive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs\u2019 performance across various chemistry tasks. The code and datasets used in this study are available at https://github.com/ChemFoundationModels/ChemLLMBench.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.14356",
        "title": "COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs",
        "abstract": "Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counter- factual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffu- sion models. We use our framework to create COCO-Counterfactuals, a mul- timodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of multi- modal vision-language models via training data augmentation. We make our code2 and the COCO-Counterfactuals dataset3 publicly available.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.09064",
        "title": "Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models",
        "abstract": "Systematic compositionality, or the ability to adapt to novel situations by creating a mental model of the world using reusable pieces of knowledge, remains a significant challenge in machine learning. While there has been considerable progress in the language domain, efforts towards systematic visual imagination, or envisioning the dynamical implications of a visual observation, are in their infancy. We introduce the Systematic Visual Imagination Benchmark (SVIB), the first benchmark designed to address this problem head-on. SVIB offers a novel framework for a minimal world modeling problem, where models are evaluated based on their ability to generate one-step image-to-image transformations under a latent world dynamics. The framework provides benefits such as the possibility to jointly optimize for systematic perception and imagination, a range of difficulty levels, and the ability to control the fraction of possible factor combinations used during training. We provide a comprehensive evaluation of various baseline models on SVIB, offering insight into the current state-of-the-art in systematic visual imagination. We hope that this benchmark will help advance visual systematic compositionality. https: //systematic-visual-imagination.github.io",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.13216",
        "title": "Diverse Community Data for Benchmarking Data Privacy Algorithms",
        "abstract": "The Collaborative Research Cycle (CRC) is a National Institute of Standards and Technology (NIST) benchmarking program intended to strengthen understanding of tabular data deidentification technologies. Deidentification algorithms are vul- nerable to the same bias and privacy issues that impact other data analytics and machine learning applications, and it can even amplify those issues by contami- nating downstream applications. This paper summarizes four CRC contributions: theoretical work on the relationship between diverse populations and challenges for equitable deidentification; public benchmark data focused on diverse popula- tions and challenging features; a comprehensive open source suite of evaluation metrology for deidentified datasets; and an archive of more than 450 deidentified data samples from a broad range of techniques. The initial set of evaluation results demonstrate the value of the CRC tools for investigations in this field.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.10088",
        "title": "Android in the Wild: A Large-Scale Dataset for Android Device Control",
        "abstract": "There is a growing interest in device-control systems that can interpret human natural language instructions and execute them on a digital device by directly con- trolling its user interface. We present a dataset for device-control research, Android in the Wild (AITW), which is orders of magnitude larger than current datasets. The dataset contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. It consists of 715k episodes spanning 30k unique instructions, four versions of Android (v10\u201313), and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It contains multi-step tasks that require semantic understanding of language and visual context. This dataset poses a new challenge: actions available through the user interface must be inferred from their visual appearance, and, instead of simple UI element-based actions, the action space consists of precise gestures (e.g., hori- zontal scrolls to operate carousel widgets). We organize our dataset to encourage robustness analysis of device-control systems, i.e., how well a system performs in the presence of new task descriptions, new applications, or new platform versions. We develop two agents and report performance across the dataset. The dataset is available at https://github.com/google-research/google-research/ tree/master/android_in_the_wild.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2404.01578",
        "title": "GLEMOS: Benchmark for Instantaneous Graph Learning Model Selection",
        "abstract": "The choice of a graph learning (GL) model (i.e., a GL algorithm and its hyperpa- rameter settings) has a significant impact on the performance of downstream tasks. However, selecting the right GL model becomes increasingly difficult and time consuming as more and more GL models are developed. Accordingly, it is of great significance and practical value to equip users of GL with the ability to perform a near-instantaneous selection of an effective GL model without manual interven- tion. Despite the recent attempts to tackle this important problem, there has been no comprehensive benchmark environment to evaluate the performance of GL model selection methods. To bridge this gap, we present GLEMOS in this work, a comprehensive benchmark for instantaneous GL model selection that makes the following contributions. (i) GLEMOS provides extensive benchmark data for fundamental GL tasks, i.e., link prediction and node classification, including the performances of 366 models on 457 graphs on these tasks. (ii) GLEMOS designs multiple evaluation settings, and assesses how effectively representative model selection techniques perform in these different settings. (iii) GLEMOS is designed to be easily extended with new models, new graphs, and new performance records. (iv) Based on the experimental results, we discuss the limitations of existing ap- proaches and highlight future research directions. To promote research on this significant problem, we make the benchmark data and code publicly available at https://github.com/facebookresearch/glemos",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.06577",
        "title": "RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation",
        "abstract": "Retinal vessel segmentation is generally grounded in image-based datasets collected with bench-top devices. The static images naturally lose the dynamic characteristics of retina fluctuation, resulting in diminished dataset richness, and the usage of bench-top devices further restricts dataset scalability due to its limited accessibility. Considering these limitations, we introduce the first video-based retinal dataset by employing handheld devices for data acquisition. The dataset comprises 635 smartphone-based fundus videos collected from four different clinics, involving 415 patients from 50 to 75 years old. It delivers comprehensive and precise annotations of retinal structures in both spatial and temporal dimensions, aiming to advance the landscape of vasculature segmentation. Specifically, the dataset provides three levels of spatial annotations: binary vessel masks for overall retinal structure delineation, general vein-artery masks for distinguishing the vein and artery, and fine-grained vein-artery masks for further characterizing the granularities of each artery and vein. In addition, the dataset offers temporal annotations that capture the vessel pulsation characteristics, assisting in detecting ocular diseases that require fine-grained recognition of hemodynamic fluctuation. In application, our dataset exhibits a significant domain shift with respect to data captured by bench-top devices, thus posing great challenges to existing methods. Thanks to rich annotations and data scales, our dataset potentially paves the path for more advanced retinal analysis and accurate disease diagnosis. In the experiments, we provide evaluation metrics and benchmark results on our dataset, reflecting both the potential and challenges it offers for vessel segmentation tasks. We hope this challenging dataset would significantly contribute to the development of eye disease diagnosis and early prevention. The dataset is available at \u0087 RVD.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.11203",
        "title": "AVOIDDS: Aircraft Vision-based Intruder Detection Dataset and Simulator",
        "abstract": "Designing robust machine learning systems remains an open problem, and there is a need for benchmark problems that cover both environmental changes and evaluation on a downstream task. In this work, we introduce AVOIDDS, a re- alistic object detection benchmark for the vision-based aircraft detect-and-avoid problem. We provide a labeled dataset consisting of 72,000 photorealistic images of intruder aircraft with various lighting conditions, weather conditions, relative geometries, and geographic locations. We also provide an interface that evaluates trained models on slices of this dataset to identify changes in performance with respect to changing environmental conditions. Finally, we implement a fully- integrated, closed-loop simulator of the vision-based detect-and-avoid problem to evaluate trained models with respect to the downstream collision avoidance task. This benchmark will enable further research in the design of robust ma- chine learning systems for use in safety-critical applications. The AVOIDDS dataset and code are publicly available at https://purl.stanford.edu/hj293cv5980 and https://github.com/sisl/VisionBasedAircraftDAA, respectively.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.17290",
        "title": "RIO: A Benchmark for Reasoning Intention-Oriented Objects in Open Environments",
        "abstract": "Intention-oriented object detection aims to detect desired objects based on specific intentions or requirements. For instance, when we desire to \"lie down and rest\", we instinctively seek out a suitable option such as a \"bed\" or a \"sofa\" that can fulfill our needs. Previous work in this area is limited either by the number of in- tention descriptions or by the affordance vocabulary available for intention objects. These limitations make it challenging to handle intentions in open environments effectively. To facilitate this research, we construct a comprehensive dataset called Reasoning Intention-Oriented Objects (RIO). In particular, RIO is specifically designed to incorporate diverse real-world scenarios and a wide range of object categories. It offers the following key features: 1) intention descriptions in RIO are represented as natural sentences rather than a mere word or verb phrase, making them more practical and meaningful; 2) the intention descriptions are contextually relevant to the scene, enabling a broader range of potential functionalities associ- ated with the objects; 3) the dataset comprises a total of 40,214 images and 130,585 intention-object pairs. With the proposed RIO, we evaluate the ability of some existing models to reason intention-oriented objects in open environments.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.12424",
        "title": "VISOGENDER: A dataset for benchmarking gender bias in image-text pronoun resolution",
        "abstract": "We introduce VISOGENDER, a novel dataset for benchmarking gender bias in vision-language models. We focus on occupation-related biases within a hege- monic system of binary gender, inspired by Winograd and Winogender schemas, where each image is associated with a caption containing a pronoun relation- ship of subjects and objects in the scene. VISOGENDER is balanced by gender representation in professional roles, supporting bias evaluation in two ways: i) resolution bias, where we evaluate the difference between pronoun resolution accu- racies for image subjects with gender presentations perceived as masculine versus feminine by human annotators and ii) retrieval bias, where we compare ratios of professionals perceived to have masculine and feminine gender presentations retrieved for a gender-neutral search query. We benchmark several state-of-the-art vision-language models and find that they demonstrate bias in resolving binary gender in complex scenes. While the direction and magnitude of gender bias de- pends on the task and the model being evaluated, captioning models are generally less biased than Vision-Language Encoders. Dataset and code are available at https://github.com/oxai/visogender.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.09191",
        "title": "A benchmark of categorical encoders for binary classification",
        "abstract": "Categorical encoders transform categorical features into numerical representa- tions that are indispensable for a wide range of machine learning models. Ex- isting encoder benchmark studies lack generalizability because of their limited choice of 1. encoders, 2. experimental factors, and 3. datasets. Addition- ally, inconsistencies arise from the adoption of varying aggregation strategies. This paper is the most comprehensive benchmark of categorical encoders to date, including an extensive evaluation of 32 configurations of encoders from diverse families, with 48 combinations of experimental factors, and on 50 datasets. The study shows the profound influence of dataset selection, exper- imental factors, and aggregation strategies on the benchmark\u2019s conclusions \u2014 aspects disregarded in previous encoder benchmarks. Our code is available at https://github.com/DrCohomology/EncoderBenchmarking. This version of the paper is identical to the one accepted at the 37th Conference on Neural Infor- mation Processing Systems (NeurIPS 2023), Track on Datasets and Benchmarks.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.19053",
        "title": "Datasets and Benchmarks for Nanophotonic Structure and Parametric Design Simulations",
        "abstract": "Nanophotonic structures have versatile applications including solar cells, anti- reflective coatings, electromagnetic interference shielding, optical filters, and light emitting diodes. To design and understand these nanophotonic structures, elec- trodynamic simulations are essential. These simulations enable us to model elec- tromagnetic fields over time and calculate optical properties. In this work, we introduce frameworks and benchmarks to evaluate nanophotonic structures in the context of parametric structure design problems. The benchmarks are instrumental in assessing the performance of optimization algorithms and identifying an optimal structure based on target optical properties. Moreover, we explore the impact of varying grid sizes in electrodynamic simulations, shedding light on how evaluation fidelity can be strategically leveraged in enhancing structure designs.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.04370",
        "title": "OpenAGI: When LLM Meets Domain Experts",
        "abstract": "Human Intelligence (HI) excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive AI Agents, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools, plugins, or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research and development platform designed for solving multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools, plugins, or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM\u2019s task-solving ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project\u2019s code, dataset, benchmarks, evaluation methods, and the UI demo to foster community involvement in AGI advancement: https://github.com/agiresearch/OpenAGI.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.13425",
        "title": "MiliPoint: A Point Cloud Dataset for mmWave Radar",
        "abstract": "Millimetre-wave (mmWave) radar has emerged as an attractive and cost-effective alternative for human activity sensing compared to traditional camera-based sys- tems. mmWave radars are also non-intrusive, providing better protection for user privacy. However, as a Radio Frequency (RF) based technology, mmWave radars rely on capturing reflected signals from objects, making them more prone to noise compared to cameras. This raises an intriguing question for the deep learning community: Can we develop more effective point set-based deep learning methods for such attractive sensors?\nTo answer this question, our work, termed MiliPoint2, delves into this idea by providing a large-scale, open dataset for the community to explore how mmWave radars can be utilised for human activity recognition. Moreover, MiliPoint stands out as it is larger in size than existing datasets, has more diverse human actions represented, and encompasses all three key tasks in human activity recognition. We have also established a range of point-based deep neural networks such as DGCNN, PointNet++ and PointTransformer, on MiliPoint, which can serve to set the ground baseline for further development.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.16342",
        "title": "LagrangeBench: A Lagrangian Fluid Mechanics Benchmarking Suite",
        "abstract": "Machine learning has been successfully applied to grid-based PDE modeling in various scientific applications. However, learned PDE solvers based on Lagrangian particle discretizations, which are the preferred approach to problems with free sur- faces or complex physics, remain largely unexplored. We present LagrangeBench, the first benchmarking suite for Lagrangian particle problems, focusing on temporal coarse-graining. In particular, our contribution is: (a) seven new fluid mechanics datasets (four in 2D and three in 3D) generated with the Smoothed Particle Hy- drodynamics (SPH) method including the Taylor-Green vortex, lid-driven cavity, reverse Poiseuille flow, and dam break, each of which includes different physics like solid wall interactions or free surface, (b) efficient JAX-based API with various recent training strategies and three neighbor search routines, and (c) JAX imple- mentation of established Graph Neural Networks (GNNs) like GNS and SEGNN with baseline results. Finally, to measure the performance of learned surrogates we go beyond established position errors and introduce physical metrics like kinetic energy MSE and Sinkhorn distance for the particle distribution. Our codebase is available under the URL: https://github.com/tumaer/lagrangebench.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.12032",
        "title": "The Waymo Open Sim Agents Challenge",
        "abstract": "Simulation with realistic, interactive agents represents a key task for autonomous vehicle software development. In this work, we introduce the Waymo Open Sim Agents Challenge (WOSAC). WOSAC is the first public challenge to tackle this task and propose corresponding metrics. The goal of the challenge is to stimulate the design of realistic simulators that can be used to evaluate and train a behavior model for autonomous driving. We outline our evaluation methodology, present results for a number of different baseline simulation agent methods, and analyze several submissions to the 2023 competition which ran from March 16, 2023 to May 23, 2023. The WOSAC evaluation server remains open for submissions and we discuss open problems for the task.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.15681",
        "title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram",
        "abstract": "Question answering (QA) in the field of healthcare has received much attention due to significant advancements in natural language processing. However, existing healthcare QA datasets primarily focus on medical images, clinical notes, or struc- tured electronic health record tables. This leaves the vast potential of combining electrocardiogram (ECG) data with these systems largely untapped. To address this gap, we present ECG-QA, the first QA dataset specifically designed for ECG analysis. The dataset comprises a total of 70 question templates that cover a wide range of clinically relevant ECG topics, each validated by an ECG expert to ensure their clinical utility. As a result, our dataset includes diverse ECG interpretation questions, including those that require a comparative analysis of two different ECGs. In addition, we have conducted numerous experiments to provide valuable insights for future research directions. We believe that ECG-QA will serve as a valuable resource for the development of intelligent QA systems capable of assisting clinicians in ECG interpretations.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.11167",
        "title": "Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and\nEinstellung Effect with the OnlyConnect Wall Dataset",
        "abstract": "The quest for human imitative AI has been an enduring topic in AI research since its inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to the cultural zeitgeist. While recent NLP evaluation benchmark tasks test some as- pects of human-imitative behavior (e.g., BIG-bench\u2019s \u2018human-like behavior\u2019 tasks), few, if not none, examine creative problem solving abilities. Creative problem solv- ing in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use the ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli \u2014 distractors dubbed red herrings \u2014 impede human performance in such tasks via the fixation effect and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographi- cally similar incorrect words to subsequent word-fragments or clues. The popular British quiz show Only Connect\u2019s Connecting Wall segment essentially mimics Mednick\u2019s Remote Associates Test (RAT) formulation with built-in, deliberate red herrings, which makes it an ideal proxy task to explore and study the fixation effect and Einstellung paradigm from cognitive neuroscience in LLMs. In this paper, we present the novel Only Connect Wall (OCW) dataset and report results from our evaluation of selected pre-trained language models and LLMs on creative problem solving tasks like grouping clue words by heterogeneous connections and identifying correct open knowledge domain connections in respective groups. We synthetically generate two additional datasets: OCW-Randomized, OCW-WordNet to further analyze our red-herrings hypothesis in language models. The code and link to the dataset are available at https://github.com/TaatiTeam/OCW.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.04657",
        "title": "BEAVERTAILS: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset",
        "abstract": "In this paper, we introduce the BEAVERTAILS dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites. google.com/view/pku-beavertails.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.04678",
        "title": "DORIS-MAE:\nScientific Document Retrieval using Multi-level Aspect-based Queries",
        "abstract": "In scientific research, the ability to effectively retrieve relevant documents based on complex, multifaceted queries is critical. Existing evaluation datasets for this task are limited, primarily due to the high cost and effort required to annotate resources that effectively represent complex queries. To address this, we propose a novel task, Scientific DOcument Retrieval using Multi-level Aspect-based quEries (DORIS- MAE), which is designed to handle the complex nature of user queries in scientific research. We developed a benchmark dataset within the field of computer science, consisting of 100 human-authored complex query cases. For each complex query, we assembled a collection of 100 relevant documents and produced annotated relevance scores for ranking them. Recognizing the significant labor of expert annotation, we also introduce Anno-GPT, a scalable framework for validating the performance of Large Language Models (LLMs) on expert-level dataset annotation tasks. LLM annotation of the DORIS-MAE dataset resulted in a 500x reduction in cost, without compromising quality. Furthermore, due to the multi-tiered structure of these complex queries, the DORIS-MAE dataset can be extended to over 4,000 sub-query test cases without requiring additional annotation. We evaluated 17 recent retrieval methods on DORIS-MAE, observing notable performance drops compared to traditional datasets. This highlights the need for better approaches to handle complex, multifaceted queries in scientific research. Our dataset and codebase are available at https://github.com/Real-Doris-Mae/Doris-Mae-Dataset.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09077",
        "title": "Estimating Generic 3D Room Structures from 2D Annotations",
        "abstract": "Indoor rooms are among the most common use cases in 3D scene understanding. Current state-of-the-art methods for this task are driven by large annotated datasets. Room layouts are especially important, consisting of structural elements in 3D, such as wall, floor, and ceiling. However, they are difficult to annotate, especially on pure RGB video. We propose a novel method to produce generic 3D room layouts just from 2D segmentation masks, which are easy to annotate for humans. Based on these 2D annotations, we automatically reconstruct 3D plane equations for the structural elements and their spatial extent in the scene, and connect adjacent elements at the appropriate contact edges. We annotate and publicly release 2246 3D room layouts on the RealEstate10k dataset, containing YouTube videos. We demonstrate the high quality of these 3D layouts annotations with extensive experiments.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.03449",
        "title": "Into the LAION\u2019s Den: Investigating Hate in Multimodal Datasets",
        "abstract": "\u2018Scale the model, scale the data, scale the compute\u2019 is the reigning sentiment in the world of generative AI today. While the impact of model scaling has been extensively studied, we are only beginning to scratch the surface of data scaling and its consequences. This is especially of critical importance in the context of vision- language datasets such as LAION. These datasets are continually growing in size and are built based on large-scale internet dumps such as the Common Crawl, which is known to have numerous drawbacks ranging from quality, legality, and content. The datasets then serve as the backbone for large generative models, contributing to the operationalization and perpetuation of harmful societal and historical biases and stereotypes. In this paper, we investigate the effect of scaling datasets on hateful content through a comparative audit of two datasets: LAION-400M and LAION-2B. Our results show that hate content increased by nearly 12% with dataset scale, measured both qualitatively and quantitatively using a metric that we term as Hate Content Rate (HCR). We also found that filtering dataset contents based on Not Safe For Work (NSFW) values calculated based on images alone does not exclude all the harmful content in alt-text. Instead, we found that trace amounts of hateful, targeted, and aggressive text remain even when carrying out conservative filtering. We end with a reflection and a discussion of the significance of our results for dataset curation and usage in the AI community. Code and the meta-data assets curated in this paper are publicly available at https://github.com/vinayprabhu/hate_scaling.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.15162",
        "title": "YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus",
        "abstract": "Machine learning for sign languages is bottlenecked by data. In this paper, we present YouTube-ASL, a large-scale, open-domain corpus of American Sign Lan- guage (ASL) videos and accompanying English captions drawn from YouTube. With ~1000 hours of videos and >2500 unique signers, YouTube-ASL is ~3x as large and has ~10x as many unique signers as the largest prior ASL dataset. We train baseline models for ASL to English translation on YouTube-ASL and evaluate them on How2Sign, where we achieve a new finetuned state of the art of 12.39 BLEU and, for the first time, report zero-shot results.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.02034",
        "title": "SAMRS: Scaling-up Remote Sensing Segmentation Dataset with Segment Anything Model",
        "abstract": "The success of the Segment Anything Model (SAM) demonstrates the significance of data-centric machine learning. However, due to the difficulties and high costs associated with annotating Remote Sensing (RS) images, a large amount of valuable RS data remains unlabeled, particularly at the pixel level. In this study, we leverage SAM and existing RS object detection datasets to develop an efficient pipeline for generating a large-scale RS segmentation dataset, dubbed SAMRS. SAMRS totally possesses 105,090 images and 1,668,241 instances, surpassing existing high-resolution RS segmentation datasets in size by several orders of magnitude. It provides object category, location, and instance information that can be used for semantic segmentation, instance segmentation, and object detection, either individually or in combination. We also provide a comprehensive analysis of SAMRS from various aspects. Moreover, preliminary experiments highlight the importance of conducting segmentation pre-training with SAMRS to address task discrepancies and alleviate the limitations posed by limited training data during fine-tuning. The code and dataset will be available at SAMRS.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.10345",
        "title": "MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset for Versatile Wireless Sensing",
        "abstract": "4D human perception plays an essential role in a myriad of applications, such as home automation and metaverse avatar simulation. However, existing solutions which mainly rely on cameras and wearable devices are either privacy intrusive or inconvenient to use. To address these issues, wireless sensing has emerged as a promising alternative, leveraging LiDAR, mmWave radar, and WiFi signals for device-free human sensing. In this paper, we propose MM-Fi, the first multi-modal non-intrusive 4D human dataset with 27 daily or rehabilitation action categories, to bridge the gap between wireless sensing and high-level human perception tasks. MM-Fi consists of over 320k synchronized frames of five modalities from 40 human subjects. Various annotations are provided to support potential sensing tasks, e.g., human pose estimation and action recognition. Extensive experiments have been conducted to compare the sensing capacity of each or several modalities in terms of multiple tasks. We envision that MM-Fi can contribute to wireless sensing research with respect to action recognition, human pose estimation, multi-modal learning, cross-modal supervision, and interdisciplinary healthcare research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.00818",
        "title": "Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset",
        "abstract": "In this paper, we present Motion-X, a large-scale 3D expressive whole-body motion dataset. Existing motion datasets predominantly contain body-only poses, lacking facial expressions, hand gestures, and fine-grained pose descriptions. Moreover, they are primarily collected from limited laboratory scenes with textual descrip- tions manually labeled, which greatly limits their scalability. To overcome these limitations, we develop a whole-body motion and text annotation pipeline, which can automatically annotate motion from either single- or multi-view videos and provide comprehensive semantic labels for each video and fine-grained whole-body pose descriptions for each frame. This pipeline is of high precision, cost-effective, and scalable for further research. Based on it, we construct Motion-X, which com- prises 15.6M precise 3D whole-body pose annotations (i.e., SMPL-X) covering 81.1K motion sequences from massive scenes. Besides, Motion-X provides 15.6M frame-level whole-body pose descriptions and 81.1K sequence-level semantic labels. Comprehensive experiments demonstrate the accuracy of the annotation pipeline and the significant benefit of Motion-X in enhancing expressive, diverse, and natural motion generation, as well as 3D whole-body human mesh recovery",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.07279",
        "title": "Scalable 3D Captioning with Pretrained Models",
        "abstract": "We introduce Cap3D, an automatic approach for generating descriptive text for 3D objects. This approach utilizes pretrained models from image captioning, image-text alignment, and LLM to consolidate captions from multiple views of a 3D asset, completely side-stepping the time-consuming and costly process of manual annotation. We apply Cap3D to the recently introduced large-scale 3D dataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted using 41k human annotations from the same dataset, demonstrates that Cap3D surpasses human-authored descriptions in terms of quality, cost, and speed. Through effective prompt engineering, Cap3D rivals human performance in generating geometric de- scriptions on 17k collected annotations from the ABO dataset. Finally, we finetune text-to-3D models on Cap3D and human captions, and show Cap3D outperforms; and benchmark the SOTA including Point\u00b7E, Shap\u00b7E, and DreamFusion.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.10453",
        "title": "Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking",
        "abstract": "Link prediction attempts to predict whether an unseen edge exists based on only a portion of edges of a graph. A flurry of methods have been introduced in re- cent years that attempt to make use of graph neural networks (GNNs) for this task. Furthermore, new and diverse datasets have also been created to better eval- uate the effectiveness of these new models. However, multiple pitfalls currently exist that hinder our ability to properly evaluate these new methods. These pit- falls mainly include: (1) Lower than actual performance on multiple baselines, (2) A lack of a unified data split and evaluation metric on some datasets, and (3) An unrealistic evaluation setting that uses easy negative samples. To overcome these challenges, we first conduct a fair comparison across prominent methods and datasets, utilizing the same dataset and hyperparameter search settings. We then create a more practical evaluation setting based on a Heuristic Related Sampling Technique (HeaRT), which samples hard negative samples via multiple heuristics. The new evaluation setting helps promote new challenges and opportunities in link prediction by aligning the evaluation with real-world situations. Our implementa- tion and data are available at https://github.com/Juanhui28/HeaRT.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.13531",
        "title": "WBCAtt: A White Blood Cell Dataset Annotated with Detailed Morphological Attributes",
        "abstract": "The examination of blood samples at a microscopic level plays a fundamental role in clinical diagnostics. For instance, an in-depth study of White Blood Cells (WBCs), a crucial component of our blood, is essential for diagnosing blood-related diseases such as leukemia and anemia. While multiple datasets containing WBC images have been proposed, they mostly focus on cell categorization, often lacking the necessary morphological details to explain such categorizations, despite the importance of explainable artificial intelligence (XAI) in medical domains. This paper seeks to address this limitation by introducing comprehensive annotations for WBC images. Through collaboration with pathologists, a thorough literature review, and manual inspection of microscopic images, we have identified 11 morphological attributes associated with the cell and its components (nucleus, cytoplasm, and granules). We then annotated ten thousand WBC images with these attributes, resulting in 113k labels (11 attributes x 10.3k images). Annotating at this level of detail and scale is unprecedented, offering unique value to AI in pathology. Moreover, we conduct experiments to predict these attributes from cell images, and also demonstrate specific applications that can benefit from our detailed annotations. Overall, our dataset paves the way for interpreting WBC recognition models, further advancing XAI in the fields of pathology and hematology.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.02997",
        "title": "When Do Neural Nets Outperform Boosted Trees on Tabular Data?",
        "abstract": "Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and question the importance of this debate. To this end, we conduct the largest tabular data analysis to date, comparing 19 algorithms across 176 datasets, and we find that the \u2018NN vs. GBDT\u2019 debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than choosing between NNs and GBDTs. A remarkable exception is the recently-proposed prior-data fitted network, TabPFN: although it is effectively limited to training sets of size 3000, we find that it outperforms all other algorithms on average, even when randomly sampling 3000 training datapoints. Next, we analyze dozens of metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better than NNs at handling skewed or heavy-tailed feature distributions and other forms of dataset irregularities. Our insights act as a guide for practitioners to determine which techniques may work best on their dataset. Finally, with the goal of accelerating tabular data research, we release the TabZilla Benchmark Suite: a collection of the 36 \u2018hardest\u2019 of the datasets we study. Our benchmark suite, codebase, and all raw results are available at https://github.com/naszilla/tabzilla.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09109",
        "title": "NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations",
        "abstract": "Recent advances in neural reconstruction enable high-quality 3D object recon- struction from casually captured image collections. Current techniques mostly analyze their progress on relatively simple image collections where Structure- from-Motion (SfM) techniques can provide ground-truth (GT) camera poses. We note that SfM techniques tend to fail on in-the-wild image collections such as image search results with varying backgrounds and illuminations. To enable sys- tematic research progress on 3D reconstruction from casual image captures, we propose \u2018NAVI\u2019: a new dataset of category-agnostic image collections of objects with high-quality 3D scans along with per-image 2D-3D alignments providing near-perfect GT camera parameters. These 2D-3D alignments allow us to extract accurate derivative annotations such as dense pixel correspondences, depth and segmentation maps. We demonstrate the use of NAVI image collections on different problem settings and show that NAVI enables more thorough evaluations that were not possible with existing datasets. We believe NAVI is beneficial for systematic research progress on 3D reconstruction and correspondence estimation. Project page: https://navidataset.github.io",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.13952",
        "title": "VidChapters-7M: Video Chapters at Scale",
        "abstract": "Segmenting long videos into chapters enables users to quickly navigate to the infor- mation of their interest. This important topic has been understudied due to the lack of publicly released datasets. To address this issue, we present VidChapters-7M, a dataset of 817K user-chaptered videos including 7M chapters in total. VidChapters- 7M is automatically created from videos online in a scalable manner by scraping user-annotated chapters and hence without any additional manual annotation. We introduce the following three tasks based on this data. First, the video chapter gen- eration task consists of temporally segmenting the video and generating a chapter title for each segment. To further dissect the problem, we also define two variants of this task: video chapter generation given ground-truth boundaries, which requires generating a chapter title given an annotated video segment, and video chapter grounding, which requires temporally localizing a chapter given its annotated title. We benchmark both simple baselines and state-of-the-art video-language models for these three tasks. We also show that pretraining on VidChapters-7M transfers well to dense video captioning tasks in both zero-shot and finetuning settings, largely improving the state of the art on the YouCook2 and ViTT benchmarks. Finally, our experiments reveal that downstream performance scales well with the size of the pretraining dataset. Our dataset, code, and models are publicly available at https://antoyang.github.io/vidchapters.html.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.05538",
        "title": "ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of Zoom and Spatial Biases in Image Classification",
        "abstract": "Image classifiers are information-discarding machines, by design. Yet, how these models discard information remains mysterious. We hypothesize that one way for image classifiers to reach high accuracy is to zoom to the most discriminative region in the image and then extract features from there to predict image labels, discarding the rest of the image. Studying six popular networks ranging from AlexNet to CLIP, we find that proper framing of the input image can lead to the correct classification of 98.91% of ImageNet images. Furthermore, we uncover positional biases in various datasets, especially a strong center bias in two popular datasets: ImageNet-A and ObjectNet. Finally, leveraging our insights into the potential of zooming, we propose a test-time augmentation (TTA) technique that improves classification accuracy by forcing models to explicitly perform zoom-in operations before making predictions. Our method is more interpretable, accurate, and faster than MEMO, a state-of-the-art (SOTA) TTA method. We introduce ImageNet-Hard, a new benchmark that challenges SOTA classifiers including large vision-language models even when optimal zooming is allowed.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.03736",
        "title": "Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent Learning",
        "abstract": "Neural MMO 2.0 is a massively multi-agent environment for reinforcement learning research. The key feature of this new version is a flexible task system that allows users to define a broad range of objectives and reward signals. We challenge researchers to train agents capable of generalizing to tasks, maps, and opponents never seen during training. Neural MMO features procedurally generated maps with 128 agents in the standard setting and support for up to. Version 2.0 is a complete rewrite of its predecessor with three-fold improved performance and compatibility with CleanRL. We release the platform as free and open-source software with comprehensive documentation available at neuralmmo.github.io and an active community Discord. To spark initial research on this new platform, we are concurrently running a competition at NeurIPS 2023.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.02080",
        "title": "benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models",
        "abstract": "Various adaptation methods, such as LoRA, prompts, and adapters, have been proposed to enhance the performance of pre-trained vision-language models in specific domains. As test samples in real-world applications usually differ from adaptation data, studying the robustness of these adaptation methods against distri- bution shifts is essential. In this study, we assess the robustness of 11 widely-used adaptation methods across 4 vision-language datasets under multimodal corrup- tions. Concretely, we introduce 7 benchmark datasets, including 96 visual and 87 textual corruptions, to investigate the robustness of different adaptation methods, the impact of available adaptation examples, and the influence of trainable parame- ter size during adaptation. Our analysis reveals that: 1) Adaptation methods are more sensitive to text corruptions than visual corruptions. 2) Full fine-tuning does not consistently provide the highest robustness; instead, adapters can achieve better robustness with comparable clean performance. 3) Contrary to expectations, our findings indicate that increasing the number of adaptation data and parameters does not guarantee enhanced robustness; instead, it results in even lower robustness. We hope this study could benefit future research in developing robust multimodal adaptation methods. The benchmark, code, and dataset used in this study can be accessed at https://adarobustness.github.io.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1409.0575",
        "title": "ImageNet Large Scale Visual Recognition Challenge",
        "abstract": "The ImageNet Large Scale Visual Recogni- tion Challenge is a benchmark in object category classi- fication and detection on hundreds of object categories and millions of images. The challenge has been run an- nually from 2010 to present, attracting participation from more than fifty institutions.\nThis paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recog- nition, provide a detailed analysis of the current state of the field of large-scale image classification and ob- ject detection, and compare the state-of-the-art com- puter vision accuracy with human accuracy. We con- clude with lessons learned in the five years of the chal- lenge, and propose future directions and improvements.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2004.02042",
        "title": "ObjectNet Dataset: Reanalysis and Correction",
        "abstract": "Recently, Barbu et al. introduced a dataset called ObjectNet which includes objects in daily life situations. They showed a dramatic performance drop of the state of the art object recognition models on this dataset. Due to the importance and implications of their results regarding generalization ability of deep models, we take a second look at their findings. We highlight a major problem with their work which is applying object recognizers to the scenes containing multiple objects rather than isolated objects. The latter results in around 20-30% performance gain using our code. Compared with the results reported in the ObjectNet paper, we observe that around 10-15% of the performance loss can be recovered, without any test time data augmentation. In accordance with Barbu et al.\u2019s conclusions, however, we also conclude that deep models suffer drastically on this dataset. Thus, we believe that ObjectNet remains a challenging dataset for testing the generalization power of models beyond datasets on which they have been trained.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.11546",
        "title": "Bullying10K: A Large-Scale Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition",
        "abstract": "The prevalence of violence in daily life poses significant threats to individuals\u2019 physical and mental well-being. Using surveillance cameras in public spaces has proven effective in proactively deterring and preventing such incidents. However, concerns regarding privacy invasion have emerged due to their widespread deploy- ment. To address the problem, we leverage Dynamic Vision Sensors (DVS) camera to detect violent incidents and preserve privacy since it captures pixel brightness variations instead of static imagery. We introduce the Bullying10K dataset, en- compassing various actions, complex movements, and occlusions from real-life scenarios. It provides three benchmarks for evaluating different tasks: action recognition, temporal action localization, and pose estimation. With 10,000 event segments, totaling 12 billion events and 255 GB of data, Bullying10K contributes significantly by balancing violence detection and personal privacy persevering. And it also poses a challenge to the neuromorphic dataset. It will serve as a valu- able resource for training and developing privacy-protecting video systems. The Bullying10K opens new possibilities for innovative approaches in these domains.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.09126",
        "title": "EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding",
        "abstract": "We introduce EgoSchema, a very long-form video question-answering dataset, and benchmark to evaluate long video understanding capabilities of modern vision and language systems. Derived from Ego4D, EgoSchema consists of over 5000 human curated multiple choice question answer pairs, spanning over 250 hours of real video data, covering a very broad range of natural human activity and behavior. For each question, EgoSchema requires the correct answer to be selected between five given options based on a three-minute-long video clip. While some prior works have proposed video datasets with long clip lengths, we posit that merely the length of the video clip does not truly capture the temporal difficulty of the video task that is being considered. To remedy this, we introduce temporal certificate sets, a general notion for capturing the intrinsic temporal understanding length associated with a broad range of video understanding tasks & datasets. Based on this metric, we find EgoSchema to have intrinsic temporal lengths over 5.7\u00d7 longer than the second closest dataset and 10\u00d7 to 100\u00d7 longer than any other video understanding dataset. Further, our evaluation of several current state-of-the-art video and language models shows them to be severely lacking in long-term video understanding capabilities. Even models with several billions of parameters achieve QA accuracy less than 33% (random is 20%) on the EgoSchema multi-choice question answering task, while humans achieve about 76% accuracy. We posit that EgoSchema, with its long intrinsic temporal structures and diverse complexity, would serve as a valuable evaluation probe for developing effective long-term video understanding systems in the future. Data and Zero-shot model evaluation code are open-sourced under the Ego4D license at egoschema.github.io.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.10224",
        "title": "RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization",
        "abstract": "Visual Reinforcement Learning (Visual RL), coupled with high-dimensional ob- servations, has consistently confronted the long-standing challenge of out-of- distribution generalization. Despite the focus on algorithms aimed at resolving visual generalization problems, we argue that the devil is in the existing benchmarks as they are restricted to isolated tasks and generalization categories, undermining a comprehensive evaluation of agents\u2019 visual generalization capabilities. To bridge this gap, we introduce RL-ViGen: a novel Reinforcement Learning Benchmark for Visual Generalization, which contains diverse tasks and a wide spectrum of generalization types, thereby facilitating the derivation of more reliable conclu- sions. Furthermore, RL-ViGen incorporates the latest generalization visual RL algorithms into a unified framework, under which the experiment results indicate that no single existing algorithm has prevailed universally across tasks. Our aspi- ration is that RL-ViGen will serve as a catalyst in this area, and lay a foundation for the future creation of universal visual generalization RL agents suitable for real-world scenarios. Access to our code and implemented algorithms is provided at https://gemcollector.github.io/RL-ViGen/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.10440",
        "title": "OpenLane-V2: A Topology Reasoning Benchmark for Unified 3D HD Mapping",
        "abstract": "Accurately depicting the complex traffic scene is a vital component for autonomous vehicles to execute correct judgments. However, existing benchmarks tend to oversimplify the scene by solely focusing on lane perception tasks. Observing that human drivers rely on both lanes and traffic signals to operate their vehicles safely, we present OpenLane-V2, the first dataset on topology reasoning for traffic scene structure. The objective of the presented dataset is to advance research in understanding the structure of road scenes by examining the relationship between perceived entities, such as traffic elements and lanes. Leveraging existing datasets, OpenLane-V2 consists of 2,000 annotated road scenes that describe traffic elements and their correlation to the lanes. It comprises three primary sub-tasks, including the 3D lane detection inherited from OpenLane, accompanied by corresponding metrics to evaluate the model\u2019s performance. We evaluate various state-of-the-art methods, and present their quantitative and qualitative results on OpenLane-V2 to indicate future avenues for investigating topology reasoning in traffic scenes.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.12423",
        "title": "Benchmarking and Analyzing 3D-aware Image Synthesis with a Modularized Codebase",
        "abstract": "Despite the rapid advance of 3D-aware image synthesis, existing studies usually adopt a mixture of techniques and tricks, leaving it unclear how each part contributes to the final performance in terms of generality. Following the most popular and effective paradigm in this field, which incorporates a neural radiance field (NeRF) into the generator of a generative adversarial network (GAN), we build a well-structured codebase, dubbed Carver, through modularizing the generation process. Such a design allows researchers to develop and replace each module independently, and hence offers an opportunity to fairly compare various approaches and recognize their contributions from the module perspective. The reproduction of a range of cutting-edge algorithms demonstrates the availability of our modularized codebase. We also perform a variety of in-depth analyses, such as the comparison across different types of point feature, the necessity of the tailing upsampler in the generator, the reliance on the camera pose prior, etc., which deepen our understanding of existing methods and point out some further directions of the research work. We release code and models here to facilitate the development and evaluation of this field.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2206.09203",
        "title": "Interactive Visual Reasoning under Uncertainty",
        "abstract": "One of the fundamental cognitive abilities of humans is to quickly resolve uncer- tainty by generating hypotheses and testing them via active trials. Encountering a novel phenomenon accompanied by ambiguous cause-effect relationships, humans make hypotheses against data, conduct inferences from observation, test their the- ory via experimentation, and correct the proposition if inconsistency arises. These iterative processes persist until the underlying mechanism becomes clear. In this work, we devise the IVRE (pronounced as ivory) environment for evaluating artificial agents\u2019 reasoning ability under uncertainty. IVRE is an interactive environment featuring rich scenarios centered around Blicket detection. Agents in\nIVRE are placed into environments with various ambiguous action-effect pairs and asked to determine each object\u2019s role. They are encouraged to propose effective and efficient experiments to validate their hypotheses based on observations and actively gather new information. The game ends when all uncertainties are resolved or the maximum number of trials is consumed. By evaluating modern artificial agents in IVRE, we notice a clear failure of today\u2019s learning methods compared to humans. Such inefficacy in interactive reasoning ability under uncertainty calls for future research in building human-like intelligence.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2206.10668",
        "title": "BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing",
        "abstract": "Recent work has shown that generation from a prompted or fine-tuned language model can perform well at semantic parsing when the output is constrained to be a valid semantic representation. We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model Parsing, that includes context-free gram- mars for seven semantic parsing datasets and two syntactic parsing datasets with varied output representations, as well as a constrained decoding interface to gen- erate only valid outputs covered by these grammars. We provide low, medium, and high resource splits for each dataset, allowing accurate comparison of various language models under different data regimes. Our benchmark supports evalua- tion of language models using prompt-based learning as well as fine-tuning. We benchmark eight language models, including two GPT-3 variants available only through an API. Our experiments show that encoder-decoder pretrained language models can achieve similar performance or surpass state-of-the-art methods for syntactic and semantic parsing when the model output is constrained to be valid.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.10768",
        "title": "Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory",
        "abstract": "Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, assessing 4 functionalities, 3 domains, and 11 behavioral and neural characteristics of WM. We jointly trained and tested state-of-the-art recurrent neural networks and transformers on all these tasks. For comparison, we also include human behavioral benchmarks. Note that all computational models were never trained with any human data or behavioral biases; yet, these models remarkably replicate some characteristics of WM in biological brains, such as primacy and recency effects. Moreover, we performed neural population analysis on these models and identified neural clusters specialized for different domains and functionalities of WM. Not all computational models exhibit a strong alignment with all human behaviors. Our experimental results also reveal several limitations in existing models to match with working memory capabilities of humans. This dataset serves as a valuable resource for communities in cognitive psychology, neuroscience, and AI, offering a standardized framework to compare and enhance WM models, investigate WM\u2019s neural underpinnings, and develop WM models with human-like capabilities. Our source code and data are available at: link.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.19252",
        "title": "Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union",
        "abstract": "Semantic segmentation datasets often exhibit two types of imbalance: class imbal- ance, where some classes appear more frequently than others and size imbalance, where some objects occupy more pixels than others. This causes traditional evalua- tion metrics to be biased towards majority classes (e.g. overall pixel-wise accuracy) and large objects (e.g. mean pixel-wise accuracy and per-dataset mean intersection over union). To address these shortcomings, we propose the use of fine-grained mIoUs along with corresponding worst-case metrics, thereby offering a more holis- tic evaluation of segmentation techniques. These fine-grained metrics offer less bias towards large objects, richer statistical information, and valuable insights into model and dataset auditing. Furthermore, we undertake an extensive benchmark study, where we train and evaluate 15 modern neural networks with the proposed metrics on 12 diverse natural and aerial segmentation datasets. Our benchmark study highlights the necessity of not basing evaluations on a single metric and confirms that fine-grained mIoUs reduce the bias towards large objects. Moreover, we identify the crucial role played by architecture designs and loss functions, which lead to best practices in optimizing fine-grained metrics. The code is available at https://github.com/zifuwanggg/JDTLosses.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.03810",
        "title": "URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates",
        "abstract": "Representation learning has significantly driven the field to develop pretrained models that can act as a valuable starting point when transferring to new datasets. With the rising demand for reliable machine learning and uncertainty quantification, there is a need for pretrained models that not only provide embeddings but also transferable uncertainty estimates. To guide the development of such models, we propose the Uncertainty-aware Representation Learning (URL) benchmark. Besides the transferability of the representations, it also measures the zero-shot transferability of the uncertainty estimate using a novel metric. We apply URL to evaluate eleven uncertainty quantifiers that are pretrained on ImageNet and transferred to eight downstream datasets. We find that approaches that focus on the uncertainty of the representation itself or estimate the prediction loss directly outperform those that are based on the probabilities of upstream classes. Yet, achieving transferable uncertainty quantification remains an open challenge. Our findings indicate that it is not necessarily in conflict with traditional representation learning goals. Code is available at https://github.com/mkirchhof/url.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.01586",
        "title": "Alexa Arena: A User-Centric Interactive Platform for Embodied AI",
        "abstract": "We introduce Alexa Arena, a user-centric sim- ulation platform for Embodied AI (EAI) re- search. Alexa Arena provides a variety of multi-room layouts and interactable objects, for the creation of human-robot interaction (HRI) missions. With user-friendly graphics and control mechanisms, Alexa Arena sup- ports the development of gamified robotic tasks readily accessible to general human users, thus opening a new venue for high-efficiency HRI data collection and EAI system evaluation. Along with the platform, we introduce a dialog- enabled instruction-following benchmark and provide baseline results for it. We make Alexa Arena1 publicly available to facilitate research in building generalizable and assistive embod- ied agents.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.10514",
        "title": "Building Socio-culturally Inclusive Stereotype Resources with Community Engagement",
        "abstract": "With rapid development and deployment of generative language models in global settings, there is an urgent need to also scale our measurements of harm, not just in the number and types of harms covered, but also how well they account for local cultural contexts, including marginalized identities and the social biases ex- perienced by them. Current evaluation paradigms are limited in their abilities to address this, as they are not representative of diverse, locally situated but global, socio-cultural perspectives. It is imperative that our evaluation resources are en- hanced and calibrated by including people and experiences from different cultures and societies worldwide, in order to prevent gross underestimations or skews in measurements of harm. In this work, we demonstrate a socio-culturally aware expansion of evaluation resources in the Indian societal context, specifically for the harm of stereotyping. We devise a community engaged effort to build a resource which contains stereotypes for axes of disparity that are uniquely present in India. The resultant resource increases the number of stereotypes known for and in the Indian context by over 1000 stereotypes across many unique identities. We also demonstrate the utility and effectiveness of such expanded resources for evalua- tions of language models. CONTENT WARNING: This paper contains examples of stereotypes that may be offensive.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2207.13332",
        "title": "REALTIME QA: What\u2019s the Answer Right Now?",
        "abstract": "We introduce REALTIME QA, a dynamic question answering (QA) platform that announces questions and evaluates systems on a regular basis (weekly in this version). REALTIME QA inquires about the current world, and QA systems need to answer questions about novel events or information. It therefore challenges static, conventional assumptions in open-domain QA datasets and pursues instantaneous applications. We build strong baseline models upon large pretrained language models, including GPT-3 and T5. Our benchmark is an ongoing effort, and this paper presents real-time evaluation results over the past year. Our experimental results show that GPT-3 can often properly update its generation results, based on newly-retrieved documents, highlighting the importance of up-to-date information retrieval. Nonetheless, we find that GPT-3 tends to return outdated answers when retrieved documents do not provide sufficient information to find an answer. This suggests an important avenue for future research: can an open-domain QA system identify such unanswerable cases and communicate with the user or even the retrieval module to modify the retrieval results? We hope that REALTIME QA will\nspur progress in instantaneous applications of question answering and beyond.2",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.08731",
        "title": "EPIC Fields\nMarrying 3D Geometry and Video Understanding",
        "abstract": "Neural rendering is fuelling a unification of learning, 3D geometry and video understanding that has been waiting for more than two decades. Progress, however, is still hampered by a lack of suitable datasets and benchmarks. To address this gap, we introduce EPIC Fields, an augmentation of EPIC-KITCHENS with 3D camera information. Like other datasets for neural rendering, EPIC Fields removes the complex and expensive step of reconstructing cameras using photogrammetry, and allows researchers to focus on modelling problems. We illustrate the challenge of photogrammetry in egocentric videos of dynamic actions and propose innovations to address them. Compared to other neural rendering datasets, EPIC Fields is better tailored to video understanding because it is paired with labelled action segments and the recent VISOR segment annotations. To further motivate the community, we also evaluate three benchmark tasks in neural rendering and segmenting dynamic objects, with strong baselines that showcase what is not possible today. We also highlight the advantage of geometry in semi-supervised video object segmentations on the VISOR annotations. EPIC Fields reconstructs 96% of videos in EPIC- KITCHENS, registering 19M frames in 99 hours recorded in 45 kitchens, and is available from: http://epic-kitchens.github.io/epic-fields",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2011.12948",
        "title": "Nerfies: Deformable Neural Radiance Fields",
        "abstract": "We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos cap- tured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local min- ima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust opti- mization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub \u201cnerfies.\u201d We evalu- ate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2011.13961",
        "title": "D-NeRF: Neural Radiance Fields for Dynamic Scenes",
        "abstract": "Neural rendering techniques combining machine learn- ing with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF) [26], which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achiev- ing an unprecedented level of photorealism on the gener- ated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different im- ages. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allow- ing to reconstruct and render novel images of objects under rigid and non-rigid motions from a single camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canon- ical space and another that maps this canonical represen- tation into the deformed scene at a particular time. Both\nmappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can ren- der novel images, controlling both the camera view and the time variable, and thus, the object movement. We demon- strate the effectiveness of our approach on scenes with ob- jects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be re- leased.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2106.13228",
        "title": "HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields",
        "abstract": "Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprece- dented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limita- tion by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this \u201chyper-space\u201d. Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly be- tween \u201cmoments\u201d, i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for interpolation and 8.6% for novel-view syn- thesis, as measured by LPIPS. Additional videos, results, and visualizations are available at hypernerf.github.io.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.15123",
        "title": "Uncovering Neural Scaling Laws\nin Molecular Representation Learning",
        "abstract": "Molecular Representation Learning (MRL) has emerged as a powerful tool for drug and materials discovery in a variety of tasks such as virtual screening and inverse design. While there has been a surge of interest in advancing model- centric techniques, the influence of both data quantity and quality on molecular representations is not yet clearly understood within this field. In this paper, we delve into the neural scaling behaviors of MRL from a data-centric viewpoint, examining four key dimensions: (1) data modalities, (2) dataset splitting, (3) the role of pre-training, and (4) model capacity. Our empirical studies confirm a consistent power-law relationship between data volume and MRL performance across these dimensions. Additionally, through detailed analysis, we identify potential avenues for improving learning efficiency. To challenge these scaling laws, we adapt seven popular data pruning strategies to molecular data and benchmark their performance. Our findings underline the importance of data-centric MRL and highlight possible directions for future research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.10161",
        "title": "Building the Bridge of Schr\u00f6dinger:\nA Continuous Entropic Optimal Transport Benchmark",
        "abstract": "Over the last several years, there has been significant progress in developing neural solvers for the Schr\u00f6dinger Bridge (SB) problem and applying them to generative modelling. This new research field is justifiably fruitful as it is interconnected with the practically well-performing diffusion models and theoretically grounded entropic optimal transport (EOT). Still, the area lacks non-trivial tests allowing a researcher to understand how well the methods solve SB or its equivalent continuous EOT problem. We fill this gap and propose a novel way to create pairs of probability distributions for which the ground truth OT solution is known by the construction. Our methodology is generic and works for a wide range of OT formulations, in particular, it covers the EOT which is equivalent to SB (the main interest of our study). This development allows us to create continuous benchmark distributions with the known EOT and SB solutions on high-dimensional spaces such as spaces of images. As an illustration, we use these benchmark pairs to test how well existing neural EOT/SB solvers actually compute the EOT solution. Our code for constructing benchmark pairs under different setups is available at:\nhttps://github.com/ngushchin/EntropicOTBenchmark",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.13023",
        "title": "Seeing is not always believing: Benchmarking Human and Model Perception of AI-Generated Images",
        "abstract": "Photos serve as a way for humans to record what they experience in their daily lives, and they are often regarded as trustworthy sources of information. However, there is a growing concern that the advancement of artificial intelligence (AI) technology may produce fake photos, which can create confusion and diminish trust in photographs. This study aims to comprehensively evaluate agents for distinguishing state-of-the-art AI-generated visual content. Our study benchmarks both human capability and cutting-edge fake image detection AI algorithms, using a newly collected large-scale fake image dataset Fake2M. In our human perception evaluation, titled HPBench, we discovered that humans struggle significantly to distinguish real photos from AI-generated ones, with a misclassification rate of 38.7%. Along with this, we conduct the model capability of AI-Generated images detection evaluation MPBench and the top-performing model from MPBench achieves a 13% failure rate under the same setting used in the human evaluation. We hope that our study can raise awareness of the potential risks of AI-generated images and facilitate further research to prevent the spread of false information. More information can refer to https://github.com/Inf-imagine/Sentry.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.05378",
        "title": "M2Hub: Unlocking the Potential of Machine Learning for Materials Discovery",
        "abstract": "We introduce M2Hub, a toolkit for advancing machine learning in materials dis- covery. Machine learning has achieved remarkable progress in modeling molecular structures, especially biomolecules for drug discovery. However, the development of machine learning approaches for modeling materials structures lag behind, which is partly due to the lack of an integrated platform that enables access to diverse tasks for materials discovery. To bridge this gap, M2Hub will enable easy access to materials discovery tasks, datasets, machine learning methods, evaluations, and benchmark results that cover the entire workflow. Specifically, the first release of M2Hub focuses on three key stages in materials discovery: virtual screening, inverse design, and molecular simulation, including 9 datasets that covers 6 types of materials with 56 tasks across 8 types of material properties. We further provide 2 synthetic datasets for the purpose of generative tasks on materials. In addition to random data splits, we also provide 3 additional data partitions to reflect the real-world materials discovery scenarios. State-of-the-art machine learning meth- ods (including those are suitable for materials structures but never compared in the literature) are benchmarked on representative tasks. Our codes and library are publicly available at https://github.com/yuanqidu/M2Hub.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2005.00707",
        "title": "Benchmarking Materials Property Prediction Methods: The Matbench Test Set and Automatminer Reference Algorithm",
        "abstract": "We present a benchmark test suite and an automated machine learning procedure for evaluating supervised machine learning (ML) models for predicting properties of inorganic bulk materials. The test suite, Matbench, is a set of 13 ML tasks that range in size from 312 to 132k samples and contain data from 10 density functional theory-derived and experimental sources. Tasks include predicting optical, thermal, electronic, thermodynamic, tensile, and elastic properties given a materials composition and/or crystal structure. The reference algorithm, Automatminer, is a highly-extensible, fully-automated ML pipeline for predicting materials properties from materials primitives (such as composition and crystal structure) without user intervention or hyperparameter tuning. We test Automatminer on the Matbench test suite and compare its predictive power with state-of-the-art crystal graph neural networks and a traditional descriptor-based Random Forest model. We find Automatminer achieves the best performance on 8 of 13 tasks in the benchmark.\nWe also show our test suite is capable of exposing predictive advantages of each algorithm \u2013 namely, that crystal graph methods appear to outperform traditional machine learning methods given ~104 or greater data points. The pre-processed, ready-to-use Matbench tasks and the Automatminer source code are open source and available online (http://hackingmaterials.lbl.gov/automatminer/). We encourage evaluating new materials ML algorithms on the Matbench benchmark and comparing them against the latest version of Automatminer.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.11155v3",
        "title": "Beyond MD17: the reactive xxMD dataset",
        "abstract": "System specific neural force fields (NFFs) have gained popularity in computational chemistry. One of the most popular datasets as a bencharmk to develop NFFs models is the MD17 dataset and its subsequent extension. These datasets comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampled from direct adiabatic dynamics. However, many chemical reactions involve significant molecular geometrical deformations, for example, bond breaking. Therefore, MD17 is inadequate to represent a chemical reaction. To address this limitation in MD17, we introduce a new dataset, called Extended Excited-state Molecular Dynamics (xxMD) dataset. The xxMD dataset involves geometries sampled from direct non-adiabatic dynamics, and the energies are computed at both multireference wavefunction theory and density functional theory. We show that the xxMD dataset involves diverse geometries which represent chemical reactions. Assessment of NFF models on xxMD dataset reveals significantly higher predictive errors than those reported for MD17 and its variants. This work underscores the challenges faced in crafting a generalizable NFF model with extrapolation capability.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2103.09382",
        "title": "SPICE: Semantic Pseudo-Labeling for Image Clustering",
        "abstract": "The similarity among samples and the discrepancy among clusters are two crucial aspects of image clustering. However, current deep clustering methods suffer from inaccurate estimation of either feature similarity or semantic discrepancy. In this paper, we present a Semantic Pseudo-labeling-based Image ClustEring (SPICE) framework, which divides the clustering network into a feature model for measuring the instance-level similarity and a clustering head for identifying the cluster-level discrepancy. We design two semantics-aware pseudo-labeling al- gorithms, prototype pseudo-labeling and reliable pseudo-labeling, which enable accurate and reliable self-supervision over cluster- ing. Without using any ground-truth label, we optimize the clus- tering network in three stages: 1) train the feature model through contrastive learning to measure the instance similarity; 2) train the clustering head with the prototype pseudo-labeling algorithm to identify cluster semantics; and 3) jointly train the feature model and clustering head with the reliable pseudo-labeling algorithm to improve the clustering performance. Extensive experimental results demonstrate that SPICE achieves significant improvements (\u223c10%) over existing methods and establishes the new state-of-the-art clustering results on six image benchmark datasets in terms of three popular metrics. Importantly, SPICE significantly reduces the gap between unsupervised and fully- supervised classification; e.g. there is only 2% (91.8% vs 93.8%) accuracy difference on CIFAR-10. Our code is made publically available at https://github.com/niuchuangnn/SPICE.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2010.09990",
        "title": "The Open Catalyst 2020 (OC20) Dataset and Community Challenges",
        "abstract": "Catalyst discovery and optimization is key to solving many societal and energy challenges including solar fuels synthesis, long-term energy storage, and renewable fertilizer production. Despite considerable effort by the catalysis community to apply machine learning models to the computational catalyst discovery process, it remains an open challenge to build mod- els that can generalize across both elemental compositions of surfaces and adsorbate iden- tity/configurations, perhaps because datasets have been smaller in catalysis than related fields. To address this we developed the OC20 dataset, consisting of 1,281,040 Density Functional Theory (DFT) relaxations (\u223c264,890,000 single point evaluations) across a wide swath of materials, surfaces, and adsorbates (nitrogen, carbon, and oxygen chemistries). We supple- mented this dataset with randomly perturbed structures, short timescale molecular dynamics, and electronic structure analyses. The dataset comprises three central tasks indicative of day- to-day catalyst modeling and comes with pre-defined train/validation/test splits to facilitate direct comparisons with future model development efforts. We applied three state-of-the-art graph neural network models (CGCNN, SchNet, DimeNet++) to each of these tasks as base- line demonstrations for the community to build on. In almost every task, no upper limit on model size was identified, suggesting that even larger models are likely to improve on initial results. The dataset and baseline models are both provided as open resources, as well as a public leader board to encourage community contributions to solve these important tasks.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.15895",
        "title": "Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias",
        "abstract": "Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model\u2019s performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, synthetic datasets generated by simple prompts exhibit significant biases, such as regional bias; secondly, attribute diversity plays a pivotal role in enhancing model performance; lastly, attributed prompts achieve the performance of simple class-conditional prompts while utilizing only 5% of the querying cost of ChatGPT associated with the latter. We release the generated dataset and used prompts to facilitate future research2.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09467",
        "title": "AQuA: A Benchmarking Tool for Label Quality Assessment",
        "abstract": "Machine learning (ML) models are only as good as the data they are trained on. But recent studies have found datasets widely used to train and evaluate ML models, e.g. ImageNet, to have pervasive labeling errors. Erroneous labels on the train set hurt ML models\u2019 ability to generalize, and they impact evaluation and model selection using the test set. Consequently, learning in the presence of labeling errors is an active area of research, yet this field lacks a comprehensive benchmark to evaluate these methods. Most of these methods are evaluated on a few computer vision datasets with significant variance in the experimental protocols. With such a large pool of methods and inconsistent evaluation, it is also unclear how ML practitioners can choose the right models to assess label quality in their data. To this end, we propose a benchmarking environment AQuA to rigorously evaluate methods that enable machine learning in the presence of label noise. We also introduce a design space to delineate concrete design choices of label error detection models. We hope that our proposed design space and benchmark enable practitioners to choose the right tools to improve their label quality and that our benchmark enables objective and rigorous evaluation of machine learning tools facing mislabeled data.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.06231",
        "title": "Learning Human Action Recognition Representations Without Real Humans",
        "abstract": "Pre-training on massive video datasets has become essential to achieve high action recognition performance on smaller downstream datasets. However, most large- scale video datasets contain images of people and hence are accompanied with issues related to privacy, ethics, and data protection, often preventing them from being publicly shared for reproducible research. Existing work has attempted to alleviate these problems by blurring faces, downsampling videos, or training on synthetic data. On the other hand, analysis on the transferability of privacy- preserving pre-trained models to downstream tasks has been limited. In this work, we study this problem by first asking the question: can we pre-train models for human action recognition with data that does not include real humans? To this end, we present, for the first time, a benchmark that leverages real-world videos with humans removed and synthetic data containing virtual humans to pre-train a model. We then evaluate the transferability of the representation learned on this data to a diverse set of downstream action recognition benchmarks. Furthermore, we propose a novel pre-training strategy, called Privacy-Preserving MAE-Align, to effectively combine synthetic data and human-removed real data. Our approach outperforms previous baselines by up to 5% and closes the performance gap between human and no-human action recognition representations on downstream tasks, for both linear probing and fine-tuning. Our benchmark, code, and models are available at https://github.com/howardzh01/PPMA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.05107",
        "title": "OV-PARTS: Towards Open-Vocabulary Part Segmentation",
        "abstract": "Segmenting and recognizing diverse object parts is a crucial ability in applications spanning various computer vision and robotic tasks. While significant progress\nhas been made in object-level Open-Vocabulary Semantic Segmentation (OVSS),\ni.e., segmenting objects with arbitrary text, the corresponding part-level research poses additional challenges. Firstly, part segmentation inherently involves intricate boundaries, while limited annotated data compounds the challenge. Secondly, part segmentation introduces an open granularity challenge due to the diverse and often ambiguous definitions of parts in the open world. Furthermore, the large-scale vision and language models, which play a key role in the open vocabulary setting, struggle to recognize parts as effectively as objects. To comprehensively investigate and tackle these challenges, we propose an Open-Vocabulary Part Segmentation (OV-PARTS) benchmark. OV-PARTS includes refined versions of two publicly available datasets: Pascal-Part-116 and ADE20K-Part-234. And it covers three specific tasks: Generalized Zero-Shot Part Segmentation, Cross-Dataset Part Segmentation, and Few-Shot Part Segmentation, providing insights into analogical reasoning, open granularity and few-shot adapting abilities of models. Moreover, we analyze and adapt two prevailing paradigms of existing object-level OVSS methods for OV-PARTS. Extensive experimental analysis is conducted to inspire future research in leveraging foundational models for OV-PARTS. The code and dataset are available at https://github.com/OpenRobotLab/OV_PARTS.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.10012",
        "title": "MAGICBRUSH : A Manually Annotated Dataset for Instruction-Guided Image Editing",
        "abstract": "Text-guided image editing is widely needed in daily life, ranging from personal use to professional applications such as Photoshop. However, existing methods are either zero-shot or trained on an automatically synthesized dataset, which contains a high volume of noise. Thus, they still require lots of manual tun- ing to produce desirable outcomes in practice. To address this issue, we intro- duce MAGICBRUSH (https://osu-nlp-group.github.io/MagicBrush/), the first large-scale, manually annotated dataset for instruction-guided real image editing that covers diverse scenarios: single-turn, multi-turn, mask-provided, and mask-free editing. MAGICBRUSH comprises over 10K manually annotated triplets (source image, instruction, target image), which supports training large-scale text- guided image editing models. We fine-tune InstructPix2Pix on MAGICBRUSH and show that the new model can produce much better images according to human evaluation. We further conduct extensive experiments to evaluate current image editing baselines from multiple dimensions including quantitative, qualitative, and human evaluations. The results reveal the challenging nature of our dataset and the gap between current baselines and real-world editing needs.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.02028",
        "title": "EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models",
        "abstract": "While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains de- identified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR. We provide an end-to-end pipeline for the community to validate and build upon its performance. Third, we define 15 few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as sample efficiency and task adaptation. Our model and dataset are available via a research data use agreement from our website. Code to reproduce our results is available here.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.01999",
        "title": "Revisiting the Evaluation of Image Synthesis with GANs",
        "abstract": "a good metric, which promises a reliable comparison between solutions, is essential for any well-defined task. Unlike most vision tasks that have per-sample ground- truth, image synthesis tasks target generating unseen data and hence are usually evaluated through a distributional distance between one set of real samples and another set of generated samples. This study presents an empirical investigation into the evaluation of synthesis performance, with generative adversarial networks (GANs) as a representative of generative models. In particular, we make in- depth analyses of various factors, including how to represent a data point in the representation space, how to calculate a fair distance using selected samples, and how many instances to use from each set. Extensive experiments conducted on multiple datasets and settings reveal several important findings. Firstly, a group of models that include both CNN-based and ViT-based architectures serve as reliable and robust feature extractors for measurement evaluation. Secondly, Centered Kernel Alignment (CKA) provides a better comparison across various extractors and hierarchical layers in one model. Finally, CKA is more sample- efficient and enjoys better agreement with human judgment in characterizing the similarity between two internal data correlations. These findings contribute to the development of a new measurement system, which enables a consistent and reliable re-evaluation of current state-of-the-art generative models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.16981",
        "title": "Reimagining Synthetic Tabular Data Generation through Data-Centric AI:\nA Comprehensive Benchmark",
        "abstract": "Synthetic data serves as an alternative in training machine learning models, par- ticularly when real-world data is limited or inaccessible. However, ensuring that synthetic data mirrors the complex nuances of real-world data is a challenging task. This paper addresses this issue by exploring the potential of integrating data-centric AI techniques which profile the data to guide the synthetic data generation process. Moreover, we shed light on the often ignored consequences of neglecting these data profiles during synthetic data generation \u2014 despite seemingly high statistical fidelity. Subsequently, we propose a novel framework to evaluate the integration of data profiles to guide the creation of more representative synthetic data. In an empirical study, we evaluate the performance of five state-of-the-art models for tabular data generation on eleven distinct tabular datasets. The findings offer critical insights into the successes and limitations of current synthetic data gener- ation techniques. Finally, we provide practical recommendations for integrating data-centric insights into the synthetic data generation process, with a specific focus on classification performance, model selection, and feature selection. This study aims to reevaluate conventional approaches to synthetic data generation and promote the application of data-centric AI techniques in improving the quality and effectiveness of synthetic data.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.17768",
        "title": "A Dataset of Relighted 3D Interacting Hands",
        "abstract": "The two-hand interaction is one of the most challenging signals to analyze due to the self-similarity, complicated articulations, and occlusions of hands. Although several datasets have been proposed for the two-hand interaction analysis, all of them do not achieve 1) diverse and realistic image appearances and 2) diverse and large-scale groundtruth (GT) 3D poses at the same time. In this work, we propose Re:InterHand, a dataset of relighted 3D interacting hands that achieve the two goals. To this end, we employ a state-of-the-art hand relighting network with our accurately tracked two-hand 3D poses. We compare our Re:InterHand with existing 3D interacting hands datasets and show the benefit of it. Our Re:InterHand is available in here.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.03153",
        "title": "MultiVENT: Multilingual Videos of Events with Aligned Natural Text",
        "abstract": "Everyday news coverage has shifted from traditional broadcasts towards a wide range of presentation formats such as first-hand, unedited video footage. Datasets that reflect the diverse array of multimodal, multilingual news sources available online could be used to teach models to benefit from this shift, but existing news video datasets focus on traditional news broadcasts produced for English-speaking audiences. We address this limitation by constructing MultiVENT, a dataset of multilingual, event-centric videos grounded in text documents across five target languages. MultiVENT includes both news broadcast videos and non-professional event footage, which we use to analyze the state of online news videos and how they can be leveraged to build robust, factually accurate models. Finally, we provide a model for complex, multilingual video retrieval to serve as a baseline for information retrieval using MultiVENT.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.02408",
        "title": "Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning",
        "abstract": "chain-of-thought prompting (CoT) and tool augmentation have been validated in recent work as effective practices for improving large language models (LLMs) to perform step-by-step reasoning on complex math-related tasks. However, most existing math reasoning datasets may be not able to fully evaluate and analyze the ability of LLMs in manipulating tools and performing reasoning, as they may only require very few invocations of tools or miss annotations for evaluating intermediate reasoning steps. To address the issue, we construct CARP, a new Chinese dataset consisting of 4,886 computation-intensive algebra problems with formulated annotations on intermediate steps. In CARP, we test four LLMs with CoT prompting, and find that they are all prone to make mistakes at the early steps of the solution, leading to wrong answers. Based on this finding, we propose a new approach that can deliberate the reasoning steps with tool interfaces, namely DELI. In DELI, we first initialize a step-by-step solution based on retrieved exemplars, then iterate two deliberation procedures that check and refine the intermediate steps of the generated solution, from the perspectives of tool manipulation and natural language reasoning, until obtaining converged solutions or reaching the maximum turn. Experimental results on CARP and six other datasets show that the proposed DELI mostly outperforms competitive baselines, and can further boost the performance of existing CoT methods. Our data and code are available in https://github.com/RUCAIBox/CARP.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.07716",
        "title": "PAD: A Dataset and Benchmark for Pose-agnostic Anomaly Detection",
        "abstract": "Object anomaly detection is an important problem in the field of machine vision and has seen remarkable progress recently. However, two significant challenges hinder its research and application. First, existing datasets lack comprehensive visual information from various pose angles. They usually have an unrealistic assumption that the anomaly-free training dataset is pose-aligned, and the testing samples have the same pose as the training data. However, in practice, anomaly may exist in any regions on a object, the training and query samples may have different poses, calling for the study on pose-agnostic anomaly detection. Second, the absence of a consensus on experimental protocols for pose-agnostic anomaly de- tection leads to unfair comparisons of different methods, hindering the research on pose-agnostic anomaly detection. To address these issues, we develop Multi-pose Anomaly Detection (MAD) dataset and Pose-agnostic Anomaly Detection (PAD) benchmark, which takes the first step to address the pose-agnostic anomaly detec- tion problem. Specifically, we build MAD using 20 complex-shaped LEGO toys including 4K views with various poses, and high-quality and diverse 3D anomalies in both simulated and real environments. Additionally, we propose a novel method OmniposeAD, trained using MAD, specifically designed for pose-agnostic anomaly detection. Through comprehensive evaluations, we demonstrate the relevance of our dataset and method. Furthermore, we provide an open-source benchmark library, including dataset and baseline methods that cover 8 anomaly detection paradigms, to facilitate future research and application in this domain. Code, data, and models are publicly available at https://github.com/EricLee0224/PAD",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.08772",
        "title": "Katakomba:\nTools and Benchmarks for Data-Driven NetHack",
        "abstract": "NetHack is known as the frontier of reinforcement learning research where learning- based methods still need to catch up to rule-based solutions. One of the promising directions for a breakthrough is using pre-collected datasets similar to recent developments in robotics, recommender systems, and more under the umbrella of offline reinforcement learning (ORL). Recently, a large-scale NetHack dataset was released; while it was a necessary step forward, it has yet to gain wide adoption in the ORL community. In this work, we argue that there are three major obstacles for adoption: resource-wise, implementation-wise, and benchmark-wise. To address them, we develop an open-source library2 that provides workflow fundamentals familiar to the ORL community: pre-defined D4RL-style tasks, uncluttered baseline implementations, and reliable evaluation tools with accompanying configs and logs synced to the cloud.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.07577",
        "title": "Benchmarking Distribution Shift in Tabular Data with TableShift",
        "abstract": "Robustness to distribution shift has become a growing concern for text and im- age models as they transition from research subjects to deployment in the real world. However, high-quality benchmarks for distribution shift in tabular machine learning tasks are still lacking despite the widespread real-world use of tabular data and differences in the models used for tabular data in comparison to text and images. As a consequence, the robustness of tabular models to distribution shift is poorly understood. To address this issue, we introduce TABLESHIFT, a distribution shift benchmark for tabular data. TABLESHIFT contains 15 binary classification tasks in total, each with an associated shift, and includes a diverse set of data sources, prediction targets, and distribution shifts. The benchmark covers domains including finance, education, public policy, healthcare, and civic participation, and is accessible using only a few lines of Python code via the TABLESHIFT API. We conduct a large-scale study comparing several state-of-the-art tabular data models alongside robust learning and domain generalization methods on the benchmark tasks. Our study demonstrates (1) a linear trend between in-distribution (ID) and out-of-distribution (OOD) accuracy; (2) domain robustness methods can reduce shift gaps but at the cost of reduced ID accuracy; (3) a strong relationship between shift gap (difference between ID and OOD performance) and shifts in the label distribution.1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.13587",
        "title": "Benchmarking Encoder-Decoder Architectures for Biplanar X-ray to 3D Shape Reconstruction",
        "abstract": "Various deep learning models have been proposed for 3D bone shape reconstruction from two orthogonal (biplanar) X-ray images. However, it is unclear how these models compare against each other since they are evaluated on different anatomy, cohort and (often privately held) datasets. Moreover, the impact of the commonly optimized image-based segmentation metrics such as dice score on the estimation of clinical parameters relevant in 2D-3D bone shape reconstruction is not well known. To move closer toward clinical translation, we propose a benchmarking framework that evaluates tasks relevant to real-world clinical scenarios, including reconstruction of fractured bones, bones with implants, robustness to population shift, and error in estimating clinical parameters. Our open-source platform pro- vides reference implementations of 8 models (many of whose implementations were not publicly available), APIs to easily collect and preprocess 6 public datasets, and the implementation of automatic clinical parameter and landmark extraction methods. We present an extensive evaluation of 8 2D-3D models on equal footing using 6 public datasets comprising images for four different anatomies. Our results show that attention-based methods that capture global spatial relationships tend to perform better across all anatomies and datasets; performance on clinically relevant subgroups may be overestimated without disaggregated reporting; ribs are substantially more difficult to reconstruct compared to femur, hip and spine; and the dice score improvement does not always bring a corresponding improvement in the automatic estimation of clinically relevant parameters.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.15376",
        "title": "ADGym: Design Choices for Deep Anomaly Detection",
        "abstract": "Deep learning (DL) techniques have recently found success in anomaly detection (AD) across various fields such as finance, medical services, and cloud computing. However, most of the current research tends to view deep AD algorithms as a whole, without dissecting the contributions of individual design choices like loss functions and network architectures. This view tends to diminish the value of preliminary steps like data preprocessing, as more attention is given to newly designed loss functions, network architectures, and learning paradigms. In this paper, we aim to bridge this gap by asking two key questions: (i) Which design choices in deep AD methods are crucial for detecting anomalies? (ii) How can we automatically select the optimal design choices for a given AD dataset, instead of relying on generic, pre- existing solutions? To address these questions, we introduce ADGym, a platform specifically crafted for comprehensive evaluation and automatic selection of AD design elements in deep methods. Our extensive experiments reveal that relying solely on existing leading methods is not sufficient. In contrast, models developed using ADGym significantly surpass current state-of-the-art techniques.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.00164",
        "title": "Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis",
        "abstract": "We consider the problem of traffic accident analysis on a road network based on road network connections and traffic volume. Previous works have designed various deep-learning methods using historical records to predict traffic accident occurrences. However, there is a lack of consensus on how accurate existing methods are, and a fundamental issue is the lack of public accident datasets for comprehensive evaluations. This paper constructs a large-scale, unified dataset of traffic accident records from official reports of various states in the US, totaling 9 million records, accompanied by road networks and traffic volume reports. Using this new dataset, we evaluate existing deep-learning methods for predicting the occurrence of accidents on road networks. Our main finding is that graph neural networks such as GraphSAGE can accurately predict the number of accidents on roads with less than 22% mean absolute error (relative to the actual count) and whether an accident will occur or not with over 87% AUROC, averaged over states. We achieve these results by using multitask learning to account for cross-state variabilities (e.g., availability of accident labels) and transfer learning to combine traffic volume with accident prediction. Ablation studies highlight the importance of road graph-structural features, amongst other features. Lastly, we discuss the implications of the analysis and develop a package for easily using our new dataset.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.04181",
        "title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
        "abstract": "Numerous benchmarks have been established to assess the performance of founda- tion models on open-ended question answering, which serves as a comprehensive test of a model\u2019s ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking frame- work, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains to probe for a broad acquisition, and raise follow-up questions to engage in a more in-depth assessment. (2) Upon evaluation, the examiner combines both scoring and ranking measurements, providing a reli- able result as it aligns closely with human annotations. (3) We additionally propose a decentralized Peer-examination method to address the biases in a single examiner. Our data and benchmarking results are available at: http://lmexam.xlore.cn.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.03030",
        "title": "Benchmarking Large Language Models on CMExam - A Comprehensive Chinese Medical Exam Dataset",
        "abstract": "Recent advancements in large language models (LLMs) have transformed the field of question answering (QA). However, evaluating LLMs in the medical field is challenging due to the lack of standardized and comprehensive datasets. To address this gap, we introduce CMExam, sourced from the Chinese National Medical Licensing Examination. CMExam consists of 60K+ multiple-choice questions for standardized and objective evaluations, as well as solution explanations for model reasoning evaluation in an open-ended manner. For in-depth analyses of LLMs, we invited medical professionals to label five additional question-wise annotations, including disease groups, clinical departments, medical disciplines, areas of com- petency, and question difficulty levels. Alongside the dataset, we further conducted thorough experiments with representative LLMs and QA algorithms on CMExam. The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great disparity when compared to human accuracy, which stood at 71.6%. For explanation tasks, while LLMs could generate relevant reasoning and demonstrate improved performance after finetuning, they fall short of a desired standard, indicating ample room for improvement. To the best of our knowledge, CMExam is the first Chinese medical exam dataset to pro- vide comprehensive medical annotations. The experiments and findings of LLM evaluation also provide valuable insights into the challenges and potential solutions in developing Chinese medical QA systems and LLM evaluation pipelines.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2209.12487",
        "title": "TARTARUS: A Benchmarking Platform for Realistic And Practical Inverse Molecular Design",
        "abstract": "The efficient exploration of chemical space to design molecules with intended properties enables the accelerated discovery of drugs, materials, and catalysts, and is one of the most important outstanding challenges in chemistry. Encouraged by the recent surge in computer power and artificial intelligence development, many algorithms have been developed to tackle this problem. However, despite the emer- gence of many new approaches in recent years, comparatively little progress has been made in developing realistic benchmarks that reflect the complexity of molec- ular design for real-world applications. In this work, we develop a set of practical benchmark tasks relying on physical simulation of molecular systems mimicking real-life molecular design problems for materials, drugs, and chemical reactions. Additionally, we demonstrate the utility and ease of use of our new benchmark set by demonstrating how to compare the performance of several well-established families of algorithms. Surprisingly, we find that model performance can strongly depend on the benchmark domain. We believe that our benchmark suite will help move the field towards more realistic molecular design benchmarks, and move the development of inverse molecular design algorithms closer to designing molecules that solve existing problems in both academia and industry alike",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.15448",
        "title": "Understanding Social Reasoning in Language Models with Language Models",
        "abstract": "As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.2",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.06399",
        "title": "Lo-Hi: Practical ML Drug Discovery Benchmark",
        "abstract": "Finding new drugs is getting harder and harder. One of the hopes of drug discov- ery is to use machine learning models to predict molecular properties. That is why models for molecular property prediction are being developed and tested on benchmarks such as MoleculeNet. However, existing benchmarks are unrealistic and are too different from applying the models in practice. We have created a new practical Lo-Hi benchmark consisting of two tasks: Lead Optimization (Lo) and Hit Identification (Hi), corresponding to the real drug discovery process. For the Hi task, we designed a novel molecular splitting algorithm that solves the Balanced Vertex Minimum k-Cut problem. We tested state-of-the-art and classic ML models, revealing which works better under practical settings. We analyzed modern benchmarks and showed that they are unrealistic and overoptimistic.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.01116",
        "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",
        "abstract": "Large language models are commonly trained on a mixture of filtered web data and curated \u201chigh-quality\u201d corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our REFINEDWEB dataset, and 1.3/7.5B parameters language models trained on it",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.03111",
        "title": "Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs",
        "abstract": "Text-to-SQL parsing, which aims at converting natural language questions into executable SQLs, has gained increasing attention in recent years. In particular, GPT-4 and Claude-2 have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database values leaving the gap between academic study and real-world applications. To mitigate this gap, we present BIRD, a BIg bench for laRge-scale Database grounded in text-to-SQL tasks, containing 12,751 text-to- SQL pairs and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty and noisy database values, external knowledge grounding between NL questions and database values, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most effective text-to-SQL models, i.e. GPT-4, only achieve 54.89% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand. We also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. We believe that BIRD will contribute to advancing real-world applications of text-to-SQL research. The leaderboard and source code are available: https://bird-bench.github.io/",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.02665",
        "title": "Digital Typhoon: Long-term Satellite Image Dataset for the Spatio-Temporal Modeling of Tropical Cyclones",
        "abstract": "This paper presents the official release of the Digital Typhoon dataset, the longest typhoon satellite image dataset for 40+ years aimed at benchmarking machine learning models for long-term spatio-temporal data. To build the dataset, we de- veloped a workflow to create an infrared typhoon-centered image for cropping using Lambert azimuthal equal-area projection referring to the best track data. We also address data quality issues such as inter-satellite calibration to create a homogeneous dataset. To take advantage of the dataset, we organized machine learning tasks by the types and targets of inference, with other tasks for me- teorological analysis, societal impact, and climate change. The benchmarking results on the analysis, forecasting, and reanalysis for the intensity suggest that the dataset is challenging for recent deep learning models, due to many choices that affect the performance of various models. This dataset reduces the barrier for machine learning researchers to meet large-scale real-world events called tropical cyclones and develop machine learning models that may contribute to advancing scientific knowledge on tropical cyclones as well as solving societal and sustain- ability issues such as disaster reduction and climate change. The dataset is publicly available at http://agora.ex.nii.ac.jp/digital-typhoon/dataset/ and https://github.com/kitamoto-lab/digital-typhoon/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.12421",
        "title": "evaluating Open-QA Evaluation",
        "abstract": "This study focuses on the evaluation of the Open Question Answering (Open- QA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes human- annotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this area. All resources are available at https://github.com/wangcunxiang/QA-Eval and it is under the Apache-2.0 License.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.13512",
        "title": "DISCO-10M: A Large-Scale Music Dataset",
        "abstract": "Music datasets play a crucial role in advancing research in machine learning for music. However, existing music datasets suffer from limited size, accessibility, and lack of audio resources. To address these shortcomings, we present DISCO-10M, a novel and extensive music dataset that surpasses the largest previously available music dataset by an order of magnitude. To ensure high-quality data, we imple- ment a multi-stage filtering process. This process incorporates similarities based on textual descriptions and audio embeddings. Moreover, we provide precom- puted CLAP embeddings alongside DISCO-10M, facilitating direct application on various downstream tasks. These embeddings enable efficient exploration of machine learning applications on the provided data. With DISCO-10M, we aim to democratize and facilitate new research to help advance the development of novel machine learning models for music",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2212.07489",
        "title": "SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning",
        "abstract": "The availability of challenging benchmarks has played a key role in the recent progress of machine learning. In cooperative multi-agent reinforcement learning, the StarCraft Multi-Agent Challenge (SMAC) has become a popular testbed for the centralised training with decentralised execution paradigm. However, after years of sustained improvement on SMAC, algorithms now achieve near-perfect performance. In this work, we conduct new analysis demonstrating that SMAC lacks the stochasticity and partial observability to require complex closed-loop policies (i.e., those that condition on the observation). In particular, we show that an open-loop policy conditioned only on the timestep can achieve non-trivial win rates for many SMAC scenarios. To address this limitation, we introduce SMACv2, a new benchmark where scenarios are procedurally generated and require agents to generalise to previously unseen settings during evaluation.2 We show that these changes ensure the benchmark requires the use of closed-loop policies. We also introduce the extended partial observability challenge (EPO), which augments SMACv2 to ensure meaningful partial observability. We evaluate state-of-the-art algorithms on SMACv2 and show that it presents significant challenges not present in the original benchmark. Our analysis illustrates that SMACv2 addresses the discovered deficiencies of SMAC and can help benchmark the next generation of MARL methods. Videos of training are available on our website.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.12567",
        "title": "Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark",
        "abstract": "Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical sce- narios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenar- ios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of- the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for safer, more reliable, and respon- sible real-world applications. The website of this project can be accessed at https://sites.google.com/view/safety-gymnasium.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.04755",
        "title": "Pairwise GUI Dataset Construction Between Android Phones and Tablets",
        "abstract": "In the current landscape of pervasive smartphones and tablets, apps frequently exist across both platforms. Although apps share most graphic user interfaces (GUIs) and functionalities across phones and tablets, developers often rebuild from scratch for tablet versions, escalating costs and squandering existing design resources. Researchers are attempting to collect data and employ deep learning in automated GUIs development to enhance developers\u2019 productivity. There are currently several publicly accessible GUI page datasets for phones, but none for pairwise GUIs between phones and tablets. This poses a significant barrier to the employment of deep learning in automated GUI development. In this paper, we introduce the Papt dataset, a pioneering pairwise GUI dataset tailored for Android phones and tablets, encompassing 10,035 phone-tablet GUI page pairs sourced from 5,593 unique app pairs. We propose novel pairwise GUI collection approaches for constructing this dataset and delineate its advantages over currently prevailing datasets in the field. Through preliminary experiments on this dataset, we analyze the present challenges of utilizing deep learning in automated GUI development",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.03919",
        "title": "Data Portraits: Recording Foundation Model Training Data",
        "abstract": "Foundation models are trained on increasingly immense and opaque datasets. Even while these models are now key in AI system building, it can be difficult to answer the straightforward question: has the model already encountered a given example during training? We therefore propose a widespread adoption of Data Portraits: artifacts that record training data and allow for downstream inspection. First we outline the properties of such an artifact and discuss how existing solutions can be used to increase transparency. We then propose and implement a solution based on data sketching, stressing fast and space efficient querying. Using our tools, we document a popular language modeling corpus (The Pile) and a recently released code modeling dataset (The Stack). We show that our solution enables answering questions about test set leakage and model plagiarism. Our tool is lightweight and fast, costing only 3% of the dataset size in overhead. We release a live interface of our tools at dataportraits.org and call on dataset and model creators to release Data Portraits as a complement to current documentation practices.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.12251",
        "title": "GADBench: Revisiting and Benchmarking Supervised Graph Anomaly Detection",
        "abstract": "With a long history of traditional Graph Anomaly Detection (GAD) algorithms and recently popular Graph Neural Networks (GNNs), it is still not clear (1) how they perform under a standard comprehensive setting, (2) whether GNNs can outperform traditional algorithms such as tree ensembles, and (3) how about their efficiency on large-scale graphs. In response, we introduce GADBench\u2014a benchmark tool dedicated to supervised anomalous node detection in static graphs. GADBench facilitates a detailed comparison across 29 distinct models on ten real-world GAD datasets, encompassing thousands to millions (\u223c6M) nodes. Our main finding is that tree ensembles with simple neighborhood aggregation can outperform the latest GNNs tailored for the GAD task. We shed light on the current progress of GAD, setting a robust groundwork for subsequent investigations in this domain. GADBench is open-sourced at https://github.com/squareRoot3/GADBench.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.10548",
        "title": "MARBLE: Music Audio Representation Benchmark for Universal Evaluation",
        "abstract": "In the era of extensive intersection between art and Artificial Intelligence (AI), such as image generation and fiction co-creation, AI for music remains relatively nascent, particularly in music understanding. This is evident in the limited work on deep music representations, the scarcity of large-scale datasets, and the absence of a universal and community-driven benchmark. To address this issue, we introduce the Music Audio Representation Benchmark for universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy with four hierarchy levels, including acoustic, performance, score, and high-level description. We establish a unified protocol based on 18 tasks on 12 public-available datasets, providing a fair and standard assessment of representations of all open-sourced pre-trained models developed on music recordings as baselines. MARBLE offers an easy-to-use, extendable, and reproducible suite for the community, with clear statements on dataset copyright. Results suggest that recently proposed large-scale pre-trained musical language models perform the best in most tasks, with room for further improvement. The leaderboard and toolkit repository are published34 to promote future music AI research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.01458",
        "title": "CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care",
        "abstract": "The recent advances in natural language processing (NLP), have led to a new trend of applying large language models (LLMs) to real-world scenarios. While the latest LLMs are astonishingly fluent when interacting with humans, they suffer from the misinformation problem by unintentionally generating factually false statements. This can lead to harmful consequences, especially when produced within sensitive contexts, such as healthcare. Yet few previous works have fo- cused on evaluating misinformation in the long-form (LF) generation of LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have been shown to perform well in different languages, misinformation evaluation has been mostly conducted in English. To this end, we present a benchmark, CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic, specifically the maternity and infant care domain; and 2) a language other than English, namely Chinese. Most importantly, we provide an innovative paradigm for building LF generation evaluation benchmarks that can be transferred to other knowledge-intensive do- mains and low-resourced languages. Our proposed benchmark fills the gap between the extensive usage of LLMs and the lack of datasets for assessing the misinfor- mation generated by these models. It contains 1,612 expert-checked questions, accompanied with human-selected references. Using our benchmark, we conduct extensive experiments and found that current Chinese LLMs are far from perfect in the topic of maternity and infant care. In an effort to minimize the reliance on human resources for performance evaluation, we offer off-the-shelf judgment models for automatically assessing the LF output of LLMs given benchmark ques- tions. Moreover, we compare potential solutions for LF generation evaluation and provide insights for building better automated metrics. Code and models are available at https://github.com/Meetyou-AI-Lab/CARE-MI.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.01426",
        "title": "DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection",
        "abstract": "A critical yet frequently overlooked challenge in the field of deepfake detection is the lack of a standardized, unified, comprehensive benchmark. This issue leads to unfair performance comparisons and potentially misleading results. Specifically, there is a lack of uniformity in data processing pipelines, resulting in inconsistent data inputs for detection models. Additionally, there are noticeable differences in experimental settings, and evaluation strategies and metrics lack standardization. To fill this gap, we present the first comprehensive benchmark for deepfake detec- tion, called DeepfakeBench, which offers three key contributions: 1) a unified data management system to ensure consistent input across all detectors, 2) an integrated framework for state-of-the-art methods implementation, and 3) standardized evalu- ation metrics and protocols to promote transparency and reproducibility. Featuring an extensible, modular-based codebase, DeepfakeBench contains 15 state-of-the-art detection methods, 9 deepfake datasets, a series of deepfake detection evaluation protocols and analysis tools, as well as comprehensive evaluations. Moreover, we provide new insights based on extensive analysis of these evaluations from various perspectives (e.g., data augmentations, backbones). We hope that our efforts could facilitate future research and foster innovation in this increasingly critical domain. All codes, evaluations, and analyses of our benchmark are publicly available at https://github.com/SCLBD/DeepfakeBench.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2301.12993",
        "title": "Benchmarking Robustness to Adversarial Image Obfuscations",
        "abstract": "Automated content filtering and moderation is an important tool that allows online platforms to build striving user communities that facilitate cooperation and prevent abuse. Unfortunately, resourceful actors try to bypass automated filters in a bid to post content that violate platform policies and codes of conduct. To reach this goal, these malicious actors may obfuscate policy violating images (e.g. overlay harmful images by carefully selected benign images or visual patterns) to prevent machine learning models from reaching the correct decision. In this paper, we invite researchers to tackle this specific issue and present a new image benchmark. This benchmark, based on IMAGENET, simulates the type of obfuscations created by malicious actors. It goes beyond IMAGENET-C and IMAGENET-C \u0304 by proposing general, drastic, adversarial modifications that preserve the original content intent. It aims to tackle a more common adversarial threat than the one considered by lp-norm bounded adversaries. We evaluate 33 pretrained models on the benchmark and train models with different augmentations, architectures and training methods on subsets of the obfuscations to measure generalization. Our hope is that this benchmark will encourage researchers to test their models and methods and try to find new approaches that are more robust to these obfuscations.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.16424",
        "title": "Realistic Synthetic Financial Transactions for Anti-Money Laundering Models",
        "abstract": "With the widespread digitization of finance and the increasing popularity of cryp- tocurrencies, the sophistication of fraud schemes devised by cybercriminals is growing. Money laundering \u2013 the movement of illicit funds to conceal their origins \u2013 can cross bank and national boundaries, producing complex transaction patterns. The UN estimates 2-5% of global GDP or $0.8 - $2.0 trillion dollars are laundered globally each year. Unfortunately, real data to train machine learning models to detect laundering is generally not available, and previous synthetic data generators have had significant shortcomings. A realistic, standardized, publicly-available benchmark is needed for comparing models and for the advancement of the area. To this end, this paper contributes a synthetic financial transaction dataset generator and a set of synthetically generated AML (Anti-Money Laundering) datasets. We have calibrated this agent-based generator to match real transactions as closely as possible and made the datasets public. We describe the generator in detail and demonstrate how the datasets generated can help compare different machine learning models in terms of their AML abilities. In a key way, using synthetic data in these comparisons can be even better than using real data: the ground truth labels are complete, whilst many laundering transactions in real data are never detected.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.05766",
        "title": "Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting",
        "abstract": "Radiology reporting is a crucial part of the communication between radiologists and other medical professionals, but it can be time- consuming and error-prone. One approach to alleviate this is structured reporting, which saves time and enables a more accurate evaluation than free-text reports. However, there is limited research on automating struc- tured reporting, and no public benchmark is available for evaluating and comparing different methods. To close this gap, we introduce Rad- ReStruct, a new benchmark dataset that provides fine-grained, hierarchi- cally ordered annotations in the form of structured reports for X-Ray im- ages. We model the structured reporting task as hierarchical visual ques- tion answering (VQA) and propose hi-VQA, a novel method that consid- ers prior context in the form of previously asked questions and answers for populating a structured radiology report. Our experiments show that hi-VQA achieves competitive performance to the state-of-the-art on the medical VQA benchmark VQARad while performing best among meth- ods without domain-specific vision-language pretraining and provides a strong baseline on Rad-ReStruct. Our work represents a significant step towards the automated population of structured radiology reports and provides a valuable first benchmark for future research in this area. Our dataset and code is available at https://github.com/ChantalMP/Rad- ReStruct.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2102.09542",
        "title": "SLAKE: A SEMANTICALLY-LABELED KNOWLEDGE-ENHANCED DATASET FOR MEDICAL VISUAL QUESTION ANSWERING",
        "abstract": "Medical visual question answering (Med-VQA) has tremendous potential in healthcare. However, the devel- opment of this technology is hindered by the lacking of publicly-available and high-quality labeled datasets for train- ing and evaluation. In this paper, we present a large bilin- gual dataset, SLAKE, with comprehensive semantic labels annotated by experienced physicians and a new structural medical knowledge base for Med-VQA. Besides, SLAKE includes richer modalities and covers more human body parts than the currently available dataset. We show that SLAKE can be used to facilitate the development and evaluation of Med-VQA systems. The dataset can be downloaded from http://www.med-vqa.com/slake.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.03830",
        "title": "QSlack: A slack-variable approach for variational quantum semi-definite programming",
        "abstract": "Solving optimization problems is a key task for which quantum computers could possibly provide a speedup over the best known classical algorithms. Particular classes of optimization problems in- cluding semi-definite programming (SDP) and linear programming (LP) have wide applicability in many domains of computer science, engineering, mathematics, and physics. Here we focus on semi- definite and linear programs for which the dimensions of the variables involved are exponentially large, so that standard classical SDP and LP solvers are not helpful for such large-scale problems. We propose the QSlack and CSlack methods for estimating their optimal values, respectively, which work by 1) introducing slack variables to transform inequality constraints to equality constraints, 2) transforming a constrained optimization to an unconstrained one via the penalty method, and 3) replacing the optimizations over all possible non-negative variables by optimizations over parame- terized quantum states and parameterized probability distributions. Under the assumption that the SDP and LP inputs are efficiently measurable observables, it follows that all terms in the resulting objective functions are efficiently estimable by either a quantum computer in the SDP case or a quantum or probabilistic computer in the LP case. Furthermore, by making use of SDP and LP duality theory, we prove that these methods provide a theoretical guarantee that, if one could find global optima of the objective functions, then the resulting values sandwich the true optimal values from both above and below. Finally, we showcase the QSlack and CSlack methods on a variety of example optimization problems and discuss details of our implementation, as well as the resulting performance. We find that our implementations of both the primal and dual for these problems approach the ground truth, typically achieving errors of the order 10\u22122.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2003.10286",
        "title": "PATHVQA: 30000+ QUESTIONS FOR MEDICAL VISUAL QUESTION ANSWERING",
        "abstract": "Is it possible to develop an \u201cAI Pathologist\" to pass the board-certified examination of the American Board of Pathology? To achieve this goal, the first step is to create a visual question answering (VQA) dataset where the AI agent is presented with a pathology image together with a question and is asked to give the correct answer. Our work makes the first attempt to build such a dataset. Different from creating general-domain VQA datasets where the images are widely accessible and there are many crowdsourcing workers available and capable of generating question-answer pairs, developing a medical VQA dataset is much more challenging. First, due to privacy concerns, pathology images are usually not publicly available. Second, only well-trained pathologists can understand pathology images, but they barely have time to help create datasets for AI research. To address these challenges, we resort to pathology textbooks and online digital libraries. We develop a semi-automated pipeline to extract pathology images and captions from textbooks and generate question-answer pairs from captions using natural language processing. We collect 32,799 open-ended questions from 4,998 pathology images where each question is manually checked to ensure correctness. To our best knowledge, this is the first dataset for pathology VQA. Our dataset will be released publicly to promote research in medical VQA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2301.03213",
        "title": "EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset",
        "abstract": "Visual object tracking is key to many egocentric vision problems. However, the full spectrum of challenges of egocentric tracking faced by an embodied AI is under- represented in many existing datasets, which tend to focus on short, third-person videos. Egocentric video has several distinguishing characteristics from those com- monly found in past datasets: frequent large camera motions and hand interactions with objects commonly lead to occlusions or objects exiting the frame, and object appearance can change rapidly due to widely different points of view, scale, or object states. Embodied tracking is also naturally long-term, and being able to con- sistently (re-)associate objects to their appearances and disappearances over as long as a lifetime is critical. Previous datasets under-emphasize this re-detection prob- lem, and their \u201cframed\u201d nature has led to adoption of various spatiotemporal priors that we find do not necessarily generalize to egocentric video. We thus introduce EgoTracks, a new dataset for long-term egocentric visual object tracking. Sourced from the Ego4D dataset, EgoTracks presents a significant challenge to recent state- of-the-art single-object trackers, which we find score more poorly on our new dataset than existing popular benchmarks, according to traditional tracking metrics. We further show improvements that can be made to the STARK tracker to signifi- cantly increase its performance on egocentric data, resulting in a baseline model we call EgoSTARK. We publicly release our annotations and benchmark (https:// github.com/EGO4D/episodic-memory/tree/main/EgoTracks), hoping our dataset leads to further advancements in tracking.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.13924",
        "title": "trajdata: A Unified Interface to Multiple Human Trajectory Datasets",
        "abstract": "The field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous large-scale, real-world human trajectory datasets for autonomous vehicles (AVs) and pedestrian motion tracking. While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets. To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets. At its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data. As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights. trajdata is permissively licensed (Apache 2.0) and can be accessed online at https://github.com/NVlabs/trajdata.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.03831",
        "title": "GEO-Bench:\nToward Foundation Models for Earth Monitoring",
        "abstract": "Recent progress in self-supervision has shown that pre-training large neural networks on vast amounts of unsupervised data can lead to substantial increases in generalization to downstream tasks. Such models, recently coined foundation models, have been transformational to the field of natural language processing. Variants have also been proposed for image data, but their applicability to remote sensing tasks is limited. To stimulate the development of foundation models for Earth monitoring, we propose a benchmark comprised of six classification and six segmentation tasks, which were carefully curated and adapted to be both relevant to the field and well-suited for model evaluation. We accompany this benchmark with a robust methodology for evaluating models and reporting aggregated results to enable a reliable assessment of progress. Finally, we report results for 20 baselines to gain information about the performance of existing models. We believe that this benchmark will be a driver of progress across a variety of Earth monitoring tasks.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.06070",
        "title": "MIND2WEB: Towards a Generalist Agent for the Web",
        "abstract": "We introduce MIND2WEB, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, MIND2WEB pro- vides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on MIND2WEB, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world web- sites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.06828",
        "title": "RoboHive: A Unified Framework for Robot Learning",
        "abstract": "We present RoboHive, a comprehensive software platform and ecosystem for re- search in the field of Robot Learning and Embodied Artificial Intelligence. Our platform encompasses a diverse range of pre-existing and novel environments, including dexterous manipulation with the Shadow Hand, whole-arm manipulation tasks with Franka and Fetch robots, quadruped locomotion, among others. Included environments are organized within and cover multiple domains such as hand ma- nipulation, locomotion, multi-task, multi-agent, muscles, etc. In comparison to prior works, RoboHive offers a streamlined and unified task interface taking depen- dency on only a minimal set of well-maintained packages, features tasks with high physics fidelity and rich visual diversity, and supports common hardware drivers for real-world deployment. The unified interface of RoboHive offers a convenient and accessible abstraction for algorithmic research in imitation, reinforcement, multi-task, and hierarchical learning. Furthermore, RoboHive includes expert demonstrations and baseline results for most environments, providing a standard for benchmarking and comparisons. Details: https://sites.google.com/view/robohiveW",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.10455",
        "title": "A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset",
        "abstract": "In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-1M Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capa- ble of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhib- ited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research within the machine learning community, progress on creating an image-based taxonomic classifier will also further the ultimate goal of all BIOSCAN research: to lay the foundation for a comprehensive survey of global biodiversity. This paper introduces the dataset and explores the classification task through the implementation and analysis of a baseline clas- sifier. The code repository of the BIOSCAN-1M-Insect dataset is available at https://github.com/zahrag/BIOSCAN-1M",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.12284",
        "title": "D4: Improving LLM Pretraining via Document De-Duplication and Diversification",
        "abstract": "Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent perfor- mance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as Min- Hash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever data selection can significantly improve LLM pre-training, calls into question the common practice of training for a single epoch on as much data as possible, and demonstrates a path to keep improving our models past the limits of randomly sampling web data.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.12477",
        "title": "American Stories: A Large-Scale Structured Text Dataset of Historical U.S. Newspapers",
        "abstract": "Existing full text datasets of U.S. public domain newspapers do not recognize the often complex layouts of newspaper scans, and as a result the digitized content scrambles texts from articles, headlines, captions, advertisements, and other lay- out regions. OCR quality can also be low. This study develops a novel, deep learn- ing pipeline for extracting full article texts from newspaper images and applies it to the nearly 20 million scans in Library of Congress\u2019s public domain Chronicling America collection. The pipeline includes layout detection, legibility classifica- tion, custom OCR, and association of article texts spanning multiple bounding boxes. To achieve high scalability, it is built with efficient architectures designed for mobile phones. The resulting American Stories dataset provides high quality data that could be used for pre-training a large language model to achieve better understanding of historical English and historical world knowledge. The dataset could also be added to the external database of a retrieval-augmented language model to make historical information - ranging from interpretations of political events to minutiae about the lives of people\u2019s ancestors - more widely accessible. Furthermore, structured article texts facilitate using transformer-based methods for popular social science applications like topic classification, detection of repro- duced content, and news story clustering. Finally, American Stories provides a massive silver quality dataset for innovating multimodal layout analysis models and other multimodal applications.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.17810",
        "title": "A Massive Scale Semantic Similarity Dataset of Historical English",
        "abstract": "A diversity of tasks use language models trained on semantic similarity data. While there are a variety of datasets that capture semantic similarity, they are either constructed from modern web data or are relatively small datasets created in the past decade by human annotators. This study utilizes a novel source, newly digitized articles from off-copyright, local U.S. newspapers, to assemble a massive-scale semantic similarity dataset spanning 70 years from 1920 to 1989 and containing nearly 400M positive semantic similarity pairs. Historically, around half of articles in U.S. local newspapers came from newswires like the Associated Press. While local papers reproduced articles from the newswire, they wrote their own headlines, which form abstractive summaries of the associated articles. We associate articles and their headlines by exploiting document layouts and language understanding. We then use deep neural methods to detect which articles are from the same underlying source, in the presence of substantial noise and abridgement. The headlines of reproduced articles form positive semantic similarity pairs. The resulting publicly available HEADLINES dataset is significantly larger than most existing semantic similarity datasets and covers a much longer span of time. It will facilitate the application of contrastively trained semantic similarity models to a variety of tasks, including the study of semantic change across space and time.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2301.02560",
        "title": "GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition",
        "abstract": "Current dataset collection methods typically scrape large amounts of data from the web. While this technique is extremely scalable, data collected in this way tends to reinforce stereotypical biases, can contain personally identifiable information, and typically originates from Europe and North America. In this work, we rethink the dataset collection paradigm and introduce GeoDE , a geographically diverse dataset with 61,940 images from 40 classes and 6 world regions, and no personally identifiable information, collected through crowd-sourcing. We analyse GeoDE to understand differences in images collected in this manner compared to web-scraping. Despite the smaller size of this dataset, we demonstrate its use as both an evaluation and training dataset, allowing us to highlight shortcomings in current models, as well as demonstrate improved performances even when training on the small dataset. We release the full dataset and code at https://geodiverse-data-collection.cs. princeton.edu/",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.05284",
        "title": "On the Need of a Modeling Language for Distribution Shifts: Illustrations on Tabular Datasets",
        "abstract": "Different distribution shifts require different interventions, and algorithms must be grounded in the specific shifts they address. However, methodological development for \u201crobust\u201d methods typically relies on structural assumptions that lack empirical validation. Advocating for an empirically grounded inductive approach to research, we build an empirical testbed compris- ing natural shifts across 5 tabular datasets and 60,000 method configurations encompassing imbalanced learning methods and distributionally robust optimization (DRO) methods. We find Y |X-shifts are most prevalent on our testbed, in stark contrast to the heavy focus on X\n(covariate)-shifts in the ML literature. The performance of \u201crobust\u201d methods varies significantly over shift types, and is no better than that of vanilla methods. To understand why, we conduct an in-depth empirical analysis of DRO methods and find that although often neglected by researchers, implementation details\u2014such as the choice of underlying model class (e.g., XGBoost) and hyperparameter selection\u2014have a bigger impact on performance than the ambiguity set or its radius. To further bridge that gap between methodological research and practice, we design case studies that illustrate how such a refined, inductive understanding of distribution shifts can enhance both data-centric and algorithmic interventions.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.10798",
        "title": "INSPECT: A Multimodal Dataset for Pulmonary Embolism Diagnosis and Prognosis",
        "abstract": "Synthesizing information from multiple data sources plays a crucial role in the prac- tice of modern medicine. Current applications of artificial intelligence in medicine often focus on single-modality data due to a lack of publicly available, multimodal medical datasets. To address this limitation, we introduce INSPECT, which con- tains de-identified longitudinal records from a large cohort of patients at risk for pulmonary embolism (PE), along with ground truth labels for multiple outcomes. INSPECT contains data from 19,402 patients, including CT images, radiology report impression sections, and structured electronic health record (EHR) data (i.e. demographics, diagnoses, procedures, vitals, and medications). Using INSPECT, we develop and release a benchmark for evaluating several baseline modeling approaches on a variety of important PE related tasks. We evaluate image-only, EHR-only, and multimodal fusion models. Trained models and the de-identified dataset are made available for non-commercial use under a data use agreement. To the best of our knowledge, INSPECT is the largest multimodal dataset integrating 3D medical imaging and EHR for reproducible methods evaluation and research",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.19909",
        "title": "Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks",
        "abstract": "Neural network based computer vision systems are typically built on a backbone, a pretrained or randomly initialized feature extractor. Several years ago, the default option was an ImageNet-trained convolutional neural network. However, the re- cent past has seen the emergence of countless backbones pretrained using various algorithms and datasets. While this abundance of choice has led to performance increases for a range of systems, it is difficult for practitioners to make informed decisions about which backbone to choose. Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from clas- sification to object detection to OOD generalization and more. Furthermore, BoB sheds light on promising directions for the research community to advance com- puter vision by illuminating strengths and weakness of existing approaches through a comprehensive analysis conducted on more than 1500 training runs. While vision transformers (ViTs) and self-supervised learning (SSL) are increasingly popular, we find that convolutional neural networks pretrained in a supervised fashion on large training sets still perform best on most tasks among the models we consider. More- over, in apples-to-apples comparisons on the same architectures and similarly sized pretraining datasets, we find that SSL backbones are highly competitive, indicating that future works should perform SSL pretraining with advanced architectures and larger pretraining datasets. We release the raw results of our experiments along with code that allows researchers to put their own backbones through the gauntlet here: https://github.com/hsouri/Battle-of-the-Backbones.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.05326",
        "title": "OpenProteinSet: Training data for structural biology at scale",
        "abstract": "Multiple sequence alignments (MSAs) of proteins encode rich biological infor- mation and have been workhorses in bioinformatic methods for tasks like protein design and protein structure prediction for decades. Recent breakthroughs like Al- phaFold2 that use transformers to attend directly over large quantities of raw MSAs have reaffirmed their importance. Generation of MSAs is highly computationally intensive, however, and no datasets comparable to those used to train AlphaFold2 have been made available to the research community, hindering progress in machine learning for proteins. To remedy this problem, we introduce OpenProteinSet, an open-source corpus of more than 16 million MSAs, associated structural homologs from the Protein Data Bank, and AlphaFold2 protein structure predictions. We have previously demonstrated the utility of OpenProteinSet by successfully retraining AlphaFold2 on it. We expect OpenProteinSet to be broadly useful as training and validation data for 1) diverse tasks focused on protein structure, function, and design and 2) large-scale multimodal machine learning research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.12829",
        "title": "Intelligent Knee Sleeves: A Real-time Multimodal Dataset for 3D Lower Body Motion Estimation Using Smart Textile",
        "abstract": "The kinematics of human movements and locomotion are closely linked to the activation and contractions of muscles. To investigate this, we present a multimodal dataset with benchmarks collected using a novel pair of Intelligent Knee Sleeves (Texavie MarsWear Knee Sleeves) for human pose estimation. Our system utilizes synchronized datasets that comprise time-series data from the Knee Sleeves and the corresponding ground truth labels from visualized motion capture camera system. We employ these to generate 3D human models solely based on the wearable data of individuals performing different activities. We demonstrate the effectiveness of this camera-free system and machine learning algorithms in the assessment of various movements and exercises, including extension to unseen exercises and individuals. The results show an average error of 7.21 degrees across all eight lower body joints when compared to the ground truth, indicating the effectiveness and reliability of the Knee Sleeve system for the prediction of different lower body joints beyond knees. The results enable human pose estimation in a seamless manner without being limited by visual occlusion or the field of view of cameras. Our results show the potential of multimodal wearable sensing in a variety of applications from home fitness to sports, healthcare, and physical rehabilitation focusing on pose and movement estimation.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.05179",
        "title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",
        "abstract": "Despite the existence of various benchmarks for evaluating natural language pro- cessing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel bench- mark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model\u2019s multimodal understanding capability; and (3) multilevel struc- ture, featuring exams from three critical educational periods to comprehensively assess a model\u2019s proficiency at different levels. In total, M3Exam contains 12,317 questions in 9 diverse languages with three educational levels, where about 23% of the questions require processing images for successful solving. We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with com- plex multimodal questions. We believe that M3Exam can be a valuable resource for comprehensively evaluating LLMs by examining their multilingual and multimodal abilities and tracking their development. Data and evaluation code is available at https://github.com/DAMO-NLP-SG/M3Exam.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.14610",
        "title": "SUGARCREPE: Fixing Hackable Benchmarks for Vision-Language Compositionality",
        "abstract": "In the last year alone, a surge of new benchmarks to measure compositional understanding of vision-language models have permeated the machine learning ecosystem. Given an image, these benchmarks probe a model\u2019s ability to identify its associated caption amongst a set of compositional distractors. Surprisingly, we find significant biases in all these benchmarks rendering them hackable. This hackability is so dire that blind models with no access to the image outperform state-of-the-art vision-language models. To remedy this rampant vulnerability, we introduce SUGARCREPE, a new benchmark for vision-language compositionality evaluation. We employ large language models, instead of rule-based templates used in previous benchmarks, to generate fluent and sensical hard negatives, and utilize an adversarial refinement mechanism to maximally reduce biases. We re-evaluate state-of-the-art models and recently proposed compositionality inducing strategies, and find that their improvements were hugely overestimated, suggesting that more innovation is needed in this important direction. We release SUGARCREPE and the code for evaluation at: https://github.com/RAIVNLab/sugar-crepe.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.13831",
        "title": "Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks",
        "abstract": "We present the Minigrid and Miniworld libraries which provide a suite of goal- oriented 2D and 3D environments. The libraries were explicitly created with a minimalistic design paradigm to allow users to rapidly develop new environments for a wide range of research-specific needs. As a result, both have received widescale adoption by the RL community, facilitating research in a wide range of areas. In this paper, we outline the design philosophy, environment details, and their world generation API. We also showcase the additional capabilities brought by the unified API between Minigrid and Miniworld through case studies on transfer learning (for both RL agents and humans) between the different observation spaces. The source code of Minigrid and Miniworld can be found at https: //github.com/Farama-Foundation/{Minigrid,Miniworld} along with their documentation at https://{minigrid,miniworld}.farama.org/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.01909",
        "title": "ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling",
        "abstract": "Modeling weather and climate is an essential endeavor to understand the near- and long-term impacts of climate change, as well as inform technology and policymak- ing for adaptation and mitigation efforts. In recent years, there has been a surging interest in applying data-driven methods based on machine learning for solving core problems such as weather forecasting and climate downscaling. Despite promising results, much of this progress has been impaired due to the lack of large-scale, open- source efforts for reproducibility, resulting in the use of inconsistent or underspeci- fied datasets, training setups, and evaluations by both domain scientists and artificial intelligence researchers. We introduce ClimateLearn, an open-source PyTorch library that vastly simplifies the training and evaluation of machine learning mod- els for data-driven climate science. ClimateLearn consists of holistic pipelines for dataset processing (e.g., ERA5, CMIP6, PRISM), implementation of state-of- the-art deep learning models (e.g., Transformers, ResNets), and quantitative and qualitative evaluation for standard weather and climate modeling tasks. We supple- ment these functionalities with extensive documentation, contribution guides, and quickstart tutorials to expand access and promote community growth. We have also performed comprehensive forecasting and downscaling experiments to showcase the capabilities and key features of our library. To our knowledge, ClimateLearn is the first large-scale, open-source effort for bridging research in weather and climate modeling with modern machine learning systems. Our library is available publicly at https://github.com/aditya-grover/climate-learn.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.14898",
        "title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
        "abstract": "Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create three interactive code environments with Bash, SQL, and Python as action spaces, leveraging data from the static NL2Bash [32], Spider [55], and MBPP [4] datasets. We demonstrate InterCode\u2019s viability as a testbed by evaluating multiple state-of-the-art LLMs configured with different prompting strategies such as ReAct [51] and Plan & Solve [43]. Our results showcase the benefits of interactive code generation and demonstrate that InterCode can serve as a challenging benchmark for advancing code understanding and generation capabilities. InterCode is designed to be easily extensible and can even be used to create new tasks such as Capture the Flag, a popular coding puzzle that is inherently multi-step and involves multiple programming languages.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.01812",
        "title": "Into the Single Cell Multiverse:\nan End-to-End Dataset for Procedural Knowledge Extraction in Biomedical Texts",
        "abstract": "Many of the most commonly explored natural language processing (NLP) informa- tion extraction tasks can be thought of as evaluations of declarative knowledge, or fact-based information extraction. Procedural knowledge extraction, i.e., breaking down a described process into a series of steps, has received much less attention, perhaps in part due to the lack of structured datasets that capture the knowledge ex- traction process from end-to-end. To address this unmet need, we present FlaMB\u00e9 (Flow annotations for Multiverse Biological entities), a collection of expert-curated datasets across a series of complementary tasks that capture procedural knowledge in biomedical texts. This dataset is inspired by the observation that one ubiquitous source of procedural knowledge that is described as unstructured text is within aca- demic papers describing their methodology. The workflows annotated in FlaMB\u00e9 are from texts in the burgeoning field of single cell research, a research area that has become notorious for the number of software tools and complexity of workflows used. Additionally, FlaMB\u00e9 provides, to our knowledge, the largest manually curated named entity recognition (NER) and disambiguation (NED) datasets for tissue/cell type, a fundamental biological entity that is critical for knowledge ex- traction in the biomedical research domain. Beyond providing a valuable dataset to enable further development of NLP models for procedural knowledge extraction, automating the process of workflow mining also has important implications for advancing reproducibility in biomedical research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.16917",
        "title": "The Drunkard\u2019s Odometry: Estimating Camera Motion in Deforming Scenes",
        "abstract": "Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodolo- gies. To tackle this issue with a common benchmark, we introduce the Drunkard\u2019s Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D build- ings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality. We further present a novel deformable odometry method, dubbed the Drunkard\u2019s Odometry, which decomposes optical flow estimates into rigid-body camera motion and non-rigid scene deformations. In order to validate our data, our work contains an evaluation of several baselines as well as a novel tracking error metric which does not require ground truth data. Dataset and code: https: //davidrecasens.github.io/TheDrunkard\u2019sOdometry/",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.15668",
        "title": "Physion++: Evaluating Physical Scene Understanding that Requires Online Inference of Different Physical Properties",
        "abstract": "General physical scene understanding requires more than simply localizing and recognizing objects \u2013 it requires knowledge that objects can have different latent properties (e.g., mass or elasticity), and that those properties affect the outcome of physical events. While there has been great progress in physical and video prediction models in recent years, benchmarks to test their performance typically do not require an understanding that objects have individual physical properties, or at best test only those properties that are directly observable (e.g., size or color). This work proposes a novel dataset and benchmark, termed Physion++, that rigorously evaluates visual physical prediction in artificial systems under circumstances where those predictions rely on accurate estimates of the latent physical properties of objects in the scene. Specifically, we test scenarios where accurate prediction relies on estimates of properties such as mass, friction, elasticity, and deformability, and where the values of those properties can only be inferred by observing how objects move and interact with other objects or fluids. We evaluate the performance of a number of state-of-the-art prediction models that span a variety of levels of learning vs. built-in knowledge, and compare that performance to a set of human predictions. We find that models that have been trained using standard regimes and datasets do not spontaneously learn to make inferences about latent properties, but also that models that encode objectness and physical states tend to make better predictions. However, there is still a huge gap between all models and human performance, and all models\u2019 predictions correlate poorly with those made by humans, suggesting that no state-of-the-art model is learning to make physical predictions in a human- like way. These results show that current deep learning models that succeed in some settings nevertheless fail to achieve human-level physical prediction in other cases, especially those where latent property inference is required. Project page: https://dingmyu.github.io/physion_v2/",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.17503",
        "title": "Pgx: Hardware-Accelerated Parallel Game Simulators for Reinforcement Learning",
        "abstract": "We propose Pgx, a suite of board game reinforcement learning (RL) environments written in JAX and optimized for GPU/TPU accelerators. By leveraging JAX\u2019s auto-vectorization and parallelization over accelerators, Pgx can efficiently scale to thousands of simultaneous simulations over accelerators. In our experiments on a DGX-A100 workstation, we discovered that Pgx can simulate RL environments 10-100x faster than existing implementations available in Python. Pgx includes RL environments commonly used as benchmarks in RL research, such as backgammon, chess, shogi, and Go. Additionally, Pgx offers miniature game sets and baseline models to facilitate rapid research cycles. We demonstrate the efficient training of the Gumbel AlphaZero algorithm with Pgx environments. Overall, Pgx provides high-performance environment simulators for researchers to accelerate their RL experiments. Pgx is available at https://github.com/sotetsuk/pgx.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.01135",
        "title": "Generating QM1B with PySCFIPU",
        "abstract": "The emergence of foundation models in Computer Vision and Natural Language Processing have resulted in immense progress on downstream tasks. This progress was enabled by datasets with billions of training examples. Similar benefits are yet to be unlocked for quantum chemistry, where the potential of deep learning is constrained by comparatively small datasets with 100k to 20M training examples. These datasets are limited in size because the labels are computed using the accurate (but computationally demanding) predictions of Density Functional Theory (DFT). Notably, prior DFT datasets were created using CPU supercomputers without leveraging hardware acceleration. In this paper, we take a first step towards utilising hardware accelerators by introducing the data generator PySCFIPU using Intelligence Processing Units (IPUs). This allowed us to create the dataset QM1B with one billion training examples containing 9-11 heavy atoms. We demonstrate that a simple baseline neural network (SchNet 9M) improves its performance by simply increasing the amount of training data without additional inductive biases. To encourage future researchers to use QM1B responsibly, we highlight several limitations of QM1B and emphasise the low-resolution of our DFT options, which also serves as motivation for even larger, more accurate datasets. Code and dataset.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.16900",
        "title": "Learning to Taste : A Multimodal Wine Dataset",
        "abstract": "We present WineSensed, a large multimodal wine dataset for studying the relations between visual perception, language, and flavor. The dataset encompasses 897k im- ages of wine labels and 824k reviews of wines curated from the Vivino platform. It has over 350k unique bottlings, annotated with year, region, rating, alcohol percent- age, price, and grape composition. We obtained fine-grained flavor annotations on a subset by conducting a wine-tasting experiment with 256 participants who were asked to rank wines based on their similarity in flavor, resulting in more than 5k pairwise flavor distances. We propose a low-dimensional concept embedding algo- rithm that combines human experience with automatic machine similarity kernels. We demonstrate that this shared concept embedding space improves upon separate embedding spaces for coarse flavor classification (alcohol percentage, country, grape, price, rating) and aligns with the intricate human perception of flavor.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.01525",
        "title": "VisAlign: Dataset for Measuring the Degree of Alignment between AI and Humans in Visual Perception",
        "abstract": "AI alignment refers to models acting towards human-intended goals, preferences, or ethical principles. Given that most large-scale deep learning models act as black boxes and cannot be manually controlled, analyzing the similarity between models and humans can be a proxy measure for ensuring AI safety. In this paper, we focus on the models\u2019 visual perception alignment with humans, further referred to as AI-human visual alignment. Specifically, we propose a new dataset for measuring AI-human visual alignment in terms of image classification, a fundamental task in machine perception. In order to evaluate AI-human visual alignment, a dataset should encompass samples with various scenarios that may arise in the real world and have gold human perception labels. Our dataset consists of three groups of samples, namely Must-Act (i.e., Must-Classify), Must-Abstain, and Uncertain, based on the quantity and clarity of visual information in an image and further divided into eight categories. All samples have a gold human perception label; even Uncertain (e.g., severely blurry) sample labels were obtained via crowd- sourcing. The validity of our dataset is verified by sampling theory, statistical theories related to survey design, and experts in the related fields. Using our dataset, we analyze the visual alignment and reliability of five popular visual perception models and seven abstention methods. Our code and data is available at https://github.com/jiyounglee-0523/VisAlign.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09126",
        "title": "STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events",
        "abstract": "While direction of arrival (DOA) of sound events is generally estimated from multichannel audio data recorded in a microphone array, sound events usually derive from visually perceptible source objects, e.g., sounds of footsteps come from the feet of a walker. This paper proposes an audio-visual sound event localization and detection (SELD) task, which uses multichannel audio and video information to estimate the temporal activation and DOA of target sound events. Audio- visual SELD systems can detect and localize sound events using signals from a microphone array and audio-visual correspondence. We also introduce an audio- visual dataset, Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23), which consists of multichannel audio data recorded with a microphone array, video data, and spatiotemporal annotation of sound events. Sound scenes in STARSS23 are recorded with instructions, which guide recording participants to ensure adequate activity and occurrences of sound events. STARSS23 also serves human-annotated temporal activation labels and human-confirmed DOA labels, which are based on tracking results of a motion capture system. Our benchmark results demonstrate the benefits of using visual object positions in audio-visual SELD tasks. The data is available at https://zenodo.org/record/7880637.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.04937",
        "title": "Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine",
        "abstract": "We propose the Multimodal Clinical Benchmark for Emergency Care (MC-BEC), a comprehensive benchmark for evaluating foundation models in Emergency Medicine using a dataset of 100K+ continuously monitored Emergency Department visits from 2020-2022. MC-BEC focuses on clinically relevant prediction tasks at timescales from minutes to days, including predicting patient decompensation, disposition, and emergency department (ED) revisit, and includes a standardized evaluation framework with train-test splits and evaluation metrics. The multimodal dataset includes a wide range of detailed clinical data, including triage information, prior diagnoses and medications, continuously measured vital signs, electrocar- diogram and photoplethysmograph waveforms, orders placed and medications administered throughout the visit, free-text reports of imaging studies, and informa- tion on ED diagnosis, disposition, and subsequent revisits. We provide performance baselines for each prediction task to enable the evaluation of multimodal, multitask models. We believe that MC-BEC will encourage researchers to develop more effective, generalizable, and accessible foundation models for multimodal clinical data.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.00723",
        "title": "HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count",
        "abstract": "We present the HOH (Human-Object-Human) Handover Dataset, a large object count dataset with 136 objects, to accelerate data-driven research on handover studies, human-robot handover implementation, and artificial intelligence (AI) on handover parameter estimation from 2D and 3D data of two-person interactions. HOH contains multi-view RGB and depth data, skeletons, fused point clouds, grasp type and handedness labels, object, giver hand, and receiver hand 2D and 3D segmentations, giver and receiver comfort ratings, and paired object metadata and aligned 3D models for 2,720 handover interactions spanning 136 objects and 20 giver-receiver pairs\u201440 with role-reversal\u2014organized from 40 participants. We also show experimental results of neural networks trained using HOH to perform grasp, orientation, and trajectory prediction. As the only fully markerless handover capture dataset, HOH represents natural human-human handover interactions, overcoming challenges with markered datasets that require specific suiting for body tracking, and lack high-resolution hand tracking. To date, HOH is the largest handover dataset in terms of object count, participant count, pairs with role reversal accounted for, and total interactions captured.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.07902",
        "title": "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark",
        "abstract": "Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture- dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2207.13250",
        "title": "Spatio-Temporal Wildfire Prediction using\nMulti-Modal Data",
        "abstract": "Due to severe societal and environmental impacts, wildfire prediction using multi-modal sensing data has become a highly sought-after data-analytical tool by various stakeholders (such as state governments and power utility companies) to achieve a more informed understanding of wildfire activities and plan preventive measures. A desirable algorithm should precisely predict fire risk and magnitude for a location in real time. In this paper, we develop a flexible spatio-temporal wildfire prediction framework using multi-modal time series data. We first predict the wildfire risk (the chance of a wildfire event) in real-time, considering the historical events using discrete mutually exciting point process models. Then we further develop a wildfire magnitude prediction set method based on the flexible distribution-free time-series conformal prediction (CP) approach. Theoretically, we prove a risk model parameter recovery guarantee, as well as coverage and set size guarantees for the CP sets. Through extensive real-data experiments with wildfire data in California, we demonstrate the effectiveness of our methods, as well as their flexibility and scalability in large regions.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.16005",
        "title": "MLFMF: Data Sets for Machine Learning for Mathematical Formalization",
        "abstract": "We introduce MLFMF, a collection of data sets for benchmarking recommendation systems used to support formalization of mathematics with proof assistants. These systems help humans identify which previous entries (theorems, constructions, datatypes, and postulates) are relevant in proving a new theorem or carrying out a new construction. Each data set is derived from a library of formalized mathematics written in proof assistants Agda or Lean. The collection includes the largest Lean 4 library Mathlib, and some of the largest Agda libraries: the standard library, the library of univalent mathematics Agda-unimath, and the TypeTopology library. Each data set represents the corresponding library in two ways: as a heterogeneous network, and as a list of s-expressions representing the syntax trees of all the entries in the library. The network contains the (modular) structure of the library and the references between entries, while the s-expressions give complete and easily parsed information about every entry. We report baseline results using standard graph and word embeddings, tree ensembles, and instance-based learning algorithms. The MLFMF data sets provide solid benchmarking support for further investigation of the numerous machine learning approaches to formalized mathematics. The methodology used to extract the networks and the s-expressions readily applies to other libraries, and is applicable to other proof assistants. With more than 250 000 entries in total, this is currently the largest collection of formalized mathematical knowledge in machine learnable format.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.03977",
        "title": "PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning",
        "abstract": "Synthetic image datasets offer unmatched advantages for designing and evaluating deep neural networks: they make it possible to (i) render as many data samples as needed, (ii) precisely control each scene and yield granular ground truth labels (and captions), (iii) precisely control distribution shifts between training and testing to isolate variables of interest for sound experimentation. Despite such promise, the use of synthetic image data is still limited \u2013 and often played down \u2013 mainly due to their lack of realism. Most works therefore rely on datasets of real images, which have often been scraped from public images on the internet, and may have issues with regards to privacy, bias, and copyright, while offering little control over how objects precisely appear. In this work, we present a path to democratize the use of photorealistic synthetic data: we develop a new generation of interactive environments for representation learning research, that offer both controllability and realism. We use the Unreal Engine, a powerful game engine well known in the entertainment industry, to produce PUG (Photorealistic Unreal Graphics) environments and datasets for representation learning. In this paper, we demonstrate the potential of PUG to enable more rigorous evaluations of vision models. The datasets can be downloaded at https://pug.metademolab.com/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.09073",
        "title": "Consensus and Subjectivity of Skin Tone Annotation for ML Fairness",
        "abstract": "Understanding different human attributes and how they affect model behavior may become a standard need for all model creation and usage, from traditional computer vision tasks to the newest multimodal generative AI systems. In computer vision specifically, we have relied on datasets augmented with perceived attribute signals (e.g., gender presentation, skin tone, and age) and benchmarks enabled by these datasets. Typically labels for these tasks come from human annotators. However, annotating attribute signals, especially skin tone, is a difficult and subjective task. Perceived skin tone is affected by technical factors, like lighting conditions, and social factors that shape an annotator\u2019s lived experience.\nThis paper examines the subjectivity of skin tone annotation through a series of annotation experiments using the Monk Skin Tone (MST) scale [59], a small pool of professional photographers, and a much larger pool of trained crowdsourced annotators. Along with this study we release the Monk Skin Tone Examples (MST- E) dataset, containing 1515 images and 31 videos spread across the full MST scale. MST-E is designed to help train human annotators to annotate MST effectively. Our study shows that annotators can reliably annotate skin tone in a way that aligns with an expert in the MST scale, even under challenging environmental conditions. We also find evidence that annotators from different geographic regions rely on different mental models of MST categories resulting in annotations that systematically vary across regions. Given this, we advise practitioners to use a diverse set of annotators and a higher replication count for each image when annotating skin tone for fairness research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.11249",
        "title": "OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning",
        "abstract": "Spatio-temporal predictive learning is a learning paradigm that enables models to learn spatial and temporal patterns by predicting future frames from given past frames in an unsupervised manner. Despite remarkable progress in recent years, a lack of systematic understanding persists due to the diverse settings, complex implementation, and difficult reproducibility. Without standardization, comparisons can be unfair and insights inconclusive. To address this dilemma, we propose OpenSTL, a comprehensive benchmark for spatio-temporal predictive learning that categorizes prevalent approaches into recurrent-based and recurrent-free models. OpenSTL provides a modular and extensible framework implementing various state-of-the-art methods. We conduct standard evaluations on datasets across various domains, including synthetic moving object trajectory, human motion, driving scenes, traffic flow, and weather forecasting. Based on our observations, we provide a detailed analysis of how model architecture and dataset properties affect spatio-temporal predictive learning performance. Surprisingly, we find that recurrent-free models achieve a good balance between efficiency and performance than recurrent models. Thus, we further extend the common MetaFormers to boost recurrent-free spatial-temporal predictive learning. We open-source the code and models at https://github.com/chengtan9907/OpenSTL.Most neural networks assume that input images have a fixed number of channels (three for RGB images). However, there are many settings where the number of channels may vary, such as microscopy images where the number of channels changes depending on instruments and experimental goals. Yet, there has not been a systemic attempt to create and evaluate neural networks that are invariant to the number and type of channels. As a result, trained models remain specific to individual studies and are hardly reusable for other microscopy settings. In this paper, we present a benchmark for investigating channel-adaptive models in microscopy imaging, which consists of 1) a dataset of varied-channel single-cell im- ages, and 2) a biologically relevant evaluation framework. In addition, we adapted several existing techniques to create channel-adaptive models and compared their performance on this benchmark to fixed-channel, baseline models. We find that channel-adaptive models can generalize better to out-of-domain tasks and can be computationally efficient. We contribute a curated dataset1 and an evaluation API2 to facilitate objective comparisons in future research and applications.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.19224",
        "title": "CHAMMI: A benchmark for channel-adaptive models in microscopy imaging",
        "abstract": "Most neural networks assume that input images have a fixed number of channels (three for RGB images). However, there are many settings where the number of channels may vary, such as microscopy images where the number of channels changes depending on instruments and experimental goals. Yet, there has not been a systemic attempt to create and evaluate neural networks that are invariant to the number and type of channels. As a result, trained models remain specific to individual studies and are hardly reusable for other microscopy settings. In this paper, we present a benchmark for investigating channel-adaptive models in microscopy imaging, which consists of 1) a dataset of varied-channel single-cell im- ages, and 2) a biologically relevant evaluation framework. In addition, we adapted several existing techniques to create channel-adaptive models and compared their performance on this benchmark to fixed-channel, baseline models. We find that channel-adaptive models can generalize better to out-of-domain tasks and can be computationally efficient. We contribute a curated dataset1 and an evaluation API2 to facilitate objective comparisons in future research and applications.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.11513",
        "title": "GENEVAL: An Object-Focused Framework for Evaluating Text-to-Image Alignment",
        "abstract": "Recent breakthroughs in diffusion models, multimodal pretraining, and efficient finetuning have led to an explosion of text-to-image generative models. Given human evaluation is expensive and difficult to scale, automated methods are critical for evaluating the increasingly large number of new models. However, most current automated evaluation metrics like FID or CLIPScore only offer a holistic measure of image quality or image-text alignment, and are unsuited for fine-grained or instance- level analysis. In this paper, we introduce GENEVAL, an object-focused framework to evaluate compositional image properties such as object co-occurrence, position, count, and color. We show that current object detection models can be leveraged to evaluate text-to-image models on a variety of generation tasks with strong human agreement, and that other discriminative vision models can be linked to this pipeline to further verify properties like object color. We then evaluate several open- source text-to-image models and analyze their relative generative capabilities on our benchmark. We find that recent models demonstrate significant improvement on these tasks, though they are still lacking in complex capabilities such as spatial relations and attribute binding. Finally, we demonstrate how GENEVAL might be used to help discover existing failure modes, in order to inform development of the next generation of text-to-image models. Our code to run the GENEVAL framework is publicly available at https://github.com/djghosh13/geneval.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.16527",
        "title": "OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents",
        "abstract": "Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks. However, the datasets used to train these models have not been released, and the collection process has not been fully specified. We introduce the OBELICS dataset, an open web-scale filtered dataset of in- terleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We describe the dataset creation process, present comprehensive filtering rules, and provide an analysis of the dataset\u2019s content. To show the viability of OBELICS, we train vision and language models of 9 and 80 billion parameters named IDEFICS, and obtain competitive performance on different multimodal benchmarks. We release our dataset, models and code.1.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.16186",
        "title": "Large-scale Training Data Search for Object Re-identification",
        "abstract": "We consider a scenario where we have access to the tar- get domain, but cannot afford on-the-fly training data an- notation, and instead would like to construct an alternative training set from a large-scale data pool such that a com- petitive model can be obtained. We propose a search and pruning (SnP) solution to this training data search prob- lem, tailored to object re-identification (re-ID), an appli- cation aiming to match the same object captured by differ- ent cameras. Specifically, the search stage identifies and merges clusters of source identities which exhibit similar distributions with the target domain. The second stage, subject to a budget, then selects identities and their im- ages from the Stage I output, to control the size of the re- sulting training set for efficient training. The two steps provide us with training sets 80% smaller than the source pool while achieving a similar or even higher re-ID accu- racy. These training sets are also shown to be superior to a few existing search methods such as random sampling and greedy sampling under the same budget on training data size. If we release the budget, training sets resulting from the first stage alone allow even higher re-ID accu- racy. We provide interesting discussions on the specificity of our method to the re-ID problem and particularly its role in bridging the re-ID domain gap. The code is available at https://github.com/yorkeyao/SnP",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.05938",
        "title": "V2X-Seq: A Large-Scale Sequential Dataset for Vehicle-Infrastructure Cooperative Perception and Forecasting",
        "abstract": "Utilizing infrastructure and vehicle-side information to track and forecast the behaviors of surrounding traf- fic participants can significantly improve decision-making and safety in autonomous driving. However, the lack of real-world sequential datasets limits research in this area. To address this issue, we introduce V2X-Seq, the first large-scale sequential V2X dataset, which includes data frames, trajectories, vector maps, and traffic lights captured from natural scenery. V2X-Seq comprises two parts: the sequential perception dataset, which includes more than 15,000 frames captured from 95 scenarios, and the trajectory forecasting dataset, which contains about 80,000 infrastructure-view scenarios, 80,000 vehicle-view scenarios, and 50,000 cooperative-view scenarios cap- tured from 28 intersections\u2019 areas, covering 672 hours of data. Based on V2X-Seq, we introduce three new tasks for vehicle-infrastructure cooperative (VIC) autonomous driv- ing: VIC3D Tracking, Online-VIC Forecasting, and Offline- VIC Forecasting. We also provide benchmarks for the intro- duced tasks. Find data, code, and more up-to-date informa- tion at https://github.com/AIR-THU/DAIR-V2X-Seq.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2212.06152",
        "title": "Accelerating Dataset Distillation via Model Augmentation",
        "abstract": "Dataset Distillation (DD), a newly emerging field, aims at generating much smaller but efficient synthetic training datasets from large ones. Existing DD methods based on gradient matching achieve leading performance; however, they are extremely computationally intensive as they re- quire continuously optimizing a dataset among thousands of randomly initialized models. In this paper, we assume that training the synthetic data with diverse models leads to better generalization performance. Thus we propose two model augmentation techniques, i.e. using early-stage models and parameter perturbation to learn an informative synthetic set with significantly reduced training cost. Exten- sive experiments demonstrate that our method achieves up to 20\u00d7 speedup and comparable performance on par with state-of-the-art methods.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.17096",
        "title": "ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing",
        "abstract": "Recent studies have shown that higher accuracy on Im- ageNet usually leads to better robustness against differ- ent corruptions. Therefore, in this paper, instead of fol- lowing the traditional research paradigm that investigates new out-of-distribution corruptions or perturbations deep models may encounter, we conduct model debugging in in- distribution data to explore which object attributes a model may be sensitive to. To achieve this goal, we create a toolkit for object editing with controls of backgrounds, sizes, po- sitions, and directions, and create a rigorous benchmark named ImageNet-E(diting) for evaluating the image clas- sifier robustness in terms of object attributes. With our ImageNet-E, we evaluate the performance of current deep learning models, including both convolutional neural net- works and vision transformers. We find that most models are quite sensitive to attribute changes. A small change in the background can lead to an average of 9.23% drop on top-1 accuracy. We also evaluate some robust models including both adversarially trained models and other ro- bust trained models and find that some models show worse robustness against attribute changes than vanilla models. Based on these findings, we discover ways to enhance at- tribute robustness with preprocessing, architecture designs, and training strategies. We hope this work can provide some insights to the community and open up a new av- enue for research in robust computer vision. The code and dataset are available at https://github.com/ alibaba/easyrobust.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.01112",
        "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
        "abstract": "Formula-driven supervised learning (FDSL) has been shown to be an effective method for pre-training vision transformers, where ExFractalDB-21k was shown to exceed the pre-training effect of ImageNet-21k. These studies also indicate that contours mattered more than textures when pre-training vision transformers. However, the lack of a systematic investigation as to why these contour-oriented synthetic datasets can achieve the same accuracy as real datasets leaves much room for skepticism. In the present work, we develop a novel methodology based on circular harmonics for systematically investigating the design space of contour-oriented synthetic datasets. This allows us to efficiently search the optimal range of FDSL parameters and maximize the variety of synthetic images in the dataset, which we found to be a critical factor. When the resulting new dataset VisualAtom-21k is used for pre-training ViT- Base, the top-1 accuracy reached 83.7% when fine-tuning on ImageNet-1k. This is close to the top-1 accuracy (84.2%) achieved by JFT-300M pre-training, while the number of images is 1/14. Unlike JFT-300M which is a static dataset, the quality of synthetic datasets will continue to improve, and the current work is a testament to this possibility. FDSL is also free of the common issues associated with real im- ages, e.g. privacy/copyright issues, labeling costs/errors, and ethical biases.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09310",
        "title": "Infinite Photorealistic Worlds using Procedural Generation",
        "abstract": "We introduce Infinigen, a procedural generator of photo- realistic 3D scenes of the natural world. Infinigen is entirely procedural: every asset, from shape to texture, is generated from scratch via randomized mathematical rules, using no external source and allowing infinite variation and composi- tion. Infinigen offers broad coverage of objects and scenes in the natural world including plants, animals, terrains, and natural phenomena such as fire, cloud, rain, and snow. In- finigen can be used to generate unlimited, diverse training data for a wide range of computer vision tasks including object detection, semantic segmentation, optical flow, and 3D reconstruction. We expect Infinigen to be a useful re- source for computer vision research and beyond. Please visit infinigen.org for videos, code and pre-generated data.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.14717",
        "title": "CelebV-Text: A Large-Scale Facial Text-Video Dataset",
        "abstract": "Text-driven generation models are flourishing in video generation and editing. However, face-centric text-to-video generation remains a challenge due to the lack of a suitable dataset containing high-quality videos and highly relevant texts. This paper presents CelebV-Text, a large-scale, di- verse, and high-quality dataset of facial text-video pairs, to facilitate research on facial text-to-video generation tasks. CelebV-Text comprises 70,000 in-the-wild face video clips with diverse visual content, each paired with 20 texts gen- erated using the proposed semi-automatic text generation strategy. The provided texts are of high quality, describ- ing both static and dynamic attributes precisely. The supe- riority of CelebV-Text over other datasets is demonstrated via comprehensive statistical analysis of the videos, texts, and text-video relevance. The effectiveness and potential of CelebV-Text are further shown through extensive self- evaluation. A benchmark is constructed with representa- tive methods to standardize the evaluation of the facial text- to-video generation task. All data and models are publicly available",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.01943",
        "title": "Spring: A High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo",
        "abstract": "While recent methods for motion and stereo estimation recover an unprecedented amount of details, such highly detailed structures are neither adequately reflected in the data of existing benchmarks nor their evaluation methodol- ogy. Hence, we introduce Spring \u2013 a large, high-resolution, high-detail, computer-generated benchmark for scene flow, optical flow, and stereo. Based on rendered scenes from the open-source Blender movie \u201cSpring\u201d, it provides photo- realistic HD datasets with state-of-the-art visual effects and ground truth training data. Furthermore, we provide a web- site to upload, analyze and compare results. Using a novel evaluation methodology based on a super-resolved UHD ground truth, our Spring benchmark can assess the quality of fine structures and provides further detailed performance statistics on different image regions. Regarding the num- ber of ground truth frames, Spring is 60\u00d7 larger than the only scene flow benchmark, KITTI 2015, and 15\u00d7 larger than the well-established MPI Sintel optical flow bench- mark. Initial results for recent methods on our benchmark show that estimating fine details is indeed challenging, as their accuracy leaves significant room for improvement. The Spring benchmark and the corresponding datasets are available at http://spring-benchmark.org.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2302.11217",
        "title": "Connecting Vision and Language with Video Localized Narratives",
        "abstract": "We propose Video Localized Narratives, a new form of multimodal video annotations connecting vision and lan- guage. In the original Localized Narratives [40], annota- tors speak and move their mouse simultaneously on an im- age, thus grounding each word with a mouse trace segment. However, this is challenging on a video. Our new protocol empowers annotators to tell the story of a video with Local- ized Narratives, capturing even complex events involving multiple actors interacting with each other and with sev- eral passive objects. We annotated 20k videos of the OVIS, UVO, and Oops datasets, totalling 1.7M words. Based on this data, we also construct new benchmarks for the video narrative grounding and video question answering tasks, and provide reference results from strong baseline models. Our annotations are available at https://google. github.io/video-localized-narratives/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.15166",
        "title": "Towards Artistic Image Aesthetics Assessment: a Large-scale Dataset and a New Method",
        "abstract": "Image aesthetics assessment (IAA) is a challenging task due to its highly subjective nature. Most of the current stud- ies rely on large-scale datasets (e.g., AVA and AADB) to learn a general model for all kinds of photography images. However, little light has been shed on measuring the aes- thetic quality of artistic images, and the existing datasets only contain relatively few artworks. Such a defect is a great obstacle to the aesthetic assessment of artistic images. To fill the gap in the field of artistic image aesthetics assess- ment (AIAA), we first introduce a large-scale AIAA dataset: Boldbrush Artistic Image Dataset (BAID), which consists of 60,337 artistic images covering various art forms, with more than 360,000 votes from online users. We then pro- pose a new method, SAAN (Style-specific Art Assessment Network), which can effectively extract and utilize style- specific and generic aesthetic information to evaluate artis- tic images. Experiments demonstrate that our proposed approach outperforms existing IAA methods on the pro- posed BAID dataset according to quantitative comparisons. We believe the proposed dataset and method can serve as a foundation for future AIAA works and inspire more re- search in this field. Dataset and code are available at: https://github.com/Dreemurr-T/BAID.git",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.14933",
        "title": "MD-VQA: Multi-Dimensional Quality Assessment for UGC Live Videos",
        "abstract": "User-generated content (UGC) live videos are often bothered by various distortions during capture procedures and thus exhibit diverse visual qualities. Such source videos are further compressed and transcoded by media server providers before being distributed to end-users. Because of the flourishing of UGC live videos, effective video qual- ity assessment (VQA) tools are needed to monitor and per- ceptually optimize live streaming videos in the distributing process. In this paper, we address UGC Live VQA prob- lems by constructing a first-of-a-kind subjective UGC Live VQA database and developing an effective evaluation tool. Concretely, 418 source UGC videos are collected in real live streaming scenarios and 3,762 compressed ones at dif- ferent bit rates are generated for the subsequent subjective VQA experiments. Based on the built database, we de- velop a Multi-Dimensional VQA (MD-VQA) evaluator to measure the visual quality of UGC live videos from seman- tic, distortion, and motion aspects respectively. Extensive experimental results show that MD-VQA achieves state-of- the-art performance on both our UGC Live VQA database and existing compressed UGC VQA databases.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2212.08051",
        "title": "Objaverse: A Universe of Annotated 3D Objects",
        "abstract": "Massive data corpora like WebText, Wikipedia, Concep- tual Captions, WebImageText, and LAION have propelled recent dramatic progress in AI. Large neural models trained on such datasets produce impressive results and top many of today\u2019s benchmarks. A notable omission within this fam- ily of large-scale datasets is 3D data. Despite considerable interest and potential applications in 3D vision, datasets of high-fidelity 3D models continue to be mid-sized with limited diversity of object categories. Addressing this gap, we present Objaverse 1.0, a large dataset of objects with 800K+ (and growing) 3D models with descriptive captions, tags, and animations. Objaverse improves upon present day 3D repositories in terms of scale, number of categories, and in the visual diversity of instances within a category. We demonstrate the large potential of Objaverse via four diverse applications: training generative 3D models, improving tail category segmentation on the LVIS benchmark, training open-vocabulary object-navigation models for Em- bodied AI, and creating a new benchmark for robustness analysis of vision models. Objaverse can open new direc- tions for research and enable new applications across the field of AI.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.05633",
        "title": "Habitat-Matterport 3D Semantics Dataset",
        "abstract": "We present the Habitat-Matterport 3D Semantics (HM3DSEM) dataset. HM3DSEM is the largest dataset of 3D real-world spaces with densely annotated seman- tics that is currently available to the academic commu- nity. It consists of 142,646 object instance annotations across 216 3D spaces and 3,100 rooms within those spaces. The scale, quality, and diversity of object annotations far exceed those of prior datasets. A key difference setting apart HM3DSEM from other datasets is the use of tex- ture information to annotate pixel-accurate object bound- aries. We demonstrate the effectiveness of HM3DSEM dataset for the Object Goal Navigation task using differ- ent methods. Policies trained using HM3DSEM perform outperform those trained on prior datasets. Introduction of HM3DSEM in the Habitat ObjectNav Challenge lead to an increase in participation from 400 submissions in 2021 to 1022 submissions in 2022. Project page: https: //aihabitat.org/datasets/hm3d-semantics/",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.01932",
        "title": "MobileBrick: Building LEGO for 3D Reconstruction on Mobile Devices",
        "abstract": "High-quality 3D ground-truth shapes are critical for 3D object reconstruction evaluation. However, it is difficult to create a replica of an object in reality, and even 3D re- constructions generated by 3D scanners have artefacts that cause biases in evaluation. To address this issue, we in- troduce a novel multi-view RGBD dataset captured using a mobile device, which includes highly precise 3D ground- truth annotations for 153 object models featuring a diverse set of 3D structures. We obtain precise 3D ground-truth shape without relying on high-end 3D scanners by utilizing LEGO models with known geometry as the 3D structures for image capture. The distinct data modality offered by high- resolution RGB images and low-resolution depth maps cap- tured on a mobile device, when combined with precise 3D geometry annotations, presents a unique opportunity for fu- ture research on high-fidelity 3D reconstruction. Further- more, we evaluate a range of 3D reconstruction algorithms on the proposed dataset.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.15443",
        "title": "GeoNet: Benchmarking Unsupervised Adaptation across Geographies",
        "abstract": "In recent years, several efforts have been aimed at im- proving the robustness of vision models to domains and environments unseen during training. An important practi- cal problem pertains to models deployed in a new geography that is under-represented in the training dataset, posing a direct challenge to fair and inclusive computer vision. In this paper, we study the problem of geographic robust- ness and make three main contributions. First, we intro- duce a large-scale dataset GeoNet for geographic adapta- tion containing benchmarks across diverse tasks like scene recognition (GeoPlaces), image classification (GeoImNet) and universal adaptation (GeoUniDA). Second, we inves- tigate the nature of distribution shifts typical to the prob- lem of geographic adaptation and hypothesize that the ma- jor source of domain shifts arise from significant varia- tions in scene context (context shift), object design (de- sign shift) and label distribution (prior shift) across ge- ographies. Third, we conduct an extensive evaluation of several state-of-the-art unsupervised domain adaptation al- gorithms and architectures on GeoNet, showing that they do not suffice for geographical adaptation, and that large-scale pre-training using large vision models also does not lead to geographic robustness. Our dataset is publicly available at https://tarun005.github.io/GeoNet.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2302.11102",
        "title": "Logical Consistency and Greater Descriptive Power for Facial Hair Attribute Learning",
        "abstract": "Face attribute research has so far used only simple bi- nary attributes for facial hair; e.g., beard / no beard. We have created a new, more descriptive facial hair annotation scheme and applied it to create a new facial hair attribute dataset, FH37K. Face attribute research also so far has not dealt with logical consistency and completeness. For ex- ample, in prior research, an image might be classified as both having no beard and also having a goatee (a type of beard). We show that the test accuracy of previous classifi- cation methods on facial hair attribute classification drops significantly if logical consistency of classifications is en- forced. We propose a logically consistent prediction loss, LCPLoss, to aid learning of logical consistency across at- tributes, and also a label compensation training strategy to eliminate the problem of no positive prediction across a set of related attributes. Using an attribute classifier trained on FH37K, we investigate how facial hair affects face recognition accuracy, including variation across de- mographics. Results show that similarity and difference in facial hairstyle have important effects on the impostor and genuine score distributions in face recognition. The code is at https://github.com/HaiyuWu/LogicalConsistency.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2301.01795",
        "title": "PACO: Parts and Attributes of Common Objects",
        "abstract": "Object models are gradually progressing from predict- ing just category labels to providing detailed descriptions of object instances. This motivates the need for large datasets which go beyond traditional object masks and pro- vide richer annotations such as part masks and attributes. Hence, we introduce PACO: Parts and Attributes of Com- mon Objects. It spans 75 object categories, 456 object- part categories and 55 attributes across image (LVIS) and video (Ego4D) datasets. We provide 641K part masks an- notated across 260K object boxes, with roughly half of them exhaustively annotated with attributes as well. We design evaluation metrics and provide benchmark results for three tasks on the dataset: part mask segmentation, ob- ject and part attribute prediction and zero-shot instance de- tection. Dataset, models, and code are open-sourced at https://github.com/facebookresearch/paco.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09780",
        "title": "Understanding Deep Generative Models with Generalized Empirical Likelihoods",
        "abstract": "Understanding how well a deep generative model captures a distribution of high-dimensional data remains an important open challenge. It is especially difficult for certain model classes, such as Generative Adversarial Networks and Diffu- sion Models, whose models do not admit exact likelihoods. In this work, we demonstrate that generalized empirical like- lihood (GEL) methods offer a family of diagnostic tools that can identify many deficiencies of deep generative models (DGMs). We show, with appropriate specification of moment conditions, that the proposed method can identify which modes have been dropped, the degree to which DGMs are mode imbalanced, and whether DGMs sufficiently capture intra-class diversity. We show how to combine techniques from Maximum Mean Discrepancy and Generalized Empiri- cal Likelihood to create not only distribution tests that retain per-sample interpretability, but also metrics that include la- bel information. We find that such tests predict the degree of mode dropping and mode imbalance up to 60% better than metrics such as improved precision/recall. We provide an im- plementation at https://github.com/deepmind/ understanding _ deep _ generative _ models _ with_generalized_empirical_likelihood/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.10036",
        "title": "Visual DNA:\nRepresenting and Comparing Images using Distributions of Neuron Activations",
        "abstract": "Selecting appropriate datasets is critical in modern com- puter vision. However, no general-purpose tools exist to evaluate the extent to which two datasets differ. For this, we propose representing images \u2013 and by extension datasets \u2013 using Distributions of Neuron Activations (DNAs). DNAs fit distributions, such as histograms or Gaussians, to activa- tions of neurons in a pre-trained feature extractor through which we pass the image(s) to represent. This extractor is frozen for all datasets, and we rely on its generally expres- sive power in feature space. By comparing two DNAs, we can evaluate the extent to which two datasets differ with granular control over the comparison attributes of inter- est, providing the ability to customise the way distances are measured to suit the requirements of the task at hand. Furthermore, DNAs are compact, representing datasets of any size with less than 15 megabytes. We demonstrate the value of DNAs by evaluating their applicability on several tasks, including conditional dataset comparison, synthetic image evaluation, and transfer learning, and across diverse datasets, ranging from synthetic cat images to celebrity faces and urban driving scenes.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2211.12914",
        "title": "Open-vocabulary Attribute Detection",
        "abstract": "Vision-language modeling has enabled open-vocabulary tasks where predictions can be queried using any text prompt in a zero-shot manner. Existing open-vocabulary tasks focus on object classes, whereas research on object attributes is limited due to the lack of a reliable attribute- focused evaluation benchmark. This paper introduces the Open-Vocabulary Attribute Detection (OVAD) task and the corresponding OVAD benchmark. The objective of the novel task and benchmark is to probe object-level attribute information learned by vision-language models. To this end, we created a clean and densely annotated test set cov- ering 117 attribute classes on the 80 object classes of MS COCO. It includes positive and negative annotations, which enables open-vocabulary evaluation. Overall, the bench- mark consists of 1.4 million annotations. For reference, we provide a first baseline method for open-vocabulary at- tribute detection. Moreover, we demonstrate the bench- mark\u2019s value by studying the attribute detection perfor- mance of several foundation models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.10448",
        "title": "ReLight My NeRF: A Dataset for\nNovel View Synthesis and Relighting of Real World Objects",
        "abstract": "In this paper, we focus on the problem of rendering novel views from a Neural Radiance Field (NeRF) under unob- served light conditions. To this end, we introduce a novel dataset, dubbed ReNe (Relighting NeRF), framing real world objects under one-light-at-time (OLAT) conditions, annotated with accurate ground-truth camera and light poses. Our acquisition pipeline leverages two robotic arms holding, respectively, a camera and an omni-directional point-wise light source. We release a total of 20 scenes depicting a variety of objects with complex geometry and challenging materials. Each scene includes 2000 images, acquired from 50 different points of views under 40 different OLAT conditions. By leveraging the dataset, we perform an ablation study on the relighting capability of variants of the vanilla NeRF architecture and identify a lightweight archi- tecture that can render novel views of an object under novel light conditions, which we use to establish a non-trivial baseline for the dataset. Dataset and benchmark are avail- able at https://eyecan-ai.github.io/rene.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2302.09997",
        "title": "A Large-Scale Homography Benchmark",
        "abstract": "We present a large-scale dataset of Planes in 3D, Pi3D, of roughly 1000 planes observed in 10 000 images from the 1DSfM dataset, and HEB, a large-scale homography estimation benchmark leveraging Pi3D. The applications of the Pi3D dataset are diverse, e.g. training or evaluat- ing monocular depth, surface normal estimation and image matching algorithms. The HEB dataset consists of 226 260 homographies and includes roughly 4M correspondences. The homographies link images that often undergo signifi- cant viewpoint and illumination changes. As applications of HEB, we perform a rigorous evaluation of a wide range of robust estimators and deep learning-based correspon- dence filtering methods, establishing the current state-of- the-art in robust homography estimation. We also evalu- ate the uncertainty of the SIFT orientations and scales w.r.t. the ground truth coming from the underlying homographies and provide codes for comparing uncertainty of custom de- tectors. The dataset is available at https://github. com/danini/homography-benchmark.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2211.13190",
        "title": "BiasBed \u2013 Rigorous Texture Bias Evaluation",
        "abstract": "The well-documented presence of texture bias in modern convolutional neural networks has led to a plethora of al- gorithms that promote an emphasis on shape cues, often to support generalization to new domains. Yet, common datasets, benchmarks and general model selection strate- gies are missing, and there is no agreed, rigorous evaluation protocol. In this paper, we investigate difficulties and limi- tations when training networks with reduced texture bias. In particular, we also show that proper evaluation and mean- ingful comparisons between methods are not trivial. We introduce BiasBed, a testbed for texture- and style-biased training, including multiple datasets and a range of exist- ing algorithms. It comes with an extensive evaluation pro- tocol that includes rigorous hypothesis testing to gauge the significance of the results, despite the considerable train- ing instability of some style bias methods. Our extensive experiments, shed new light on the need for careful, sta- tistically founded evaluation protocols for style bias (and beyond). E.g., we find that some algorithms proposed in the literature do not significantly mitigate the impact of style bias at all. With the release of BiasBed, we hope to fos- ter a common understanding of consistent and meaning- ful comparisons, and consequently faster progress towards learning methods free of texture bias. Code is available at https://github.com/D1noFuzi/BiasBed",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.01816",
        "title": "Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation",
        "abstract": "Human evaluation is critical for validating the perfor- mance of text-to-image generative models, as this highly cognitive process requires deep comprehension of text and images. However, our survey of 37 recent papers reveals that many works rely solely on automatic measures (e.g., FID) or perform poorly described human evaluations that are not reliable or repeatable. This paper proposes a stan- dardized and well-defined human evaluation protocol to fa- cilitate verifiable and reproducible human evaluation in fu- ture works. In our pilot data collection, we experimentally show that the current automatic measures are incompatible with human perception in evaluating the performance of the text-to-image generation results. Furthermore, we provide insights for designing human evaluation experiments reli- ably and conclusively. Finally, we make several resources publicly available to the community to facilitate easy and fast implementations.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.13611",
        "title": "A New Comprehensive Benchmark for Semi-supervised Video Anomaly Detection and Anticipation",
        "abstract": "Semi-supervised video anomaly detection (VAD) is a critical task in the intelligent surveillance system. How- ever, an essential type of anomaly in VAD named scene- dependent anomaly has not received the attention of re- searchers. Moreover, there is no research investigating anomaly anticipation, a more significant task for preventing the occurrence of anomalous events. To this end, we pro- pose a new comprehensive dataset, NWPU Campus, con- taining 43 scenes, 28 classes of abnormal events, and 16 hours of videos. At present, it is the largest semi-supervised VAD dataset with the largest number of scenes and classes of anomalies, the longest duration, and the only one con- sidering the scene-dependent anomaly. Meanwhile, it is also the first dataset proposed for video anomaly antici- pation. We further propose a novel model capable of de- tecting and anticipating anomalous events simultaneously. Compared with 7 outstanding VAD algorithms in recent years, our method can cope with scene-dependent anomaly detection and anomaly anticipation both well, achieving state-of-the-art performance on ShanghaiTech, CUHK Av- enue, IITB Corridor and the newly proposed NWPU Cam- pus datasets consistently. Our dataset and code is available at: https://campusvad.github.io.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.16940",
        "title": "BEDLAM: A Synthetic Dataset of\nBodies Exhibiting Detailed Lifelike Animated Motion",
        "abstract": "We show, for the first time, that neural networks trained only on synthetic data achieve state-of-the-art accuracy on the problem of 3D human pose and shape (HPS) estima- tion from real images. Previous synthetic datasets have been small, unrealistic, or lacked realistic clothing. Achiev- ing sufficient realism is non-trivial and we show how to do this for full bodies in motion. Specifically, our BED- LAM dataset contains monocular RGB videos with ground- truth 3D bodies in SMPL-X format. It includes a diver- sity of body shapes, motions, skin tones, hair, and cloth- ing. The clothing is realistically simulated on the moving bodies using commercial clothing physics simulation. We render varying numbers of people in realistic scenes with varied lighting and camera motions. We then train vari- ous HPS regressors using BEDLAM and achieve state-of- the-art accuracy on real-image benchmarks despite train- ing with synthetic data. We use BEDLAM to gain insights\ninto what model design choices are important for accu- racy. With good synthetic training data, we find that a basic method like HMR approaches the accuracy of the current SOTA method (CLIFF). BEDLAM is useful for a variety of tasks and all images, ground truth bodies, 3D clothing, support code, and more are available for research purposes. Additionally, we provide detailed information about our synthetic data generation pipeline, enabling oth- ers to generate their own datasets. See the project page: https://bedlam.is.tue.mpg.de/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.14968",
        "title": "Blind Image Quality Assessment via Vision-Language Correspondence: A Multitask Learning Perspective",
        "abstract": "We aim at advancing blind image quality assessment (BIQA), which predicts the human perception of image quality without any reference information. We develop a general and automated multitask learning scheme for BIQA to exploit auxiliary knowledge from other tasks, in a way that the model parameter sharing and the loss weighting are determined automatically. Specifically, we first describe all candidate label combinations (from multiple tasks) us- ing a textual template, and compute the joint probability from the cosine similarities of the visual-textual embed- dings. Predictions of each task can be inferred from the joint distribution, and optimized by carefully designed loss functions. Through comprehensive experiments on learn- ing three tasks - BIQA, scene classification, and distor- tion type identification, we verify that the proposed BIQA method 1) benefits from the scene classification and dis- tortion type identification tasks and outperforms the state- of-the-art on multiple IQA datasets, 2) is more robust in the group maximum differentiation competition, and 3) re- aligns the quality annotations from different IQA datasets more effectively. The source code is available at https: //github.com/zwx8981/LIQE .",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.01946",
        "title": "AFFECTION: LEARNING AFFECTIVE EXPLANATIONS FOR REAL-WORLD VISUAL DATA",
        "abstract": "In this work, we explore the emotional reactions that real-world images tend to induce by using natural language as the medium to express the rationale behind an affective response to a given visual stimulus. To embark on this journey, we introduce and share with the research community a large-scale dataset that contains emotional reactions and free-form textual explanations for 85,007 publicly available images, analyzed by 6,283 annotators who were asked to indicate and explain how and why they felt in a particular way when observing a particular image, producing a total of 526,749 responses. Even though emotional reactions are subjective and sensitive to context (personal mood, social status, past experiences) \u2013 we show that there is significant common ground to capture potentially plausible emotional responses with a large support in the subject population. In light of this key observation, we ask the following questions: i) Can we develop multi-modal neural networks that provide reasonable affective responses to real-world visual data, explained with language? ii) Can we steer such methods towards producing explanations with varying degrees of pragmatic language or justifying different emotional reactions while adapting to the underlying visual stimulus? Finally, iii) How can we evaluate the performance of such methods for this novel task? With this work, we take the first steps in addressing all of these questions, thus paving the way for richer, more human-centric, and emotionally-aware image analysis systems. Our introduced dataset and all developed methods are available on https://affective-explanations.org.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2207.01398",
        "title": "A LARGE-SCALE ROBUSTNESS ANALYSIS OF VIDEO ACTION RECOGNITION MODELS",
        "abstract": "We have seen a great progress in video action recognition in recent years. There are several models based on convolutional neural network (CNN) and some recent transformer based approaches which provide top performance on existing benchmarks. In this work, we perform a large-scale robustness analysis of these existing models for video action recognition. We focus on robustness against real- world distribution shift perturbations instead of adversarial perturbations. We propose four different benchmark datasets, HMDB51-P, UCF101-P, Kinetics400-P, and SSv2-P to perform this analysis. We study robustness of six state-of-the-art action recognition models against 90 different perturbations. The study reveals some interesting findings, 1) transformer based models are consistently more robust compared to CNN based models, 2) Pretraining improves robustness for Transformer based models more than CNN based models, and 3) All of the studied models are robust to temporal perturbations for all datasets but SSv2; suggesting the importance of temporal information for action recognition varies based on the dataset and activities. Next, we study the role of augmentations in model robustness and present a real-world dataset, UCF101-DS, which contains realistic distribution shifts, to further validate some of these findings. We believe this study will serve as a benchmark for future research in robust video action recognition",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.00576",
        "title": "MammalNet: A Large-scale Video Benchmark\nfor Mammal Recognition and Behavior Understanding",
        "abstract": "Monitoring animal behavior can facilitate conservation efforts by providing key insights into wildlife health, popula- tion status, and ecosystem function. Automatic recognition of animals and their behaviors is critical for capitalizing on the large unlabeled datasets generated by modern video devices and for accelerating monitoring efforts at scale. However, the development of automated recognition systems is cur- rently hindered by a lack of appropriately labeled datasets. Existing video datasets 1) do not classify animals according to established biological taxonomies; 2) are too small to fa- cilitate large-scale behavioral studies and are often limited to a single species; and 3) do not feature temporally localized annotations and therefore do not facilitate localization of targeted behaviors within longer video sequences. Thus, we propose MammalNet, a new large-scale animal behav- ior dataset with taxonomy-guided annotations of mammals and their common behaviors. MammalNet contains over 18K videos totaling 539 hours, which is \u223c10 times larger than the largest existing animal behavior dataset [36]. It covers 17 orders, 69 families, and 173 mammal categories for animal categorization and captures 12 high-level animal behaviors that received focus in previous animal behavior studies. We establish three benchmarks on MammalNet: standard animal and behavior recognition, compositional low-shot animal and behavior recognition, and behavior detection. Our dataset and code have been made available at: https://mammal-net.github.io.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.14152",
        "title": "Fantastic Breaks: A Dataset of Paired 3D Scans of Real-World Broken Objects and Their Complete Counterparts",
        "abstract": "Automated shape repair approaches currently lack ac- cess to datasets that describe real-world damaged geome- try. We present Fantastic Breaks (and Where to Find Them: https://terascale-all-sensing-research- studio.github.io/FantasticBreaks), a dataset containing scanned, waterproofed, and cleaned 3D meshes for 150 broken objects, paired and geometrically aligned with complete counterparts. Fantastic Breaks contains class and material labels, proxy repair parts that join to broken meshes to generate complete meshes, and manually anno- tated fracture boundaries. Through a detailed analysis of fracture geometry, we reveal differences between Fantastic Breaks and synthetic fracture datasets generated using ge- ometric and physics-based methods. We show experimental shape repair evaluation with Fantastic Breaks using mul- tiple learning-based approaches pre-trained with synthetic datasets and re-trained with subset of Fantastic Breaks",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.05947",
        "title": "Visual Localization using Imperfect 3D Models from the Internet",
        "abstract": "Visual localization is a core component in many applica- tions, including augmented reality (AR). Localization algo- rithms compute the camera pose of a query image w.r.t. a scene representation, which is typically built from images. This often requires capturing and storing large amounts of data, followed by running Structure-from-Motion (SfM) algorithms. An in- teresting, and underexplored, source of data for building scene representations are 3D models that are readily available on the Internet, e.g., hand-drawn CAD models, 3D models generated from building footprints, or from aerial images. These mod- els allow to perform visual localization right away without the time-consuming scene capturing and model building steps. Yet, it also comes with challenges as the available 3D models are of- ten imperfect reflections of reality. E.g., the models might only have generic or no textures at all, might only provide a simple approximation of the scene geometry, or might be stretched. This paper studies how the imperfections of these models af- fect localization accuracy. We create a new benchmark for this task and provide a detailed experimental evaluation based on multiple 3D models per scene. We show that 3D models from the Internet show promise as an easy-to-obtain scene represen- tation. At the same time, there is significant room for improve- ment for visual localization pipelines. To foster research on this interesting and challenging task, we release our benchmark at v-pnk.github.io/cadloc.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.13174",
        "title": "3D-POP - An automated annotation approach to facilitate markerless 2D-3D tracking of freely moving birds with marker-based motion capture",
        "abstract": "Recent advances in machine learning and computer vi- sion are revolutionizing the field of animal behavior by en- abling researchers to track the poses and locations of freely moving animals without any marker attachment. However, large datasets of annotated images of animals for marker- less pose tracking, especially high-resolution images taken from multiple angles with accurate 3D annotations, are still scant. Here, we propose a method that uses a motion cap- ture (mo-cap) system to obtain a large amount of annotated data on animal movement and posture (2D and 3D) in a semi-automatic manner. Our method is novel in that it ex- tracts the 3D positions of morphological keypoints (e.g eyes, beak, tail) in reference to the positions of markers attached to the animals. Using this method, we obtained, and offer here, a new dataset - 3D-POP with approximately 300k an- notated frames (4 million instances) in the form of videos having groups of one to ten freely moving birds from 4 dif- ferent camera views in a 3.6m x 4.2m area. 3D-POP is the first dataset of flocking birds with accurate keypoint annota- tions in 2D and 3D along with bounding box and individual identities and will facilitate the development of solutions for problems of 2D to 3D markerless pose, trajectory tracking, and identification in birds.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2203.06111",
        "title": "Multi-sensor large-scale dataset for multi-view 3D reconstruction",
        "abstract": "We present a new multi-sensor dataset for multi-view 3D surface reconstruction. It includes registered RGB and depth data from sensors of different resolutions and modalities: smartphones, Intel RealSense, Microsoft Kinect, industrial cameras, and structured-light scanner. The scenes are se- lected to emphasize a diverse set of material properties challenging for existing algorithms. We provide around 1.4 million images of 107 different scenes acquired from 100 viewing directions under 14 lighting conditions. We expect our dataset will be useful for evaluation and training of 3D reconstruction algorithms and for related tasks. The dataset is available at skoltech3d.appliedai.tech.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.05772",
        "title": "An Image Quality Assessment Dataset for Portraits",
        "abstract": "Year after year, the demand for ever-better smartphone photos continues to grow, in particular in the domain of portrait photography. Manufacturers thus use perceptual quality criteria throughout the development of smartphone cameras. This costly procedure can be partially replaced by automated learning-based methods for image quality as- sessment (IQA). Due to its subjective nature, it is necessary to estimate and guarantee the consistency of the IQA pro- cess, a characteristic lacking in the mean opinion scores (MOS) widely used for crowdsourcing IQA. In addition, existing blind IQA (BIQA) datasets pay little attention to the difficulty of cross-content assessment, which may de- grade the quality of annotations. This paper introduces PIQ23, a portrait-specific IQA dataset of 5116 images of 50 predefined scenarios acquired by 100 smartphones, cov- ering a high variety of brands, models, and use cases. The dataset includes individuals of various genders and ethnic- ities who have given explicit and informed consent for their photographs to be used in public research. It is annotated by pairwise comparisons (PWC) collected from over 30 im- age quality experts for three image attributes: face detail preservation, face target exposure, and overall image qual- ity. An in-depth statistical analysis of these annotations allows us to evaluate their consistency over PIQ23. Fi- nally, we show through an extensive comparison with ex- isting baselines that semantic information (image context) can be used to improve IQA predictions. The dataset along with the proposed statistical analysis and BIQA algorithms are available: https : / / github . com / DXOMARK - Research/PIQ2023",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.08401",
        "title": "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming",
        "abstract": "Open-domain dialogue systems have made promising progress in recent years. While the state-of-the-art dialogue agents are built upon large-scale text-based social media data and large pre-trained models, there is no guaran- tee these agents could also perform well in fast-growing scenarios, such as live stream- ing, due to the bounded transferability of pre- trained models and biased distributions of pub- lic datasets from Reddit and Weibo, etc. To improve the essential capability of responding and establish a benchmark in the live open- domain scenario, we introduce the LiveChat dataset, composed of 1.33 million real-life Chi- nese dialogues with almost 3800 average ses- sions across 351 personas and fine-grained pro- files for each persona. LiveChat is automati- cally constructed by processing numerous live videos on the Internet and naturally falls within the scope of multi-party conversations, where the issues of Who says What to Whom should be considered. Therefore, we target two criti- cal tasks of response modeling and addressee recognition and propose retrieval-based base- lines grounded on advanced techniques. Ex- perimental results have validated the positive effects of leveraging persona profiles and larger average sessions per persona. In addition, we also benchmark the transferability of advanced generation-based models on LiveChat and pose some future directions for current challenges.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2403.07354",
        "title": "BID: Boundary-Interior Decoding\nfor Unsupervised Temporal Action Localization Pre-Training",
        "abstract": "Skeleton-based motion representations are robust for action localization and understanding for their invariance to perspective, lighting, and occlu- sion, compared with images. Yet, they are of- ten ambiguous and incomplete when taken out of context, even for human annotators. As in- fants discern gestures before associating them with words, actions can be conceptualized be- fore being grounded with labels. Therefore, we propose the first unsupervised pre-training frame- work, Boundary-Interior Decoding (BID), that partitions a skeleton-based motion sequence into discovered semantically meaningful pre-action segments. By fine-tuning our pre-training net- work with a small number of annotated data, we show results out-performing SOTA methods by a large margin.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2105.14550",
        "title": "Blind Quality Assessment for in-the-Wild Images\nvia Hierarchical Feature Fusion and Iterative Mixed\nDatabase Training",
        "abstract": "Image quality assessment (IQA) is very important for both end-users and service providers since a high-quality image can significantly improve the user\u2019s quality of experience (QoE) and also benefit lots of computer vision algorithms. Most existing blind image quality assessment (BIQA) models were developed for synthetically distorted images, however, they perform poorly on in-the-wild images, which are widely existed in various practical applications. In this paper, we propose a novel BIQA model for in-the-wild images by addressing two critical problems in this field: how to learn better quality-aware feature representation, and how to solve the problem of insufficient training samples in terms of their content and distortion diversity. Considering that perceptual visual quality is affected by both low-level visual features (e.g. distortions) and high-level semantic information (e.g. content), we first propose a staircase structure to hierarchically integrate the features from intermediate layers into the final feature representation, which enables the model to make full use of visual information from low-level to high- level. Then an iterative mixed database training (IMDT) strategy is proposed to train the BIQA model on multiple databases simultaneously, so the model can benefit from the increase in both training samples and image content and distortion diversity and can learn a more general feature representation. Experimental results show that the proposed model outperforms other state- of-the-art BIQA models on six in-the-wild IQA databases by a large margin. Moreover, the proposed model shows an excellent performance in the cross-database evaluation experiments, which further demonstrates that the learned feature representation is robust to images with diverse distortions and content. The code is available at https://github.com/sunwei925/StairIQA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1910.06180",
        "title": "KonIQ-10k: An ecologically valid database for deep learning of blind image quality assessment",
        "abstract": "Deep learning methods for image quality assessment (IQA) are limited due to the small size of existing datasets. Extensive datasets require substantial resources both for generating publishable content and annotating it accurately. We present a systematic and scalable approach to creating KonIQ-10k, the largest IQA dataset to date, con- sisting of 10,073 quality scored images. It is the first in-the-wild database aiming for ecological validity, concerning the authenticity of distortions, the diversity of content, and quality-related indicators. Through the use of crowdsourcing, we obtained 1.2 million reliable quality ratings from 1,459 crowd workers, paving the way for more general IQA models. We propose a novel, deep learning model (KonCept512), to show an excel- lent generalization beyond the test set (0.921 SROCC), to the current state-of-the-art database LIVE-in-the-Wild (0.825 SROCC). The model derives its core performance from the InceptionResNet architecture, being trained at a higher resolution than previous models (512 \u00d7 384). Correlation analysis shows that KonCept512 performs similar to having 9 subjective scores for each test image.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2204.08040",
        "title": "NICO++: Towards Better Benchmarking for Domain Generalization",
        "abstract": "Despite the remarkable performance that modern deep neural networks have achieved on in- dependent and identically distributed (I.I.D.) data, they can crash under distribution shifts. Most current evaluation methods for domain generalization (DG) adopt the leave-one-out strategy as a compromise on the limited number of domains. We propose a large-scale benchmark with extensive labeled domains named NICO++\u2021 along with more rational evaluation methods for comprehensively evaluating DG algorithms. To evaluate DG datasets, we propose two metrics to quantify covariate shift and concept shift, respectively. Two novel generalization bounds from the perspective of data construction are proposed to prove that limited concept shift and significant covariate shift favor the evaluation capability for generalization. Through extensive experiments, NICO++ shows its superior evaluation capability compared with current DG datasets and its contribution in alleviating unfairness caused by the leak of oracle knowledge in model selection.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09944",
        "title": "REALIMPACT: A Dataset of Impact Sound Fields for Real Objects",
        "abstract": "Objects make unique sounds under different perturba- tions, environment conditions, and poses relative to the listener. While prior works have modeled impact sounds and sound propagation in simulation, we lack a standard dataset of impact sound fields of real objects for audio- visual learning and calibration of the sim-to-real gap. We present REALIMPACT, a large-scale dataset of real object impact sounds recorded under controlled conditions. RE- ALIMPACT contains 150,000 recordings of impact sounds of 50 everyday objects with detailed annotations, includ- ing their impact locations, microphone locations, contact force profiles, material labels, and RGBD images.* We make preliminary attempts to use our dataset as a reference to current simulation methods for estimating object impact sounds that match the real world. Moreover, we demon- strate the usefulness of our dataset as a testbed for acoustic and audio-visual learning via the evaluation of two bench- mark tasks, including listener location classification and vi- sual acoustic matching.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.03321",
        "title": "Planetarium\ud83e\ude90: A Rigorous Benchmark for Translating Text to Structured Planning Languages",
        "abstract": "Many recent works have explored using language models for planning problems. One line of research focuses on translating natural language descriptions of plan- ning tasks into structured planning languages, such as the planning domain defini- tion language (PDDL). While this approach is promising, accurately measuring the quality of generated PDDL code continues to pose significant challenges. First, generated PDDL code is typically evaluated using planning validators that check whether the problem can be solved with a planner. This method is insufficient because a language model might generate valid PDDL code that does not align with the natural language description of the task. Second, existing evaluation sets often have natural language descriptions of the planning task that closely resemble the ground truth PDDL, reducing the challenge of the task. To bridge this gap, we introduce Planetarium, a benchmark designed to evaluate language models\u2019 ability to generate PDDL code from natural language descriptions of planning tasks. We begin by creating a PDDL equivalence algorithm that rigorously evaluates the correctness of PDDL code generated by language models by flexibly comparing it against a ground truth PDDL. Then, we present a dataset of 132, 037 text-to-PDDL pairs across 13 different tasks, with varying levels of difficulty. Finally, we eval- uate several API-access and open-weight language models that reveal this task\u2019s complexity. For example, 87.6% of the PDDL problem descriptions generated by GPT-4o are syntactically parseable, 82.2% are valid, solve-able problems, but only 35.1% are semantically correct, highlighting the need for a more rigorous benchmark for this problem.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.03236",
        "title": "CATT: Character-based Arabic Tashkeel Transformer",
        "abstract": "Tashkeel, or Arabic Text Diacritization (ATD), greatly enhances the comprehension of Ara- bic text by removing ambiguity and minimiz- ing the risk of misinterpretations caused by its absence. It plays a crucial role in improv- ing Arabic text processing, particularly in ap- plications such as text-to-speech and machine translation. This paper introduces a new ap- proach to training ATD models. First, we finetuned two transformers, encoder-only and encoder-decoder, that were initialized from a pretrained character-based BERT. Then, we applied the Noisy-Student approach to boost the performance of the best model. We evalu- ated our models alongside 11 commercial and open-source models using two manually la- beled benchmark datasets: WikiNews and our CATT dataset. Our findings show that our top model surpasses all evaluated models by rela- tive Diacritic Error Rates (DERs) of 30.83% and 35.21% on WikiNews and CATT, respec- tively, achieving state-of-the-art in ATD. In ad- dition, we show that our model outperforms GPT-4-turbo on CATT dataset by a relative DER of 9.36%. We open-source our CATT models and benchmark dataset for the research community",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.03239",
        "title": "Solving the inverse problem of microscopy deconvolution with a residual Beylkin-Coifman-Rokhlin neural network",
        "abstract": "Optic deconvolution in light microscopy (LM) refers to re- covering the object details from images, revealing the ground truth of samples. Traditional explicit methods in LM rely on the point spread function (PSF) during image acquisition. Yet, these approaches often fall short due to inaccurate PSF models and noise artifacts, hampering the overall restoration quality. In this paper, we approached the optic decon- volution as an inverse problem. Motivated by the nonstandard-form com- pression scheme introduced by Beylkin, Coifman, and Rokhlin (BCR), we proposed an innovative physics-informed neural network Multi-Stage Residual-BCR Net (m-rBCR) to approximate the optic deconvolution. We validated the m-rBCR model on four microscopy datasets - two sim- ulated microscopy datasets from ImageNet and BioSR, real dSTORM microscopy images, and real widefield microscopy images. In contrast to the explicit deconvolution methods (e.g. Richardson-Lucy) and other state-of-the-art NN models (U-Net, DDPM, CARE, DnCNN, ESRGAN, RCAN, Noise2Noise, MPRNet, and MIMO-U-Net), the m-rBCR model demonstrates superior performance to other candidates by PSNR and SSIM in two real microscopy datasets and the simulated BioSR dataset. In the simulated ImageNet dataset, m-rBCR ranks the second-best place (right after MIMO-U-Net). With the backbone from the optical physics, m-rBCR exploits the trainable parameters with better performances (from \u223c30 times fewer than the benchmark MIMO-U-Net to \u223c210 times than ESRGAN). This enables m-rBCR to achieve a shorter runtime (from \u223c3 times faster than MIMO-U-Net to \u223c300 times faster than DDPM). To summarize, by leveraging physics constraints our model re- duced potentially redundant parameters significantly in expertise-oriented NN candidates and achieved high efficiency with superior performance.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2403.13317v1",
        "title": "Flickr30K-CFQ: A Compact and Fragmented Query Dataset for Text-image Retrieval",
        "abstract": "Withtheexplosivegrowthofmulti-modalinformationonthe Internet, unimodal search cannot satisfy the requirement of Internet ap- plications. Text-image retrieval research is needed to realize high-quality and efficient retrieval between different modalities. Existing text-image retrieval research is mostly based on general vision-language datasets (e.g. MS-COCO, Flickr30K), in which the query utterance is rigid and unnatural (i.e. verbosity and formality). To overcome the shortcoming, we construct a new Compact and Fragmented Query challenge dataset (named Flickr30K-CFQ) to model text-image retrieval task consid- ering multiple query content and style, including compact and fine- grained entity-relation corpus. We propose a novel LLM-based Query- enhanced method using prompt engineering based on LLM. Experiments show that our proposed Flickr30-CFQ reveals the insufficiency of ex- isting vision-language datasets in realistic text-image tasks. Our LLM- based Query-enhanced method applied on different existing text-image retrieval models improves query understanding performance both on public dataset and our challenge set Flickr30-CFQ with over 0.9% and 2.4% respectively. Our project can be available anonymously in https: //sites.google.com/view/Flickr30K-cfq.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1505.04870",
        "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
        "abstract": "The Flickr30k dataset has become a standard benchmark for sentence-based image description. This pa- per presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains, link- ing mentions of the same entities across different captions for the same image, and associating them with 276k man- ually annotated bounding boxes. Such annotations are es- sential for continued progress in automatic image descrip- tion and grounded language understanding. They enable us to define a new benchmark for localization of textual entity mentions in an image. We present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards se- lecting larger objects. While our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2104.08541",
        "title": "TransVG: End-to-End Visual Grounding with Transformers",
        "abstract": "In this paper, we present a neat yet effective transformer- based framework for visual grounding, namely TransVG, to address the task of grounding a language query to the corresponding region onto an image. The state-of-the-art methods, including two-stage or one-stage ones, rely on a complex module with manually-designed mechanisms to perform the query reasoning and multi-modal fusion. How- ever, the involvement of certain mechanisms in fusion mod- ule design, such as query decomposition and image scene graph, makes the models easily overfit to datasets with spe- cific scenarios, and limits the plenitudinous interaction be- tween the visual-linguistic context. To avoid this caveat, we propose to establish the multi-modal correspondence by leveraging transformers, and empirically show that the complex fusion modules (e.g., modular attention network, dynamic graph, and multi-modal tree) can be replaced by a simple stack of transformer encoder layers with higher per- formance. Moreover, we re-formulate the visual grounding as a direct coordinates regression problem and avoid mak- ing predictions out of a set of candidates (i.e., region pro- posals or anchor boxes). Extensive experiments are con- ducted on five widely used datasets, and a series of state- of-the-art records are set by our TransVG. We build the benchmark of transformer-based visual grounding frame- work and make the code available at https://github. com/djiajunustc/TransVG.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.13216",
        "title": "Exploiting CNNs for Semantic Segmentation with Pascal VOC",
        "abstract": "In this paper, we present a comprehensive study on semantic segmentation with the Pascal VOC dataset. Here, we have to label each pixel with a class which in turn segments the entire image based on the objects/entities present. To tackle this, we firstly use a Fully Convolution Network (FCN) baseline which gave 71.31% pixel accuracy and 0.0527 mean IoU. We analyze its performance and working and subsequently address the issues in the baseline with three improvements - a) cosine annealing learning rate scheduler(pixel accuracy: 72.86%, IoU: 0.0529), b) data augmentation(pixel accuracy: 69.88%, IoU: 0.0585) c) class imbalance weights(pixel accuracy: 68.98%, IoU: 0.0596). Apart from these changes in train- ing pipeline, we also explore three different architectures - a) Our proposed model - Advanced FCN (pixel accuracy: 67.20%, IoU: 0.0602) b) Transfer Learning with ResNet (Best performance) (pixel accuracy: 71.33%, IoU: 0.0926 ) c) U- Net(pixel accuracy: 72.15%, IoU: 0.0649). We observe that the improvements help in greatly improving the performance, as reflected both, in metrics and seg- mentation maps. Interestingly, we observe that among the improvements, dataset augmentation has the greatest contribution. Also, note that transfer learning model performs the best on the pascal dataset. We analyse the performance of these using loss, accuracy and IoU plots along with segmentation maps, which help us draw valuable insights about the working of the models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2009.05686",
        "title": "LQR-augmented neural networks",
        "abstract": "In this paper we propose a new computa- tional method for designing optimal regulators for high- dimensional nonlinear systems. The proposed approach leverages physics-informed machine learning to solve high-dimensional Hamilton-Jacobi-Bellman equations aris- ing in optimal feedback control. Concretely, we augment linear quadratic regulators with neural networks to handle nonlinearities. We train the augmented models on data generated without discretizing the state space, enabling application to high-dimensional problems. We use the pro- posed method to design a candidate optimal regulator for an unstable Burgers\u2019 equation, and through this example, demonstrate improved robustness and accuracy compared to existing neural network formulations.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.03247",
        "title": "Bridging Model Heterogeneity in Federated Learning via Uncertainty-based Asymmetrical Reciprocity Learning",
        "abstract": "This paper presents FedType, a simple yet pio- neering framework designed to fill research gaps in heterogeneous model aggregation within feder- ated learning (FL). FedType introduces small identical proxy models for clients, serving as agents for information exchange, ensuring model security, and achieving efficient communication simultaneously. To transfer knowledge between large private and small proxy models on clients, we propose a novel uncertainty-based asymmetri- cal reciprocity learning method, eliminating the need for any public data. Comprehensive experi- ments conducted on benchmark datasets demon- strate the efficacy and generalization ability of FedType across diverse settings. Our approach redefines federated learning paradigms by bridg- ing model heterogeneity, eliminating reliance on public data, prioritizing client privacy, and reduc- ing communication costs.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.04414",
        "title": "CIFAR-10-WAREHOUSE: BROAD AND MORE REALIS- TIC TESTBEDS IN MODEL GENERALIZATION ANALYSIS",
        "abstract": "Analyzing model performance in various unseen environments is a critical research problem in the machine learning community. To study this problem, it is important to construct a testbed with out-of-distribution test sets that have broad coverage of environmental discrepancies. However, existing testbeds typically either have a small number of domains or are synthesized by image corruptions, hindering algo- rithm design that demonstrates real-world effectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of 180 datasets collected by prompting image search engines and diffusion models in various ways. Generally sized between 300 and 8,000 images, the datasets contain natural images, cartoons, certain colors, or objects that do not naturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen the understanding of two generalization tasks: domain gener- alization and model accuracy prediction in various out-of-distribution environments. We conduct extensive benchmarking and comparison experiments and show that CIFAR-10-W offers new and interesting insights inherent to these tasks. We also discuss other fields that would benefit from CIFAR-10-W. Data and code are avail- able at https://sites.google.com/view/CIFAR-10-warehouse/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.03251",
        "title": "ACTRESS: Active Retraining for Semi-supervised Visual Grounding",
        "abstract": "Semi-Supervised Visual Grounding (SSVG) is a new chal- lenge for its sparse labeled data with the need for multimodel under- standing. A previous study, RefTeacher [30], makes the first attempt to tackle this task by adopting the teacher-student framework to provide pseudo confidence supervision and attention-based supervision. However, this approach is incompatible with current state-of-the-art visual ground- ing models, which follow the Transformer-based pipeline. These pipelines directly regress results without region proposals or foreground binary classification, rendering them unsuitable for fitting in RefTeacher due to the absence of confidence scores. Furthermore, the geometric difference in teacher and student inputs, stemming from different data augmentations, induces natural misalignment in attention-based constraints. To estab- lish a compatible SSVG framework, our paper proposes the ACTive REtraining approach for Semi-Supervised Visual Grounding, abbrevi- ated as ACTRESS. Initially, the model is enhanced by incorporating an additional quantized detection head to expose its detection confi- dence. Building upon this, ACTRESS consists of an active sampling strategy and a selective retraining strategy. The active sampling strategy iteratively selects high-quality pseudo labels by evaluating three crucial aspects: Faithfulness, Robustness, and Confidence, optimizing the uti- lization of unlabeled data. The selective retraining strategy retrains the model with periodic re-initialization of specific parameters, facilitating the model\u2019s escape from local minima. Extensive experiments demon- strates our superior performance on widely-used benchmark datasets.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1503.04424",
        "title": "Bridging Social Media via Distant Supervision",
        "abstract": "Microblog classification has received a lot of attention in recent years. Different classification tasks have been investigated, most of them fo- cusing on classifying microblogs into a small number of classes (five or less) using a training set of manually annotated tweets. Unfortunately, labelling data is tedious and expensive, and finding tweets that cover all the classes of interest is not always straightforward, especially when some of the classes do not frequently arise in practice. In this paper we study an approach to tweet classification based on distant supervision, whereby we automatically transfer labels from one social medium to another for a single-label multi-class classi- fication task. In particular, we apply YouTube video classes to tweets linking to these videos. This provides for free a virtually unlimited number of labelled instances that can be used as training data. The classification experiments we have run show that training a tweet classifier via these automatically labelled data achieves substantially better performance than training the same clas- sifier with a limited amount of manually labelled data; this is advantageous, given that the automatically labelled data come at no cost. Further investiga- tion of our approach shows its robustness when applied with different numbers of classes and across different languages",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1908.01456",
        "title": "A Deep Learning Approach for Tweet Classification and Rescue Scheduling for Effective Disaster Management (Industrial)",
        "abstract": "Every activity in disaster management such as managing evacu- ation plan, and running rescue missions demands accurate and up-to-date information to allow a quick, easy, and cost-effective response to reduce the possible loss of lives and properties. It is a challenging and complex task to acquire information from different regions of a disaster-affected area in a timely fashion. The exten- sive spread and reach of social media and networks allow people to share information in real-time. However, the processing of social media data and gathering of valuable information require a series of operations such as (1) processing each specific tweet for a text classi- fication, (2) possible location determination of people needing help based on tweets, and (3) priority calculations of rescue tasks based on the classification of tweets. These are three primary challenges in developing an effective rescue scheduling operation using social media data. In this paper, first, we propose a deep learning model combining attention based Bi-directional Long Short-Term Mem- ory (BLSTM) and Convolutional Neural Network (CNN) to classify the tweets under different categories. We use pre-trained crisis word vectors and global vectors for word representation (GLoVe) for capturing semantic meaning from tweets. Next, we perform feature engineering to create an auxiliary feature map which dra- matically increases the model accuracy. In our experiments using real data sets from Hurricanes Harvey and Irma, it is observed that our proposed approach performs better compared to other classifi- cation methods based on Precision, Recall, F1-score, and Accuracy, and is highly effective to determine the correct priority of a tweet. Furthermore, to evaluate the effectiveness and robustness of the proposed classification model a merged dataset comprises of 4 dif- ferent datasets from CrisisNLP and another 15 different disasters data from CrisisLex are used. Finally, we develop an adaptive multi- task hybrid scheduling algorithm considering resource constraints to perform an effective rescue scheduling operation considering different rescue priorities.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.12793",
        "title": "ShareGPT4V: Improving Large Multi-Modal Models with Better Captions",
        "abstract": "In the realm of large multi-modal models (LMMs), efficient modality alignment is crucial yet often constrained by the scarcity of high-quality image-text data. To address this bottleneck, we introduce the ShareGPT4V dataset, a pio- neering large-scale resource featuring 1.2 million highly descriptive captions, which surpasses existing datasets in diversity and information content, covering world knowl- edge, object properties, spatial relationships, and aesthetic evaluations. Specifically, ShareGPT4V originates from a curated 100K high-quality captions collected from ad- vanced GPT4-Vision and has been expanded to 1.2M with a superb caption model trained on this subset. ShareGPT4V\nfirst demonstrates its effectiveness for the Supervised Fine-Tuning (SFT) phase, by substituting an equivalent quantity of detailed captions in existing SFT datasets with a subset of our high-quality captions, significantly enhancing the LMMs like LLaVA-7B, LLaVA-1.5-13B, and Qwen- VL-Chat-7B on the MME and MMBench benchmarks, with respective gains of 222.8/22.0/22.3 and 2.7/1.3/1.5. We further incorporate ShareGPT4V data into both the pre-training and SFT phases, obtaining ShareGPT4V-7B, a superior LMM based on a simple architecture that has remarkable performance across a majority of the multi-modal benchmarks. This project is available at https : / / ShareGPT4V . github . io to serve as a pivotal resource for advancing the LMMs community.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1812.08658",
        "title": "nocaps: novel object captioning at scale",
        "abstract": "Image captioning models have achieved impressive results on datasets containing limited visual concepts and large amounts of paired image-caption training data. However, if these models are to ever function in the wild, a much larger variety of visual concepts must be learned, ideally from less supervision. To encourage the development of im- age captioning models that can learn visual concepts from alternative data sources, such as object detection datasets, we present the first large-scale benchmark for this task. Dubbed \u2018nocaps\u2019, for novel object captioning at scale, our benchmark consists of 166,100 human-generated captions describing 15,100 images from the Open Images valida- tion and test sets. The associated training data consists of COCO image-caption pairs, plus Open Images image- level labels and object bounding boxes. Since Open Images contains many more classes than COCO, nearly 400 object classes seen in test images have no or very few associated training captions (hence, nocaps). We extend existing novel object captioning models to establish strong baselines for this benchmark and provide analysis to guide future work.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1612.00837",
        "title": "Making the V in VQA Matter:\nElevating the Role of Image Understanding in Visual Question Answering",
        "abstract": "Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than vi- sual modalities, resulting in models that ignore visual infor- mation, leading to an inflated sense of their capability.\nWe propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset [3] by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the origi- nal VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at http://visualqa.org/ as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0).\nWe further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform sig- nificantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language pri- ors. This finding provides the first concrete empirical evi- dence for what seems to be a qualitative sense among prac- titioners.\nFinally, our data collection protocol for identifying com- plementary images enables us to develop a novel inter- pretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter- example based explanation. Specifically, it identifies an im- age that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.13245",
        "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
        "abstract": "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) intro- duce grouped-query attention (GQA), a gener- alization of multi-query attention which uses an intermediate (more than one, less than num- ber of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1906.00067",
        "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge",
        "abstract": "Visual Question Answering (VQA) in its ideal form lets us study reasoning in the joint space of vision and lan- guage and serves as a proxy for the AI task of scene understanding. However, most VQA benchmarks to date are focused on questions such as simple counting, visual attributes, and object detection that do not require rea- soning or knowledge beyond what is in the image. In this paper, we address the task of knowledge-based visual question answering and provide a benchmark, called OK- VQA, where the image content is not sufficient to answer the questions, encouraging methods that rely on external knowledge resources. Our new dataset includes more than 14,000 questions that require external knowledge to an- swer. We show that the performance of the state-of-the-art VQA models degrades drastically in this new setting. Our analysis shows that our knowledge-based VQA task is di- verse, difficult, and large compared to previous knowledge- based VQA datasets. We hope that this dataset enables re- searchers to open up new avenues for research in this do- main. See http://okvqa.allenai.org to download and browse the dataset.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2206.01718",
        "title": "A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge",
        "abstract": "The Visual Question Answering (VQA) task aspires to provide a meaningful testbed for the development of AI models that can jointly reason over visual and natural language inputs. Despite a proliferation of VQA datasets, this goal is hindered by a set of common limitations. These include a reliance on relatively simplistic questions that are repetitive in both concepts and linguistic structure, little world knowledge needed outside of the paired image, and limited reasoning required to arrive at the correct answer. We introduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer. In contrast to the existing knowledge-based VQA datasets, the questions generally cannot be answered by simply querying a knowledge base, and instead require some form of commonsense reasoning about the scene depicted in the image. We demonstrate the potential of this new dataset through a detailed analysis of its contents and baseline performance measurements over a variety of state-of-the-art vision\u2013language models.\nhttp://a-okvqa.allenai.org/",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2402.11684",
        "title": "ALLaVA: Harnessing GPT4V-Synthesized Data for Lite Vision-Language Models",
        "abstract": "Large vision-language models (LVLMs) have shown premise in a broad range of vision-language tasks with their strong reasoning and generalization capabilities. However, they require considerable computational resources for training and de- ployment. This study aims to bridge the performance gap between traditional-scale LVLMs and resource-friendly lite versions by adopting high-quality training data. To this end, we propose a comprehensive pipeline for generating a synthetic dataset. The key idea is to leverage strong proprietary models to generate (i) fine-grained image annotations for vision-language alignment and (ii) complex reasoning vi- sual question-answering pairs for visual instruction fine-tuning, yielding 1.3M samples in total. We train a series of lite VLMs on the synthetic dataset and ex- perimental results demonstrate the effectiveness of the proposed scheme, where they achieve competitive performance on 17 benchmarks among 4B LVLMs, and even perform on par with 7B/13B-scale models on various benchmarks. This work highlights the feasibility of adopting high-quality data in crafting more efficient LVLMs. We name our dataset ALLaVA, and open-source it to research community for developing better resource-efficient LVLMs for wider usage.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2006.01038",
        "title": "DocBank: A Benchmark Dataset for Document Layout Analysis",
        "abstract": "Document layout analysis usually relies on computer vision models to understand documents while ignoring textual information that is vital to capture. Meanwhile, high quality labeled datasets with both visual and textual information are still insufficient. In this paper, we present DocBank, a benchmark dataset that contains 500K document pages with fine-grained token- level annotations for document layout analysis. DocBank is constructed using a simple yet effective way with weak supervision from the LATEX documents available on the arXiv.com. With DocBank, models from different modalities can be compared fairly and multi-modal ap- proaches will be further investigated and boost the performance of document layout analysis. We build several strong baselines and manually split train/dev/test sets for evaluation. Ex- periment results show that models trained on DocBank accurately recognize the layout infor- mation for a variety of documents. The DocBank dataset is publicly available at https: //github.com/doc-analysis/DocBank.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1908.07836",
        "title": "PubLayNet: largest dataset ever for document layout analysis",
        "abstract": "Recognizing the layout of unstructured digital documents is an important step when parsing the documents into structured machine-readable format for downstream applications. Deep neural networks that are developed for computer vision have been proven to be an effective method to analyze layout of document images. However, document layout datasets that are currently publicly available are several magnitudes smaller than established computing vision datasets. Models have to be trained by transfer learning from a base model that is pre-trained on a traditional computer vision dataset. In this paper, we develop the PubLayNet dataset for document layout analysis by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed CentralTM. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated. The experiments demonstrate that deep neural networks trained on PubLayNet accurately recognize the layout of scientific articles. The pre-trained models are also a more effective base mode for transfer learning on a different document domain. We release the dataset (https://github.com/ibm-aur-nlp/PubLayNet) to support development and evaluation of more advanced models for document layout analysis.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1912.03879",
        "title": "AI2D-RST: A multimodal corpus of 1000 primary school science diagrams",
        "abstract": "This article introduces AI2D-RST, a multimodal corpus of 1000 English-language diagrams that represent topics in primary school natural sciences, such as food webs, life cycles, moon phases and human physiology. The corpus is based on the Allen Institute for Artificial Intelli- gence Diagrams (AI2D) dataset, a collection of diagrams with crowd-sourced descriptions, which was originally developed to support research on automatic diagram understanding and visual question answering. Building on the segmentation of diagram layouts in AI2D, the AI2D-RST corpus presents a new multi-layer annotation schema that provides a rich description of their multimodal structure. Annotated by trained experts, the layers describe (1) the grouping of diagram elements into perceptual units, (2) the connections set up by diagrammatic elements such as arrows and lines, and (3) the discourse relations between diagram elements, which are described using Rhetorical Structure Theory (RST). Each annotation layer in AI2D-RST is represented using a graph. The corpus is freely available for research and teaching.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.11833v1",
        "title": "MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs",
        "abstract": "Generating natural and meaningful responses to communicate with multi-modal hu- man inputs is a fundamental capability of Large Vision-Language Models (LVLMs). While current open-source LVLMs demonstrate promising performance in simpli- fied scenarios such as single-turn single-image input, they fall short in real-world conversation scenarios such as following instructions in a long context history with multi-turn and multi-images. Existing LVLM benchmarks primarily focus on single-choice questions or short-form responses, which do not adequately as- sess the capabilities of LVLMs in real-world human-AI interaction applications. Therefore, we introduce MMDU, a comprehensive benchmark, and MMDU-45k, a large-scale instruction tuning dataset, designed to evaluate and improve LVLMs\u2019 abilities in multi-turn and multi-image conversations. We employ the clustering algorithm to find the relevant images and textual descriptions from the open-source Wikipedia and construct the question-answer pairs by human annotators with the assistance of the GPT-4o model. MMDU has a maximum of 18k image+text tokens, 20 images, and 27 turns, which is at least 5\u00d7 longer than previous benchmarks and poses challenges to current LVLMs. Our in-depth analysis of 15 representative LVLMs using MMDU reveals that open-source LVLMs lag behind closed-source counterparts due to limited conversational instruction tuning data. We demonstrate that fine-tuning open-source LVLMs on MMDU-45k significantly address this gap, generating longer and more accurate conversations, and improving scores on MMDU and existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA:\n+1.2%). Our contributions pave the way for bridging the gap between current LVLM models and real-world application demands. This project is available at https://github.com/Liuziyu77/MMDU.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2110.13214",
        "title": "IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning",
        "abstract": "Current visual question answering (VQA) tasks mainly consider answering human- annotated questions for natural images. However, aside from natural images, abstract diagrams with semantic richness are still understudied in visual under- standing and reasoning research. In this work, we introduce a new challenge of Icon Question Answering (IconQA) with the goal of answering a question in an icon image context. We release IconQA, a large-scale dataset that consists of 107,439 questions and three sub-tasks: multi-image-choice, multi-text-choice, and filling-in- the-blank. The IconQA dataset is inspired by real-world diagram word problems that highlight the importance of abstract diagram understanding and comprehensive cognitive reasoning. Thus, IconQA requires not only perception skills like object recognition and text understanding, but also diverse cognitive reasoning skills, such as geometric reasoning, commonsense reasoning, and arithmetic reasoning. To facilitate potential IconQA models to learn semantic representations for icon images, we further release an icon dataset Icon645 which contains 645,687 colored icons on 377 classes. We conduct extensive user studies and blind experiments and reproduce a wide range of advanced VQA methods to benchmark the IconQA task. Also, we develop a strong IconQA baseline Patch-TRM that applies a pyramid cross-modal Transformer with input diagram embeddings pre-trained on the icon dataset. IconQA and Icon645 are available at https://iconqa.github.io.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2404.18585",
        "title": "FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering",
        "abstract": "Table Question Answering (TQA) aims at com- posing an answer to a question based on tabu- lar data. While prior research has shown that TQA models lack robustness, understanding the underlying cause and nature of this issue remains predominantly unclear, posing a sig- nificant obstacle to the development of robust TQA systems. In this paper, we formalize three major desiderata for a fine-grained evaluation of robustness of TQA systems. They should (i) answer questions regardless of alterations in table structure, (ii) base their responses on the content of relevant cells rather than on biases, and (iii) demonstrate robust numerical reason- ing capabilities. To investigate these aspects, we create and publish a novel TQA evaluation benchmark in English. Our extensive experi- mental analysis reveals that none of the exam- ined state-of-the-art TQA systems consistently excelsinthesethreeaspects. Ourbenchmark is a crucial instrument for monitoring the be- havior of TQA systems and paves the way for the development of robust TQA systems. We release our benchmark publicly.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1811.00232",
        "title": "Textbook Question Answering with Multi-modal Context Graph Understanding and Self-supervised Open-set Comprehension",
        "abstract": "In this work, we introduce a novel algorithm for solving the textbook question answering (TQA) task which describes more realistic QA problems compared to other recent tasks. We mainly focus on two related issues with anal- ysis of the TQA dataset. First, solving the TQA problems requires to comprehend multi- modal contexts in complicated input data. To tackle this issue of extracting knowledge fea- tures from long text lessons and merging them with visual features, we establish a context graph from texts and images, and propose a new module f-GCN based on graph con- volutional networks (GCN). Second, scien- tific terms are not spread over the chapters and subjects are split in the TQA dataset. To overcome this so called \u2018out-of-domain\u2019 is- sue, before learning QA problems, we intro- duce a novel self-supervised open-set learn- ing process without any annotations. The ex- perimental results show that our model signifi- cantly outperforms prior state-of-the-art meth- ods. Moreover, ablation studies validate that both methods of incorporating f-GCN for ex- tracting knowledge from multi-modal contexts and our newly proposed self-supervised learn- ing process are effective for TQA problems.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1801.08163",
        "title": "DVQA: Understanding Data Visualizations via Question Answering",
        "abstract": "Bar charts are an effective way to convey numeric in- formation, but today\u2019s algorithms cannot parse them. Ex- isting methods fail when faced with even minor variations in appearance. Here, we present DVQA, a dataset that tests many aspects of bar chart understanding in a ques- tion answering framework. Unlike visual question answer- ing (VQA), DVQA requires processing words and answers that are unique to a particular bar chart. State-of-the-art VQA algorithms perform poorly on DVQA, and we pro- pose two strong baselines that perform considerably better. Our work will enable algorithms to automatically extract numeric and semantic information from vast quantities of bar charts found in scientific publications, Internet articles, business reports, and many other areas.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2403.00816",
        "title": "CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering",
        "abstract": "Document Visual Question Answering (DVQA) is a task that involves responding to queries based on the content of images. Existing work is limited to locating information within a single page and does not facilitate cross-page question-and-answer interaction. Furthermore, the token length limitation imposed on inputs to the model may lead to truncation of segments pertinent to the answer. In this study, we introduce a simple but effective methodology called CFRet-DVQA, which focuses on retrieval and efficient tuning to address this critical issue effectively. For that, we initially retrieve multiple segments from the document that correlate with the question at hand. Subsequently, we leverage the advanced reasoning abilities of the large language model (LLM), further augmenting its performance through instruction tuning. This approach enables the generation of answers that align with the style of the document labels. The experiments demonstrate that our methodology achieved state-of-the-art or competitive results with both single-page and multi-page documents in various fields.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2203.10244",
        "title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning",
        "abstract": "Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic opera- tions. They also commonly refer to visual fea- tures of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their ques- tions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and log- ical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a uni- fied way to answer questions. While our mod- els achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1905.13319",
        "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
        "abstract": "We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map prob- lems to operation programs. Due to an- notation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational an- notations over diverse problem types. We introduce a new representation language to model precise operation programs correspond- ing to each math problem that aim to im- prove both the performance and the inter- pretability of the learned models. Using this representation language, our new dataset, MathQA, significantly enhances the AQuA dataset with fully-specified operational pro- grams. We additionally introduce a neu- ral sequence-to-program model enhanced with automatic problem categorization. Our exper- iments show improvements over competitive baselines in our MathQA as well as the AQuA datasets. The results are still significantly lower than human performance indicating that the dataset poses new challenges for future re- search. Our dataset is available at: https: //math-qa.github.io/math-QA/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2105.04165",
        "title": "Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning",
        "abstract": "Geometry problem solving has attracted much attention in the NLP community recently. The task is challenging as it requires abstract prob- lem understanding and symbolic reasoning with axiomatic knowledge. However, current datasets are either small in scale or not pub- licly available. Thus, we construct a new large- scale benchmark, Geometry3K, consisting of 3,002 geometry problems with dense annota- tion in formal language. We further propose a novel geometry solving approach with for- mal language and symbolic reasoning, called Interpretable Geometry Problem Solver (Inter- GPS). Inter-GPS first parses the problem text and diagram into formal language automati- cally via rule-based text parsing and neural ob- ject detecting, respectively. Unlike implicit learning in existing methods, Inter-GPS in- corporates theorem knowledge as conditional rules and performs symbolic reasoning step by step. Also, a theorem predictor is designed to infer the theorem application sequence fed to the symbolic solver for the more efficient and reasonable searching path. Extensive experi- ments on the Geometry3K and GEOS datasets demonstrate that Inter-GPS achieves signifi- cant improvements over existing methods.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2205.09947",
        "title": "PGDP5K: A Diagram Parsing Dataset for Plane Geometry Problems",
        "abstract": "Diagram parsing is an important foundation for geometry problem solving, attracting increasing attention in the field of intelligent education and document image understanding. Due to the complex layout and between-primitive relationship, plane geometry diagram parsing (PGDP) is still a challenging task deserving further research and exploration. An appropriate dataset is critical for the research of PGDP. Although some datasets with rough annotations have been proposed to solve geometric problems, they are either small in scale or not publicly available. The rough annotations also make them not very useful. Thus, we propose a new large-scale geometry diagram dataset named PGDP5K and a novel annotation method. Our dataset consists of 5000 diagram samples composed of 16 shapes, covering 5 positional relations, 22 symbol types and 6 text types. Different from previous datasets, our PGDP5K dataset is labeled with more fine-grained annotations at primitive level, including primitive classes, locations and relationships. What is more, combined with above annotations and geometric prior knowledge, it can generate intelligible geometric propositions automatically and uniquely. We performed experiments on PGDP5K and IMP- Geometry3K datasets reveal that the state-of-the-art (SOTA) method achieves only 66.07% F1 value. This shows that PGDP5K presents a challenge for future research. Our dataset is available at http://www.nlpr.ia.ac.cn/databases/CASIA-PGDP5K/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2209.14610",
        "title": "DYNAMIC PROMPT LEARNING VIA POLICY GRADIENT FOR SEMI-STRUCTURED MATHEMATICAL REASONING",
        "abstract": "Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex prob- lems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TABMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathe- matical reasoning on both textual and tabular data. Each question in TABMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi- choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TABMWP, in- cluding the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TABMWP. To mitigate this, we further propose a novel approach, PROMPTPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and re- duces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1909.05405",
        "title": "SuPer: A Surgical Perception Framework for Endoscopic Tissue Manipulation with Surgical Robotics",
        "abstract": "Traditional control and task automation have been successfully demonstrated in a variety of structured, controlled environments through the use of highly specialized modeled robotic systems in conjunction with multiple sensors. However, the application of autonomy in endoscopic surgery is very challenging, particularly in soft tissue work, due to the lack of high-quality images and the unpredictable, constantly deforming environment. In this work, we propose a novel surgical perception framework, SuPer, for surgical robotic control. This framework continuously collects 3D geometric information that allows for mapping a deformable surgical field while tracking rigid instruments within the field. To achieve this, a model-based tracker is employed to localize the surgical tool with a kinematic prior in conjunction with a model-free tracker to reconstruct the deformable environment and provide an estimated point cloud as a mapping of the environment. The proposed framework was implemented on the da Vinci Surgical\u20ddR System in real-time with an end-effector controller where the target configurations are set and regulated through the framework. Our proposed framework successfully completed soft tissue manipulation tasks with high accuracy. The demonstration of this novel framework is promising for the future of surgical autonomy. In addition, we provide our dataset for further surgical research",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.16890",
        "title": "TouchStone: Evaluating Vision-Language Models by Language Models",
        "abstract": "Large vision-language models (LVLMs) have recently witnessed rapid advancements, exhibiting a remarkable capacity for perceiving, understanding, and processing visual information by connecting visual receptor with large language models (LLMs). However, current assessments mainly focus on recognizing and reasoning abilities, lacking direct evaluation of conversational skills and neglecting visual storytelling abilities. In this paper, we propose an evaluation method that uses strong LLMs as judges to comprehensively evaluate the various abilities of LVLMs. Firstly, we construct a comprehensive visual dialogue dataset TouchStone, consisting of open-world images and questions, covering five major categories of abilities and 27 subtasks. This dataset not only covers funda- mental recognition and comprehension but also extends to literary creation. Secondly, by integrating detailed image annotations we effectively transform the multimodal in- put content into a form understandable by LLMs. This enables us to employ advanced LLMs for directly evaluating the quality of the multimodal dialogue without requiring human intervention. Through validation, we demonstrate that powerful LVLMs, such as GPT-4, can effectively score dialogue quality by leveraging their textual capabilities alone, aligning with human preferences. We hope our work can serve as a touchstone for LVLMs\u2019 evaluation and pave the way for building stronger LVLMs. The evaluation code is available at https://github.com/OFA-Sys/TouchStone.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1904.08920",
        "title": "Towards VQA Models That Can Read",
        "abstract": "Studies have shown that a dominant class of questions\nasked by visually impaired users on images of their surroundings involves reading text in the image. but today's VQA models can not read! Our paper takes a first step to-\nwards addressing this problem. First, we introduce a new\n\u201cTextVQA\u201d dataset to facilitate progress on this important\nproblem. Existing datasets either have a small proportion of\nquestions about text (e.g., the VQA dataset) or are too small\n(e.g., the VizWiz dataset). TextVQA contains 45,336 ques-\ntions on 28,408 images that require reasoning about text to\nanswer. Second, we introduce a novel model architecture\nthat reads text in the image, reasons about it in the con-\ntext of the image and the question, and predicts an answer\nwhich might be a deduction based on the text and the image\nor is composed of the strings found in the image. Conse-\nquently, we call our approach Look, Read, Reason & An-swer(lorra). we show th that lorra outperforms existing state-of-the-art vqa models on our textVQA dataset. we find that the gap between human performance and machine performance is significantly larger on textVQA than on\nVQA 2.0, suggesting that TextVQA is well-suited to bench-mark progress along directions complementary to VQA 2.0.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.02255",
        "title": "MATHVISTA: EVALUATING MATHEMATICAL REASON- ING OF FOUNDATION MODELS IN VISUAL CONTEXTS",
        "abstract": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MATHVISTA, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 ex- amples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging.\nWith MATHVISTA, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT- 4V is mainly attributed to its enhanced visual perception and mathematical rea- soning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MATHVISTA will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.02490",
        "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
        "abstract": "We propose MM-Vet1, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown var- ious intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the compli- cated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capa- bilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combination. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models. Code and data are available at https://github.com/yuweihao/MM-Vet.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.16502",
        "title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI",
        "abstract": "We introduce MMMU: a new benchmark designed to eval- uate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and text- books, covering six core disciplines: Art & Design, Busi- ness, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly het- erogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike ex- isting benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 28 open-source LMMs as well as the propri- etary GPT-4V(ision) and Gemini highlights the substantial\nchallenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56% and 59% re- spectively, indicating significant room for improvement. We believe MMMU will stimulate the community to build next- generation multimodal foundation models towards expert artificial general intelligence",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2407.00468",
        "title": "MMEVALPRO: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation",
        "abstract": "Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding and reasoning abilities, often assessed through multiple-choice questions (MCQs) that include an image, a question, and several options. However, many benchmarks used for such evaluations suffer from systematic biases. Remarkably, Large Lan- guage Models (LLMs) without any visual perception capabilities achieve non-trivial performance, undermining the credibility of these evaluations. To address this issue while maintaining the efficiency of MCQ evaluations, we propose MMEVALPRO, a benchmark designed to avoid Type-I errors through a trilogy evaluation pipeline and more rigorous metrics. For each original question from existing benchmarks, human annotators augment it by creating one perception question and one knowl- edge anchor question through a meticulous annotation process. MMEVALPRO comprises 2, 138 question triplets, totaling 6, 414 distinct questions. Two-thirds of these questions are manually labeled by human experts, while the rest are sourced from existing benchmarks (MMMU, ScienceQA, and MathVista). Compared with the existing benchmarks, our experiments with the latest LLMs and LMMs demon- strate that MMEVALPRO is more challenging (the best LMM lags behind human performance by 31.73%, compared to an average gap of 8.03% in previous bench- marks) and more trustworthy (the best LLM trails the best LMM by 23.09%, whereas the gap for previous benchmarks is just 14.64%). Our in-depth analysis explains the reason for the large performance gap and justifies the trustworthiness of evaluation, underscoring its significant potential for advancing future research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.00784",
        "title": "ViP-LLaVA:\nMaking Large Multimodal Models Understand Arbitrary Visual Prompts",
        "abstract": "While existing large vision-language multimodal mod- els focus on whole image understanding, there is a promi- nent gap in achieving region-specific comprehension. Cur- rent approaches that use textual coordinates or spatial en- codings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we intro- duce a novel multimodal model capable of decoding arbi- trary (free-form) visual prompts. This allows users to intu- itively mark images and interact with the model using nat- ural cues like a \u201cred bounding box\u201d or \u201cpointed arrow\u201d. Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encod- ings, yet achieves state-of-the-art performance on region- understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.13951",
        "title": "MLLM-BENCH: EVALUATING MULTIMODAL LLMS WITH PER-SAMPLE CRITERIA",
        "abstract": "Multimodal large language models (MLLMs) (e.g., GPT-4V, LLaVA, and Claude- 3) have broadened the scope of AI applications. Yet, evaluating their performance presents a significant challenge owing to the inherently subjective nature of tasks that do not yield clear-cut solutions especially for those open-ended queries. Exist- ing automatic evaluation methodologies are mainly limited in evaluating objective queries without considering real-world user experiences, inadequately addressing the nuances of creative and associative multimodal tasks. In our paper, we pro- pose a new evaluation paradigm for MLLMs, which is evaluating MLLMs with per-sample criteria using potent MLLM as the judge. To validate the feasibility and effectiveness of this paradigm, we design a benchmark, dubbed MLLM-Bench, with the evaluation samples across six critical levels following the revised Bloom\u2019s Taxonomy with the ethical consideration. We benchmark 21 popular MLLMs in a pairwise-comparison fashion, showing diverse performance across models. More- over, the validity of our benchmark manifests itself in reaching 88.02% agreement with human evaluation. We contend that the proposed paradigm explores the poten- tial of MLLMs as effective evaluation tools with the help of per-sample criteria, and that MLLM-Bench will serve as a catalyst for encouraging the development of user- centric MLLMs tailored to real-world applications. Our benchmark data, online leaderboard and submission entry are at https://mllm-bench.llmzoo.com/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2404.16790",
        "title": "SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension",
        "abstract": "Comprehending text-rich visual content is paramount for the practical application of Multimodal Large Language Models (MLLMs), since text-rich scenarios are ubiquitous in the real world, which are characterized by the presence of extensive texts embedded within images. Recently, the advent of MLLMs with impressive versatility has raised the bar for what we can expect from MLLMs. However, their proficiency in text-rich scenarios has yet to be comprehen- sively and objectively assessed, since current MLLM bench- marks primarily focus on evaluating general visual compre- hension. In this work, we introduce SEED-Bench-2-Plus, a benchmark specifically designed for evaluating text-rich vi- sual comprehension of MLLMs. Our benchmark comprises 2.3K multiple-choice questions with precise human anno- tations, spanning three broad categories: Charts, Maps, and Webs, each of which covers a wide spectrum of text- rich scenarios in the real world. These categories, due to their inherent complexity and diversity, effectively sim- ulate real-world text-rich environments. We further conduct a thorough evaluation involving 34 prominent MLLMs (in- cluding GPT-4V, Gemini-Pro-Vision and Claude-3-Opus) and emphasize the current limitations of MLLMs in text- rich visual comprehension. We hope that our work can serve as a valuable addition to existing MLLM benchmarks, providing insightful observations and inspiring further re- search in the area of text-rich visual comprehension with MLLMs. The dataset and evaluation code can be accessed at https://github.com/AILab-CVC/SEED-Bench.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.06281",
        "title": "MMBench: Is Your Multi-modal Model an All-around Player?",
        "abstract": "Large vision-language models (VLMs) have recently achieved remarkable progress, exhibiting impressive multimodal perception and reasoning abilities. However, effectively evaluating these large VLMs remains a major challenge, hindering future development in this domain. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but lack fine-grained abil- ity assessment and robust evaluation metrics. Meanwhile, subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model\u2019s abilities by incor- porating human labor, which is not scalable and may display significant bias. In response to these challenges, we propose MMBench, a bilingual benchmark for assessing the multi-modal capabilities of VLMs. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of the following key features: 1. MMBench is meticulously curated with well-designed quality con- trol schemes, surpassing existing similar benchmarks in terms of the number and variety of evaluation questions and abilities; 2. MMBench introduces a rigorous CircularEval strategy and incorporates large language models to convert free-form predictions into pre-defined choices, which helps to yield accurate evaluation results for models with limited instruction-following capabilities. 3. MMBench incorpo- rates multiple-choice questions in both English and Chinese versions, enabling an apples-to-apples comparison of VLMs\u2019 performance under a bilingual context. To summarize, MMBench is a systematically designed objective benchmark for a robust and holistic evaluation of vision-language models. We hope MMBench will assist the research community in better evaluating their models and facilitate future progress in this area. The evalutation code of MMBench has been integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit. 1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.14515",
        "title": "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding",
        "abstract": "The advent of large vision-language models (LVLMs) has spurred research into their applications in multi-modal contexts, particularly in video understanding. Traditional VideoQA benchmarks, despite providing quantitative metrics, often fail to encompass the full spectrum of video content and inadequately assess models\u2019 temporal comprehension. To address these limitations, we introduce MMBench- Video, a quantitative benchmark designed to rigorously evaluate LVLMs\u2019 pro- ficiency in video understanding. MMBench-Video incorporates lengthy videos from YouTube and employs free-form questions, mirroring practical use cases. The benchmark is meticulously crafted to probe the models\u2019 temporal reasoning skills, with all questions human-annotated according to a carefully constructed ability taxonomy. We employ GPT-4 for automated assessment, demonstrating superior accuracy and robustness over earlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted comprehensive evaluations that include both proprietary and open-source LVLMs for images and videos. MMBench-Video stands as a valuable resource for the research community, facilitating improved evaluation of LVLMs and catalyzing progress in the field of video understanding. The evalutation code of MMBench-Video will be integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.16125",
        "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension",
        "abstract": "Based on powerful Large Language Models (LLMs), recent generative Multi- modal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and gen- eration. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (\u00d76 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.07559",
        "title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
        "abstract": "\nLarge Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accu- racy without ground-truth. Retrieval-Augmented Generation (RAG) models have been proposed to reduce hallucinations and provide provenance for how an an- swer was generated. Applying such models to the scientific literature may enable large-scale, systematic processing of scientific knowledge. We present PaperQA, a RAG agent for answering questions over the scientific literature. PaperQA is an agent that performs information retrieval across full-text scientific articles, as- sesses the relevance of sources and passages, and uses RAG to provide answers. Viewing this agent as a question-answering model, we find it exceeds performance of existing LLMs and LLM agents on current science QA benchmarks. To push the field closer to how humans perform research on scientific literature, we also introduce LitQA, a more complex benchmark that requires retrieval and synthesis of information from full-text scientific papers across the literature. Finally, we demonstrate PaperQA\u2019s matches expert human researchers on LitQA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.11416",
        "title": "Scaling Instruction-Finetuned Language Models",
        "abstract": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation, RealToxicityPrompts). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PaLM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2207.13242",
        "title": "Uncertainty-based Visual Question Answering: Estimating Semantic Inconsistency between Image and Knowledge Base",
        "abstract": "Knowledge-based visual question answering\n(KVQA) task aims to answer questions that require additional external knowledge as well as an understanding of images and questions. Recent studies on KVQA inject an external knowledge in a multi-modal form, and as more knowledge is used, irrelevant information may be added and can confuse the question answering. In order to properly use the knowledge, this study proposes the following: 1) we introduce a novel semantic inconsistency measure computed from caption uncertainty and semantic similarity; 2) we suggest a new external knowledge assimilation method based on the semantic inconsistency measure and apply it to integrate explicit knowledge and implicit knowledge for KVQA; 3) the proposed method is evaluated with the OK-VQA dataset and achieves the state-of-the-art performance.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.08622",
        "title": "Multiple-Question Multiple-Answer Text-VQA",
        "abstract": "We present Multiple-Question Multiple- Answer (MQMA), a novel approach to do text-VQA in encoder-decoder transformer models. The text-VQA task requires a model to answer a question by understanding multi-modal content: text (typically from OCR) and an associated image. To the best of our knowledge, almost all previous approaches for text-VQA process a single question and its associated content to predict a single answer. In order to answer multiple questions from the same image, each question and content are fed into the model multiple times. In contrast, our proposed MQMA approach takes multiple questions and content as input at the encoder and predicts multiple answers at the decoder in an auto-regressive manner at the same time. We make several novel architectural modifica- tions to standard encoder-decoder transformers to support MQMA. We also propose a novel MQMA denoising pre-training task which is designed to teach the model to align and delineate multiple questions and content with associated answers. MQMA pre-trained model achieves state-of-the-art results on multiple text-VQA datasets, each with strong baselines. Specifically, on OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%), DocVQA (+1.1%) absolute improvements over the previous state-of-the-art approaches.\n",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2404.18397",
        "title": "ViOCRVQA: Novel Benchmark Dataset and Vision Reader for Visual Question Answering by Understanding Vietnamese Text in Images",
        "abstract": "\nOptical Character Recognition - Visual Question Answering (OCR-VQA) is the task of answering text information contained in images that have just been sig- nificantly developed in the English language in recent years. However, there are limited studies of this task in low-resource languages such as Vietnamese. To this end, we introduce a novel dataset, ViOCRVQA (Vietnamese Optical Character Recognition - Visual Question Answering dataset), consisting of 28,000+ im- ages and 120,000+ question-answer pairs. In this dataset, all the images contain text and questions about the information relevant to the text in the images. We deploy ideas from state-of-the-art methods proposed for English to conduct ex- periments on our dataset, revealing the challenges and difficulties inherent in a Vietnamese dataset. Furthermore, we introduce a novel approach, called Vision- Reader, which achieved 0.4116 in EM and 0.6990 in the F1-score on the test set. Through the results, we found that the OCR system plays a very important role in VQA models on the ViOCRVQA dataset. In addition, the objects in the image also play a role in improving model performance. We open access to our dataset at link for further research in OCR-VQA task in Vietnamese.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1905.13648",
        "title": "Scene Text Visual Question Answering",
        "abstract": "Current visual question answering datasets do not con- sider the rich semantic information conveyed by text within an image. In this work, we present a new dataset, ST-VQA, that aims to highlight the importance of exploiting high- level semantic information present in images as textual cues in the Visual Question Answering process. We use this dataset to define a series of tasks of increasing difficulty for which reading the scene text in the context provided by the visual information is necessary to reason and generate an appropriate answer. We propose a new evaluation metric for these tasks to account both for reasoning errors as well as shortcomings of the text recognition module. In addition we put forward a series of baseline methods, which provide further insight to the newly released dataset, and set the scene for further research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2112.12494",
        "title": "LaTr: Layout-Aware Transformer for Scene-Text VQA",
        "abstract": "We propose a novel multimodal architecture for Scene Text Visual Question Answering (STVQA), named Layout- Aware Transformer (LaTr). The task of STVQA requires models to reason over different modalities. Thus, we first investigate the impact of each modality, and reveal the im- portance of the language module, especially when enriched with layout information. Accounting for this, we propose a single objective pre-training scheme that requires only text and spatial cues. We show that applying this pre-training scheme on scanned documents has certain advantages over using natural images, despite the domain gap. Scanned documents are easy to procure, text-dense and have a vari- ety of layouts, helping the model learn various spatial cues (e.g. left-of, below etc.) by tying together language and layout information. Compared to existing approaches, our method performs vocabulary-free decoding and, as shown, generalizes well beyond the training vocabulary. We further demonstrate that LaTr improves robustness towards OCR errors, a common reason for failure cases in STVQA. In addition, by leveraging a vision transformer, we eliminate the need for an external object detector. LaTr outperforms state-of-the-art STVQA methods on multiple datasets. In particular, +7.6% on TextVQA, +10.8% on ST-VQA and +4.0% on OCR-VQA (all absolute accuracy numbers).",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1707.06209",
        "title": "Crowdsourcing Multiple Choice Science Questions",
        "abstract": "We present a novel method for obtain- ing high-quality, domain-targeted multi- ple choice questions from crowd workers. Generating these questions can be difficult without trading away originality, relevance or diversity in the answer options. Our method addresses these problems by lever- aging a large corpus of domain-specific text and a small set of existing ques- tions. It produces model suggestions for document selection and answer distractor choice which aid the human question gen- eration process. With this method we have assembled SciQ, a dataset of 13.7K mul- tiplechoicescienceexamquestions.1 We demonstrate that the method produces in- domain questions by providing an analysis of this new dataset and by showing that hu- mans cannot distinguish the crowdsourced questions from original questions. When using SciQ as additional training data to existing questions, we observe accuracy improvements on real science exams.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2203.14371",
        "title": "MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering",
        "abstract": "This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answer- ing (MCQA) dataset designed to address real- world medical entrance exam questions. More than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a ques- tion, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects & top- ics. A detailed explanation of the solution, along with the above information, is provided in this study.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2404.19024",
        "title": "Multi-Page Document Visual Question Answering using Self-Attention Scoring Mechanism",
        "abstract": "Documentsare2-dimensionalcarriersofwrittencommunication,and as such their interpretation requires a multi-modal approach where textual and visual information are efficiently combined. Document Visual Question Answer- ing (Document VQA), due to this multi-modal nature, has garnered significant interest from both the document understanding and natural language processing communities. The state-of-the-art single-page Document VQA methods show im- pressive performance, yet in multi-page scenarios, these methods struggle. They have to concatenate all pages into one large page for processing, demanding sub- stantial GPU resources, even for evaluation. In this work, we propose a novel method and efficient training strategy for multi-page Document VQA tasks. In particular, we employ a visual-only document representation, leveraging the en- coder from a document understanding model, Pix2Struct. Our approach utilizes a self-attention scoring mechanism to generate relevance scores for each docu- ment page, enabling the retrieval of pertinent pages. This adaptation allows us to extend single-page Document VQA models to multi-page scenarios without constraints on the number of pages during evaluation, all with minimal demand for GPU resources. Our extensive experiments demonstrate not only achieving state-of-the-art performance without the need for Optical Character Recognition (OCR), but also sustained performance in scenarios extending to documents of nearly 800 pages compared to a maximum of 20 pages in the MP-DocVQA dataset. Our code is publicly available at https://github.com/leitro/ SelfAttnScoring- MPDocVQA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2007.00398",
        "title": "DocVQA: A Dataset for VQA on Document Images",
        "abstract": "We present a new dataset for Visual Question Answering (VQA) on document images called DocVQA. The dataset consists of 50,000 questions defined on 12,000+ document images. Detailed analysis of the dataset in comparison with similar datasets for VQA and reading comprehension is pre- sented. We report several baseline results by adopting exist- ing VQA and reading comprehension models. Although the existing models perform reasonably well on certain types of questions, there is large performance gap compared to human performance (94.36% accuracy). The models need to improve specifically on questions where understanding structure of the document is crucial. The dataset, code and leaderboard are available at docvqa.org",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.00295",
        "title": "Making the V in Text-VQA Matter",
        "abstract": "Text-based VQA aims at answering questions by reading the text present in the images. It requires a large amount of scene-text relationship understanding compared to the VQA task. Recent studies have shown that the question- answer pairs in the dataset are more focused on the text present in the image but less importance is given to visual features and some questions do not require understanding the image. The models trained on this dataset predict bi- ased answers due to the lack of understanding of visual con- text. For example, in questions like \u201cWhat is written on the signboard?\u201d, the answer predicted by the model is always \u201cSTOP\u201d which makes the model to ignore the image. To address these issues, we propose a method to learn visual features (making V matter in TextVQA) along with the OCR features and question features using VQA dataset as exter- nal knowledge for Text-based VQA. Specifically, we com- bine the TextVQA dataset and VQA dataset and train the model on this combined dataset. Such a simple, yet effec- tive approach increases the understanding and correlation between the image features and text present in the image, which helps in the better answering of questions. We fur- ther test the model on different datasets and compare their qualitative and quantitative results.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2006.06434v1",
        "title": "TableQA: a Large-Scale Chinese Text-to-SQL Dataset for Table-Aware SQL Generation",
        "abstract": "Parsing natural language to corresponding SQL (NL2SQL) with data driven approaches like deep neural networks at- tracts much attention in recent years. Existing NL2SQL datasets assume that condition values should appear exactly in natural language questions and the queries are answerable given the table. However, these assumptions may fail in prac- tical scenarios, because user may use different expressions for the same content in the table, and query information out- side the table without the full picture of contents in table. Therefore we present TableQA, a large-scale cross-domain Natural Language to SQL dataset in Chinese language con- sisting 64,891 questions and 20,311 unique SQL queries on over 6,000 tables. Different from exisiting NL2SQL datasets, TableQA requires to generalize well not only to SQL skele- tons of different questions and table schemas, but also to the various expressions for condition values. Experiment results show that the state-of-the-art model with 95.1% condition value accuracy on WikiSQL only gets 46.8% condition value accuracy and 43.0% logic form accuracy on TableQA, indi- cating the proposed dataset is challenging and necessary to handle. Two table-aware approaches are proposed to alleviate the problem, the end-to-end approaches obtains 51.3% and 47.4% accuracy on the condition value and logic form tasks, with improvement of 4.7% and 3.4% respectively.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2301.09715v2",
        "title": "PRIMEQA: The Prime Repository for State-of-the-Art Multilingual Question Answering Research and Development",
        "abstract": "The field of Question Answering (QA) has made remarkable progress in recent years, thanks to the advent of large pre-trained language models, newer realistic benchmark datasets with leaderboards, and novel algo- rithms for key components such as retriev- ers and readers. In this paper, we introduce PRIMEQA: a one-stop and open-source QA repository with an aim to democratize QA re- search and facilitate easy replication of state- of-the-art (SOTA) QA methods. PRIMEQA supports core QA functionalities like retrieval and reading comprehension as well as auxil- iary capabilities such as question generation. It has been designed as an end-to-end toolkit for various use cases: building front-end ap- plications, replicating SOTA methods on pub- lic benchmarks, and expanding pre-existing methods. PRIMEQA is available at: https: //github.com/primeqa.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.01733",
        "title": "DocFormerv2: Local Features for Document Understanding",
        "abstract": "We propose DocFormerv2, a multi-modal transformer for Visual Document Understanding (VDU). The VDU do- main entails understanding documents (beyond mere OCR predictions) e.g., extracting information from a form, VQA for documents and other tasks. VDU is challenging as it needs a model to make sense of multiple modalities (visual, language and spatial) to make a prediction. Our approach, termed DocFormerv2 is an encoder-decoder transformer which takes as input - vision, language and spatial fea- tures. DocFormerv2 is pre-trained with unsupervised tasks employed asymmetrically i.e., two novel document tasks on encoder and one on the auto-regressive decoder. The un- supervised tasks have been carefully designed to ensure that the pre-training encourages local-feature alignment between multiple modalities. DocFormerv2 when evalu- ated on nine datasets shows state-of-the-art performance over strong baselines e.g. TabFact (4.3%), InfoVQA (1.4%), FUNSD (1%). Furthermore, to show generalization ca- pabilities, on three VQA tasks involving scene-text, Doc- Formerv2 outperforms previous comparably-sized models and even does better than much larger models (such as GIT2, PaLi and Flamingo) on some tasks. Extensive ab- lations show that due to its pre-training, DocFormerv2 un- derstands multiple modalities better than prior-art in VDU.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1909.02164",
        "title": "TABFACT: A LARGE-SCALE DATASET FOR TABLE- BASED FACT VERIFICATION\n",
        "abstract": "The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natu- ral language sentences and documents, news, etc), while verification under struc- tured evidence, such as tables, graphs, and databases, remains under-explored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, we construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural lan- guage statements, which are labeled as either ENTAILED or REFUTED. TabFact is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning. To address these reasoning challenges, we design two different mod- els: Table-BERT and Latent Program Algorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language model to encode the linearized tables and statements into continuous vectors for verification. LPA parses statements into programs and executes them against the tables to obtain the returned binary value for verification. Both methods achieve similar accuracy but still lag far be- hind human performance. We also perform a comprehensive analysis to demon- strate great future opportunities. The data and code of the dataset are provided in https://github.com/wenhuchen/Table-Fact-Checking.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2104.12756",
        "title": "InfographicVQA",
        "abstract": "Infographics communicate information using a combi- nation of textual, graphical and visual elements. In this work, we explore automatic understanding of infographic images by using a Visual Question Answering technique. To this end, we present InfographicVQA, a new dataset that comprises a diverse collection of infographics along with question-answer annotations. The questions require methods to jointly reason over the document layout, tex- tual content, graphical elements, and data visualizations. We curate the dataset with emphasis on questions that re- quire elementary reasoning and basic arithmetic skills. For VQA on the dataset, we evaluate two strong baselines based on state-of-the-art Transformer-based, scene text VQA and document understanding models. Poor performance of both the approaches compared to near perfect human perfor- mance suggests that VQA on infographics that are designed to communicate information quickly and clearly to human brain, is ideal for benchmarking machine understanding of complex document images. The dataset, code and leader- board will be made available at docvqa.org",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2104.00369v1",
        "title": "FeTaQA: Free-form Table Question Answering",
        "abstract": "Existing table question answering datasets contain abundant factual questions that pri- marily evaluate the query and schema com- prehension capability of a system, but they fail to include questions that require com- plex reasoning and integration of informa- tion due to the constraint of the associated short-form answers. To address these is- sues and to demonstrate the full challenge of table question answering, we introduce Fe- TaQA, a new dataset with 10K Wikipedia- based {table, question, free-form answer, supporting table cells} pairs. FeTaQA yields a more challenging table question answering set- ting because it requires generating free-form text answers after retrieval, inference, and inte- gration of multiple discontinuous facts from a structured knowledge source. Unlike datasets of generative QA over text in which answers are prevalent with copies of short text spans from the source, answers in our dataset are human-generated explanations involving enti- ties and their high-level relations. We provide two benchmark methods for the proposed task: a pipeline method based on semantic parsing- based QA systems and an end-to-end method based on large pretrained text generation mod- els, and show that FeTaQA poses a challenge for both methods.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2201.06223v2",
        "title": "Korean-Specific Dataset for Table Question Answering",
        "abstract": "Existing question answering systems mainly focus on dealing with text data. However, much of the data produced daily is stored in the form of tables that can be found in documents and relational databases, or on the web. To solve the task of question answering over tables, there exist many datasets for table question answering written in English, but few Korean datasets. In this paper, we demonstrate how we construct Korean-specific datasets for table question answering: Korean tabular dataset is a collection of 1.4M tables with corresponding descriptions for unsupervised pre-training language models. Korean table question answering corpus consists of 70k pairs of questions and answers created by crowd-sourced workers. Subsequently, we then build a pre-trained language model based on Transformer and fine-tune the model for table question answering with these datasets. We then report the evaluation results of our model. We make our datasets publicly available via our GitHub repository and hope that those datasets will help further studies for question answering over tables, and for the transformation of table formats.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2007.02871v2",
        "title": "DART: Open-Domain Structured Data Record to Text Generation",
        "abstract": "We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-Text an- notations can be a costly process, especially when dealing with tables which are the ma- jor source of structured data and contain non- trivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploit- ing the semantic dependencies among table headers and the table title. Our dataset con- struction framework effectively merged hetero- geneous sources from open domain semantic parsing and dialogue-act-based meaning rep- resentation tasks by utilizing techniques such as: tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post- editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain gen- eralization. Our data and code can be found at https://github.com/Yale-LILY/ dart.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1903.00161v2",
        "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
        "abstract": "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. How- ever, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We in- troduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96k- question benchmark, a system must resolve references in a question, perhaps to multiple in- put positions, and perform discrete operations over them (such as addition, counting, or sort- ing). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and se- mantic parsing literatures on this dataset and show that the best systems only achieve 32.7% F1 on our generalized accuracy metric, while expert human performance is 96.4%. We ad- ditionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F1.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2106.12944v1",
        "title": "AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry",
        "abstract": "Recent advances in transformers have enabled Table Question Answering (Table QA) systems to achieve high accuracy and SOTA results on open domain datasets like WikiTableQuestions and WikiSQL. Such transformers are frequently pre- trained on open-domain content such as Wikipedia, where they effectively encode questions and corresponding tables from Wikipedia as seen in Table QA dataset. However, web tables in Wikipedia are notably flat in their layout, with the first row as the sole column header. The layout lends to a relational view of tables where each row is a tuple. Whereas, tables in domain-specific business or scientific documents often have a much more complex layout, including hierarchical row and column headers, in addition to having specialized vocabulary terms from that domain.\nTo address this problem, we introduce the domain-specific Table QA dataset AIT- QA (Airline Industry Table QA). The dataset consists of 515 questions authored by human annotators on 116 tables extracted from public U.S. SEC filings1 of major airline companies for the fiscal years 2017-2019. We also provide annotations pertaining to the nature of questions, marking those that require hierarchical head- ers, domain-specific terminology, and paraphrased forms. Our zero-shot baseline evaluation of three transformer-based SOTA Table QA methods - TaPAS (end-to- end), TaBERT (semantic parsing-based), and RCI (row-column encoding-based) - clearly exposes the limitation of these methods in this practical setting, with the best accuracy at just 51.8% (RCI). We also present pragmatic table pre-processing steps used to pivot and project these complex tables into a layout suitable for the SOTA Table QA models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.15147v2",
        "title": "S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models",
        "abstract": "\nThe rapid development of Large Language Models (LLMs) has led to great strides in model capabilities like long-context under- standing and reasoning. However, as LLMs are able to process longer contexts, it becomes more challenging to evaluate whether they have acquired certain capabilities, since the length of text (e.g., 200K tokens) they can process far exceeds what humans can reliably assess in a reasonable duration. In this paper, we propose using complex synthetic tasks as a proxy evaluation method, and present S3EVAL, a Synthetic, Scalable, Systematic evaluation suite for LLMs evaluation. The synthetic nature of S3EVAL provides users full control over the dataset, allowing them to systematically probe LLM capabilities by scaling text length and varying task difficulty across diverse scenar- ios. The strong correlation between S3EVAL and real-world benchmarks demonstrates the soundness of using S3EVAL for evaluation of LLMs. S3EVAL provides a flexible and infi- nite long-context data generation method. We have generated a comprehensive dataset called S3EVAL-Standard, and experimental results have shown that it poses significant challenges for all existing LLMs. Our code is available at https://github.com/lfy79001/S3Eval.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2001.08034v1",
        "title": "MANYMODALQA: Modality Disambiguation and QA over Diverse Inputs",
        "abstract": "We present a new multimodal question answering challenge, MANYMODALQA, in which an agent must answer a ques- tion by considering three distinct modalities: text, images, and tables. We collect our data by scraping Wikipedia and then utilize crowdsourcing to collect question-answer pairs. Our questions are ambiguous, in that the modality that con- tains the answer is not easily determined based solely upon the question. To demonstrate this ambiguity, we construct a modality selector (or disambiguator) network, and this model gets substantially lower accuracy on our challenge set, com- pared to existing datasets, indicating that our questions are more ambiguous. By analyzing this model, we investigate which words in the question are indicative of the modal- ity. Next, we construct a simple baseline MANYMODALQA model, which, based on the prediction from the modality se- lector, fires a corresponding pre-trained state-of-the-art uni- modal QA model. We focus on providing the community with a new manymodal evaluation set and only provide a fine-tuning set, with the expectation that existing datasets and approaches will be transferred for most of the training, to encourage low-resource generalization without large, mono- lithic training sets for each new task. There is a significant gap between our baseline models and human performance; therefore, we hope that this challenge encourages research in end-to-end modality disambiguation and multimodal QA models, as well as transfer learning.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.07288v1",
        "title": "Open-WikiTable: Dataset for Open Domain Question Answering with Complex Reasoning over Table",
        "abstract": "Despite recent interest in open domain ques- tion answering (ODQA) over tables, many studies still rely on datasets that are not truly optimal for the task with respect to utilizing structural nature of table. These datasets as- sume answers reside as a single cell value and do not necessitate exploring over multi- ple cells such as aggregation, comparison, and sorting. Thus, we release Open-WikiTable, the first ODQA dataset that requires complex rea- soning over tables. Open-WikiTable is built upon WikiSQL and WikiTableQuestions to be applicable in the open-domain setting. As each question is coupled with both textual answers and SQL queries, Open-WikiTable opens up a wide range of possibilities for future research, as both reader and parser methods can be ap- plied. The dataset and code are publicly avail- able1 .",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2402.01155v3",
        "title": "CABINET: CONTENT RELEVANCE BASED NOISE REDUCTION FOR TABLE QUESTION ANSWERING",
        "abstract": "Table understanding capability of Large Language Models (LLMs) has been ex- tensively studied through the task of question-answering (QA) over tables. Typ- ically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse Re- ductioN for TablE QuesTion-Answering) \u2013 a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET com- prises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input ques- tion before feeding it to the question-answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that gen- erates a parsing statement describing the criteria of rows and columns relevant to the question and highlights the content of corresponding table cells. CABINET significantly outperforms various tabular LLM baselines, as well as GPT3-based in-context learning methods, is more robust to noise, maintains outperformance on tables of varying sizes, and establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We release our code and datasets here.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2101.11272",
        "title": "VisualMRC: Machine Reading Comprehension on Document Images",
        "abstract": "Recent studies on machine reading comprehension have focused on text-level understanding but have not yet reached the level of human understanding of the visual layout and content of real-world documents. In this study, we introduce a new visual machine reading comprehension dataset, named VisualMRC, wherein given a question and a document image, a machine reads and comprehends texts in the image to answer the question in natural language. Compared with existing visual question answering (VQA) datasets that contain texts in images, VisualMRC focuses more on developing natural language understanding and generation abilities. It contains 30,000+ pairs of a question and an abstractive answer for 10,000+ document images sourced from multiple domains of webpages. We also introduce a new model that extends existing sequence-to-sequence models, pre-trained with largescale text corpora, to take into account the visual layout and content of documents. Experiments with VisualMRC show that this model outperformed the base sequence-to-sequence models and a state-of-the-art VQA model. However, its performance is still below that of humans on most automatic evaluation metrics. The dataset will facilitate research aimed at connecting vision and language understanding.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2404.12720",
        "title": "PDF-MVQA: A Dataset for Multimodal Information Retrieval in PDF-based Visual Question Answering",
        "abstract": "PDF-MVQA: A Dataset for Multimodal Information Retrieval in PDF-based Visual Question Answering",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.04325",
        "title": "ShareGPT4Video: Improving Video Understanding and Generation with Better Captions",
        "abstract": "We present the ShareGPT4Video series, aiming to facilitate the video understand- ing of large video-language models (LVLMs) and the video generation of text-to- video models (T2VMs) via dense and precise captions. The series comprises: 1) ShareGPT4Video, 40K GPT4V annotated dense captions of videos with various lengths and sources, developed through carefully designed data filtering and an- notating strategy. 2) ShareCaptioner-Video, an efficient and capable captioning model for arbitrary videos, with 4.8M high-quality aesthetic videos annotated by it. 3) ShareGPT4Video-8B, a simple yet superb LVLM that reached SOTA per- formance on three advancing video benchmarks. To achieve this, taking aside the non-scalable costly human annotators, we find using GPT4V to caption video with a naive multi-frame or frame-concatenation input strategy leads to less detailed and sometimes temporal-confused results. We argue the challenge of designing a high-quality video captioning strategy lies in three aspects: 1) Inter-frame precise temporal change understanding. 2) Intra-frame detailed content description. 3)Frame-numberscalabilityforarbitrary-lengthvideos. Tothisend,wemetic- ulously designed a differential video captioning strategy, which is stable, scalable, and efficient for generating captions for videos with arbitrary resolution, aspect ratios, and length. Based on it, we construct ShareGPT4Video, which contains 40K high-quality videos spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. Based on ShareGPT4Video, we further develop ShareCaptioner-Video, a superior captioner capable of efficiently generating high-quality captions for arbitrary videos. We annotated 4.8M aesthetically appealing videos by it and verified their effectiveness on a 10-second text2video generation task. For video understanding, we verified the effectiveness of ShareGPT4Video on several current LVLM architectures and presented our superb new LVLM ShareGPT4Video-8B. All the models, strategies, and annotations2 will be open-sourced and we hope this project can serve as a pivotal resource for advancing both the LVLMs and T2VMs community.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1906.02467",
        "title": "ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering",
        "abstract": "Recent developments in modeling language and vision have been successfully applied to image question answering. It is both crucial and natural to extend this research direction to the video domain for video question answering (VideoQA). Compared to the image domain where large scale and fully annotated benchmark datasets exists, VideoQA datasets are limited to small scale and are automatically generated, etc. These limitations restrict their applicability in practice. Here we introduce ActivityNet-QA, a fully annotated and large scale VideoQA dataset. The dataset consists of 58,000 QA pairs on 5,800 complex web videos derived from the popular ActivityNet dataset. We present a statistical analysis of our ActivityNet-QA dataset and conduct extensive experiments on it by comparing existing VideoQA baselines. Moreover, we explore various video representation strategies to im- prove VideoQA performance, especially for long videos. The dataset is available at https://github.com/MILVLG/ activitynet-qa",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1705.08421v4",
        "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions",
        "abstract": "This paper introduces a video dataset of spatio- temporally localized Atomic Visual Actions (AVA). The AVA dataset densely annotates 80 atomic visual actions in 430 15-minute video clips, where actions are localized in space and time, resulting in 1.58M action labels with multiple labels per person occurring frequently. The key charac- teristics of our dataset are: (1) the definition of atomic visual actions, rather than composite actions; (2) precise spatio-temporal annotations with possibly multiple annota- tions for each person; (3) exhaustive annotation of these atomic actions over 15-minute video clips; (4) people tem- porally linked across consecutive segments; and (5) using movies to gather a varied set of action representations. This departs from existing datasets for spatio-temporal action recognition, which typically provide sparse annotations for composite actions in short video clips.\nAVA, with its realistic scene and action complexity, ex- poses the intrinsic difficulty of action recognition. To bench- mark this, we present a novel approach for action local- ization that builds upon the current state-of-the-art meth- ods, and demonstrates better performance on JHMDB and UCF101-24 categories. While setting a new state of the art on existing datasets, the overall results on AVA are low at 15.6% mAP, underscoring the need for developing new ap- proaches for video understanding.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1808.06492v1",
        "title": "Benchmarking Automatic Machine Learning Frameworks",
        "abstract": "AutoML serves as the bridge between varying lev- els of expertise when designing machine learning systems and expedites the data science process. A wide range of techniques is taken to address this, however there does not exist an objective comparison of these techniques. We present a benchmark of current open source AutoML so- lutions using open source datasets. We test auto- sklearn, TPOT, auto ml, and H2Os AutoML solu- tion against a compiled set of regression and clas- sification datasets sourced from OpenML and find that auto-sklearn performs the best across classifi- cation datasets and TPOT performs the best across regression datasets.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1705.06950v1",
        "title": "The Kinetics Human Action Video Dataset",
        "abstract": "We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as play- ing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1609.08675v1",
        "title": "YouTube-8M: A Large-Scale Video Classification Benchmark",
        "abstract": "Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learn- ing and inexpensive commodity hardware have reduced the bar- rier of entry for exploring novel approaches at scale. It is possible to train models over millions of examples within a few days. Al- though large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets.\nIn this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of \u223c8 million videos\u2014500K hours of video\u2014annotated with a vocabulary of 4800 visual en- tities. To get the videos and their (multiple) labels, we used a YouTube video annotation system, which labels videos with the main topics in them. While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals, so they repre- sent an excellent target for content-based annotation approaches. We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre- trained on ImageNet to extract the hidden representation immedi- ately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download. The dataset contains frame-level features for over 1.9 billion video frames and 8 million videos, making it the largest public multi-label video dataset.\nWe trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using the publicly-available TensorFlow framework. We plan to release code for training a basic TensorFlow model and for computing metrics.\nWe show that pre-training on large data generalizes to other datasets like Sports-1M and ActivityNet. We achieve state-of-the-art on Ac- tivityNet, improving mAP from 53.8% to 77.6%. We hope that the unprecedented scale and diversity of YouTube-8M will lead to ad- vances in video understanding and representation learning.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1905.04757v2",
        "title": "NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding",
        "abstract": "Research on depth-based human activity analysis achieved outstanding performance and demonstrated the effectiveness of 3D representation for action recognition. The existing depth-based and RGB+D-based action recognition benchmarks have a number of limitations, including the lack of large-scale training samples, realistic number of distinct class categories, diversity in camera views, varied environmental conditions, and variety of human subjects. In this work, we introduce a large-scale dataset for RGB+D human action recognition, which is collected from 106 distinct subjects and contains more than 114 thousand video samples and 8 million frames. This dataset contains 120 different action classes including daily, mutual, and health-related activities. We evaluate the performance of a series of existing 3D activity analysis methods on this dataset, and show the advantage of applying deep learning methods for 3D-based human action recognition. Furthermore, we investigate a novel one-shot 3D activity recognition problem on our dataset, and a simple yet effective Action-Part Semantic Relevance-aware (APSR) framework is proposed for this task, which yields promising results for recognition of the novel action classes. We believe the introduction of this large-scale dataset will enable the community to apply, adapt, and develop various data-hungry learning techniques for depth-based and RGB+D-based human activity understanding. [The dataset is available at: http:// rose1.ntu.edu.sg/ Datasets/ actionRecognition.asp.]",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.08345v1",
        "title": "VALOR: Vision-Audio-Language\nOmni-Perception Pretraining Model and Dataset",
        "abstract": "In this paper, we propose a Vision-Audio-Language Omni-peRception pretraining model (VALOR) for multi-modal understanding and generation. Different from widely-studied vision-language pretraining models, VALOR jointly models relationships of vision, audio and language in an end-to-end manner. It contains three separate encoders for single modality representations, and a decoder for multimodal conditional text generation. We design two pretext tasks to pretrain VALOR model, including Multimodal Grouping Alignment (MGA) and Multimodal Grouping Captioning (MGC). MGA projects vision, language and audio to the same common space, building vision-language, audio-language and audiovisual-language alignment simultaneously. MGC learns how to generate text tokens in conditions of vision, audio or their both. To promote vision-audio-language pretraining research, we construct a large-scale high-quality tri-modality dataset named VALOR-1M, which contains 1M audiable videos with human annotated audiovisual captions. Extensive experiments show that VALOR can learn strong multimodal correlations and be generalized to various downstream tasks (e.g., retrieval, captioning and question answering), with different input modalities (e.g., vision-language, audio-language and audiovisual-language). VALOR achieves new state-of-the-art performances on series of public cross-modality benchmarks. Code and data are available at project page https://casia-iva-group.github.io/projects/VALOR.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.18500v2",
        "title": "VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset",
        "abstract": "Vision and text have been fully explored in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including Vision, Audio, and Subtitle, and Text by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open- domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vision-text, audio-text, and multi-modal video-text tasks (retrieval, captioning and QA). Extensive experiments have been conducted to demonstrate the effectiveness of our proposed VAST-27M corpus and VAST foundation model. VAST achieves 22 new state-of-the-art results on various cross-modality benchmarks. Code, model and dataset will be released at https://github.com/TXH-mercury/VAST.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1804.04527v2",
        "title": "SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos",
        "abstract": "In this paper, we introduce SoccerNet, a benchmark for action spotting in soccer videos. The dataset is composed of 500 complete soccer games from six main European leagues, covering three seasons from 2014 to 2017 and a total duration of 764 hours. A total of 6,637 temporal anno- tations are automatically parsed from online match reports at a one minute resolution for three main classes of events (Goal, Yellow/Red Card, and Substitution). As such, the dataset is easily scalable. These annotations are manually refined to a one second resolution by anchoring them at a single timestamp following well-defined soccer rules. With an average of one event every 6.9 minutes, this dataset fo- cuses on the problem of localizing very sparse events within long videos. We define the task of spotting as finding the anchors of soccer events in a video. Making use of recent developments in the realm of generic action recognition and detection in video, we provide strong baselines for detecting soccer events. We show that our best model for classifying temporal segments of length one minute reaches a mean Av- erage Precision (mAP) of 67.8%. For the spotting task, our baseline reaches an Average-mAP of 49.7% for tolerances \u03b4 ranging from 5 to 60 seconds. Our dataset and models are available at https://silviogiancola.github.io/SoccerNet.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2012.09434v2",
        "title": "Multi-shot Temporal Event Localization: a Benchmark",
        "abstract": "Current developments in temporal event or action lo- calization usually target actions captured by a single cam- era. However, extensive events or actions in the wild may be captured as a sequence of shots by multiple cameras at different positions. In this paper, we propose a new and challenging task called multi-shot temporal event lo- calization, and accordingly, collect a large-scale dataset called MUlti-Shot EventS (MUSES). MUSES has 31,477 event instances for a total of 716 video hours. The core nature of MUSES is the frequent shot cuts, for an average of 19 shots per instance and 176 shots per video, which in- duces large intra-instance variations. Our comprehensive evaluations show that the state-of-the-art method in tem- poral action localization only achieves an mAP of 13.1% at IoU=0.5. As a minor contribution, we present a sim- ple baseline approach for handling the intra-instance vari- ations, which reports an mAP of 18.9% on MUSES and 56.9% on THUMOS14 at IoU=0.5. To facilitate research in this direction, we release the dataset and the project code at https://songbai.site/muses/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2007.05515v3",
        "title": "AViD Dataset: Anonymized Videos from Diverse Countries",
        "abstract": "We introduce a new public video dataset for action recognition: Anonymized Videos from Diverse countries (AViD). Unlike existing public video datasets, AViD is a collection of action videos from many different countries. The motivation is to create a public dataset that would benefit training and pretraining of action recognition models for everybody, rather than making it useful for limited countries. Further, all the face identities in the AViD videos are properly anonymized to protect their privacy. It also is a static dataset where each video is licensed with the creative commons license. We confirm that most of the existing video datasets are statistically biased to only capture action videos from a limited number of countries. We experimentally illustrate that models trained with such biased datasets do not transfer perfectly to action videos from the other countries, and show that AViD addresses such problem. We also confirm that the new AViD dataset could serve as a good dataset for pretraining the models, performing comparably or better than prior datasets",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2010.05654v1",
        "title": "The MECCANO Dataset: Understanding Human-Object Interactions from Egocentric Videos in an Industrial-like Domain",
        "abstract": "Wearable cameras allow to collect images and videos of humans interacting with the world. While human-object in- teractions have been thoroughly investigated in third person vision, the problem has been understudied in egocentric set- tings and in industrial scenarios. To fill this gap, we intro- duce MECCANO, the first dataset of egocentric videos to study human-object interactions in industrial-like settings. MECCANO has been acquired by 20 participants who were asked to build a motorbike model, for which they had to interact with tiny objects and tools. The dataset has been explicitly labeled for the task of recognizing human-object interactions from an egocentric perspective. Specifically, each interaction has been labeled both temporally (with ac- tion segments) and spatially (with active object bounding boxes). With the proposed dataset, we investigate four dif- ferent tasks including 1) action recognition, 2) active ob- ject detection, 3) active object recognition and 4) egocen- tric human-object interaction detection, which is a revisited version of the standard human-object interaction detection task. Baseline results show that the MECCANO dataset is a challenging benchmark to study egocentric human- object interactions in industrial-like scenarios. We publicy release the dataset at https://iplab.dmi.unict. it/MECCANO.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1911.03911v2",
        "title": "Contract Discovery: Dataset and a Few-Shot Semantic Retrieval Challenge with Competitive Baselines",
        "abstract": "We propose a new shared task of semantic re- trieval from legal texts, in which a so-called contract discovery is to be performed\u2013where legal clauses are extracted from documents, given a few examples of similar clauses from other legal acts. The task differs substantially from conventional NLI and shared tasks on legal information extraction (e.g., one has to identify text span instead of a single document, page, or paragraph). The specification of the proposed task is followed by an evaluation of multiple solutions within the unified frame- work proposed for this branch of methods. It is shown that state-of-the-art pretrained encoders fail to provide satisfactory results on the task proposed. In contrast, Language Model-based solutions perform better, especially when un- supervised fine-tuning is applied. Besides the ablation studies, we addressed questions re- garding detection accuracy for relevant text fragments depending on the number of exam- ples available. In addition to the dataset and reference results, LMs specialized in the legal domain were made publicly available.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.07574",
        "title": "To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning",
        "abstract": "Existing visual instruction tuning methods typically prompt large language mod- els with textual descriptions to generate instruction-following data. Despite the promising performance achieved, these descriptions are derived from image an- notations, which are oftentimes coarse-grained. Furthermore, the instructions might even contradict the visual content without observing the entire visual context. To address this challenge, we introduce a fine-grained visual instruction dataset, LVIS-INSTRUCT4V, which contains 220K visually aligned and context-aware instructions produced by prompting the powerful GPT-4V with images from LVIS. Through experimental validation and case studies, we demonstrate that high-quality visual instructional data could improve the performance of LLaVA-1.5, a state- of-the-art large multimodal model, across a wide spectrum of benchmarks by clear margins. Notably, by simply replacing the LLaVA-Instruct with our LVIS- INSTRUCT4V, we achieve better results than LLaVA on most challenging LMM benchmarks, e.g., LLaVAw (76.7 vs. 70.7) and MM-Vet (40.2 vs. 35.4). We release our data and model at https://github.com/X2FD/LVIS-INSTRUCT4V.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2205.06207",
        "title": "CiteSum: Citation Text-guided Scientific Extreme Summarization and Domain Adaptation with Limited Supervision",
        "abstract": "Scientific extreme summarization (TLDR) aims to form ultra-short summaries of scien- tific papers. Previous efforts on curating sci- entific TLDR datasets failed to scale up due to the heavy human annotation and domain ex- pertise required. In this paper, we propose a simple yet effective approach to automatically extracting TLDR summaries for scientific pa- pers from their citation texts. Based on the pro- posed approach, we create a new benchmark CiteSum without human annotation, which is around 30 times larger than the previous human-curated dataset SciTLDR. We conduct a comprehensive analysis of CiteSum, exam- ining its data characteristics and establishing strong baselines. We further demonstrate the usefulness of CiteSum by adapting models pre- trained on CiteSum (named CITES) to new tasks and domains with limited supervision. For scientific extreme summarization, CITES outperforms most fully-supervised methods on SciTLDR without any fine-tuning and ob- tains state-of-the-art results with only 128 ex- amples. For news extreme summarization, CITES achieves significant gains on XSum over its base model (not pre-trained on Cite- Sum), e.g., +7.2 ROUGE-1 zero-shot per- formance and state-of-the-art few-shot perfor- mance. For news headline generation, CITES performs the best among unsupervised and zero-shot methods on Gigaword.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.14197v3",
        "title": "Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models",
        "abstract": "The integration of large language models (LLMs) with external content has enabled more up-to-date and wide-ranging applications of LLMs, such as Microsoft Copilot. However, this integration has also exposed LLMs to the risk of indirect prompt injection attacks, where an attacker can embed malicious instructions within external content, compromising LLM output and causing responses to deviate from user expectations. To investigate this important but underexplored issue, we introduce the first benchmark for indirect prompt injection attacks, named BIPIA, to evaluate the risk of such attacks. Based on the evaluation, our work makes a key analysis of the underlying reason for the success of the attack, namely the inability of LLMs to distinguish between instructions and external content and the absence of LLMs\u2019 awareness to not execute instructions within external content. Building upon this analysis, we develop two black-box methods based on prompt learning and a white-box defense method based on fine-tuning with adversarial training accordingly. Experimental results demonstrate that black-box defenses are highly effective in mitigating these attacks, while the white-box defense reduces the attack success rate to near-zero levels. Overall, our work systematically investigates indirect prompt injection attacks by introducing a benchmark, analyzing the underlying reason for the success of the attack, and developing an initial set of defenses",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1606.02858",
        "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task",
        "abstract": "Enabling a computer to understand a docu- ment so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solu- tion by machine learned systems is the lim- ited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language under- standing is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by show- ing that simple, carefully designed systems can obtain accuracies of 73.6% and 76.6% on these two datasets, exceeding current state-of-the-art results by 7\u201310% and ap- proaching what we believe is the ceiling for performance on this task.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1804.11283",
        "title": "NEWSROOM: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies",
        "abstract": "We present NEWSROOM, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrow- ing words and phrases from articles at vary- ing rates. We analyze the extraction strategies used in NEWSROOM summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1906.01749",
        "title": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
        "abstract": "Automatic generation of summaries from mul- tiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) sys- tems have benefited from advances in neu- ral encoder-decoder model thanks to the avail- ability of large datasets. However, multi- document summarization (MDS) of news ar- ticles has been limited to datasets of a couple of hundred examples. In this paper, we in- troduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a tradi- tional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark sev- eral methods on Multi-News and release our data and code in hope that this work will pro- mote advances in summarization in the multi- document setting1.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2110.08499",
        "title": "PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization",
        "abstract": "We introduce PRIMERA, a pre-trained model for multi-document representation with a fo- cus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and ag- gregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, PRIMERA outper- forms current state-of-the-art dataset-specific and pre-trained models on most of these set- tings with large margins.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1906.03741",
        "title": "BIGPATENT: A Large-Scale Dataset\nfor Abstractive and Coherent Summarization",
        "abstract": "Most existing text summarization datasets are compiled from the news domain, where sum- maries have a flattened discourse structure. In such datasets, summary-worthy content of- ten appears in the beginning of input arti- cles. Moreover, large segments from input ar- ticles are present verbatim in their respective summaries. These issues impede the learn- ing and evaluation of systems that can under- stand an article\u2019s global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consist- ing of 1.3 million records of U.S. patent doc- uments along with human written abstractive summaries. Compared to existing summa- rization datasets, BIGPATENT has the follow- ing properties: i) summaries contain a richer discourse structure with more recurring enti- ties, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Fi- nally, we train and evaluate baselines and pop- ular learning models on BIGPATENT to shed light on new challenges and motivate future di- rections for summarization research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2104.06486",
        "title": "MS\u02c62: A Dataset for Multi-Document Summarization of Medical Studies",
        "abstract": "To assess the effectiveness of any medical in- tervention, researchers must conduct a time- intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MS\u02c62 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory ev- idence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical do- main. We experiment with a summarization system based on BART, with promising early results, though significant work remains to achieve higher summarization quality. We for- mulate our summarization inputs and targets in both free text and structured forms and mod- ify a recently proposed metric to assess the quality of our system\u2019s generated summaries. Data and models are available at https:// github.com/allenai/ms2.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.04440v1",
        "title": "OPENASP: A Benchmark for Multi-document Open Aspect-based Summarization",
        "abstract": "The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting spe- cific information needs of users in real-world scenarios, particularly when a targeted sum- mary is sought, such as in the useful aspect- based summarization setting targeted in this paper. Previous datasets and studies for this set- ting have predominantly concentrated on a lim- ited set of pre-defined aspects, focused solely on single document inputs, or relied on syn- thetic data. To advance research on more realis- tic scenarios, we introduce OPENASP, a bench- mark for multi-document open aspect-based summarization. This benchmark is created us- ing a novel and cost-effective annotation proto- col, by which an open aspect dataset is derived from existing generic multi-document summa- rization datasets. We analyze the properties of OPENASP showcasing its high-quality content. Further, we show that the realistic open-aspect setting realized in OPENASP poses a challenge for current state-of-the-art summarization mod- els, as well as for large language models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.09369v2",
        "title": "Embrace Divergence for Richer Insights:\nA Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles",
        "abstract": "Previous research in multi-document news sum- marization has typically concentrated on col- lating information that all sources agree upon. However, the summarization of diverse infor- mation dispersed across multiple articles about an event remains underexplored. In this pa- per, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we present a data collection schema for identifying diverse information and curated a dataset named DIVERSESUMM. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Next, to enable consistent automatic evaluation, we con- duct a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries. Through correlation analyses, we outline the best practices for effectively using automatic LLM-based metrics on the DIVERS- ESUMM dataset. Finally, we study how LLMs summarize multiple news articles by analyz- ing which type of diverse information LLMs are capable of identifying. Our analyses sug- gest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover under 40% of the diverse information on average.1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2301.11312v1",
        "title": "LoRaLay: A Multilingual and Multimodal Dataset for Long Range and Layout-Aware Summarization",
        "abstract": "Text Summarization is a popular task and an active area of research for the Natural Lan- guage Processing community. It requires ac- counting for long input texts, a characteristic which poses computational challenges for neu- ral models. Moreover, real-world documents come in a variety of complex, visually-rich, layouts. This information is of great relevance, whether to highlight salient content or to en- code long-range interactions between textual passages. Yet, all publicly available summa- rization datasets only provide plain text con- tent. To facilitate research on how to exploit vi- sual/layout information to better capture long- range dependencies in summarization models, we present LoRaLay, a collection of datasets for long-range summarization with accompa- nying visual/layout information. We extend existing and popular English datasets (arXiv and PubMed) with visual/layout information and propose four novel datasets \u2013 consistently built from scholar resources \u2013 covering French, Spanish, Portuguese, and Korean languages. Further, we propose new baselines merging layout-aware and long-range models \u2013 two or- thogonal approaches \u2013 and obtain state-of-the- art results, showing the importance of combin- ing both lines of research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.19240v1",
        "title": "M4LE: A MULTI-ABILITY MULTI-RANGE MULTI- TASK MULTI-DOMAIN LONG-CONTEXT EVALUATION BENCHMARK FOR LARGE LANGUAGE MODELS",
        "abstract": "Managing long sequences has become an important and necessary feature for large language models (LLMs). However, it is still an open question of how to com- prehensively and systematically evaluate the long-sequence capability of LLMs. One of the reasons is that conventional and widely-used benchmarks mainly con- sist of short sequences. In this paper, we propose M4LE, a Multi-ability, Multi- range, Multi-task, Multi-domain benchmark for Long-context Evaluation. M4LE is based on a diverse NLP task pool comprising 36 NLP datasets, 11 task types and 12 domains. To alleviate the scarcity of tasks with naturally long sequences and incorporate multiple-ability assessment, we propose an automatic approach (but with negligible human annotations) to convert short-sequence tasks into a unified long-sequence scenario where LLMs have to identify single or multiple relevant spans in long contexts based on explicit or semantic hints. Specifically, the scenario includes five different types of abilities: (1) explicit single-span; (2) semantic single-span; (3) explicit multiple-span; (4) semantic multiple-span; and (5) global context understanding. The resulting samples in M4LE are evenly dis- tributed from 1k to 8k input length.1 We conducted a systematic evaluation on 11 well-established LLMs, especially those optimized for long-sequence inputs. Our results reveal that: 1) Current LLMs struggle to understand long context, particu- larly when tasks require multiple-span attention. 2) Semantic retrieval task is more difficult for competent LLMs. 3) Models fine-tuned on longer text with position interpolation have comparable performance to those using Neural Tangent Ker- nel (NTK) aware scaling methods without fine-tuning. We make our benchmark publicly available to encourage future research in this challenging area 2.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.11088v3",
        "title": "L-EVAL: INSTITUTING STANDARDIZED EVALUATION FOR LONG CONTEXT LANGUAGE MODELS",
        "abstract": "Recently, there has been growing interest in extending the context length of large language models (LLMs), aiming to effectively process long inputs of one turn or conversations with more extensive histories. While proprietary models such as GPT-4 and Claude can largely preserve the reasoning ability in an extended context, open-source models are still progressing through the early stages of de- velopment. To bridge this gap, we propose L-Eval to institute a more standardized evaluation for long context language models (LCLMs) addressing two key as- pects: dataset construction and evaluation metrics. On the one hand, we build a new evaluation suite containing 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs encompassing diverse question styles, domains, and input length (3k\u223c200k tokens). On the other hand, we investigate the effectiveness in evalution metrics for LCLMs. Results show that popular n- gram matching metrics generally can not correlate well with human judgment, and thus we strongly advocate for length-instruction-enhanced (LIE) evaluation and employing LLM judges. We conducted a comprehensive study of 4 popular commercial LLMs and 12 open-source counterparts using the L-Eval benchmark. Our empirical findings offer useful insights into the study of LCLMs and lay the groundwork for the development of more principled evaluation of these models.1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1902.01069",
        "title": "A Comprehensive Exploration on WikiSQL with Table-Aware Word Contextualization",
        "abstract": "We present SQLOVA, the first Natural-language-to-SQL (NL2SQL) model to achieve human performance in WikiSQL dataset. We revisit and discuss diverse popular methods in NL2SQL literature, take a full advantage of BERT (Devlin et al., 2018) through an effective table contextualization method, and coherently combine them, outperforming the previous state of the art by 8.2% and 2.5% in logical form and execution accuracy, respectively. We particularly note that BERT with a seq2seq decoder leads to a poor performance in the task, indicating the importance of a careful design when using such large pretrained models. We also provide a comprehensive analysis on the dataset and our model, which can be helpful for designing future NL2SQL datsets and models. We especially show that our model\u2019s performance is near the upper bound in WikiSQL, where we observe that a large portion of the evaluation errors are due to wrong annotations, and our model is already exceeding human performance by 1.3% in execution accuracy.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1810.09305",
        "title": "WikiHow: A Large Scale Text Summarization Dataset",
        "abstract": "Sequence-to-sequence models have recently gained the state of the art performance in sum- marization. However, not too many large-scale high-quality datasets are available and almost all the available ones are mainly news articles with specific writing style. Moreover, abstrac- tive human-style systems involving descrip- tion of the content at a deeper level require data with higher levels of abstraction. In this paper, we present WikiHow, a dataset of more than 230,000 article and summary pairs ex- tracted and constructed from an online knowl- edge base written by different human authors. The articles span a wide range of topics and therefore represent high diversity styles. We evaluate the performance of the existing meth- ods on WikiHow to present its challenges and set some baselines to further improve it.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1811.00783",
        "title": "Abstractive Summarization of Reddit Posts with Multi-level Memory Networks",
        "abstract": "\nWe address the problem of abstractive summa- rization in two directions: proposing a novel dataset and a new model. First, we collect Reddit TIFU dataset, consisting of 120K posts from the online discussion forum Reddit. We use such informal crowd-generated posts as text source, in contrast with existing datasets that mostly use formal documents as source such as news articles. Thus, our dataset could less suffer from some biases that key sentences usually locate at the beginning of the text and favorable summary candidates are already in- side the text in similar forms. Second, we pro- pose a novel abstractive summarization model named multi-level memory networks (MMN), equipped with multi-level memory to store the information of text from different levels of abstraction. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the Reddit TIFU dataset is highly ab- stractive and the MMN outperforms the state- of-the-art summarization models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2004.10706",
        "title": "CORD-19: The COVID-19 Open Research Dataset",
        "abstract": "The COVID-19 Open Research Dataset (CORD-19) is a growing1 resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded2 over 200K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomed- ical experts, and policy makers in the search for effective treatments and management policies for COVID-19.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1910.00523",
        "title": "BillSum: A Corpus for Automatic Summarization of US Legislation",
        "abstract": "Automatic summarization methods have been studied on a variety of domains, includ- ing news and scientific articles. Yet, leg- islation has not previously been considered for this task, despite US Congress and state governments releasing tens of thousands of bills every year. In this paper, we in- troduce BillSum, the first dataset for sum- marization of US Congressional and Cali- fornia state bills (https://github.com/ FiscalNote/BillSum). We explain the properties of the dataset that make it more challenging to process than other domains. Then, we benchmark extractive methods that consider neural sentence representations and traditional contextual features. Finally, we demonstrate that models built on Congres- sional bills can be used to summarize Cali- fornia bills, thus, showing that methods devel- oped on this dataset can transfer to states with- out human-written summaries.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2211.09783v6",
        "title": "UNISUMM and SUMMZOO:\nUnified Model and Diverse Benchmark for Few-Shot Summarization",
        "abstract": "The high annotation costs and diverse demands of various summarization tasks motivate the development of few-shot summarization. How- ever, despite the emergence of many summa- rization tasks and datasets, the current training paradigm for few-shot summarization systems ignores potentially shareable knowledge in het- erogeneous datasets. To this end, we propose UNISUMM, a unified few-shot summarization model pre-trained with multiple summarization tasks and can be prefix-tuned to excel at any few-shot summarization task. Meanwhile, to better evaluate few-shot summarizers, under the principles of diversity and robustness, we assemble and release a new benchmark SUMM- ZOO. It consists of 8 summarization tasks with multiple sets of few-shot samples for each task, covering diverse domains. Experimental results and analysis show that UNISUMM outperforms strong baselines by a large margin across all sub-tasks in SUMMZOO under both automatic and human evaluations and achieves compara- ble results in human evaluation compared with a GPT-3.5 model.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2301.13298v1",
        "title": "LONGEVAL: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization",
        "abstract": "\nWhile human evaluation remains best prac- tice for accurately judging the faithfulness of automatically-generated summaries, few solu- tions exist to address the increased difficulty and workload when evaluating long-form sum- maries. Through a survey of 162 papers on long-form summarization, we first shed light on current human evaluation practices surrounding long-form summaries. We find that 73% of these papers do not perform any human evaluation on model-generated sum- maries, while other works face new difficul- ties that manifest when dealing with long documents (e.g., low inter-annotator agree- ment). Motivated by our survey, we present LONGEVAL, a set of guidelines for human evaluation of faithfulness in long-form sum- maries that addresses the following challenges: (1) How can we achieve high inter-annotator agreement on faithfulness scores? (2) How can we minimize annotator workload while maintaining accurate faithfulness scores? and (3) Do humans benefit from automated align- ment between summary and source snippets? We deploy LONGEVAL in annotation studies on two long-form summarization datasets in different domains (SQuALITY and PubMed), and we find that switching to a finer granu- larity of judgment (e.g., clause-level) reduces inter-annotator variance in faithfulness scores (e.g., std-dev from 18.5 to 6.8). We also show that scores from a partial annotation of fine-grained units highly correlates with scores from a full annotation workload (0.89 Kendall\u2019s \u03c4 using 50% judgments). We release our human judgments, annotation templates, and our software for future research.1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2011.07636v3",
        "title": "Open4Business (O4B): An Open Access Dataset for Summarizing Business Documents",
        "abstract": "A major challenge in fine-tuning deep learning models for automatic summarization is the need for large domain specific datasets. One of the barriers to curating such data from resources like online publications is navigating the license regulations applicable to their re-use, especially for commercial purposes. As a result, despite the availability of several business journals there are no large scale datasets for sum- marizing business documents. In this work, we introduce Open4Business (O4B), a dataset of 17,458 open access business articles and their reference summaries. The dataset introduces a new challenge for summarization in the business domain, requiring highly abstractive and more concise summaries as compared to other existing datasets. Additionally, we evaluate existing models on it and consequently show that models trained on O4B and a 7x larger non\u2013open access dataset achieve comparable performance on summarization. We release the dataset, along with the code 2 which can be leveraged to similarly gather data for multiple domains.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1709.00103",
        "title": "SEQ2SQL: GENERATING STRUCTURED QUERIES FROM NATURAL LANGUAGE USING REINFORCEMENT LEARNING",
        "abstract": "Relational databases store a significant amount of the worlds data. However, ac- cessing this data currently requires users to understand a query language such as SQL. We propose Seq2SQL, a deep neural network for translating natural lan- guage questions to corresponding SQL queries. Our model uses rewards from in- the-loop query execution over the database to learn a policy to generate the query, which contains unordered parts that are less suitable for optimization via cross en- tropy loss. Moreover, Seq2SQL leverages the structure of SQL to prune the space of generated queries and significantly simplify the generation problem. In addition to the model, we release WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia that is an order of magnitude larger than comparable datasets. By applying policy- based reinforcement learning with a query execution environment to WikiSQL, Seq2SQL outperforms a state-of-the-art semantic parser, improving execution ac- curacy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1711.04436v1",
        "title": "SQLNet: GENERATING STRUCTURED QUERIES FROM NATURAL LANGUAGE WITHOUT REINFORCEMENT LEARNING",
        "abstract": "Synthesizing SQL queries from natural language is a long-standing open problem and has been attracting considerable interest recently. Toward solving the prob- lem, the de facto approach is to employ a sequence-to-sequence-style model. Such an approach will necessarily require the SQL queries to be serialized. Since the same SQL query may have multiple equivalent serializations, training a sequence- to-sequence-style model is sensitive to the choice from one of them. This phe- nomenon is documented as the \u201corder-matters\u201d problem. Existing state-of-the-art approaches rely on reinforcement learning to reward the decoder when it generates any of the equivalent serializations. However, we observe that the improvement from reinforcement learning is limited.\nIn this paper, we propose a novel approach, i.e., SQLNet, to fundamentally solve this problem by avoiding the sequence-to-sequence structure when the order does not matter. In particular, we employ a sketch-based approach where the sketch contains a dependency graph so that one prediction can be done by taking into consideration only the previous predictions that it depends on. In addition, we propose a sequence-to-set model as well as the column attention mechanism to synthesize the query based on the sketch. By combining all these novel tech- niques, we show that SQLNet can outperform the prior art by 9% to 13% on the WikiSQL task.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2004.02349v2",
        "title": "TAPAS: Weakly Supervised Table Parsing via Pre-training",
        "abstract": "Answering natural language questions over ta- bles is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training seman- tic parsers from weak supervision poses diffi- culties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this pa- per, we present TAPAS, an approach to ques- tion answering over tables without generating logical forms. TAPAS trains from weak super- vision, and predicts the denotation by select- ing table cells and optionally applying a cor- responding aggregation operator to such selec- tion. TAPAS extends BERT\u2019s architecture to encode tables as input, initializes from an ef- fective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three differ- ent semantic parsing datasets, and find that TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architec- ture. We additionally find that transfer learn- ing, which is trivial in our setting, from WIK- ISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2202.00454v1",
        "title": "TableQuery: Querying tabular data with natural language",
        "abstract": "This paper presents TableQuery, a novel tool for querying tabular data using deep learning models pre-trained to answer questions on free text. Existing deep learning methods for question answering on tabular data have various limitations, such as having to feed the entire table as input into a neural network model, making them unsuitable for most real-world applications. Since real-world data might contain millions of rows, it may not entirely fit into the memory. Moreover, data could be stored in live databases, which are updated in real-time, and it is impractical to serialize an entire database to a neural network-friendly format each time it is updated. In TableQuery, we use deep learning models pre-trained for question answering on free text to convert natural language queries to structured queries, which can be run against a database or a spreadsheet. This method eliminates the need for fitting the entire data into memory as well as serializing databases. Furthermore, deep learning models pre-trained for question answering on free text are readily available on platforms such as HuggingFace Model Hub (7). TableQuery does not require re-training; when a newly trained model for question answering with better performance is available, it can replace the existing model in TableQuery.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2404.18585v1",
        "title": "FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering",
        "abstract": "Table Question Answering (TQA) aims at com- posing an answer to a question based on tabu- lar data. While prior research has shown that TQA models lack robustness, understanding the underlying cause and nature of this issue remains predominantly unclear, posing a sig- nificant obstacle to the development of robust TQA systems. In this paper, we formalize three major desiderata for a fine-grained evaluation of robustness of TQA systems. They should (i) answer questions regardless of alterations in table structure, (ii) base their responses on the content of relevant cells rather than on biases, and (iii) demonstrate robust numerical reason- ing capabilities. To investigate these aspects, we create and publish a novel TQA evaluation benchmark in English. Our extensive experi- mental analysis reveals that none of the exam- ined state-of-the-art TQA systems consistently excelsinthesethreeaspects. Ourbenchmark is a crucial instrument for monitoring the be- havior of TQA systems and paves the way for the development of robust TQA systems. We release our benchmark publicly.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2404.13207v2",
        "title": "STARK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases",
        "abstract": "Answering real-world complex queries, such as complex product search, often requires accurate retrieval from semi-structured knowledge bases that involve blend of unstructured (e.g., textual descriptions of products) and structured (e.g., entity relations of products) information. However, previous works have mostly studied textual and relational retrieval tasks as separate topics. To address the gap, we develop STARK, a large-scale Semi-structure retrieval benchmark on Textual and Relational Knowledge Bases. Our benchmark covers three domains/datasets: product search, academic paper search, and queries in precision medicine. We design a novel pipeline to synthesize realistic user queries that integrate diverse relational information and complex textual properties, together with their ground- truth answers (items). We conduct rigorous human evaluation to validate the quality of our synthesized queries. We further enhance the benchmark with high-quality human-generated queries to provide an authentic reference. STARK serves as a comprehensive testbed for evaluating the performance of retrieval systems driven by large language models (LLMs). Our experiments suggest that STARK presents significant challenges to the current retrieval and LLM systems, indicating the demand for building more capable retrieval systems. The benchmark data and code are available on https://github.com/snap-stanford/stark.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1807.03168v1",
        "title": "NAPS: Natural Program Synthesis Dataset",
        "abstract": "We present a program synthesis-oriented dataset consisting of human written problem statements and solutions for these problems. The prob- lem statements were collected via crowdsourcing and the program solutions were extracted from human-written solutions in programming compe- titions, accompanied by input/output examples. We propose using this dataset for the program synthesis tasks aimed for working with real user- generated data. As a baseline we present few models, with the best model achieving 8.8% ac- curacy, showcasing both the complexity of the dataset and large room for future research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2301.07695v5",
        "title": "EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records",
        "abstract": "We present a new text-to-SQL dataset for electronic health records (EHRs). The utterances were collected from 222 hospital staff members, including physicians, nurses, and insurance review and health records teams. To construct the QA dataset on structured EHR data, we conducted a poll at a university hospital and used the responses to create seed questions. We then manually linked these questions to two open-source EHR databases, MIMIC-III and eICU, and included various time expressions and held-out unanswerable questions in the dataset, which were also collected from the poll. Our dataset poses a unique set of challenges: the model needs to 1) generate SQL queries that reflect a wide range of needs in the hospital, including simple retrieval and complex operations such as calculating survival rate, 2) understand various time expressions to answer time-sensitive questions in healthcare, and 3) distinguish whether a given question is answerable or unanswerable. We believe our dataset, EHRSQL, can serve as a practical benchmark for developing and assessing QA models on structured EHR data and take a step further towards bridging the gap between text-to-SQL research and its real-life deployment in healthcare. EHRSQL is available at https://github. com/glee4810/EHRSQL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2106.05006v1",
        "title": "Text-to-SQL in the Wild: A Naturally-Occurring Dataset Based on Stack Exchange Data",
        "abstract": "Most available semantic parsing datasets, com- prising of pairs of natural utterances and log- ical forms, were collected solely for the pur- pose of training and evaluation of natural lan- guage understanding systems. As a result, they do not contain any of the richness and vari- ety of natural-occurring utterances, where hu- mans ask about data they need or are curi- ous about. In this work, we release SEDE, a dataset with 12,023 pairs of utterances and SQL queries collected from real usage on the Stack Exchange website. We show that these pairs contain a variety of real-world challenges which were rarely reflected so far in any other semantic parsing dataset, propose an evalu- ation metric based on comparison of partial query clauses that is more suitable for real- world queries, and conduct experiments with strong baselines, showing a large gap between the performance on SEDE compared to other common datasets.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1908.01839v2",
        "title": "Text-to-SQL Generation for Question Answering on Electronic Medical Records",
        "abstract": "Electronic medical records (EMR) contain comprehensive patient information and are typically stored in a relational database with multiple tables. Effective and efficient patient information retrieval from EMR data is a challenging task for medical experts. Question- to-SQL generation methods tackle this problem by first predicting the SQL query for a given question about a database, and then, executing the query on the database. However, most of the existing approaches have not been adapted to the healthcare domain due to a lack of healthcare Question-to-SQL dataset for learning models specific to this domain. In addition, wide use of the abbreviation of terminologies and possible typos in questions introduce additional challenges for accurately generating the corresponding SQL queries. In this paper, we tackle these challenges by developing a deep learn- ing based TRanslate-Edit Model for Question-to-SQL (TREQS) gen- eration, which adapts the widely used sequence-to-sequence model to directly generate the SQL query for a given question, and further performs the required edits using an attentive-copying mechanism and task-specific look-up tables. Based on the widely used publicly available electronic medical database, we create a new large-scale Question-SQL pair dataset, named MIMICSQL, in order to perform the Question-to-SQL generation task in healthcare domain. An extensive set of experiments are conducted to evaluate the perfor- mance of our proposed model on MIMICSQL. Both quantitative and qualitative experimental results indicate the flexibility and effi- ciency of our proposed method in predicting condition values and its robustness to random questions with abbreviations and typos.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1804.09769v1",
        "title": "TypeSQL: Knowledge-based Type-Aware Neural Text-to-SQL Generation",
        "abstract": "Interacting with relational databases through natural language helps users of any back- ground easily query and analyze a vast amount of data. This requires a system that under- stands users\u2019 questions and converts them to SQL queries automatically. In this paper we present a novel approach, TYPESQL, which views this problem as a slot filling task. Addi- tionally, TYPESQL utilizes type information to better understand rare entities and numbers in natural language questions. We test this idea on the WikiSQL dataset and outperform the prior state-of-the-art by 5.5% in much less time. We also show that accessing the con- tent of databases can significantly improve the performance when users\u2019 queries are not well- formed. TYPESQL gets 82.6% accuracy, a 17.5% absolute improvement compared to the previous content-sensitive model.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2106.04441v2",
        "title": "CLTR: An End-to-End, Transformer-Based System for Cell Level Table Retrieval and Table Question Answering",
        "abstract": "We present the first end-to-end, transformer- based table question answering (QA) system that takes natural language questions and mas- sive table corpus as inputs to retrieve the most relevant tables and locate the correct table cells to answer the question 1. Our system, CLTR, extends the current state-of-the-art QA over ta- bles model to build an end-to-end table QA ar- chitecture. This system has successfully tack- led many real-world table QA problems with a simple, unified pipeline. Our proposed sys- tem can also generate a heatmap of candi- date columns and rows over complex tables and allow users to quickly identify the cor- rect cells to answer questions. In addition, we introduce two new open-domain benchmarks, E2E WTQ and E2E GNQ, consisting of 2,005 natural language questions over 76,242 ta- bles. The benchmarks are designed to validate CLTR as well as accommodate future table re- trieval and end-to-end table QA research and experiments. Our experiments demonstrate that our system is the current state-of-the-art model on the table retrieval task and produces promising results for end-to-end table QA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.16265v3",
        "title": "UNITE: A Unified Benchmark for Text-to-SQL Evaluation",
        "abstract": "A practical text-to-SQL system should gen- eralize well on a wide variety of natural lan- guage questions, unseen database schemas, and novel SQL query structures. To comprehen- sively evaluate text-to-SQL systems, we intro- duce a UNIfied benchmark for Text-to-SQL Evaluation (UNITE). It is composed of pub- licly available text-to-SQL datasets, containing natural language questions from more than 12 domains, SQL queries from more than 3.9K patterns, and 29K databases. Compared to the widely used Spider benchmark (Yu et al., 2018), we introduce \u223c120K additional exam- ples and a threefold increase in SQL patterns, such as comparative and boolean questions. We conduct a systematic study of six state-of-the- art (SOTA) text-to-SQL parsers on our new benchmark and show that: 1) Codex performs surprisingly well on out-of-domain datasets; 2) specially designed decoding methods (e.g. constrained beam search) can improve perfor- mance for both in-domain and out-of-domain settings; 3) explicitly modeling the relation- ship between questions and schemas further improves the Seq2Seq models. More impor- tantly, our benchmark presents key challenges towards compositional generalization and ro- bustness issues \u2014 which these SOTA models cannot address well. 1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2109.05153v1",
        "title": "Natural SQL: Making SQL Easier to Infer from Natural Language Specifications",
        "abstract": "Addressing the mismatch between natural lan- guage descriptions and the corresponding SQL queries is a key challenge for text-to-SQL translation. To bridge this gap, we pro- pose an SQL intermediate representation (IR) called Natural SQL (NatSQL). Specifically, NatSQL preserves the core functionalities of SQL, while it simplifies the queries as fol- lows: (1) dispensing with operators and key- words such as GROUP BY, HAVING, FROM, JOIN ON, which are usually hard to find coun- terparts for in the text descriptions; (2) re- moving the need for nested subqueries and set operators; and (3) making schema link- ing easier by reducing the required number of schema items. On Spider, a challenging text- to-SQL benchmark that contains complex and nested SQL queries, we demonstrate that Nat- SQL outperforms other IRs, and significantly improves the performance of several previous SOTA models. Furthermore, for existing mod- els that do not support executable SQL gener- ation, NatSQL easily enables them to generate executable SQL queries, and achieves the new state-of-the-art execution accuracy ",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2106.12944v1",
        "title": "AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry",
        "abstract": "Recent advances in transformers have enabled Table Question Answering (Table QA) systems to achieve high accuracy and SOTA results on open domain datasets like WikiTableQuestions and WikiSQL. Such transformers are frequently pre- trained on open-domain content such as Wikipedia, where they effectively encode questions and corresponding tables from Wikipedia as seen in Table QA dataset. However, web tables in Wikipedia are notably flat in their layout, with the first row as the sole column header. The layout lends to a relational view of tables where each row is a tuple. Whereas, tables in domain-specific business or scientific documents often have a much more complex layout, including hierarchical row and column headers, in addition to having specialized vocabulary terms from that domain.\nTo address this problem, we introduce the domain-specific Table QA dataset AIT- QA (Airline Industry Table QA). The dataset consists of 515 questions authored by human annotators on 116 tables extracted from public U.S. SEC filings1 of major airline companies for the fiscal years 2017-2019. We also provide annotations pertaining to the nature of questions, marking those that require hierarchical head- ers, domain-specific terminology, and paraphrased forms. Our zero-shot baseline evaluation of three transformer-based SOTA Table QA methods - TaPAS (end-to- end), TaBERT (semantic parsing-based), and RCI (row-column encoding-based) - clearly exposes the limitation of these methods in this practical setting, with the best accuracy at just 51.8% (RCI). We also present pragmatic table pre-processing steps used to pivot and project these complex tables into a layout suitable for the SOTA Table QA models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2010.02840v1",
        "title": "Semantic Evaluation for Text-to-SQL with Distilled Test Suites",
        "abstract": "\nWe propose test suite accuracy to approxi- mate semantic accuracy for Text-to-SQL mod- els. Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases. At evaluation time, it computes the denotation accuracy of the predicted queries on the distilled test suite, hence calculating a tight upper-bound for se- mantic accuracy efficiently. We use our pro- posed method to evaluate 21 models submitted to the Spider leader board and manually verify that our method is always correct on 100 ex- amples. In contrast, the current Spider metric leads to a 2.5% false negative rate on average and 8.1% in the worst case, indicating that test suite accuracy is needed. Our implementation, along with distilled test suites for eleven Text- to-SQL datasets, is publicly available.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2207.03637v1",
        "title": "OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering",
        "abstract": "The information in tables can be an important complement to text, making table-based ques- tion answering (QA) systems of great value. The intrinsic complexity of handling tables of- ten adds an extra burden to both model design and data annotation. In this paper, we aim to develop a simple table-based QA model with minimal annotation effort. Motivated by the fact that table-based QA requires both align- ment between questions and tables and the ability to perform complicated reasoning over multiple table elements, we propose an om- nivorous pretraining approach that consumes both natural and synthetic data to endow mod- els with these respective abilities. Specifically, given freely available tables, we leverage re- trieval to pair them with relevant natural sen- tences for mask-based pretraining, and syn- thesize NL questions by converting SQL sam- pled from tables for pretraining with a QA loss. We perform extensive experiments in both few-shot and full settings, and the re- sults clearly demonstrate the superiority of our model OmniTab, with the best multitasking ap- proach achieving an absolute gain of 16.2% and 2.7% in 128-shot and full settings respec- tively, also establishing a new state-of-the-art on WikiTableQuestions. Detailed ablations and analyses reveal different characteristics of natural and synthetic data, shedding light on future directions in omnivorous pretraining.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2106.11455v1",
        "title": "KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers",
        "abstract": "The goal of database question answering is to enable natural language querying of real-life relational databases in diverse application do- mains. Recently, large-scale datasets such as Spider and WikiSQL facilitated novel modeling techniques for text-to-SQL parsing, improving zero-shot generalization to unseen databases. In this work, we examine the challenges that still prevent these techniques from practical de- ployment. First, we present KaggleDBQA, a new cross-domain evaluation dataset of real Web databases, with domain-specific data types, original formatting, and unrestricted questions. Second, we re-examine the choice of evalua- tion tasks for text-to-SQL parsers as applied in real-life settings. Finally, we augment our in-domain evaluation task with database do- cumentation, a naturally occurring source of implicit domain knowledge. We show that KaggleDBQA presents a challenge to state-of- the-art zero-shot parsers but a more realistic evaluation setting and creative use of associated database documentation boosts their accuracy by over 13.2%, doubling their performance.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2010.01891v1",
        "title": "A Pilot Study of Text-to-SQL Semantic Parsing for Vietnamese",
        "abstract": "Semantic parsing is an important NLP task. However, Vietnamese is a low-resource lan- guage in this research area. In this paper, we present the first public large-scale Text-to- SQL semantic parsing dataset for Vietnamese. We extend and evaluate two strong seman- tic parsing baselines EditSQL (Zhang et al., 2019) and IRNet (Guo et al., 2019) on our dataset. We compare the two baselines with key configurations and find that: automatic Vietnamese word segmentation improves the parsing results of both baselines; the normal- ized pointwise mutual information (NPMI) score (Bouma, 2009) is useful for schema link- ing; latent syntactic features extracted from a neural dependency parser for Vietnamese also improve the results; and the monolingual language model PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) helps produce higher performances than the recent best mul- tilingual language model XLM-R (Conneau et al., 2020).",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2401.06786v1",
        "title": "CLOUDEVAL-YAML: A PRACTICAL BENCHMARK FOR CLOUD CONFIGURATION GENERATION",
        "abstract": "Among the thriving ecosystem of cloud computing and the proliferation of Large Language Model (LLM)-based code generation tools, there is a lack of benchmarking for code generation in cloud-native applications. In response to this need, we present CloudEval-YAML, a practical benchmark for cloud configuration generation. CloudEval-YAML tackles the diversity challenge by focusing on YAML, the de facto standard of numerous cloud-native tools. We develop the CloudEval-YAML benchmark with practicality in mind: the dataset consists of hand-written problems with unit tests targeting practical scenarios. We further enhanced the dataset to meet practical needs by rephrasing questions in a concise, abbreviated, and bilingual manner. The dataset consists of 1011 problems that take more than 1200 human hours to complete. To improve practicality during evaluation, we build a scalable evaluation platform for CloudEval-YAML that achieves a 20 times speedup over a single machine. To the best of our knowledge, the CloudEval-YAML dataset is the first hand-written dataset targeting cloud-native applications. We present an in-depth evaluation of 12 LLMs, leading to a deeper understanding of the problems and LLMs, as well as effective methods to improve task performance and reduce cost. We release the dataset along with the evaluation framework at https://github.com/alibaba/CloudEval-YAML.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2108.12144v3",
        "title": "Lyra: A Benchmark for Turducken-Style Code Generation",
        "abstract": "Recently, neural techniques have been used to gen- erate source code automatically. While promising for declarative languages, these approaches achieve much poorer performance on datasets for imperative languages. Since a declarative language is typically embedded in an imperative language (i.e., the tur- ducken-style programming) in real-world software development, the promising results on declarative languages can hardly lead to significant reduction of manual software development efforts.\nIn this paper, we define a new code generation task: given a natural language comment, this task aims to generate a program in a base imperative lan- guage with an embedded declarative language. To our knowledge, this is the first turducken-style code generation task. For this task, we present Lyra: a dataset in Python with embedded SQL. This dataset contains 2,000 carefully annotated database manip- ulation programs from real-world projects. Each program is paired with both a Chinese comment and an English comment. In our experiment, we adopted Transformer, BERT-style, and GPT-style models as baselines. In the best setting, the gen- eration performance of GPT-style models is better than others, where the AST exact matching accu- racy is 24% and 25.5% when using Chinese and English comments, respectively. Therefore, we be- lieve that Lyra provides a new challenge for code generation. Yet, overcoming this challenge may sig- nificantly boost the applicability of code generation techniques for real-world software development.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1902.04260v8",
        "title": "Table2answer: Read the database and answer without SQL",
        "abstract": "Semantic parsing is the task of mapping natural language to logic form. In question answering, semantic parsing can be used to map the question to logic form and execute the logic form to get the answer. One key problem for semantic parsing is the hard label work. We study this problem in another way: we do not use the logic form any more. Instead we only use the schema and answer info. We think that the logic form step can be injected into the deep model. The reason why we think removing the logic form step is possible is that human can do the task without explicit logic form. We use BERT-based model and do the experiment in the WikiSQL dataset, which is a large natural language to SQL dataset. Our experimental evaluations that show that our model can achieves the baseline results in WikiSQL dataset.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.07288v1",
        "title": "Open-WikiTable: Dataset for Open Domain Question Answering with Complex Reasoning over Table",
        "abstract": "Despite recent interest in open domain ques- tion answering (ODQA) over tables, many studies still rely on datasets that are not truly optimal for the task with respect to utilizing structural nature of table. These datasets as- sume answers reside as a single cell value and do not necessitate exploring over multi- ple cells such as aggregation, comparison, and sorting. Thus, we release Open-WikiTable, the first ODQA dataset that requires complex rea- soning over tables. Open-WikiTable is built upon WikiSQL and WikiTableQuestions to be applicable in the open-domain setting. As each question is coupled with both textual answers and SQL queries, Open-WikiTable opens up a wide range of possibilities for future research, as both reader and parser methods can be ap- plied. The dataset and code are publicly avail- able",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.13196v1",
        "title": "NameGuess: Column Name Expansion for Tabular Data",
        "abstract": "\nRecent advances in large language models have revolutionized many sectors, including the database industry. One common challenge when dealing with large volumes of tabular data is the pervasive use of abbreviated column names, which can negatively impact perfor- mance on various data search, access, and un- derstanding tasks. To address this issue, we in- troduce a new task, called NAMEGUESS, to ex- pand column names (used in database schema) as a natural language generation problem. We create a training dataset of 384K abbreviated- expanded column pairs using a new data fab- rication method and a human-annotated evalu- ation benchmark that includes 9.2K examples from real-world tables. To tackle the com- plexities associated with polysemy and am- biguity in NAMEGUESS, we enhance auto- regressive language models by conditioning on table content and column header names \u2013 yielding a fine-tuned model (with 2.7B param- eters) that matches human performance. Fur- thermore, we conduct a comprehensive anal- ysis (on multiple LLMs) to validate the effec- tiveness of table content in NAMEGUESS and identify promising future opportunities. Code has been made available at https://github. com/amazon-science/nameguess.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.07860v1",
        "title": "BookSQL: A Large Scale Text-to-SQL Dataset for Accounting Domain",
        "abstract": "Several large-scale datasets (e.g., WikiSQL, Spider) for developing natural language inter- faces to databases have recently been proposed. These datasets cover a wide breadth of domains but fall short on some essential domains, such as finance and accounting. Given that account- ing databases are used worldwide, particularly by non-technical people, there is an imminent need to develop models that could help extract information from accounting databases via nat- ural language queries. In this resource paper, we aim to fill this gap by proposing a new large- scale Text-to-SQL dataset for the accounting and financial domain: BookSQL. The dataset consists of 100k natural language queries-SQL pairs, and accounting databases of 1 million records. We experiment with and analyze exist- ing state-of-the-art models (including GPT-4) for the Text-to-SQL task on BookSQL. We find significant performance gaps, thus pointing to- wards developing more focused models for this domain.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.15891v1",
        "title": "CSS: A Large-scale Cross-schema Chinese Text-to-SQL Medical Dataset",
        "abstract": "\nThe cross-domain text-to-SQL task aims to build a system that can parse user questions into SQL on complete unseen databases, and the single-domain text-to-SQL task evaluates the performance on identical databases. Both of these setups confront unavoidable difficulties in real-world applications. To this end, we intro- duce the cross-schema text-to-SQL task, where the databases of evaluation data are different from that in the training data but come from the same domain. Furthermore, we present CSS1 , a large-scale CrosS-Schema Chinese text-to-SQL dataset, to carry on correspond- ing studies. CSS originally consisted of 4,340 question/SQL pairs across 2 databases. In or- der to generalize models to different medical systems, we extend CSS and create 19 new databases along with 29,280 corresponding dataset examples. Moreover, CSS is also a large corpus for single-domain Chinese text- to-SQL studies. We present the data collec- tion approach and a series of analyses of the data statistics. To show the potential and use- fulness of CSS, benchmarking baselines have been conducted and reported. Our dataset is publicly available at https://huggingface. co/datasets/zhanghanchong/css.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2302.04580",
        "title": "Generating a Structured Summary of Numerous Academic Papers: Dataset and Method",
        "abstract": "Writing a survey paper on one research topic usu- ally needs to cover the salient content from nu- merous related papers, which can be modeled as a multi-document summarization (MDS) task. Ex- isting MDS datasets usually focus on producing the structureless summary covering a few input docu- ments. Meanwhile, previous structured summary generation works focus on summarizing a single document into a multi-section summary. These ex- isting datasets and methods cannot meet the require- ments of summarizing numerous academic papers into a structured summary. To deal with the scarcity of available data, we propose BigSurvey, the first large-scale dataset for generating comprehensive summaries of numerous academic papers on each topic. We collect target summaries from more than seven thousand survey papers and utilize their 430 thousand reference papers\u2019 abstracts as input docu- ments. To organize the diverse content from dozens of input documents and ensure the efficiency of pro- cessing long text sequences, we propose a summa- rization method named category-based alignment and sparse transformer (CAST). The experimental results show that our CAST method outperforms various advanced summarization methods.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2004.15011",
        "title": "TLDR: Extreme Summarization of Scientific Documents",
        "abstract": "We introduce TLDR generation, a new form of extreme summarization, for scientific pa- pers. T L D R generation involves high source compression and requires expert background knowledge and understanding of complex domain-specific language. To facilitate study on this task, we introduce SCITLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SCITLDR contains both author-written and expert-derived TLDRs, where the latter are collected using a novel annotation pro- tocol that produces high-quality summaries while minimizing annotation burden. We pro- pose CATTS, a simple yet effective learn- ing strategy for generating TLDRs that ex- ploits titles as an auxiliary training signal. CATTS improves upon strong baselines un- der both automated metrics and human evalua- tions. Data and code are publicly available at https://github.com/allenai/scitldr.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2205.15051",
        "title": "X-SCITLDR: Cross-Lingual\nExtreme Summarization of Scholarly Documents",
        "abstract": "The number of scientific publications nowadays is rapidly increas- ing, causing information overload for researchers and making it hard for scholars to keep up to date with current trends and lines of work. Consequently, recent work on applying text mining tech- nologies for scholarly publications has investigated the application of automatic text summarization technologies, including extreme summarization, for this domain. However, previous work has con- centrated only on monolingual settings, primarily in English. In this paper, we fill this research gap and present an abstractive cross-lingual summarization dataset for four different languages in the scholarly domain, which enables us to train and evaluate models that process English papers and generate summaries in German, Italian, Chinese and Japanese. We present our new X- SCITLDR dataset for multilingual summarization and thoroughly benchmark different models based on a state-of-the-art multilingual pre-trained model, including a two-stage \u2018summarize and translate\u2019 approach and a direct cross-lingual model. We additionally explore the benefits of intermediate-stage training using English monolin- gual summarization and machine translation as intermediate tasks and analyze performance in zero- and few-shot scenarios.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2205.11465",
        "title": "SQuALITY: Building a Long-Document Summarization Dataset the Hard Way",
        "abstract": "Summarization datasets are often assembled either by scraping naturally occurring public- domain summaries\u2014which are nearly always in difficult-to-work-with technical domains\u2014 or by using approximate heuristics to extract them from everyday text\u2014which frequently yields unfaithful summaries. In this work, we turn to a slower but more straightforward approach to developing summarization bench- mark data: We hire highly-qualified contrac- tors to read stories and write original sum- maries from scratch. To amortize reading time, we collect five summaries per document, with the first giving an overview and the subsequent four addressing specific questions. We use this protocol to collect SQuALITY, a dataset of question-focused summaries built on the same public-domain short stories as the multiple- choice dataset QuALITY (Pang et al., 2021b). Experiments with state-of-the-art summariza- tion systems show that our dataset is challeng- ing and that existing automatic evaluation met- rics are weak indicators of quality.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2104.05938",
        "title": "MSum: A New Benchmark for Query-based Multi-domain Meeting Summarization",
        "abstract": "Meetings are a key component of human col- laboration. As increasing numbers of meetings are recorded and transcribed, meeting sum- maries have become essential to remind those who may or may not have attended the meet- ings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multi- ple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting sum- marization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum con- sists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we in- vestigate a locate-then-summarize method and evaluate a set of strong summarization base- lines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summa- rization for future research. Dataset is avail- able at https://github.com/Yale-LILY/ QMSum.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2010.12694",
        "title": "AQUAMUSE: Automatically Generating Datasets for Query-Based Multi-Document Summarization",
        "abstract": "Summarization is the task of compressing source document(s) into coherent and succinct passages. This is a valuable tool to present users with concise and accurate sketch of the top ranked documents related to their queries. Query-based multi-document summarization (qMDS) addresses this pervasive need, but the research is severely limited due to lack of training and evaluation datasets as existing single-document and multi-document summa- rization datasets are inadequate in form and scale. We propose a scalable approach called AQUAMUSE to automatically mine qMDS ex- amples from question answering datasets and large document corpora. Our approach is unique in the sense that it can general a dual dataset \u2014 for extractive and abstractive sum- maries both. We publicly release a specific in- stance of an AQUAMUSE dataset with 5,519 query-based summaries, each associated with an average of 6 input documents selected from an index of 355M documents from Common Crawl1 . Extensive evaluation of the dataset along with baseline summarization model ex- periments are provided.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2012.14774",
        "title": "Generating Query Focused Summaries from Query-Free Resources",
        "abstract": "The availability of large-scale datasets has driven the development of neural models that create generic summaries from single or mul- tiple documents. In this work we consider query focused summarization (QFS), a task for which training data in the form of queries, doc- uments, and summaries is not readily available. We propose to decompose QFS into (1) query modeling (i.e., finding supportive evidence within a set of documents for a query) and (2) conditional language modeling (i.e., sum- mary generation). We introduce MARGE, a Masked ROUGE Regression framework for evidence estimation and ranking which relies on a unified representation for summaries and queries, so that summaries in generic data can be converted into proxy queries for learning a query model. Experiments across QFS bench- marks and query types show that our model achieves state-of-the-art performance despite learning from weak supervision.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2106.00130",
        "title": "Bringing Structure into Summaries:\na Faceted Summarization Dataset for Long Scientific Documents",
        "abstract": "Faceted summarization provides briefings of a document from different perspectives. Read- ers can quickly comprehend the main points of a long document with the help of a structured outline. However, little research has been con- ducted on this subject, partially due to the lack of large-scale faceted summarization datasets. In this study, we present FacetSum, a faceted summarization benchmark built on Emerald journal articles, covering a diverse range of do- mains. Different from traditional document- summary pairs, FacetSum provides multi- ple summaries, each targeted at specific sec- tions of a long document, including the pur- pose, method, findings, and value. Analy- ses and empirical results on our dataset reveal the importance of bringing structure into sum- maries. We believe FacetSum will spur fur- ther advances in summarization research and foster the development of NLP systems that can leverage the structured information in both long texts and summaries.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2110.08296",
        "title": "ASPECTNEWS: Aspect-Oriented Summarization of News Documents",
        "abstract": "Generic summaries try to cover an entire doc- ument and query-based summaries try to an- swer document-specific questions. But real users\u2019 needs often fall in between these ex- tremes and correspond to aspects, high-level topics discussed among similar types of docu- ments. In this paper, we collect a dataset of realistic aspect-oriented summaries, ASPECT- NEWS, which covers different subtopics about articles in news sub-domains. We annotate data across two domains of articles, earth- quakes and fraud investigations, where each ar- ticle is annotated with two distinct summaries focusing on different aspects for each domain. A system producing a single generic summary cannot concisely satisfy both aspects. Our fo- cus in evaluation is how well existing tech- niques can generalize to these domains with- out seeing in-domain training data, so we turn to techniques to construct synthetic training data that have been used in query-focused sum- marization work. We compare several train- ing schemes that differ in how strongly key- words are used and how oracle summaries are extracted. Our evaluation shows that our final approach yields (a) focused summaries, better than those from a generic summarization sys- tem or from keyword matching; (b) a system sensitive to the choice of keywords",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2403.16941",
        "title": "SPACE-IDEAS: A Dataset for Salient Information Detection in Space Innovation",
        "abstract": "Detecting salient parts in text using natural language processing has been widely used to mitigate the effects of information overflow. Nevertheless, most of the datasets available for this task are derived mainly from academic publications. We introduce SPACE-IDEAS, a dataset for salient information detection from innovation ideas related to the Space domain. The text in SPACE-IDEAS varies greatly and includes informal, technical, academic and business-oriented writing styles. In addition to a manually annotated dataset we release an extended version that is annotated using a large generative language model. We train different sentence and sequential sentence classifiers, and show that the automatically annotated dataset can be leveraged using multitask learning to train better classifiers.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2011.07832",
        "title": "WikiAsp: A Dataset for Multi-domain Aspect-based Summarization",
        "abstract": "Aspect-based summarization is the task of generating focused summaries based on spe- cific points of interest. Such summaries aid efficient analysis of text, such as quickly un- derstanding reviews or opinions from differ- ent angles. However, due to large differences in the type of aspects for different domains (e.g., sentiment, product features), the de- velopment of previous models has tended to be domain-specific. In this paper, we propose WikiAsp,1 a large-scale dataset for multi-domain aspect-based summarization that attempts to spur research in the direc- tion of open-domain aspect-based summa- rization. Specifically, we build the dataset using Wikipedia articles from 20 different domains, using the section titles and bound- aries of each article as a proxy for aspect an- notation. We propose several straightforward baseline models for this task and conduct ex- periments on the dataset. Results highlight key challenges that existing summarization models face in this setting, such as proper pronoun handling of quoted sources and con- sistent explanation of time-sensitive events.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2212.09233",
        "title": "OASum: Large-Scale Open Domain Aspect-based Summarization",
        "abstract": "Aspect or query-based summarization has re- cently caught more attention, as it can gener- ate differentiated summaries based on users\u2019 interests. However, the current dataset for as- pect or query-based summarization either fo- cuses on specific domains, contains relatively small-scale instances, or includes only a few aspect types. Such limitations hinder further explorations in this direction. In this work, we take advantage of crowd-sourcing knowledge on Wikipedia.org and automatically create a high-quality, large-scale open-domain aspect- based summarization dataset named OASum, which contains more than 3.7 million instances with around 1 million different aspects on 2 mil- lion Wikipedia pages. We provide benchmark results on OASum and demonstrate its ability for diverse aspect-based summarization genera- tion. To overcome the data scarcity problem on specific domains, we also perform zero-shot, few-shot, and fine-tuning on seven downstream datasets. Specifically, zero/few-shot and fine- tuning results show that the model pre-trained on our corpus demonstrates a strong aspect or query-focused generation ability compared with the backbone model. Our dataset and pre- trained checkpoints are publicly available.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.04440v1",
        "title": "OPENASP: A Benchmark for Multi-document Open Aspect-based Summarization",
        "abstract": "The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting spe- cific information needs of users in real-world scenarios, particularly when a targeted sum- mary is sought, such as in the useful aspect- based summarization setting targeted in this paper. Previous datasets and studies for this set- ting have predominantly concentrated on a lim- ited set of pre-defined aspects, focused solely on single document inputs, or relied on syn- thetic data. To advance research on more realis- tic scenarios, we introduce OPENASP, a bench- mark for multi-document open aspect-based summarization. This benchmark is created us- ing a novel and cost-effective annotation proto- col, by which an open aspect dataset is derived from existing generic multi-document summa- rization datasets. We analyze the properties of OPENASP showcasing its high-quality content. Further, we show that the realistic open-aspect setting realized in OPENASP poses a challenge for current state-of-the-art summarization mod- els, as well as for large language models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.14196",
        "title": "ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding",
        "abstract": "We introduce ZeroSCROLLS, a zero-shot bench- mark for natural language understanding over long texts, which contains only test and small validation sets, without training data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregat- ing the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outper- forms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open chal- lenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2201.03533",
        "title": "SCROLLS: Standardized CompaRison Over Long Language Sequences",
        "abstract": "NLP benchmarks have largely focused on short texts, such as sentences and paragraphs, even though long texts comprise a consider- able amount of natural language in the wild. We introduce SCROLLS, a suite of tasks that require reasoning over long texts. We exam- ine existing long-text datasets, and handpick ones where the text is naturally long, while pri- oritizing tasks that involve synthesizing infor- mation across the input. SCROLLS contains summarization, question answering, and nat- ural language inference tasks, covering multi- ple domains, including literature, science, busi- ness, and entertainment. Initial baselines, in- cluding Longformer Encoder-Decoder, indi- cate that there is ample room for improvement on SCROLLS. We make all datasets available in a unified text-to-text format and host a live leaderboard to facilitate research on model ar- chitecture and pretraining methods",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.11088",
        "title": "L-EVAL: INSTITUTING STANDARDIZED EVALUATION FOR LONG CONTEXT LANGUAGE MODELS",
        "abstract": "Recently, there has been growing interest in extending the context length of large language models (LLMs), aiming to effectively process long inputs of one turn or conversations with more extensive histories. While proprietary models such as GPT-4 and Claude can largely preserve the reasoning ability in an extended context, open-source models are still progressing through the early stages of de- velopment. To bridge this gap, we propose L-Eval to institute a more standardized evaluation for long context language models (LCLMs) addressing two key as- pects: dataset construction and evaluation metrics. On the one hand, we build a new evaluation suite containing 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs encompassing diverse question styles, domains, and input length (3k\u223c200k tokens). On the other hand, we investigate the effectiveness in evalution metrics for LCLMs. Results show that popular n- gram matching metrics generally can not correlate well with human judgment, and thus we strongly advocate for length-instruction-enhanced (LIE) evaluation and employing LLM judges. We conducted a comprehensive study of 4 popular commercial LLMs and 12 open-source counterparts using the L-Eval benchmark. Our empirical findings offer useful insights into the study of LCLMs and lay the groundwork for the development of more principled evaluation of these models.1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2404.06480",
        "title": "Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks",
        "abstract": "Recently, the large language model (LLM)\ncommunity has shown increasing interest in en-\nhancing LLMs\u2019 capability to handle extremely\nlong documents. As various long-text tech-\nniques and model architectures emerge, the pre-\ncise and detailed evaluation of models\u2019 long-\ntext capabilities has become increasingly im-\nportant. Existing long-text evaluation bench-\nmarks, such as L-Eval and LongBench, con-\nstruct long-text test sets based on open-source\ndatasets, focusing mainly on QA and summa-\nrization tasks. These datasets include test samples of varying lengths (from 2k to 32k+) \nentangled together, making it challenging to as-\nsess model capabilities across different length\nranges. Moreover, they do not cover the ul-\ntralong settings (100k+ tokens) that the latest\nLLMs claim to achieve. In this paper, we intro-\nduce Ada-LEval, a length-adaptable benchmark\nfor evaluating the long-context understanding\nof LLMs. Ada-LEval includes two challeng-\ning subsets, TSort and BestAnswer, which en-\nable a more reliable evaluation of LLMs\u2019 long\ncontext capabilities. These benchmarks sup-\nBestAnswer introduced in Ada-LEval. Understanding 0\nport intricate manipulation of the length of\nand reasoning over the full text are required to solve 4\ntest cases, and can easily produce text sam-\nples up to 128k tokens. We evaluate 4 state-of-\nthe-art closed-source API models and 6 open-\nsource models with Ada-LEval. The evaluation\nthat affects LLM performance is the \u2018context win- r\nresults demonstrate the limitations of current\nLLMs, especially in ultra-long-context settings.\nOur code is available at https://github.\nsimultaneously. This window\u2019s size is pivotal in com/open-compass/Ada-LEval.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.14508",
        "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",
        "abstract": "Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only han- dle texts a few thousand tokens long, limit- ing their applications on longer sequence in- puts, such as books, reports, and codebases. Recent works have proposed methods to im- prove LLMs\u2019 long context capabilities by ex- tending context windows and more sophisti- cated memory mechanisms. However, com- prehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilin- gual, multi-task benchmark for long context understanding, enabling a more rigorous eval- uation of long context understanding. Long- Bench comprises 21 datasets across 6 task cat- egories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single- doc QA, multi-doc QA, summarization, few- shot learning, synthetic tasks, and code com- pletion. All datasets in LongBench are stan- dardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on Long- Bench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open- sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substan- tial improvement on long context understand- ing. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the perfor- mance still lags behind models that have strong long context understanding capability.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2404.16821",
        "title": "How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites",
        "abstract": "In this report, we introduce InternVL 1.5, an open-source\nmultimodal large language model (MLLM) to bridge the\ncapability gap between open-source and proprietary commercial models in multimodal understanding. We introduce\nthree simple improvements: (1) Strong Vision Encoder: we\nexplored a continuous learning strategy for the large-scale\nvision foundation model\u2014InternViT-6B, boosting its visual\nunderstanding capabilities, and making it can be transferred and reused in different LLMs. (2) Dynamic HighResolution: we divide images into tiles ranging from 1 to 40\nof 448\u00d7448 pixels according to the aspect ratio and resolution of the input images, which supports up to 4K resolution\ninput. (3) High-Quality Bilingual Dataset: we carefully collected a high-quality bilingual dataset that covers common\nscenes, document images, and annotated them with English\nand Chinese question-answer pairs, significantly enhancing performance in OCR- and Chinese-related tasks. We\nevaluate InternVL 1.5 through a series of benchmarks and\ncomparative studies. Compared to both open-source and\nproprietary commercial models, InternVL 1.5 shows competitive performance, achieving state-of-the-art results in 8\nof 18 multimodal benchmarks.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2403.17297",
        "title": "InternLM2 Technical Report",
        "abstract": "The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4\nhas sparked discussions on the advent of Artificial General Intelligence\n(AGI). However, replicating such advancements in open-source models has\nbeen challenging. This paper introduces InternLM2, an open-source LLM\nthat outperforms its predecessors in comprehensive evaluations across 6\ndimensions and 30 benchmarks, long-context modeling, and open-ended\nsubjective evaluations through innovative pre-training and optimization\ntechniques. The pre-training process of InternLM2 is meticulously detailed,\nhighlighting the preparation of diverse data types including text, code, and\nlong-context data. InternLM2 efficiently captures long-term dependencies,\ninitially trained on 4k tokens before advancing to 32k tokens in pre-training\nand fine-tuning stages, exhibiting remarkable performance on the 200k\n\u201cNeedle-in-a-Haystack\u201d test. InternLM2 is further aligned using Supervised\nFine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning\nfrom Human Feedback (COOL RLHF) strategy that addresses conflicting\nhuman preferences and reward hacking. By releasing InternLM2 models in\ndifferent training stages and model sizes, we provide the community with\ninsights into the model\u2019s evolution.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2308.10755",
        "title": "WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models",
        "abstract": "The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the\ndevelopment of large models, leading to the creation of numerous impressive\nlarge language models(LLMs) and multimodal large language models (MLLMs).\nThese cutting-edge models owe their remarkable performance to high-quality data.\nHowever, the details of the training data used in leading paradigms are often kept\nconfidential. This lack of transparency, coupled with the scarcity of open-source\ndata, impedes further developments within the community. As a response, this\npaper presents \"Wan Juan\", a large-scale multimodal dataset composed of both\nChinese and English data, collected from a wide range of web sources. The dataset\nincorporates text, image-text, and video modalities, with a total volume exceeding\n2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a\nsimilar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.10774",
        "title": "MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning",
        "abstract": "With the rapid development of large language\nmodels (LLMs) and their integration into large\nmultimodal models (LMMs), there has been\nimpressive progress in zero-shot completion\nof user-oriented vision-language tasks. However, a gap remains in the domain of chart\nimage understanding due to the distinct abstract components in charts. To address this,\nwe introduce a large-scale MultiModal Chart\nInstruction (MMC-Instruction) dataset comprising 600k instances supporting diverse tasks\nand chart types. Leveraging this data, we develop MultiModal Chart Assistant (MMCA),\nan LMM that achieves state-of-the-art performance on existing chart QA benchmarks.\nRecognizing the need for a comprehensive\nevaluation of LMM chart understanding, we\nalso propose a MultiModal Chart Benchmark\n(MMC-Benchmark), a comprehensive humanannotated benchmark with nine distinct tasks\nevaluating reasoning capabilities over charts.\nExtensive experiments on MMC-Benchmark\nreveal the limitations of existing LMMs on correctly interpreting charts, even for the most\nrecent GPT-4V model. Our work provides\nan instruction-tuning methodology and benchmark to advance multimodal understanding of\ncharts. Code and data are available at https:\n//github.com/FuxiaoLiu/MMC.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1708.09585",
        "title": "ICDAR2017 Competition on Reading Chinese Text in the Wild (RCTW-17)",
        "abstract": "Chinese is the most widely used language in the\nworld. Algorithms that read Chinese text in natural images\nfacilitate applications of various kinds. Despite the large potential\nvalue, datasets and competitions in the past primarily focus on\nEnglish, which bares very different characteristics than Chinese.\nThis report introduces RCTW, a new competition that focuses\non Chinese text reading. The competition features a largescale dataset with 12,263 annotated images. Two tasks, namely\ntext localization and end-to-end recognition, are set up. The\ncompetition took place from January 20 to May 31, 2017. 23 valid\nsubmissions were received from 19 teams. This report includes\ndataset description, task definitions, evaluation protocols, and\nresults summaries and analysis. Through this competition, we call\nfor more future research on the Chinese text reading problem.\nThe official website for the competition is http://rctw.vlrlab.net",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1712.02170",
        "title": "Detecting Curve Text in the Wild: New Dataset and New Solution",
        "abstract": "Scene text detection has been made great progress in recent years. The detection manners are evolving from axisaligned rectangle to rotated rectangle and further to quadrangle. However, current datasets contain very little curve\ntext, which can be widely observed in scene images such\nas signboard, product name and so on. To raise the concerns of reading curve text in the wild, in this paper, we\nconstruct a curve text dataset named CTW1500, which includes over 10k text annotations in 1,500 images (1000\nfor training and 500 for testing). Based on this dataset,\nwe pioneering propose a polygon based curve text detector (CTD) which can directly detect curve text without empirical combination. Moreover, by seamlessly integrating\nthe recurrent transverse and longitudinal offset connection\n(TLOC), the proposed method can be end-to-end trainable\nto learn the inherent connection among the position offsets. This allows the CTD to explore context information instead of predicting points independently, resulting in\nmore smooth and accurate detection. We also propose two\nsimple but effective post-processing methods named nonpolygon suppress (NPS) and polygonal non-maximum suppression (PNMS) to further improve the detection accuracy.\nFurthermore, the proposed approach in this paper is designed in an universal manner, which can also be trained\nwith rectangular or quadrilateral bounding boxes without\nextra efforts. Experimental results on CTW-1500 demonstrate our method with only a light backbone can outperform state-of-the-art methods with a large margin. By evaluating only in the curve or non-curve subset, the CTD +\nTLOC can still achieve the best results. Code is available\nat https://github.com/Yuliang-Liu/Curve-Text-Detector.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1909.07741",
        "title": "ICDAR 2019 Competition on Large-scale Street View Text with Partial Labeling -RRC-LSVT",
        "abstract": "Robust text reading from street view images\nprovides valuable information for various applications. Performance improvement of existing methods in such a challenging\nscenario heavily relies on the amount of fully annotated training\ndata, which is costly and in-efficient to obtain. To scale\nup the amount of training data while keeping the labeling\nprocedure cost-effective, this competition introduces a new\nchallenge on Large-scale Street View Text with Partial Labeling\n(LSVT), providing 50, 000 and 400, 000 images in full and weak\nannotations, respectively. This competition aims to explore the\nabilities of state-of-the-art methods to detect and recognize text\ninstances from large-scale street view images, closing the gap\nbetween research benchmarks and real applications. During\nthe competition period, a total of 41 teams participated in\nthe two proposed tasks with 132 valid submissions, i.e., text\ndetection and end-to-end text spotting. This paper includes\ndataset descriptions, task definitions, evaluation protocols and\nresults summaries of the ICDAR 2019-LSVT challenge.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1912.09641",
        "title": "ICDAR 2019 Robust Reading Challenge on Reading Chinese Text on Signboard",
        "abstract": "Chinese scene text reading is one of the most\nchallenging problems in computer vision and has attracted great\ninterest. Different from English text, Chinese has more than 6000\ncommonly used characters and Chinese characters can be\narranged in various layouts with numerous fonts. The Chinese\nsignboards in street view are a good choice for Chinese scene text\nimages since they have different backgrounds, fonts and layouts.\nWe organized a competition called ICDAR2019-ReCTS, which\nmainly focuses on reading Chinese text on signboard. This report\npresents the final results of the competition. A large-scale dataset\nof 25,000 annotated signboard images, in which all the text lines\nand characters are annotated with locations and transcriptions,\nwere released. Four tasks, namely character recognition, text line\nrecognition, text line detection and end-to-end recognition were set\nup. Besides, considering the Chinese text ambiguity issue, we\nproposed a multi ground truth (multi-GT) evaluation method to\nmake evaluation fairer. The competition started on March 1, 2019\nand ended on April 30, 2019. 262 submissions from 46 teams are\nreceived. Most of the participants come from universities, research\ninstitutes, and tech companies in China. There are also some\nparticipants from the United States, Australia, Singapore, and\nKorea. 21 teams submit results for Task 1, 23 teams submit results\nfor Task 2, 24 teams submit results for Task 3, and 13 teams submit\nresults for Task 4. The official website for the competition is\nhttp://rrc.cvc.uab.es/?ch=12. ",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.09014",
        "title": "ART: Automatic multi-step reasoning and tool-use for large language models",
        "abstract": "Large language models (LLMs) can perform\ncomplex reasoning in few- and zero-shot settings by generating intermediate chain of\nthought (CoT) reasoning steps. Further, each\nreasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work\non CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of\nmodel generations with tool use. We introduce Automatic Reasoning and Tool-use\n(ART), a framework that uses frozen LLMs\nto automatically generate intermediate reasoning steps as a program. Given a new task to\nsolve, ART selects demonstrations of multistep reasoning and tool use from a task library. At test time, ART seamlessly pauses\ngeneration whenever external tools are called,\nand integrates their output before resuming\ngeneration. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench\nand MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks. ART is also extensible,\nand makes it easy for humans to improve performance by correcting errors in task-specific\nprograms or incorporating new tools, which\nwe demonstrate by drastically improving performance on select tasks with minimal human\nintervention.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2106.15499",
        "title": "Self-Contrastive Learning: Single-viewed Supervised Contrastive Framework using Sub-network",
        "abstract": "Contrastive loss has significantly improved performance in\nsupervised classification tasks by using a multi-viewed framework that leverages augmentation and label information. The\naugmentation enables contrast with another view of a single image but enlarges training time and memory usage. To\nexploit the strength of multi-views while avoiding the high\ncomputation cost, we introduce a multi-exit architecture that\noutputs multiple features of a single image in a single-viewed\nframework. To this end, we propose Self-Contrastive (SelfCon) learning, which self-contrasts within multiple outputs\nfrom the different levels of a single network. The multi-exit\narchitecture efficiently replaces multi-augmented images and\nleverages various information from different layers of a network. We demonstrate that SelfCon learning improves the classification performance of the encoder network, and empirically\nanalyze its advantages in terms of the single-view and the subnetwork. Furthermore, we provide theoretical evidence of the\nperformance increase based on the mutual information bound.\nFor ImageNet classification on ResNet-50, SelfCon improves\naccuracy by +0.6% with 59% memory and 48% time of Supervised Contrastive learning, and a simple ensemble of multi-exit\noutputs boosts performance up to +1.5%. Our code is available\nat https://github.com/raymin0223/self-contrastive-learning.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2102.08981",
        "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts",
        "abstract": "The availability of large-scale image captioning and\nvisual question answering datasets has contributed significantly to recent successes in vision-and-language pretraining. However, these datasets are often collected with\noverrestrictive requirements inherited from their original\ntarget tasks (e.g., image caption generation), which limit\nthe resulting dataset scale and diversity. We take a step\nfurther in pushing the limits of vision-and-language pretraining data by relaxing the data collection pipeline used\nin Conceptual Captions 3M (CC3M) [70] and introduce\nthe Conceptual 12M (CC12M), a dataset with 12 million\nimage-text pairs specifically meant to be used for visionand-language pre-training. We perform an analysis of this\ndataset and benchmark its effectiveness against CC3M on\nmultiple downstream tasks with an emphasis on long-tail visual recognition. Our results clearly illustrate the benefit of\nscaling up pre-training data for vision-and-language tasks,\nas indicated by the new state-of-the-art results on both the\nnocaps and Conceptual Captions benchmarks.1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2003.12462",
        "title": "TextCaps: a Dataset for Image Captioning with Reading Comprehension",
        "abstract": "Image descriptions can help visually impaired people to\nquickly understand the image content. While we made significant progress\nin automatically describing images and optical character recognition, current approaches are unable to include written text in their descriptions,\nalthough text is omnipresent in human environments and frequently critical to understand our surroundings. To study how to comprehend text in\nthe context of an image we collect a novel dataset, TextCaps, with 145k\ncaptions for 28k images. Our dataset challenges a model to recognize\ntext, relate it to its visual context, and decide what part of the text\nto copy or paraphrase, requiring spatial, semantic, and visual reasoning\nbetween multiple text tokens and visual entities, such as objects. We\nstudy baselines and adapt existing approaches to this new task, which\nwe refer to as image captioning with reading comprehension. Our analysis\nwith automatic and human studies shows that our new TextCaps dataset\nprovides many new technical challenges over previous datasets.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.07147",
        "title": "GLOBAL EXPLAINABILITY OF GNNS VIA LOGIC COMBINATION OF LEARNED CONCEPTS",
        "abstract": "While instance-level explanation of GNN is a well-studied problem with plenty of\napproaches being developed, providing a global explanation for the behaviour of a\nGNN is much less explored, despite its potential in interpretability and debugging.\nExisting solutions either simply list local explanations for a given class, or generate a synthetic prototypical graph with maximal score for a given class, completely\nmissing any combinatorial aspect that the GNN could have learned. In this work,\nwe propose GLGExplainer (Global Logic-based GNN Explainer), the first Global\nExplainer capable of generating explanations as arbitrary Boolean combinations\nof learned graphical concepts. GLGExplainer is a fully differentiable architecture\nthat takes local explanations as inputs and combines them into a logic formula over\ngraphical concepts, represented as clusters of local explanations. Contrary to existing solutions, GLGExplainer provides accurate and human-interpretable global\nexplanations that are perfectly aligned with ground-truth explanations (on synthetic data) or match existing domain knowledge (on real-world data). Extracted\nformulas are faithful to the model predictions, to the point of providing insights\ninto some occasionally incorrect rules learned by the model, making GLGExplainer a promising diagnostic tool for learned GNNs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1503.01445",
        "title": "Toxicity Prediction using Deep Learning",
        "abstract": "Everyday we are exposed to various chemicals\nvia food additives, cleaning and cosmetic products and medicines \u2014 and some of them might be\ntoxic. However testing the toxicity of all existing\ncompounds by biological experiments is neither\nfinancially nor logistically feasible. Therefore\nthe government agencies NIH, EPA and FDA\nlaunched the Tox21 Data Challenge within the\n\u201cToxicology in the 21st Century\u201d (Tox21) initiative. The goal of this challenge was to assess\nthe performance of computational methods in\npredicting the toxicity of chemical compounds.\nState of the art toxicity prediction methods build\nupon specifically-designed chemical descriptors\ndeveloped over decades. Though Deep Learning\nis new to the field and was never applied to toxicity prediction before, it clearly outperformed\nall other participating methods. In this application paper we show that deep nets automatically\nlearn features resembling well-established toxicophores. In total, our Deep Learning approach\nwon both of the panel-challenges (nuclear receptors and stress response) as well as the overall\nGrand Challenge, and thereby sets a new standard in tox prediction.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2402.09649",
        "title": "ProtChatGPT: Towards Understanding Proteins with Large Language Models",
        "abstract": "Protein research is crucial in various fundamental disciplines, but understanding their intricate\nstructure-function relationships remains challenging. Recent Large Language Models (LLMs) have\nmade significant strides in comprehending taskspecific knowledge, suggesting the potential for\nChatGPT-like systems specialized in protein to\nfacilitate basic research. In this work, we introduce ProtChatGPT, which aims at learning and\nunderstanding protein structures via natural languages. ProtChatGPT enables users to upload\nproteins, ask questions, and engage in interactive\nconversations to produce comprehensive answers.\nThe system comprises protein encoders, a ProteinLanguage Pertaining Transformer (PLP-former),\na projection adapter, and an LLM. The protein\nfirst undergoes protein encoders and PLP-former\nto produce protein embeddings, which are then\nprojected by the adapter to conform with the LLM.\nThe LLM finally combines user questions with\nprojected embeddings to generate informative answers. Experiments show that ProtChatGPT can\nproduce promising responses to proteins and their\ncorresponding questions. We hope that ProtChatGPT could form the basis for further exploration\nand application in protein research. Code and our\npre-trained model will be publicly available.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2209.09178",
        "title": "ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver Distraction Detection",
        "abstract": "Ensuring traffic safety and mitigating accidents in\nmodern driving is of paramount importance, and computer vision\ntechnologies have the potential to significantly contribute to this\ngoal. This paper presents a multi-modal Vision Transformer\nfor Driver Distraction Detection (termed ViT-DD), which incorporates inductive information from training signals related\nto both distraction detection and driver emotion recognition.\nAdditionally, a self-learning algorithm is developed, allowing for\nthe seamless integration of driver data without emotion labels\ninto the multi-task training process of ViT-DD. Experimental\nresults reveal that the proposed ViT-DD surpasses existing stateof-the-art methods for driver distraction detection by 6.5% and\n0.9% on the SFDDD and AUCDD datasets, respectively.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2307.07253",
        "title": "Frustration-induced magnetic bimerons in transition metal halide CoX2 (X = Cl, Br) monolayers",
        "abstract": "With the field of two-dimensional (2D) magnetic materials expanding rapidly, noncollinear topological magnetic\ntextures in 2D materials are attracting growing interest recently. As the in-plane counterpart of magnetic skyrmions,\nmagnetic bimerons have the same topological advantages, but are rarely observed in experiments. Employing\nfirst-principles calculations and Monte Carlo simulations, we predict that the centrosymmetric transition metal halide\nCoX2 (X = Cl, Br) monolayers can be promising candidates for observing the frustration-induced bimerons. These\nbimerons crystallize into stable triangular lattice under an appropriate magnetic field. Compared to the skyrmions driven\nby the Dzyaloshinskii-Moriya interaction or the long-ranged magnetic dipole-dipole interactions, these\nfrustration-induced bimerons have much smaller size and flexible tunability. Furthermore, the biaxial strain provides an\neffective method to tune the frustration and thereby to tune the bimeron lattice. In detail, for CoCl2 monolayer, tensile\nstrain can be applied to generate bimeron lattice, further shrink bimeron size and increase the density of bimerons. For\nCoBr2 monolayer with inherent bimeron lattice state, a unique orientation rotation of bimeron lattice controlled by\ncompressive strain is predicted. ",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2007.08663",
        "title": "TUDataset: A collection of benchmark datasets for learning with graphs",
        "abstract": "Recently, there has been an increasing interest in\n(supervised) learning with graph data, especially\nusing graph neural networks. However, the\ndevelopment of meaningful benchmark datasets\nand standardized evaluation procedures is lagging, consequently hindering advancements\nin this area. To address this, we introduce\nthe TUDATASET for graph classification and\nregression. The collection consists of over 120\ndatasets of varying sizes from a wide range of\napplications. We provide Python-based data\nloaders, kernel and graph neural network baseline implementations, and evaluation tools. Here,\nwe give an overview of the datasets, standardized\nevaluation procedures, and provide baseline\nexperiments. All datasets are available at\nwww.graphlearning.io. The experiments\nare fully reproducible from the code available at\nwww.github.com/chrsmrrs/tudataset.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/cs/9508101",
        "title": "Using Qualitative Hypotheses to Identify Inaccurate Data",
        "abstract": "Identifying inaccurate data has long been regarded as a signicant and di_x000E_cult problem in AI. In this paper, we present a new method for identifying inaccurate data on the basis of qualitative correlations among related data. First, we introduce the denitions of related data and qualitative correlations among related data. Then we put forward a new concept called support coe_x000E_cient function (SCF ). SCF can be used to extract, represent, and calculate qualitative correlations among related data within a dataset. We propose an approach to determining dynamic shift intervals of inaccurate data, and an approach to calculating possibility of identifying inaccurate data, respectively. Both of the approaches are based on SCF . Finally we present an algorithm for identifying inaccurate data by using qualitative correlations among related data as conrmatory or disconrmatory evidence. We have developed a practical system for interpreting infrared spectra by applying the method, and have fully tested the system against several hundred real spectra. The experimental results show that the method is signicantly better than the conventional methods used in many similar systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/bayes-an/9510001",
        "title": "Bayesian Variable Selection with Related Predictors",
        "abstract": "In data sets with many predictors, algorithms for identifying a good subset of predictors are often used. Most such algorithms do not account for any relationships between predictors. For example, stepwise regression might select a model containing an interaction AB but neither main e_x000B_ect A or B. This paper develops mathematical representations of this and other relations between predictors, which may then be incorporated in a model\nselection procedure. A Bayesian approach that goes beyond the standard independence prior for variable selection is adopted, and preference for certain models is interpreted as prior information. Priors relevant to arbitrary interactions and polynomials, dummy variables for categorical factors, competing predictors, and restrictions on the size of the models are developed. Since the relations developed are for priors, they may be incorporated in any Bayesian variable selection algorithm for any type of linear model. The application of the methods here is illustrated via the Stochastic Search Variable Selection algorithm of George and McCulloch (1993), which is modi_x000C_ed to utilize the new priors. The performance of the approach is illustrated with two constructed examples and a computer performance dataset.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2210.09261",
        "title": "Challenging BIG-Bench tasks and whether chain-of-thought can solve them",
        "abstract": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond\nthe capabilities of current language models. Language models have already made good progress on this\nbenchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results\non 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of\naverage human-rater performance, and are those tasks actually unsolvable by current language models?\nIn this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH).\nThese are the task for which prior language model evaluations did not outperform the average human-rater. We\nfind that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average humanrater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater\nperformance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting\nwithout CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the\nbest performance and capabilities of language models, which is better captured via CoT prompting. As further\nanalysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent\ntask performance on several BBH tasks with otherwise flat scaling curves.1",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2003.05002",
        "title": "TYDI QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages",
        "abstract": "Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TYDI QA\n\u2014a question answering dataset covering 11\ntypologically diverse languages with 204K\nquestion-answer pairs. The languages of\nTYDI QA are diverse with regard to their\ntypology\u2014the set of linguistic features each\nlanguage expresses\u2014such that we expect\nmodels performing well on this set to generalize across a large number of the world\u2019s\nlanguages. We present a quantitative analysis of the data quality and example-level\nqualitative linguistic analyses of observed\nlanguage phenomena that would not be\nfound in English-only corpora. To provide a\nrealistic information-seeking task and avoid\npriming effects, questions are written by\npeople who want to know the answer, but\ndon\u2019t know the answer yet, and the data is\ncollected directly in each language without\nthe use of translation.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.03057",
        "title": "LANGUAGE MODELS ARE MULTILINGUAL CHAIN-OF-THOUGHT REASONERS",
        "abstract": "We evaluate the reasoning abilities of large language models in multilingual\nsettings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K\ndataset (Cobbe et al., 2021) into ten typologically diverse languages. We find\nthat the ability to solve MGSM problems via chain-of-thought prompting emerges\nwith increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali\nand Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and wordin-context semantic judgment. The MGSM benchmark is publicly available at\nhttps://github.com/google-research/url-nlp.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2302.11713",
        "title": "Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?",
        "abstract": "Pre-trained vision and language models (Chen\net al., 2023b,a; Dai et al., 2023; Li et al.,\n2023b) have demonstrated state-of-the-art capabilities over existing tasks involving images\nand texts, including visual question answering. However, it remains unclear whether these\nmodels possess the capability to answer questions that are not only querying visual content but knowledge-intensive and informationseeking. In this study, we introduce INFOSEEK1\n, a visual question answering dataset tailored for information-seeking questions that\ncannot be answered with only common sense\nknowledge. Using INFOSEEK, we analyze\nvarious pre-trained visual question answering\nmodels and gain insights into their characteristics. Our findings reveal that state-of-the-art\npre-trained multi-modal models (e.g., PaLI-X,\nBLIP2, etc.) face challenges in answering visual information-seeking questions, but finetuning on the INFOSEEK dataset elicits models\nto use fine-grained knowledge that was learned\nduring their pre-training. Furthermore, we\nshow that accurate visual entity recognition can\nbe used to improve performance on INFOSEEK\nby retrieving relevant documents, showing a\nsignificant space for improvement.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09224",
        "title": "Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories",
        "abstract": "We propose Encyclopedic-VQA, a large scale visual\nquestion answering (VQA) dataset featuring visual questions about detailed properties of fine-grained categories\nand instances. It contains 221k unique question+answer\npairs each matched with (up to) 5 images, resulting\nin a total of 1M VQA samples. Moreover, our dataset\ncomes with a controlled knowledge base derived from\nWikipedia, marking the evidence to support each answer.\nEmpirically, we show that our dataset poses a hard\nchallenge for large vision+language models as they perform poorly on our dataset: PaLI [14] is state-of-the-art\non OK-VQA [37], yet it only achieves 13.0% accuracy\non our dataset. Moreover, we experimentally show\nthat progress on answering our encyclopedic questions\ncan be achieved by augmenting large models with a\nmechanism that retrieves relevant information from the\nknowledge base. An oracle experiment with perfect retrieval achieves 87.0% accuracy on the single-hop portion\nof our dataset, and an automatic retrieval-augmented\nprototype yields 48.8%. We believe that our dataset enables\nfuture research on retrieval-augmented vision+language\nmodels. It is available at https://github.com/\ngoogle-research/google-research/tree/\nmaster/encyclopedic_vqa.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2009.11462",
        "title": "REALTOXICITYPROMPTS: Evaluating Neural Toxic Degeneration in Language Models",
        "abstract": "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such\ntoxic degeneration. We create and release REALTOXICITYPROMPTS, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web\ntext, paired with toxicity scores from a widelyused toxicity classifier. Using REALTOXICITYPROMPTS, we find that pretrained LMs can\ndegenerate into toxic text even from seemingly\ninnocuous prompts. We empirically assess several controllable generation methods, and find\nthat while data- or compute-intensive methods\n(e.g., adaptive pretraining on non-toxic data)\nare more effective at steering away from toxicity than simpler solutions (e.g., banning \u201cbad\u201d\nwords), no current method is failsafe against\nneural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to\npretrain several LMs (including GPT-2; Radford et al., 2019), and find a significant amount\nof offensive, factually unreliable, and otherwise toxic content. Our work provides a test\nbed for evaluating toxic generations by LMs\nand stresses the need for better data selection\nprocesses for pretraining.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.01733",
        "title": "DocFormerv2: Local Features for Document Understanding",
        "abstract": "We propose DocFormerv2, a multi-modal transformer\nfor Visual Document Understanding (VDU). The VDU domain entails understanding documents (beyond mere OCR\npredictions) e.g., extracting information from a form, VQA\nfor documents and other tasks. VDU is challenging as it\nneeds a model to make sense of multiple modalities (visual,\nlanguage and spatial) to make a prediction. Our approach,\ntermed DocFormerv2 is an encoder-decoder transformer\nwhich takes as input - vision, language and spatial features. DocFormerv2 is pre-trained with unsupervised tasks\nemployed asymmetrically i.e., two novel document tasks on\nencoder and one on the auto-regressive decoder. The unsupervised tasks have been carefully designed to ensure\nthat the pre-training encourages local-feature alignment\nbetween multiple modalities. DocFormerv2 when evaluated on nine datasets shows state-of-the-art performance\nover strong baselines e.g. TabFact (4.3%), InfoVQA (1.4%),\nFUNSD (1%). Furthermore, to show generalization capabilities, on three VQA tasks involving scene-text, DocFormerv2 outperforms previous comparably-sized models\nand even does better than much larger models (such as\nGIT2, PaLi and Flamingo) on some tasks. Extensive ablations show that due to its pre-training, DocFormerv2 understands multiple modalities better than prior-art in VDU",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1905.13538",
        "title": "FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents",
        "abstract": "We present a new dataset for form understanding\nin noisy scanned documents (FUNSD) that aims at extracting\nand structuring the textual content of forms. The dataset\ncomprises 199 real, fully annotated, scanned forms. The documents are noisy and vary widely in appearance, making\nform understanding (FoUn) a challenging task. The proposed\ndataset can be used for various tasks, including text detection,\noptical character recognition, spatial layout analysis, and entity\nlabeling/linking. To the best of our knowledge, this is the first\npublicly available dataset with comprehensive annotations to\naddress FoUn task. We also present a set of baselines and\nintroduce metrics to evaluate performance on the FUNSD\ndataset, which can be downloaded at https://guillaumejaume.\ngithub.io/FUNSD/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1705.06950",
        "title": "The Kinetics Human Action Video Dataset",
        "abstract": "We describe the DeepMind Kinetics human action video\ndataset. The dataset contains 400 human action classes,\nwith at least 400 video clips for each action. Each clip lasts\naround 10s and is taken from a different YouTube video. The\nactions are human focussed and cover a broad range of\nclasses including human-object interactions such as playing instruments, as well as human-human interactions such\nas shaking hands. We describe the statistics of the dataset,\nhow it was collected, and give some baseline performance\nfigures for neural network architectures trained and tested\nfor human action classification on this dataset. We also\ncarry out a preliminary analysis of whether imbalance in\nthe dataset leads to bias in the classifiers.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1904.11451",
        "title": "Large Scale Holistic Video Understanding",
        "abstract": "Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a largescale \u201cHolistic Video Understanding Dataset\u201d (HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx. 572k videos in total with 9 million annotations for training, validation and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes and concepts which naturally captures the real-world scenarios. We demonstrate the generalisation capability of HVU on three challenging tasks: 1.) Video classification, 2.) Video captioning and 3.) Video clustering tasks. In particular for video classification, we introduce a new spatio-temporal deep neural network architecture called \u201cHolistic Appearance and Temporal Network\u201d (HATNet) that builds on fusing 2D and 3D architectures into one by combining intermediate representations of appearance and temporal cues. HATNet focuses on the multi-label and multi-task learning problem and is trained in an end-to-end manner. Via our experiments, we validate the idea that holistic representation learning is complementary, and can play a key role in enabling many real-world applications. https://holistic-video-understanding.github.io/",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1712.09374",
        "title": "HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization",
        "abstract": "This paper presents a new large-scale dataset for recognition and temporal localization of human actions collected\nfrom Web videos. We refer to it as HACS (Human Action\nClips and Segments). We leverage both consensus and disagreement among visual classifiers to automatically mine\ncandidate short clips from unlabeled videos, which are subsequently validated by human annotators. The resulting\ndataset is dubbed HACS Clips. Through a separate process we also collect annotations defining action segment\nboundaries. This resulting dataset is called HACS Segments. Overall, HACS Clips consists of 1.5M annotated\nclips sampled from 504K untrimmed videos, and HACS Segments contains 139K action segments densely annotated\nin 50K untrimmed videos spanning 200 action categories.\nHACS Clips contains more labeled examples than any existing video benchmark. This renders our dataset both a\nlarge-scale action recognition benchmark and an excellent\nsource for spatiotemporal feature learning. In our transfer\nlearning experiments on three target datasets, HACS Clips\noutperforms Kinetics-600, Moments-In-Time and Sports1M\nas a pretraining source. On HACS Segments, we evaluate\nstate-of-the-art methods of action proposal generation and\naction localization, and highlight the new challenges posed\nby our dense temporal annotations.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2208.01006",
        "title": "Multi-Document Summarization with Centroid-Based Pretraining",
        "abstract": "In Multi-Document Summarization (MDS), the input can be modeled as a set of documents, and the output is its summary. In this paper, we focus on pretraining objectives for MDS. Specifically, we introduce a novel pretraining objective, which involves selecting the ROUGEbased centroid of each document cluster as a proxy for its summary. Our objective thus does not require human written summaries and can be utilized for pretraining on a dataset consisting solely of document sets. Through zero-shot, few-shot, and fully supervised experiments on multiple MDS datasets, we show that our model Centrum is better or comparable to a state-ofthe-art model. We make the pretrained and finetuned models freely available to the research community1 .",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1906.01749",
        "title": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
        "abstract": "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multidocument summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and release our data and code in hope that this work will promote advances in summarization in the multidocument setting1 .",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2010.14235",
        "title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles",
        "abstract": "Multi-document summarization is a challenging task for which there exists little largescale datasets. We propose Multi-XScience,\na large-scale multi-document summarization\ndataset created from scientific articles. MultiXScience introduces a challenging multidocument summarization task: writing the\nrelated-work section of a paper based on its\nabstract and the articles it references. Our\nwork is inspired by extreme summarization, a\ndataset construction protocol that favours abstractive modeling approaches. Descriptive\nstatistics and empirical results\u2014using several\nstate-of-the-art models trained on the MultiXScience dataset\u2014reveal that Multi-XScience\nis well suited for abstractive models.\n1",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2110.08499",
        "title": "PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization",
        "abstract": "We introduce PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins.1",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2106.05006",
        "title": "Text-to-SQL in the Wild: A Naturally-Occurring Dataset Based on Stack\nExchange Data",
        "abstract": "Most available semantic parsing datasets, comprising of pairs of natural utterances and logical forms, were collected solely for the purpose of training and evaluation of natural language understanding systems. As a result, they do not contain any of the richness and variety of natural-occurring utterances, where humans ask about data they need or are curious about. In this work, we release SEDE, a dataset with 12,023 pairs of utterances and SQL queries collected from real usage on the Stack Exchange website. We show that these pairs contain a variety of real-world challenges which were rarely reflected so far in any other semantic parsing dataset, propose an evaluation metric based on comparison of partial query clauses that is more suitable for realworld queries, and conduct experiments with strong baselines, showing a large gap between the performance on SEDE compared to other common datasets.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2204.14243",
        "title": "Training Naturalized Semantic Parsers with Very Little Data",
        "abstract": "Semantic parsing is an important NLP problem,\nparticularly for voice assistants such as Alexa and\nGoogle Assistant. State-of-the-art (SOTA) semantic parsers are seq2seq architectures based on large\nlanguage models that have been pretrained on vast\namounts of text. To better leverage that pretraining, recent work has explored a reformulation of\nsemantic parsing whereby the output sequences\nare themselves natural language sentences, but in\na controlled fragment of natural language. This\napproach delivers strong results, particularly for\nfew-shot semantic parsing, which is of key importance in practice and the focus of our paper. We\npush this line of work forward by introducing an\nautomated methodology that delivers very significant additional improvements by utilizing modest amounts of unannotated data, which is typically easy to obtain. Our method is based on a\nnovel synthesis of four techniques: joint training\nwith auxiliary unsupervised tasks; constrained decoding; self-training; and paraphrasing. We show\nthat this method delivers new SOTA few-shot performance on the Overnight dataset, particularly in\nvery low-resource settings, and very compelling\nfew-shot results on a new semantic parsing dataset.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1508.00305",
        "title": "Compositional Semantic Parsing on Semi-Structured Tables",
        "abstract": "Two important aspects of semantic parsing for question answering are the breadth\nof the knowledge source and the depth of\nlogical compositionality. While existing\nwork trades off one aspect for another, this\npaper simultaneously makes progress on\nboth fronts through a new task: answering\ncomplex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from\ntwo compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality\nresults in a combinatorial explosion in\nthe space of logical forms. We propose\na logical-form driven parsing algorithm\nguided by strong typing constraints and\nshow that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033\ncomplex questions on Wikipedia tables,\nwhich is made publicly available.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1809.08887",
        "title": "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task",
        "abstract": "We present Spider, a large-scale, complex\nand cross-domain semantic parsing and textto-SQL dataset annotated by 11 college students. It consists of 10,181 questions and\n5,693 unique complex SQL queries on 200\ndatabases with multiple tables, covering 138\ndifferent domains. We define a new complex\nand cross-domain semantic parsing and textto-SQL task where different complex SQL\nqueries and databases appear in train and test\nsets. In this way, the task requires the model\nto generalize well to both new SQL queries\nand new database schemas. Spider is distinct\nfrom most of the previous semantic parsing\ntasks because they all use a single database\nand the exact same programs in the train set\nand the test set. We experiment with various state-of-the-art models and the best model\nachieves only 12.4% exact matching accuracy on a database split setting. This shows\nthat Spider presents a strong challenge for future research. Our dataset and task are publicly available at https://yale-lily.\ngithub.io/spider.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1709.00103",
        "title": "SEQ2SQL: GENERATING STRUCTURED QUERIES FROM NATURAL LANGUAGE USING REINFORCEMENT LEARNING",
        "abstract": "Relational databases store a significant amount of the worlds data. However, accessing this data currently requires users to understand a query language such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model uses rewards from inthe-loop query execution over the database to learn a policy to generate the query, which contains unordered parts that are less suitable for optimization via cross entropy loss. Moreover, Seq2SQL leverages the structure of SQL to prune the space of generated queries and significantly simplify the generation problem. In addition to the model, we release WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia that is an order of magnitude larger than comparable datasets. By applying policybased reinforcement learning with a query execution environment to WikiSQL, Seq2SQL outperforms a state-of-the-art semantic parser, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.17221",
        "title": "Federated Learning for Semantic Parsing: Task Formulation, Evaluation Setup, New Algorithms",
        "abstract": "This paper studies a new task of federated learning (FL) for semantic parsing, where multiple clients collaboratively train one global model without sharing their semantic parsing data. By leveraging data from multiple clients, the FL paradigm can be especially beneficial for clients that have little training data to develop a data-hungry neural semantic parser on their own. We propose an evaluation setup to study this task, where we re-purpose widely-used single-domain text-to-SQL datasets as clients to form a realistic heterogeneous FL setting and collaboratively train a global model. As standard FL algorithms suffer from the high client heterogeneity in our realistic setup, we further propose a novel LOss Reduction Adjusted Reweighting (Lorar) mechanism to mitigate the performance degradation, which adjusts each client\u2019s contribution to the global model update based on its training loss reduction during each round. Our intuition is that the larger the loss reduction, the further away the current global model is from the client\u2019s local optimum, and the larger weight the client should get. By applying Lorar to three widely adopted FL algorithms (FedAvg, FedOPT and FedProx), we observe that their performance can be improved substantially on average (4%-20% absolute gain under MacroAvg) and that clients with smaller datasets enjoy larger performance gains. In addition, the global model converges faster for almost all the clients.1",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1702.01168",
        "title": "Type- and Content-Driven Synthesis of SQL Queries from Natural Language",
        "abstract": "This paper presents a new technique for automatically synthesizing SQL queries from natural language. Our technique is fully automated, works for any database without requiring additional customization, and does not require users to know the underlying database schema. Our method achieves these goals by combining natural language processing, program synthesis, and automated program repair. Given the user\u2019s English description, our technique first uses semantic parsing to generate a query sketch, which is subsequently completed using type-directed program synthesis and assigned a confidence score using database contents. However, since the user\u2019s description may not accurately reflect the actual database schema, our approach also performs fault localization and repairs the erroneous part of the sketch. This synthesize-repair loop is repeated until the algorithm infers a query with a sufficiently high confidence score. We have implemented the proposed technique in a tool called Sqlizer and evaluate it on three different databases. Our experiments show that the desired query is ranked within the top 5 candidates in close to 90% of the cases.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2112.06311",
        "title": "Weakly Supervised Text-to-SQL Parsing through Question Decomposition",
        "abstract": "Text-to-SQL parsers are crucial in enabling non-experts to effortlessly query relational data. Training such parsers, by contrast, generally requires expertise in annotating natural language (NL) utterances with corresponding SQL queries. In this work, we propose a weak supervision approach for training text-to-SQL parsers. We take advantage of the recently proposed question meaning representation called QDMR, an intermediate between NL and formal query languages. Given questions, their QDMR structures (annotated by non-experts or automatically predicted), and the answers, we are able to automatically synthesize SQL queries that are used to train text-to-SQL models. We test our approach by experimenting on five benchmark datasets. Our results show that the weakly supervised models perform competitively with those trained on annotated NLSQL data. Overall, we effectively train textto-SQL parsers, while using zero SQL annotations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2112.10684",
        "title": "Efficient Large Scale Language Modeling with Mixtures of Experts",
        "abstract": "Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive\nMoE language models scale in comparison\nwith dense models in a wide range of settings:\nin- and out-of-domain language modeling,\nzero- and few-shot priming, and full-shot finetuning. With the exception of fine-tuning, we\nfind MoEs to be substantially more compute\nefficient. At more modest training budgets,\nMoEs can match the performance of dense\nmodels using \u223c4 times less compute. This gap\nnarrows at scale, but our largest MoE model\n(1.1T parameters) consistently outperforms a\ncompute-equivalent dense model (6.7B parameters). Overall, this performance gap varies\ngreatly across tasks and domains, suggesting\nthat MoE and dense models generalize differently in ways that are worthy of future study.\nWe make our code and models publicly available for research use.1",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2004.07633",
        "title": "A Methodology for Creating Question Answering Corpora Using Inverse Data Annotation",
        "abstract": "In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. For this, we introduce an intermediate representation that is based on the logical query plan in a database called Operation Trees (OT). This representation allows us to invert the annotation process without losing flexibility in the types of queries that we generate. Furthermore, it allows for fine-grained alignment of query tokens to OT operations. In our method, we randomly generate OTs from a context-free grammar. Afterwards, annotators have to write the appropriate natural language question that is represented by the OT. Finally, the annotators assign the tokens to the OT operations. We apply the method to create a new corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases. We compare OTTA to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries. Finally, we train a state-of-the-art semantic parsing model on our data and show that our corpus is a challenging dataset and that the token alignment can be leveraged to increase the performance significantly.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1812.10037",
        "title": "Building a Neural Semantic Parser from a Domain Ontology",
        "abstract": "Semantic parsing is the task of converting natural language utterances into machine interpretable meaning representations which can be executed against a real-world environment such as a database. Scaling semantic parsing to arbitrary domains faces two interrelated challenges: obtaining broad coverage training data effectively and cheaply; and developing a model that generalizes to compositional utterances and complex intentions. We address these challenges with a framework which allows to elicit training data from a domain ontology and bootstrap a neural parser which recursively builds derivations of logical forms. In our framework meaning representations are described by sequences of natural language templates, where each template corresponds to a decomposed fragment of the underlying meaning representation. Although artificial, templates can be understood and paraphrased by humans to create natural utterances, resulting in parallel triples of utterances, meaning representations, and their decompositions. These allow us to train a neural semantic parser which learns to compose rules in deriving meaning representations. We crowdsource training data on six domains, covering both single-turn utterances which exhibit rich compositionality, and sequential utterances where a complex task is procedurally performed in steps. We then develop neural semantic parsers which perform such compositional tasks. In general, our approach allows to deploy neural semantic parsers quickly and cheaply from a given domain ontology.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.19227",
        "title": "Aligning Teacher with Student Preferences for Tailored Training Data Generation",
        "abstract": "Large Language Models (LLMs) have shown significant promise as copilots in various tasks. Local deployment of LLMs on edge devices is necessary when handling privacy-sensitive data or latency-sensitive tasks. The computational constraints of such devices make direct deployment of powerful large-scale LLMs impractical, necessitating the Knowledge Distillation from large-scale models to lightweight models. Lots of work has been done to elicit diversity and quality training examples from LLMs, but little attention has been paid to aligning teacher instructional content based on student preferences, akin to \"responsive teaching\" in pedagogy. Thus, we propose ARTE, dubbed Aligning TeacheR with StudenT PreferencEs, a framework that aligns the teacher model with student preferences to generate tailored training examples for Knowledge Distillation. Specifically, we elicit draft questions and rationales from the teacher model, then collect student preferences on these questions and rationales using students' performance with in-context learning as a proxy, and finally align the teacher model with student preferences. In the end, we repeat the first step with the aligned teacher model to elicit tailored training examples for the student model on the target task. Extensive experiments on academic benchmarks demonstrate the superiority of ARTE over existing instruction-tuning datasets distilled from powerful LLMs. Moreover, we thoroughly investigate the generalization of ARTE, including the generalization of fine-tuned student models in reasoning ability and the generalization of aligned teacher models to generate tailored training data across tasks and students. In summary, our contributions lie in proposing a novel framework for tailored training example generation, demonstrating its efficacy in experiments, and investigating the generalization of both student & aligned teacher models in ARTE.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2406.19564",
        "title": "Voices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects",
        "abstract": "Yor\u00f9b\u00e1 an African language with roughly 47 million speakers encompasses a continuum with several dialects. Recent efforts to develop NLP technologies for African languages have focused on their standard dialects, resulting in disparities for dialects and varieties for which there are little to no resources or tools. We take steps towards bridging this gap by introducing a new high-quality parallel text and speech corpus YOR\u00d9LECT across three domains and four regional Yor\u00f9b\u00e1 dialects. To develop this corpus, we engaged native speakers, travelling to communities where these dialects are spoken, to collect text and speech data. Using our newly created corpus, we conducted extensive experiments on (text) machine translation, automatic speech recognition, and speech-to-text translation. Our results reveal substantial performance disparities between standard Yor\u00f9b\u00e1 and the other dialects across all tasks. However, we also show that with dialect-adaptive finetuning, we are able to narrow this gap. We believe our dataset and experimental analysis will contribute greatly to developing NLP tools for Yor\u00f9b\u00e1 and its dialects, and potentially for other African languages, by improving our understanding of existing challenges and offering a high-quality dataset for further development. We release YOR\u00d9LECT dataset and models publicly under an open license.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.19538",
        "title": "Context Matters: An Empirical Study of the Impact of Contextual Information in Temporal Question Answering Systems",
        "abstract": "Large language models (LLMs) often struggle with temporal reasoning, crucial for tasks like historical event analysis and time-sensitive information retrieval. Despite advancements, state-of-the-art models falter in handling temporal information, especially when faced with irrelevant or noisy contexts. This paper addresses this gap by empirically examining the robustness of temporal question-answering (TQA) systems trained on various context types, including relevant, irrelevant, slightly altered, and no context. Our findings indicate that training with a mix of these contexts enhances model robustness and accuracy. Additionally, we show that the position of context relative to the question significantly impacts performance, with question-first positioning yielding better results. We introduce two new context-rich TQA datasets, ContextAQA and ContextTQE, and provide comprehensive evaluations and guidelines for training robust TQA models. Our work lays the foundation for developing reliable and context-aware temporal QA systems, with broader implications for enhancing LLM robustness against diverse and potentially adversarial information.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.19502",
        "title": "Investigating How Large Language Models Leverage Internal Knowledge to Perform Complex Reasoning",
        "abstract": "Despite significant advancements, there is a limited understanding of how large language models (LLMs) utilize knowledge for reasoning. To address this, we propose a method that deconstructs complex real-world questions into a graph, representing each question as a node with parent nodes of background knowledge needed to solve the question. We develop the DepthQA dataset, deconstructing questions into three depths: (i) recalling conceptual knowledge, (ii) applying procedural knowledge, and (iii) analyzing strategic knowledge. Based on a hierarchical graph, we quantify forward discrepancy, discrepancies in LLMs' performance on simpler sub-problems versus complex questions. We also measure backward discrepancy, where LLMs answer complex questions but struggle with simpler ones. Our analysis shows that smaller models have more discrepancies than larger models. Additionally, guiding models from simpler to complex questions through multi-turn interactions improves performance across model sizes, highlighting the importance of structured intermediate steps in knowledge reasoning. This work enhances our understanding of LLM reasoning and suggests ways to improve their problem-solving abilities.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.19465",
        "title": "Can Large Language Models Generate High-quality Patent Claims?",
        "abstract": "Large language models (LLMs) have shown exceptional performance across various text generation tasks but remain under-explored in the patent domain, which offers highly structured and precise language. This paper constructs a dataset to investigate the performance of current LLMs in patent claim generation. Our results demonstrate that generating claims based on patent descriptions outperforms previous research relying on abstracts. Interestingly, current patent-specific LLMs perform much worse than state-of-the-art general LLMs, highlighting the necessity for future research on in-domain LLMs. We also find that LLMs can produce high-quality first independent claims, but their performances markedly decrease for subsequent dependent claims. Moreover, fine-tuning can enhance the completeness of inventions' features, conceptual clarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the best performance in comprehensive human evaluations by patent experts, with better feature coverage, conceptual clarity, and technical coherence. Despite these capabilities, comprehensive revision and modification are still necessary to pass rigorous patent scrutiny and ensure legal robustness.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.19415",
        "title": "An Analysis of Multilingual FActScore",
        "abstract": "FActScore has gained popularity as a metric to estimate the factuality of long-form texts generated by Large Language Models (LLMs) in English. However, there has not been any work in studying the behavior of FActScore in other languages. This paper studies the limitations of each component in the four-component pipeline of FActScore in the multilingual setting. We introduce a new dataset for FActScore on texts generated by strong multilingual LLMs. Our evaluation shows that LLMs exhibit distinct behaviors in both fact extraction and fact scoring tasks. No LLM produces consistent and reliable FActScore across languages with varying levels of resources. We also find that the knowledge source plays an important role in the quality of the estimated FActScore. Using Wikipedia as the knowledge source may hinder the true FActScore of long-form text due to its limited coverage in medium- and low-resource languages. We also incorporate three mitigations to our knowledge source that ultimately improve FActScore estimation across all languages.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.20098",
        "title": "Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs",
        "abstract": "Multimodal large language models (MLLMs) have shown impressive success across modalities such as image, video, and audio in a variety of understanding and generation tasks. However, current MLLMs are surprisingly poor at understanding webpage screenshots and generating their corresponding HTML code. To address this problem, we propose Web2Code, a benchmark consisting of a new large-scale webpage-to-code dataset for instruction tuning and an evaluation framework for the webpage understanding and HTML code translation abilities of MLLMs. For dataset construction, we leverage pretrained LLMs to enhance existing webpage-to-code datasets as well as generate a diverse pool of new webpages rendered into images. Specifically, the inputs are webpage images and instructions, while the responses are the webpage's HTML code. We further include diverse natural language QA pairs about the webpage content in the responses to enable a more comprehensive understanding of the web content. To evaluate model performance in these tasks, we develop an evaluation framework for testing MLLMs' abilities in webpage understanding and web-to-code generation. Extensive experiments show that our proposed dataset is beneficial not only to our proposed tasks but also in the general visual domain, while previous datasets result in worse performance. We hope our work will contribute to the development of general MLLMs suitable for web-based content generation and task automation. Our data and code will be available at this https URL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.20095",
        "title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy",
        "abstract": "Large Language Models (LLMs) equipped with extensive world knowledge and strong reasoning skills can tackle diverse tasks across domains, often by posing them as conversation-style instruction-response pairs. In this paper, we propose LLaRA: Large Language and Robotics Assistant, a framework which formulates robot action policy as conversations, and provides improved responses when trained with auxiliary data that complements policy learning. LLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity to process state information as visual-textual prompts and generate optimal policy decisions in text. To train such action policy VLMs, we first introduce an automated pipeline to generate diverse high-quality robotics instruction data from existing behavior cloning data. A VLM finetuned with the resulting collection of datasets based on a conversation-style formulation tailored for robotics tasks, can generate meaningful robot action policy decisions. Our experiments across multiple simulated and real-world environments demonstrate the state-of-the-art performance of the proposed LLaRA framework. The code, datasets, and pretrained models are available at this https URL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.19760",
        "title": "Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case Reformulation",
        "abstract": "Legal case retrieval for sourcing similar cases is critical in upholding judicial fairness. Different from general web search, legal case retrieval involves processing lengthy, complex, and highly specialized legal documents. Existing methods in this domain often overlook the incorporation of legal expert knowledge, which is crucial for accurately understanding and modeling legal cases, leading to unsatisfactory retrieval performance. This paper introduces KELLER, a legal knowledge-guided case reformulation approach based on large language models (LLMs) for effective and interpretable legal case retrieval. By incorporating professional legal knowledge about crimes and law articles, we enable large language models to accurately reformulate the original legal case into concise sub-facts of crimes, which contain the essential information of the case. Extensive experiments on two legal case retrieval benchmarks demonstrate superior retrieval performance and robustness on complex legal case queries of KELLER over existing methods.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2406.19966",
        "title": "Simulating Financial Market via Large Language Model based Agents",
        "abstract": "Most economic theories typically assume that financial market participants are fully rational individuals and use mathematical models to simulate human behavior in financial markets. However, human behavior is often not entirely rational and is challenging to predict accurately with mathematical models. In this paper, we propose \\textbf{A}gent-based \\textbf{S}imulated \\textbf{F}inancial \\textbf{M}arket (ASFM), which first constructs a simulated stock market with a real order matching system. Then, we propose a large language model based agent as the stock trader, which contains the profile, observation, and tool-learning based action module. The trading agent can comprehensively understand current market dynamics and financial policy information, and make decisions that align with their trading strategy. In the experiments, we first verify that the reactions of our ASFM are consistent with the real stock market in two controllable scenarios. In addition, we also conduct experiments in two popular economics research directions, and we find that conclusions drawn in our \\model align with the preliminary findings in economics research. Based on these observations, we believe our proposed ASFM provides a new paradigm for economic research.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2406.19954",
        "title": "BESTOW: Efficient and Streamable Speech Language Model with the Best of Two Worlds in GPT and T5",
        "abstract": "Incorporating speech understanding capabilities into pretrained large-language models has become a vital research direction (SpeechLLM). The previous architectures can be categorized as: i) GPT-style, prepend speech prompts to the text prompts as a sequence of LLM inputs like a decoder-only model; ii) T5-style, introduce speech cross-attention to each layer of the pretrained LLMs. We propose BESTOW architecture to bring the BESt features from TwO Worlds into a single model that is highly efficient and has strong multitask capabilities. Moreover, there is no clear streaming solution for either style, especially considering the solution should generalize to speech multitask. We reformulate streamable SpeechLLM as a read-write policy problem and unifies the offline and streaming research with BESTOW architecture. Hence we demonstrate the first open-source SpeechLLM solution that enables Streaming and Multitask at scale (beyond ASR) at the same time. This streamable solution achieves very strong performance on a wide range of speech tasks (ASR, AST, SQA, unseen DynamicSuperb). It is end-to-end optimizable, with lower training/inference cost, and demonstrates LLM knowledge transferability to speech.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2406.19949",
        "title": "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring",
        "abstract": "Generating rationales that justify scoring decisions has been a promising way to facilitate explainability in automated scoring systems. However, existing methods do not match the accuracy of classifier-based methods. Plus, the generated rationales often contain hallucinated information. To address these issues, we propose a novel framework capable of generating more faithful rationales and, more importantly, matching performance with classifier-based black-box scoring systems. We first mimic the human assessment process by querying Large Language Models (LLMs) to generate a thought tree. We then summarise intermediate assessment decisions from each thought tree path for creating synthetic rationale data and rationale preference data. Finally, we utilise the generated synthetic data to calibrate LLMs through a two-step training process: supervised fine-tuning and preference optimization. Extensive experimental results demonstrate that our framework achieves a 38% assessment performance improvement in the QWK score compared to prior work while producing higher-quality rationales, as recognised by human evaluators and LLMs. Our work sheds light on the effectiveness of performing preference optimization using synthetic preference data obtained from thought tree paths.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2406.19650",
        "title": "DECOR: Improving Coherence in L2 English Writing with a Novel Benchmark for Incoherence Detection, Reasoning, and Rewriting",
        "abstract": "Coherence in writing, an aspect that second-language (L2) English learners often struggle with, is crucial in assessing L2 English writing. Existing automated writing evaluation systems primarily use basic surface linguistic features to detect coherence in writing. However, little effort has been made to correct the detected incoherence, which could significantly benefit L2 language learners seeking to improve their writing. To bridge this gap, we introduce DECOR, a novel benchmark that includes expert annotations for detecting incoherence in L2 English writing, identifying the underlying reasons, and rewriting the incoherent sentences. To our knowledge, DECOR is the first coherence assessment dataset specifically designed for improving L2 English writing, featuring pairs of original incoherent sentences alongside their expert-rewritten counterparts. Additionally, we fine-tuned models to automatically detect and rewrite incoherence in student essays. We find that incorporating specific reasons for incoherence during fine-tuning consistently improves the quality of the rewrites, achieving a result that is favored in both automatic and human evaluations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2406.19884",
        "title": "Investigating the Timescales of Language Processing with EEG and Language Models",
        "abstract": "This study explores the temporal dynamics of language processing by examining the alignment between word representations from a pre-trained transformer-based language model, and EEG data. Using a Temporal Response Function (TRF) model, we investigate how neural activity corresponds to model representations across different layers, revealing insights into the interaction between artificial language models and brain responses during language comprehension. Our analysis reveals patterns in TRFs from distinct layers, highlighting varying contributions to lexical and compositional processing. Additionally, we used linear discriminant analysis (LDA) to isolate part-of-speech (POS) representations, offering insights into their influence on neural responses and the underlying mechanisms of syntactic processing. These findings underscore EEG's utility for probing language processing dynamics with high temporal resolution. By bridging artificial language models and neural activity, this study advances our understanding of their interaction at fine timescales.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2406.19874",
        "title": "Detecting Subtle Differences between Human and Model Languages Using Spectrum of Relative Likelihood",
        "abstract": "Human and model-generated texts can be distinguished by examining the magnitude of likelihood in language. However, it is becoming increasingly difficult as language model's capabilities of generating human-like texts keep evolving. This study provides a new perspective by using the relative likelihood values instead of absolute ones, and extracting useful features from the spectrum-view of likelihood for the human-model text detection task. We propose a detection procedure with two classification methods, supervised and heuristic-based, respectively, which results in competitive performances with previous zero-shot detection methods and a new state-of-the-art on short-text detection. Our method can also reveal subtle differences between human and model languages, which find theoretical roots in psycholinguistics studies. Our code is available at this https URL",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2406.19820",
        "title": "BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question Answering",
        "abstract": "Large language models (LLMs) have demonstrated strong reasoning capabilities. Nevertheless, they still suffer from factual errors when tackling knowledge-intensive tasks. Retrieval-augmented reasoning represents a promising approach. However, significant challenges still persist, including inaccurate and insufficient retrieval for complex questions, as well as difficulty in integrating multi-source knowledge. To address this, we propose Beam Aggregation Reasoning, BeamAggR, a reasoning framework for knowledge-intensive multi-hop QA. BeamAggR explores and prioritizes promising answers at each hop of question. Concretely, we parse the complex questions into trees, which include atom and composite questions, followed by bottom-up reasoning. For atomic questions, the LLM conducts reasoning on multi-source knowledge to get answer candidates. For composite questions, the LLM combines beam candidates, explores multiple reasoning paths through probabilistic aggregation, and prioritizes the most promising trajectory. Extensive experiments on four open-domain multi-hop reasoning datasets show that our method significantly outperforms SOTA methods by 8.5%. Furthermore, our analysis reveals that BeamAggR elicits better knowledge collaboration and answer aggregation.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2406.19803",
        "title": "Scalable and Domain-General Abstractive Proposition Segmentation",
        "abstract": "Segmenting text into fine-grained units of meaning is important to a wide range of NLP applications. The default approach of segmenting text into sentences is often insufficient, especially since sentences are usually complex enough to include multiple units of meaning that merit separate treatment in the downstream task. We focus on the task of abstractive proposition segmentation: transforming text into simple, self-contained, well-formed sentences. Several recent works have demonstrated the utility of proposition segmentation with few-shot prompted LLMs for downstream tasks such as retrieval-augmented grounding and fact verification. However, this approach does not scale to large amounts of text and may not always extract all the facts from the input text. In this paper, we first introduce evaluation metrics for the task to measure several dimensions of quality. We then propose a scalable, yet accurate, proposition segmentation model. We model proposition segmentation as a supervised task by training LLMs on existing annotated datasets and show that training yields significantly improved results. We further show that by using the fine-tuned LLMs as teachers for annotating large amounts of multi-domain synthetic distillation data, we can train smaller student models with results similar to the teacher LLMs. We then demonstrate that our technique leads to effective domain generalization, by annotating data in two domains outside the original training data and evaluating on them. Finally, as a key contribution of the paper, we share an easy-to-use API for NLP practitioners to use.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2406.19764",
        "title": "Belief Revision: The Adaptability of Large Language Models Reasoning",
        "abstract": "The capability to reason from text is crucial for real-world NLP applications. Real-world scenarios often involve incomplete or evolving data. In response, individuals update their beliefs and understandings accordingly. However, most existing evaluations assume that language models (LMs) operate with consistent information. We introduce Belief-R, a new dataset designed to test LMs' belief revision ability when presented with new evidence. Inspired by how humans suppress prior inferences, this task assesses LMs within the newly proposed delta reasoning (\u0394R) framework. Belief-R features sequences of premises designed to simulate scenarios where additional information could necessitate prior conclusions drawn by LMs. We evaluate \u223c30 LMs across diverse prompting strategies and found that LMs generally struggle to appropriately revise their beliefs in response to new information. Further, models adept at updating often underperformed in scenarios without necessary updates, highlighting a critical trade-off. These insights underscore the importance of improving LMs' adaptiveness to changing information, a step toward more reliable AI systems.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.19934",
        "title": "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis",
        "abstract": "We explore multi-step reasoning in vision-language models (VLMs). The problem is challenging, as reasoning data consisting of multiple steps of visual and language processing are barely available. To overcome the challenge, we first introduce a least-to-most visual reasoning paradigm, which interleaves steps of decomposing a question into sub-questions and invoking external tools for resolving sub-questions. Based on the paradigm, we further propose a novel data synthesis approach that can automatically create questions and multi-step reasoning paths for an image in a bottom-up manner. Our approach divides the complex synthesis task into a few simple sub-tasks, and (almost entirely) relies on open-sourced models to accomplish the sub-tasks. Therefore, the entire synthesis process is reproducible and cost-efficient, and the synthesized data is quality guaranteed. With the approach, we construct 50k visual reasoning examples. Then, we develop a visual reasoner through supervised fine-tuning, which is capable of generally enhancing the reasoning abilities of a wide range of existing VLMs in a plug-and-play fashion. Extensive experiments indicate that the visual reasoner can consistently and significantly improve four VLMs on four VQA benchmarks. Our code and dataset are available at this https URL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2103.03095",
        "title": "A Survey on Spoken Language Understanding: Recent Advances and New Frontiers",
        "abstract": "Spoken Language Understanding (SLU) aims to\nextract the semantics frame of user queries, which\nis a core component in a task-oriented dialog system. With the burst of deep neural networks and\nthe evolution of pre-trained language models, the\nresearch of SLU has obtained significant breakthroughs. However, there remains a lack of a\ncomprehensive survey summarizing existing approaches and recent trends, which motivated the\nwork presented in this article. In this paper, we\nsurvey recent advances and new frontiers in SLU.\nSpecifically, we give a thorough review of this\nresearch field, covering different aspects including (1) new taxonomy: we provide a new perspective for SLU filed, including single model\nvs. joint model, implicit joint modeling vs. explicit joint modeling in joint model, non pre-trained\nparadigm vs. pre-trained paradigm; (2) new frontiers: some emerging areas in complex SLU as\nwell as the corresponding challenges; (3) abundant\nopen-source resources: to help the community, we\nhave collected, organized the related papers, baseline projects and leaderboard on a public website1\nwhere SLU researchers could directly access to the\nrecent progress. We hope that this survey can shed\na light on future research in SLU field.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2312.09899",
        "title": "SQA-SAM: Segmentation Quality Assessment for Medical Images Utilizing the Segment Anything Model",
        "abstract": "Segmentation quality assessment (SQA) plays a critical role\nin the deployment of a medical image based AI system. Users need to be\ninformed/alerted whenever an AI system generates unreliable/incorrect\npredictions. With the introduction of the Segment Anything Model (SAM),\na general foundation segmentation model, new research opportunities\nemerged in how one can utilize SAM for medical image segmentation. In\nthis paper, we propose a novel SQA method, called SQA-SAM, which\nexploits SAM to enhance the accuracy of quality assessment for medical\nimage segmentation. When a medical image segmentation model (MedSeg) produces predictions for a test image, we generate visual prompts\nbased on the predictions, and SAM is utilized to generate segmentation maps corresponding to the visual prompts. How well MedSeg\u2019s segmentation aligns with SAM\u2019s segmentation indicates how well MedSeg\u2019s\nsegmentation aligns with the general perception of objectness and image region partition. We develop a score measure for such alignment.\nIn experiments, we find that the generated scores exhibit moderate to\nstrong positive correlation (in Pearson correlation and Spearman correlation) with Dice coefficient scores reflecting the true segmentation\nquality. The code and updates will be made available at github.com/yizhezhang2000/SQA-SAM.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1809.06142",
        "title": "Open Subtitles Paraphrase Corpus for Six Languages",
        "abstract": "This paper accompanies the release of Opusparcus, a new paraphrase corpus for six European languages: German, English, Finnish, French, Russian, and Swedish. The corpus consists of paraphrases, that is, pairs of sentences in the same language that mean approximately the same thing. The paraphrases are extracted from the OpenSubtitles2016 corpus, which contains subtitles from movies and TV shows. The informal and colloquial genre that occurs in subtitles makes such data a very interesting language resource, for instance, from the perspective of computer assisted language learning. For each target language, the Opusparcus data have been\npartitioned into three types of data sets: training, development and test sets. The training sets are large, consisting of millions of sentence\npairs, and have been compiled automatically, with the help of probabilistic ranking functions. The development and test sets consist\nof sentence pairs that have been checked manually; each set contains approximately 1000 sentence pairs that have been verified to be\nacceptable paraphrases by two annotators.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2212.01145",
        "title": "Towards Diverse, Relevant and Coherent Open-Domain Dialogue Generation via Hybrid Latent Variables",
        "abstract": "Conditional variational models, using either continuous or\ndiscrete latent variables, are powerful for open-domain dialogue response generation. However, previous works show\nthat continuous latent variables tend to reduce the coherence\nof generated responses. In this paper, we also found that discrete latent variables have difficulty capturing more diverse\nexpressions. To tackle these problems, we combine the merits\nof both continuous and discrete latent variables and propose\na Hybrid Latent Variable (HLV) method. Specifically, HLV\nconstrains the global semantics of responses through discrete\nlatent variables and enriches responses with continuous latent\nvariables. Thus, we diversify the generated responses while\nmaintaining relevance and coherence. In addition, we propose Conditional Hybrid Variational Transformer (CHVT)\nto construct and to utilize HLV with transformers for dialogue generation. Through fine-grained symbolic-level semantic information and additive Gaussian mixing, we construct the distribution of continuous variables, prompting the\ngeneration of diverse expressions. Meanwhile, to maintain\nthe relevance and coherence, the discrete latent variable is\noptimized by self-separation training. Experimental results\non two dialogue generation datasets (DailyDialog and\nOpensubtitles) show that CHVT is superior to traditional transformer-based variational mechanism w.r.t. diversity, relevance and coherence metrics. Moreover, we also\ndemonstrate the benefit of applying HLV to fine-tuning two\npre-trained dialogue models (PLATO and BART-base).",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2206.04253",
        "title": "CLTS+: A New Chinese Long Text\nSummarization Dataset with Abstractive\nSummaries",
        "abstract": "The abstractive methods lack of creative ability is particularly a problem in automatic text summarization. The summaries generated by models are mostly extracted from the source articles. One of the\nmain causes for this problem is the lack of dataset with abstractiveness,\nespecially for Chinese. In order to solve this problem, we paraphrase the\nreference summaries in CLTS, the Chinese Long Text Summarization\ndataset, correct errors of factual inconsistencies, and propose the first\nChinese Long Text Summarization dataset with a high level of abstractiveness, CLTS+, which contains more than 180K article-summary pairs\nand is available online1\n. Additionally, we introduce an intrinsic metric\nbased on co-occurrence words to evaluate the dataset we constructed.\nWe analyze the extraction strategies used in CLTS+ summaries against\nother datasets to quantify the abstractiveness and difficulty of our new\ndata and train several baselines on CLTS+ to verify the utility of it for\nimproving the creative ability of models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2110.10874",
        "title": "CNewSum: A Large-scale Chinese News\nSummarization Dataset with Human-annotated\nAdequacy and Deducibility Level",
        "abstract": "Automatic text summarization aims to produce a brief but crucial summary\nfor the input documents. Both extractive and abstractive methods have witnessed\ngreat success in English datasets in recent years. However, there has been a minimal exploration of text summarization in Chinese, limited by the lack of largescale datasets. In this paper, we present a large-scale Chinese news summarization dataset CNewSum, which consists of 304,307 documents and human-written\nsummaries for the news feed. It has long documents with high-abstractive summaries, which can encourage document-level understanding and generation for\ncurrent summarization models. An additional distinguishing feature of CNewSum\nis that its test set contains adequacy and deducibility annotations for the summaries.\nThe adequacy level measures the degree of summary information covered by the\ndocument, and the deducibility indicates the reasoning ability the model needs to\ngenerate the summary. These annotations can help researchers analyze and target\ntheir model performance bottleneck. We examine recent methods on CNewSum\nand release our dataset1\nto provide a solid testbed for automatic Chinese summarization research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2209.07351",
        "title": "Rethinking Round-Trip Translation for Machine Translation Evaluation",
        "abstract": "Automatic evaluation on low-resource language translation suffers from a deficiency of\nparallel corpora. Round-trip translation could\nbe served as a clever and straightforward technique to alleviate the requirement of the parallel evaluation corpus. However, there was an\nobservation of obscure correlations between\nthe evaluation scores by forward and roundtrip translations in the era of statistical machine translation (SMT). In this paper, we report the surprising finding that round-trip translation can be used for automatic evaluation\nwithout the references. Firstly, our revisit on\nthe round-trip translation in SMT evaluation\nunveils that its long-standing misunderstanding is essentially caused by copying mechanism. After removing copying mechanism in\nSMT, round-trip translation scores can appropriately reflect the forward translation performance. Then, we demonstrate the rectification\nis overdue as round-trip translation could benefit multiple machine translation evaluation\ntasks. To be more specific, round-trip translation could be used i) to predict corresponding forward translation scores; ii) to improve\nthe performance of the recently advanced quality estimation model; and iii) to identify adversarial competitors in shared tasks via crosssystem verification.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1710.10639",
        "title": "JESC: Japanese-English Subtitle Corpus",
        "abstract": "In this paper we describe the Japanese-English Subtitle Corpus (JESC). JESC is a large Japanese-English parallel corpus covering the underrepresented domain of conversational dialogue. It consists of more than 3.2 million examples, making it the largest freely available dataset of its kind. The corpus was assembled by crawling and aligning subtitles found on the web. The assembly process incorporates a number of novel preprocessing elements to ensure high monolingual fluency and accurate bilingual alignments. We summarize its contents and evaluate its quality using human experts and baseline machine translation (MT) systems.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1903.00089",
        "title": "Massively Multilingual Neural Machine Translation",
        "abstract": "Multilingual neural machine translation\n(NMT) enables training a single model that\nsupports translation from multiple source languages into multiple target languages. In this\npaper, we push the limits of multilingual NMT\nin terms of the number of languages being\nused. We perform extensive experiments in\ntraining massively multilingual NMT models,\ntranslating up to 102 languages to and from\nEnglish within a single model. We explore\ndifferent setups for training such models and\nanalyze the trade-offs between translation\nquality and various modeling decisions. We\nreport results on the publicly available TED\ntalks multilingual corpus where we show\nthat massively multilingual many-to-many\nmodels are effective in low resource settings,\noutperforming the previous state-of-the-art\nwhile supporting up to 59 languages. Our\nexperiments on a large-scale dataset with\n102 languages to and from English and up to\none million examples per direction also show\npromising results, surpassing strong bilingual\nbaselines and encouraging future work on\nmassively multilingual NMT.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2312.02071",
        "title": "Evaluating the Claims of \u201cSAT Requires Exhaustive Search\u201d\u2217",
        "abstract": "In this paper, we take a closer look at the claims made by Xu and Zhou in their paper \u201cSAT\nRequires Exhaustive Search\u201d [XZ23], which claims to provide a lower bound on the complexity of\nthe so-called Model RB. Xu and Zhou conclude that their result implies a separation between P\nand NP, since the lower bound purportedly proves that the Strong Exponential Time Hypothesis\n(SETH) is true. In examining Xu and Zhou\u2019s arguments, we find a flaw in their main theorems.\nThe authors assume that an algorithm for Model RB must have a certain structure that can\nleverage downward self-reducibility, and argue that such an algorithm cannot run in polynomial\ntime. We argue that this structure is not guaranteed to exist and thus their paper neither proves\nSETH to be true nor proves P 6= NP.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1609.07843",
        "title": "Pointer Sentinel Mixture Models",
        "abstract": "Recent neural network sequence models with\nsoftmax classifiers have achieved their best language modeling performance only with very\nlarge hidden states and large vocabularies. Even\nthen they struggle to predict rare or unseen words\neven if the context makes the prediction unambiguous. We introduce the pointer sentinel\nmixture architecture for neural sequence models\nwhich has the ability to either reproduce a word\nfrom the recent context or produce a word from a\nstandard softmax classifier. Our pointer sentinelLSTM model achieves state of the art language\nmodeling performance on the Penn Treebank\n(70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to\nevaluate how well language models can exploit\nlonger contexts and deal with more realistic vocabularies and larger corpora we also introduce\nthe freely available WikiText corpus.1",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1909.00156",
        "title": "NCLS: Neural Cross-Lingual Summarization",
        "abstract": "Cross-lingual summarization (CLS) is the task\nto produce a summary in one particular language for a source document in a different language. Existing methods simply divide this task into two steps: summarization\nand translation, leading to the problem of error propagation. To handle that, we present\nan end-to-end CLS framework, which we refer to as Neural Cross-Lingual Summarization\n(NCLS), for the first time. Moreover, we propose to further improve NCLS by incorporating two related tasks, monolingual summarization and machine translation, into the training process of CLS under multi-task learning. Due to the lack of supervised CLS data,\nwe propose a round-trip translation strategy\nto acquire two high-quality large-scale CLS\ndatasets based on existing monolingual summarization datasets. Experimental results have\nshown that our NCLS achieves remarkable improvement over traditional pipeline methods\non both English-to-Chinese and Chinese-toEnglish CLS human-corrected test sets. In addition, NCLS with multi-task learning can further significantly improve the quality of generated summaries. We make our dataset and\ncode publicly available here: http://www.\nnlpr.ia.ac.cn/cip/dataset.htm.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.15696",
        "title": "EcomGPT-CT: Continual Pre-training of E-commerce Large Language Models with Semi-structured Data",
        "abstract": "Large Language Models (LLMs) pre-trained\non massive corpora have exhibited remarkable\nperformance on various NLP tasks. However,\napplying these models to specific domains still\nposes significant challenges, such as lack of\ndomain knowledge, limited capacity to leverage domain knowledge and inadequate adaptation to domain-specific data formats. Considering the exorbitant cost of training LLMs\nfrom scratch and the scarcity of annotated data\nwithin particular domains, in this work, we focus on domain-specific continual pre-training\nof LLMs using E-commerce domain as an exemplar. Specifically, we explore the impact\nof continual pre-training on LLMs employing unlabeled general and E-commercial corpora. Furthermore, we design a mixing strategy\namong different data sources to better leverage E-commercial semi-structured data. We\nconstruct multiple tasks to assess LLMs\u2019 fewshot In-context Learning ability and their zeroshot performance after instruction tuning in\nE-commerce domain. Experimental results\ndemonstrate the effectiveness of continual pretraining of E-commerce LLMs and the efficacy\nof our devised data mixing strategy",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1506.05865",
        "title": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset",
        "abstract": "Automatic text summarization is widely\nregarded as the highly difficult problem,\npartially because of the lack of large\ntext summarization data set. Due to the\ngreat challenge of constructing the large\nscale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging\nwebsite Sina Weibo, which is released to\nthe public1\n. This corpus consists of over\n2 million real Chinese short texts with\nshort summaries given by the author of\neach text. We also manually tagged the\nrelevance of 10,666 short summaries with\ntheir corresponding short texts. Based on\nthe corpus, we introduce recurrent neural\nnetwork for the summary generation and\nachieve promising results, which not only\nshows the usefulness of the proposed corpus for short text summarization research,\nbut also provides a baseline for further research on this topic.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1809.06106",
        "title": "Merge Non-Dominated Sorting Algorithm for Many-Objective Optimization",
        "abstract": "Many Pareto-based multi-objective evolutionary algorithms require to rank the solutions of the population in each iteration according to the dominance principle, what can become a costly operation particularly in the case of dealing with manyobjective optimization problems. In this paper, we present a new efficient algorithm for computing the non-dominated sorting procedure, called Merge Non-Dominated Sorting (MNDS), which has a best computational complexity of \u0398(NlogN) and a worst computational complexity of \u0398(MN2 ). Our approach is based on the computation of the dominance set of each solution by taking advantage of the characteristics of the merge sort algorithm. We compare the MNDS against four well-known techniques that can be considered as the state-of-the-art. The results indicate that the MNDS algorithm outperforms the other techniques in terms of number of comparisons as well as the total running time.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1705.04304",
        "title": "A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION",
        "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intraattention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \u201cexposure bias\u201d \u2013 they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2105.08209",
        "title": "BOOKSUM: A Collection of Datasets for Long-form Narrative Summarization",
        "abstract": "The majority of existing text summarization\ndatasets include short-form source documents\nthat lack long-range causal and temporal dependencies, and often contain strong layout\nand stylistic biases. While relevant, such\ndatasets will offer limited challenges for future\ntext summarization systems. We address these\nissues by introducing BOOKSUM, a collection of datasets for long-form narrative summarization. Our dataset covers documents from\nthe literature domain, such as novels, plays\nand stories, and includes highly abstractive,\nhuman written summaries on three levels of\ngranularity of increasing difficulty: paragraph-\n, chapter-, and book-level. The domain and\nstructure of our dataset poses a unique set of\nchallenges for summarization systems, which\ninclude: processing very long documents, nontrivial causal and temporal dependencies, and\nrich discourse structures. To facilitate future\nwork, we trained and evaluated multiple extractive and abstractive summarization models\nas baselines for our dataset.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.04473",
        "title": "Leveraging Key Information Modeling to Improve Less-Data Constrained News Headline Generation via Duality Fine-Tuning",
        "abstract": "Recent language generative models are mostly\ntrained on large-scale datasets, while in some\nreal scenarios, the training datasets are often\nexpensive to obtain and would be small-scale.\nIn this paper we investigate the challenging\ntask of less-data constrained generation, especially when the generated news headlines are\nshort yet expected by readers to keep readable and informative simultaneously. We highlight the key information modeling task and\npropose a novel duality fine-tuning method\nby formally defining the probabilistic duality\nconstraints between key information prediction and headline generation tasks. The proposed method can capture more information\nfrom limited data, build connections between\nseparate tasks, and is suitable for less-data constrained generation tasks. Furthermore, the\nmethod can leverage various pre-trained generative regimes, e.g., autoregressive and encoderdecoder models. We conduct extensive experiments to demonstrate that our method is effective and efficient to achieve improved performance in terms of language modeling metric\nand informativeness correctness metric on two\npublic datasets.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2403.15112",
        "title": "Text clustering with LLM embeddings",
        "abstract": "Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. However, the effectiveness of text clustering heavily relies on the choice of textual embeddings and clustering algorithms. We argue that recent advances in large language models (LLMs) can potentially improve this task. In this research, we investigated how different textual embeddings\u2014particularly those used in LLMs\u2014and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and model size adjustment. Findings reveal that LLM embeddings excel at capturing subtleties in structured language, while BERT leads the lightweight options in performance. In addition, we observe that increasing model dimensionality and employing summarization techniques do not consistently lead to improvements in clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a complex balance between the need for refined text representation and computational feasibility in text clustering applications. This study extends traditional text clustering frameworks by incorporating embeddings from LLMs, providing a path for improved methodologies, while informing new avenues for future research in various types of textual analysis.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2310.16568",
        "title": "1-PAGER: One Pass Answer Generation and Evidence Retrieval",
        "abstract": "We present 1-PAGER the first system that answers a question and retrieves evidence using\na single Transformer-based model and decoding process. 1-PAGER incrementally partitions\nthe retrieval corpus using constrained decoding to select a document and answer string,\nand we show that this is competitive with comparable retrieve-and-read alternatives according to both retrieval and answer accuracy metrics. 1-PAGER also outperforms the equivalent \u2018closed-book\u2019 question answering model,\nby grounding predictions in an evidence corpus. While 1-PAGER is not yet on-par with\nmore expensive systems that read many more\ndocuments before generating an answer, we argue that it provides an important step toward\nattributed generation by folding retrieval into\nthe sequence-to-sequence paradigm that is currently dominant in NLP. We also show that the\nsearch paths used to partition the corpus are\neasy to read and understand, paving a way forward for interpretable neural retrieval.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1711.05073",
        "title": "DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications",
        "abstract": "This paper introduces DuReader, a new\nlarge-scale, open-domain Chinese machine reading comprehension (MRC)\ndataset, designed to address real-world\nMRC. DuReader has three advantages\nover previous MRC datasets: (1) data\nsources: questions and documents are\nbased on Baidu Search and Baidu Zhidao1\n; answers are manually generated.\n(2) question types: it provides rich\nannotations for more question types,\nespecially yes-no and opinion questions,\nthat leaves more opportunity for the\nresearch community. (3) scale: it contains\n200K questions, 420K answers and 1M\ndocuments; it is the largest Chinese\nMRC dataset so far. Experiments show\nthat human performance is well above\ncurrent state-of-the-art baseline systems,\nleaving plenty of room for the community\nto make improvements. To help the\ncommunity make these improvements,\nboth DuReader2\nand baseline systems3\nhave been posted online. We also organize\na shared competition to encourage the\nexploration of more models. Since the\nrelease of the task, there are significant\nimprovements over the baselines.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.06984",
        "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models",
        "abstract": "Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching\nfails completely when a plausible candidate answer does not appear in the list of gold answers,\nwhich is increasingly the case as we shift from\nextractive to generative models. The recent success of large language models (LLMs) for QA\naggravates lexical matching failures since candidate answers become longer, thereby making\nmatching with the gold answers even more challenging. Without accurate evaluation, the true\nprogress in open-domain QA remains unknown.\nIn this paper, we conduct a thorough analysis\nof various open-domain QA models, including\nLLMs, by manually evaluating their answers\non a subset of NQ-OPEN, a popular benchmark.\nOur assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT\n(zero-shot) LLM increases by nearly +60%,\nmaking it on par with existing top models,\nand the InstructGPT (few-shot) model actually\nachieves a new state-of-the-art on NQ-OPEN.\nWe also find that more than 50% of lexical\nmatching failures are attributed to semantically\nequivalent answers. We further demonstrate\nthat regex matching ranks QA models consistent with human judgments, although still suffering from unnecessary strictness. Finally, we\ndemonstrate that automated evaluation models\nare a reasonable surrogate for lexical matching\nin some circumstances, but not for long-form\nanswers generated by LLMs. The automated\nmodels struggle in detecting hallucinations in\nLLM answers and are thus unable to evaluate\nLLMs. At this time, there appears to be no\nsubstitute for human evaluation.1",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1806.00920",
        "title": "DRCD: a Chinese Machine Reading Comprehension Dataset",
        "abstract": "In this paper, we introduce DRCD (Delta Reading Comprehension Dataset), an open domain traditional Chinese machine reading comprehension (MRC) dataset. This dataset aimed to be a standard Chinese machine reading comprehension dataset, which can be a source dataset in transfer learning. The dataset contains 10,014 paragraphs from 2,108 Wikipedia articles and 30,000+ questions generated by annotators. We build a baseline model that achieves an F1 score of 89.59%. F1 score of Human performance is 93.30%. The dataset is available at https://github.com/DRCKnowledgeTeam/ DRCD.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2102.01226",
        "title": "Self-Teaching Machines to Read and Comprehend with Large-Scale Multi-Subject Question-Answering Data",
        "abstract": "In spite of much recent research in the\narea, it is still unclear whether subject-area\nquestion-answering data is useful for machine reading comprehension (MRC) tasks.\nIn this paper, we investigate this question. We collect a large-scale multi-subject\nmultiple-choice question-answering dataset,\nExamQA, and use incomplete and noisy\nsnippets returned by a web search engine\nas the relevant context for each questionanswering instance to convert it into a\nweakly-labeled MRC instance. We then\npropose a self-teaching paradigm to better\nuse the generated weakly-labeled MRC instances to improve a target MRC task. Experimental results show that we can obtain +5.1% in accuracy on a multiplechoice MRC dataset, C3\n, and +3.8% in exact match on an extractive MRC dataset,\nCMRC 2018 over state-of-the-art MRC\nbaselines, demonstrating the effectiveness\nof our framework and the usefulness of\nlarge-scale subject-area question-answering\ndata for different types of machine reading\ncomprehension tasks.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1804.07927",
        "title": "DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension",
        "abstract": "We propose DuoRC, a novel dataset for\nReading Comprehension (RC) that motivates several new challenges for neural\napproaches in language understanding beyond those offered by existing RC datasets.\nDuoRC contains 186,089 unique questionanswer pairs created from a collection of\n7680 pairs of movie plots where each pair\nin the collection reflects two versions of the\nsame movie - one from Wikipedia and the\nother from IMDb - written by two different\nauthors. We asked crowdsourced workers\nto create questions from one version of the\nplot and a different set of workers to extract\nor synthesize answers from the other version. This unique characteristic of DuoRC\nwhere questions and answers are created\nfrom different versions of a document narrating the same underlying story, ensures\nby design, that there is very little lexical\noverlap between the questions created from\none version and the segments containing\nthe answer in the other version. Further,\nsince the two versions have different levels\nof plot detail, narration style, vocabulary,\netc., answering questions from the second\nversion requires deeper language understanding and incorporating external background knowledge. Additionally, the narrative style of passages arising from movie\nplots (as opposed to typical descriptive passages in existing datasets) exhibits the need\nto perform complex reasoning over events\nacross multiple sentences. Indeed, we observe that state-of-the-art neural RC models\nwhich have achieved near human performance on the SQuAD dataset (Rajpurkar\net al., 2016b), even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very\npoor performance (F1 score of 37.42% on\nDuoRC v/s 86% on SQuAD dataset). This\nopens up several interesting research avenues wherein DuoRC could complement\nother RC datasets to explore novel neural\napproaches for studying language understanding.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1705.03551",
        "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
        "abstract": "We present TriviaQA, a challenging reading comprehension dataset contain- ing over 650K question-answer-evidence triples. TriviaQA includes 95K question- answer pairs authored by trivia enthusi- asts and independently gathered evidence documents, six per question on average, that provide high quality distant super- vision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sen- tences, and (3) requires more cross sen- tence reasoning to find answers. We also present two baseline algorithms: a feature- based classifier and a state-of-the-art neu- ral network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that Trivi- aQA is a challenging testbed that is worth significant future study.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2101.00408v2",
        "title": "End-to-End Training of Neural Retrievers for Open-Domain Question Answering",
        "abstract": "Recent work on training neural retrievers for open-domain question answering (OpenQA) has employed both supervised and unsuper- vised approaches. However, it remains un- clear how unsupervised and supervised meth- ods can be used most effectively for neural retrievers. In this work, we systematically study retriever pre-training. We first propose an approach of unsupervised pre-training with the Inverse Cloze Task and masked salient spans, followed by supervised finetuning using question-context pairs. This approach leads to absolute gains of 2+ points over the previ- ous best result in the top-20 retrieval accuracy on Natural Questions and TriviaQA datasets. We next explore two approaches for end-to- end training of the reader and retriever compo- nents in OpenQA models, which differ in the manner the reader ingests the retrieved docu- ments. Our experiments demonstrate the ef- fectiveness of these approaches as we obtain state-of-the-art results. On the Natural Ques- tions dataset, we obtain a top-20 retrieval ac- curacy of 84%, an improvement of 5 points over the recent DPR model. We also showcase good results on answer extraction, outperform- ing recent models such as REALM and RAG by 3+ points. Our code is available at: https: //github.com/NVIDIA/Megatron- LM.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2304.01373v2",
        "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
        "abstract": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To an- swer these questions, we introduce Pythia, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public ac- cess to 154 checkpoints for each one of the 16 models, alongside tools to download and recon- struct their exact training dataloaders for further study. We intend Pythia to facilitate research in many areas, and we present several case stud- ies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynam- ics. Trained models, analysis code, training code, and training data can be found at https: //github.com/EleutherAI/pythia.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1906.00300v3",
        "title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering",
        "abstract": "Recent work on open domain question answer- ing (QA) assumes strong supervision of the supporting evidence and/or assumes a black- box information retrieval (IR) system to re- trieve evidence candidates. We argue that both are suboptimal, since gold evidence is not al- ways available, and QA is fundamentally dif- ferent from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evi- dence retrieval from all of Wikipedia is treated as a latent variable. Since this is impracti- cal to learn from scratch, we pre-train the re- triever with an Inverse Cloze Task. We evalu- ate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2402.18041v1",
        "title": "Datasets for Large Language Models: A Comprehensive Survey",
        "abstract": "This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examina- tion of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this sur- vey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Lan- guage Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Addition- ally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains. Information from 20 dimensions is incorporated into the dataset statistics. The total data size surveyed surpasses 774.5 TB for pre- training corpora and 700M instances for other datasets. We aim to present the entire landscape of LLM text datasets, serving as a comprehensive reference for researchers in this field and contributing to future studies. Related resources are available at: https://github.com/lmmlzn/Awesome-LLMs-Datasets.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2112.09924v2",
        "title": "The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus",
        "abstract": "In order to address increasing demands of real-world applications, the research for knowledge-intensive NLP (KI-NLP) should advance by capturing the challenges of a truly open-domain environment: web-scale knowledge, lack of structure, inconsistent quality and noise. To this end, we propose a new setup for evaluating existing knowledge intensive tasks in which we generalize the background corpus to a universal web snapshot. We investigate a slate of NLP tasks which rely on knowledge - either factual or common sense, and ask systems to use a subset of CCNet - the Sphere corpus - as a knowledge source. In contrast to Wikipedia, otherwise a common background corpus in KI-NLP, Sphere is orders of magnitude larger and better reflects the full diversity of knowledge on the web. Despite potential gaps in coverage, challenges of scale, lack of structure and lower quality, we find that retrieval from Sphere enables a state of the art system to match and even outperform Wikipedia-based models on several tasks. We also observe that while a dense index can outperform a sparse BM25 baseline on Wikipedia, on Sphere this is not yet possible. To facilitate further research and minimise the community's reliance on proprietary, black-box search engines, we share our indices, evaluation metrics and infrastructure.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1809.09600v1",
        "title": "HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
        "abstract": "Existing question answering (QA) datasets fail to train QA systems to perform complex rea- soning and provide explanations for answers. We introduce HOTPOTQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions re- quire finding and reasoning over multiple sup- porting documents to answer; (2) the ques- tions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level sup- porting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems\u2019 ability to extract relevant facts and perform necessary comparison. We show that HOTPOTQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make ex- plainable predictions.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2009.02252v4",
        "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks",
        "abstract": "Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, exter- nal knowledge sources. While some models do well on individual tasks, developing gen- eral models is difficult as each task might re- quire computationally expensive indexing of custom knowledge sources, in addition to ded- icated infrastructure. To catalyze research on models that condition on specific informa- tion in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reduc- ing engineering turnaround through the re- use of components, as well as accelerating research into task-agnostic memory architec- tures. We test both task-specific and gen- eral baselines, evaluating downstream perfor- mance in addition to the ability of the mod-\nin open-domain question answering (Chen et al., 2017) models need to find answers within a large corpus of text. Fact checking a claim (Thorne et al., 2018a) requires models to find evidence, often on the web. In knowledgeable open dialogue (Dinan et al., 2019), models need access to knowledge from large corpora to sustain informed conversa- tions.\nIn general, solving knowledge-intensive tasks requires\u2013even for humans\u2013access to a large body of information. Like in Information Retrieval (IR) this involves satisfying an information need lever- aging large collections of text (Manning et al., 2008). However, while IR focuses of finding rel- evant material (usually documents), the tasks we consider focus on more fine-grained behavior, such as producing specific answers to queries. For such knowledge-intensive tasks, general infrastructure and architectures across tasks have yet to emerge, and fundamental research questions remain open. For example, while it was long assumed that non- parametric and explicit memory accessed through retrieval is strictly required for competitive re- sults (Chen et al., 2017), recent large pre-trained sequence-to-sequence models such as T5 (Raffel et al., 2019a) and BART (Lewis et al., 2019) store all knowledge in their parameters while performing remarkably well (Petroni et al., 2019). Likewise, while the classical approach of information extrac- tion for populating a Knowledge Base (KB, Riedel et al., 2013; Surdeanu and Ji, 2014) seems out- of-fashion, recent results show that they remain contenders (Fan et al., 2019a; Xiong et al., 2019).\nWhile there are numerous datasets for knowledge-intensive tasks (e.g. Thorne et al., 2018a; Dinan et al., 2019; Kwiatkowski et al., 2019, to name just a few), it is difficult to answer the above questions generally across them. Each dataset comes in a different format, is pre-processed with different assumptions, and requires different loaders, evaluations, and analysis\nels to provide provenance. we find that\na shared dense vector index coupled with\na seq2seq model is a strong baseline, out-\nperforming more tailor-made approaches for fact checking, open-domain question answer- ing and dialogue, and yielding competitive re- sults on entity linking and slot filling, by gen- erating disambiguated text. KILT data and code are available at https://github.com/ facebookresearch/KILT.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2203.10232v4",
        "title": "DuReaderretrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine",
        "abstract": "In this paper, we present DuReaderretrieval, a large-scale Chinese dataset for passage re- trieval. DuReaderretrieval contains more than 90K queries and over 8M unique passages from a commercial search engine. To alle- viate the shortcomings of other datasets and ensure the quality of our benchmark, we (1) reduce the false negatives in development and test sets by manually annotating results pooled from multiple retrievers, and (2) re- move the training queries that are semanti- cally similar to the development and testing queries. Additionally, we provide two out- of-domain testing sets for cross-domain eval- uation, as well as a set of human translated queries for for cross-lingual retrieval eval- uation. The experiments demonstrate that DuReaderretrieval is challenging and a num- ber of problems remain unsolved, such as the salient phrase mismatch and the syntactic mis- match between queries and paragraphs. These experiments also show that dense retrievers do not generalize well across domains, and cross-lingual retrieval is essentially challeng- ing. DuReaderretrieval is publicly available at https://github.com/baidu/DuReader/ tree/master/DuReader-Retrieval.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1711.05073v4",
        "title": "DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications",
        "abstract": "This paper introduces DuReader, a new large-scale, open-domain Chinese ma- chine reading comprehension (MRC) dataset, designed to address real-world MRC. DuReader has three advantages over previous MRC datasets: (1) data sources: questions and documents are based on Baidu Search and Baidu Zhi- dao1 ; answers are manually generated. (2) question types: it provides rich annotations for more question types, especially yes-no and opinion questions, that leaves more opportunity for the research community. (3) scale: it contains 200K questions, 420K answers and 1M documents; it is the largest Chinese MRC dataset so far. Experiments show that human performance is well above current state-of-the-art baseline systems, leaving plenty of room for the community to make improvements. To help the community make these improvements, both DuReader2 and baseline systems3 have been posted online. We also organize a shared competition to encourage the exploration of more models. Since the release of the task, there are significant improvements over the baselines.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.19593",
        "title": "SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs",
        "abstract": "Synthetic data generation has gained significant attention recently for its utility in training large vision and language models. However, the application of synthetic data to the training of multimodal context-augmented generation systems has been relatively unexplored. This gap in existing work is important because existing vision and language models (VLMs) are not trained specifically for context-augmented generation. Resources for adapting such models are therefore crucial for enabling their use in retrieval-augmented generation (RAG) settings, where a retriever is used to gather relevant information that is then subsequently provided to a generative model via context augmentation. To address this challenging problem, we generate SK-VQA: a large synthetic multimodal dataset containing over 2 million question-answer pairs which require external knowledge to determine the final answer. Our dataset is both larger and significantly more diverse than existing resources of its kind, possessing over 11x more unique questions and containing images from a greater variety of sources than previously-proposed datasets. Through extensive experiments, we demonstrate that our synthetic dataset can not only serve as a challenging benchmark, but is also highly effective for adapting existing generative multimodal models for context-augmented generation.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.20094",
        "title": "Scaling Synthetic Data Creation with 1,000,000,000 Personas",
        "abstract": "We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce Persona Hub -- a collection of 1 billion diverse personas automatically curated from web data. These 1 billion personas (~13% of the world's total population), acting as distributed carriers of world knowledge, can tap into almost every perspective encapsulated within the LLM, thereby facilitating the creation of diverse synthetic data at scale for various scenarios. By showcasing Persona Hub's use cases in synthesizing high-quality mathematical and logical reasoning problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs and tools (functions) at scale, we demonstrate persona-driven data synthesis is versatile, scalable, flexible, and easy to use, potentially driving a paradigm shift in synthetic data creation and applications in practice, which may have a profound impact on LLM research and development.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.00993",
        "title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents",
        "abstract": "With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction. However, there is a scarcity of benchmarks available for LLM-based mobile agents. Benchmarking these agents generally faces three main challenges: (1) The inefficiency of UI-only operations imposes limitations to task evaluation. (2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents. (3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents. First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion. Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs. To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios. Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.00981",
        "title": "VisEval: A Benchmark for Data Visualization in the Era of Large Language Models",
        "abstract": "Translating natural language to visualization (NL2VIS) has shown great promise for visual data analysis, but it remains a challenging task that requires multiple low-level implementations, such as natural language processing and visualization design. Recent advancements in pre-trained large language models (LLMs) are opening new avenues for generating visualizations from natural language. However, the lack of a comprehensive and reliable benchmark hinders our understanding of LLMs' capabilities in visualization generation. In this paper, we address this gap by proposing a new NL2VIS benchmark called VisEval. Firstly, we introduce a high-quality and large-scale dataset. This dataset includes 2,524 representative queries covering 146 databases, paired with accurately labeled ground truths. Secondly, we advocate for a comprehensive automated evaluation methodology covering multiple dimensions, including validity, legality, and readability. By systematically scanning for potential issues with a number of heterogeneous checkers, VisEval provides reliable and trustworthy evaluation outcomes. We run VisEval on a series of state-of-the-art LLMs. Our evaluation reveals prevalent challenges and delivers essential insights for future advancements.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.00379",
        "title": "GraphArena: Benchmarking Large Language Models on Graph Computational Problems",
        "abstract": "The \"arms race\" of Large Language Models (LLMs) demands novel, challenging, and diverse benchmarks to faithfully examine their progresses. We introduce GraphArena, a benchmarking tool designed to evaluate LLMs on graph computational problems using million-scale real-world graphs from diverse scenarios such as knowledge graphs, social networks, and molecular structures. GraphArena offers a suite of 10 computational tasks, encompassing four polynomial-time (e.g., Shortest Distance) and six NP-complete challenges (e.g., Travelling Salesman Problem). It features a rigorous evaluation framework that classifies LLM outputs as correct, suboptimal (feasible but not optimal), or hallucinatory (properly formatted but infeasible). Evaluation of 10 leading LLMs, including GPT-4o and LLaMA3-70B-Instruct, reveals that even top-performing models struggle with larger, more complex graph problems and exhibit hallucination issues. Despite the application of strategies such as chain-of-thought prompting, these issues remain unresolved. GraphArena contributes a valuable supplement to the existing LLM benchmarks and is open-sourced at this https URL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.00377",
        "title": "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention",
        "abstract": "Prompt-based \"diversity interventions\" are commonly adopted to improve the diversity of Text-to-Image (T2I) models depicting individuals with various racial or gender traits. However, will this strategy result in nonfactual demographic distribution, especially when generating real historical figures? In this work, we propose DemOgraphic FActualIty Representation (DoFaiR), a benchmark to systematically quantify the trade-off between using diversity interventions and preserving demographic factuality in T2I models. DoFaiR consists of 756 meticulously fact-checked test instances to reveal the factuality tax of various diversity prompts through an automated evidence-supported evaluation pipeline. Experiments on DoFaiR unveil that diversity-oriented instructions increase the number of different gender and racial groups in DALLE-3's generations at the cost of historically inaccurate demographic distributions. To resolve this issue, we propose Fact-Augmented Intervention (FAI), which instructs a Large Language Model (LLM) to reflect on verbalized or retrieved factual information about gender and racial compositions of generation subjects in history, and incorporate it into the generation context of T2I models. By orienting model generations using the reflected historical truths, FAI significantly improves the demographic factuality under diversity interventions while preserving diversity.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.02883",
        "title": "CoIR: A Comprehensive Benchmark for Code Information Retrieval Models",
        "abstract": "Despite the substantial success of Information Retrieval (IR) in various NLP tasks, most IR systems predominantly handle queries and corpora in natural language, neglecting the domain of code retrieval. Code retrieval is critically important yet remains under-explored, with existing methods and benchmarks inadequately representing the diversity of code in various domains and tasks. Addressing this gap, we present COIR (Code Information Retrieval Benchmark), a robust and comprehensive benchmark specifically designed to assess code retrieval capabilities. COIR comprises ten meticulously curated code datasets, spanning eight distinctive retrieval tasks across seven diverse domains. We first discuss the construction of COIR and its diverse dataset composition. Further, we evaluate nine widely used retrieval models using COIR, uncovering significant difficulties in performing code retrieval tasks even with state-of-the-art systems. To facilitate easy adoption and integration within existing research workflows, COIR has been developed as a user-friendly Python framework, readily installable via pip. It shares same data schema as other popular benchmarks like MTEB and BEIR, enabling seamless cross-benchmark evaluations. Through COIR, we aim to invigorate research in the code retrieval domain, providing a versatile benchmarking tool that encourages further development and exploration of code retrieval systems3.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.14780",
        "title": "ACR: A Benchmark for Automatic Cohort Retrieval",
        "abstract": "Identifying patient cohorts is fundamental to numerous healthcare tasks, including clinical trial recruitment and retrospective studies. Current cohort retrieval methods in healthcare organizations rely on automated queries of structured data combined with manual curation, which are time-consuming, labor-intensive, and often yield low-quality results. Recent advancements in large language models (LLMs) and information retrieval (IR) offer promising avenues to revolutionize these systems. Major challenges include managing extensive eligibility criteria and handling the longitudinal nature of unstructured Electronic Medical Records (EMRs) while ensuring that the solution remains cost-effective for real-world application. This paper introduces a new task, Automatic Cohort Retrieval (ACR), and evaluates the performance of LLMs and commercial, domain-specific neuro-symbolic approaches. We provide a benchmark task, a query dataset, an EMR dataset, and an evaluation framework. Our findings underscore the necessity for efficient, high-quality ACR systems capable of longitudinal reasoning across extensive patient databases.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.03203",
        "title": "TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts",
        "abstract": "Proving mathematical theorems using computer-verifiable formal languages like Lean significantly impacts mathematical reasoning. One approach to formal theorem proving involves generating complete proofs using Large Language Models (LLMs) based on Natural Language (NL) proofs. Similar methods have shown promising results in code generation. However, most modern LLMs exhibit suboptimal performance due to the scarcity of aligned NL and Formal Language (FL) theorem-proving data. This scarcity results in a paucity of methodologies for training LLMs and techniques to fully utilize their capabilities in composing formal proofs. To address the challenges, this paper proposes **TheoremLlama**, an end-to-end framework to train a general-purpose LLM to become a Lean4 expert. This framework encompasses NL-FL aligned dataset generation methods, training approaches for the LLM formal theorem prover, and techniques for LLM Lean4 proof writing. Using the dataset generation method, we provide *Open Bootstrapped Theorems* (OBT), an NL-FL aligned and bootstrapped dataset. A key innovation in this framework is the NL-FL bootstrapping method, where NL proofs are integrated into Lean4 code for training datasets, leveraging the NL reasoning ability of LLMs for formal reasoning. The **TheoremLlama** framework achieves cumulative accuracies of 36.48% and 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline of 22.95% and 25.41%. We have also open-sourced our model checkpoints and generated dataset, and will soon make all the code publicly available.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.03103",
        "title": "Cactus: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory",
        "abstract": "Recently, the demand for psychological counseling has significantly increased as more individuals express concerns about their mental health. This surge has accelerated efforts to improve the accessibility of counseling by using large language models (LLMs) as counselors. To ensure client privacy, training open-source LLMs faces a key challenge: the absence of realistic counseling datasets. To address this, we introduce Cactus, a multi-turn dialogue dataset that emulates real-life interactions using the goal-oriented and structured approach of Cognitive Behavioral Therapy (CBT). We create a diverse and realistic dataset by designing clients with varied, specific personas, and having counselors systematically apply CBT techniques in their interactions. To assess the quality of our data, we benchmark against established psychological criteria used to evaluate real counseling sessions, ensuring alignment with expert evaluations. Experimental results demonstrate that Camel, a model trained with Cactus, outperforms other models in counseling skills, highlighting its effectiveness and potential as a counseling agent. We make our data, model, and code publicly available.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.03040",
        "title": "Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction Tuning for Large Language Model",
        "abstract": "Instruction tuning as an effective technique aligns the outputs of large language models (LLMs) with human preference. But how to generate the seasonal multi-turn dialogues from raw documents for instruction tuning still requires further exploration. In this paper, we present a novel framework named R2S that leverages the CoD-Chain of Dialogue logic to guide large language models (LLMs) in generating knowledge-intensive multi-turn dialogues for instruction tuning. By integrating raw documents from both open-source datasets and domain-specific web-crawled documents into a benchmark K-BENCH, we cover diverse areas such as Wikipedia (English), Science (Chinese), and Artifacts (Chinese). Our approach first decides the logic flow of the current dialogue and then prompts LLMs to produce key phrases for sourcing relevant response content. This methodology enables the creation of the G I NSTRUCT instruction dataset, retaining raw document knowledge within dialoguestyle interactions. Utilizing this dataset, we fine-tune GLLM, a model designed to transform raw documents into structured multi-turn dialogues, thereby injecting comprehensive domain knowledge into the SFT model for enhanced instruction tuning. This work signifies a stride towards refining the adaptability and effectiveness of LLMs in processing and generating more accurate, contextually nuanced responses across various fields.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.02680",
        "title": "KGym: A Platform and Dataset to Benchmark Large Language Models on Linux Kernel Crash Resolution",
        "abstract": "Large Language Models (LLMs) are consistently improving at increasingly realistic software engineering (SE) tasks. In real-world software stacks, significant SE effort is spent developing foundational system software like the Linux kernel. Unlike application-level software, a systems codebase like Linux is multilingual (low-level C/Assembly/Bash/Rust); gigantic (>20 million lines); critical (impacting billions of devices worldwide), and highly concurrent (involving complex multi-threading). To evaluate if ML models are useful while developing such large-scale systems-level software, we introduce kGym (a platform) and kBench (a dataset). The kGym platform provides a SE environment for large-scale experiments on the Linux kernel, including compiling and running kernels in parallel across several virtual machines, detecting operations and crashes, inspecting logs, and querying and patching the code base. We use kGym to facilitate evaluation on kBench, a crash resolution benchmark drawn from real-world Linux kernel bugs. An example bug in kBench contains crashing stack traces, a bug-reproducer file, a developer-written fix, and other associated data. To understand current performance, we conduct baseline experiments by prompting LLMs to resolve Linux kernel crashes. Our initial evaluations reveal that the best performing LLM achieves 0.72% and 5.38% in the unassisted and assisted (i.e., buggy files disclosed to the model) settings, respectively. These results highlight the need for further research to enhance model performance in SE tasks. Improving performance on kBench requires models to master new learning skills, including understanding the cause of crashes and repairing faults, writing memory-safe and hardware-aware code, and understanding concurrency. As a result, this work opens up multiple avenues of research at the intersection of machine learning and systems software.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.00455",
        "title": "Polarization and Morality: Lexical Analysis of Abortion Discourse on Reddit",
        "abstract": "This study investigates whether division on political topics is mapped with the distinctive patterns of language use. We collect a total 145,832 Reddit comments on the abortion debate and explore the languages of subreddit communities r/prolife and r/prochoice. With consideration of the Moral Foundations Theory, we examine lexical patterns in three ways. First, we compute proportional frequencies of lexical items from the Moral Foundations Dictionary in order to make inferences about each group's moral considerations when forming arguments for and against abortion. We then create n-gram models to reveal frequent collocations from each stance group and better understand how commonly used words are patterned in their linguistic context and in relation to morality values. Finally, we use Latent Dirichlet Allocation to identify underlying topical structures in the corpus data. Results show that the use of morality words is mapped with the stances on abortion.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01212",
        "title": "EconNLI: Evaluating Large Language Models on Economics Reasoning",
        "abstract": "Large Language Models (LLMs) are widely used for writing economic analysis reports or providing financial advice, but their ability to understand economic knowledge and reason about potential results of specific economic events lacks systematic evaluation. To address this gap, we propose a new dataset, natural language inference on economic events (EconNLI), to evaluate LLMs' knowledge and reasoning abilities in the economic domain. We evaluate LLMs on (1) their ability to correctly classify whether a premise event will cause a hypothesis event and (2) their ability to generate reasonable events resulting from a given premise. Our experiments reveal that LLMs are not sophisticated in economic reasoning and may generate wrong or hallucinated answers. Our study raises awareness of the limitations of using LLMs for critical decision-making involving economic reasoning and analysis. The dataset and codes are available at this https URL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01393",
        "title": "POLygraph: Polish Fake News Dataset",
        "abstract": "This paper presents the POLygraph dataset, a unique resource for fake news detection in Polish. The dataset, created by an interdisciplinary team, is composed of two parts: the \"fake-or-not\" dataset with 11,360 pairs of news articles (identified by their URLs) and corresponding labels, and the \"fake-they-say\" dataset with 5,082 news articles (identified by their URLs) and tweets commenting on them. Unlike existing datasets, POLygraph encompasses a variety of approaches from source literature, providing a comprehensive resource for fake news detection. The data was collected through manual annotation by expert and non-expert annotators. The project also developed a software tool that uses advanced machine learning techniques to analyze the data and determine content authenticity. The tool and dataset are expected to benefit various entities, from public sector institutions to publishers and fact-checking organizations. Further dataset exploration will foster fake news detection and potentially stimulate the implementation of similar models in other languages. The paper focuses on the creation and composition of the dataset, so it does not include a detailed evaluation of the software tool for content authenticity analysis, which is planned at a later stage of the project.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01725",
        "title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
        "abstract": "Can the rapid advances in code generation, function calling, and data analysis using large language models (LLMs) help automate the search and verification of hypotheses purely from a set of provided datasets? To evaluate this question, we present DiscoveryBench, the first comprehensive benchmark that formalizes the multi-step process of data-driven discovery. The benchmark is designed to systematically assess current model capabilities in discovery tasks and provide a useful resource for improving them. Our benchmark contains 264 tasks collected across 6 diverse domains, such as sociology and engineering, by manually deriving discovery workflows from published papers to approximate the real-world challenges faced by researchers, where each task is defined by a dataset, its metadata, and a discovery goal in natural language. We additionally provide 903 synthetic tasks to conduct controlled evaluations across task complexity. Furthermore, our structured formalism of data-driven discovery enables a facet-based evaluation that provides useful insights into different failure modes. We evaluate several popular LLM-based reasoning frameworks using both open and closed LLMs as baselines on DiscoveryBench and find that even the best system scores only 25%. Our benchmark, thus, illustrates the challenges in autonomous data-driven discovery and serves as a valuable resource for the community to make progress.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.02395",
        "title": "Is Your AI-Generated Code Really Secure? Evaluating Large Language Models on Secure Code Generation with CodeSecEval",
        "abstract": "Large language models (LLMs) have brought significant advancements to code generation and code repair, benefiting both novice and experienced developers. However, their training using unsanitized data from open-source repositories, like GitHub, raises the risk of inadvertently propagating security vulnerabilities. Despite numerous studies investigating the safety of code LLMs, there remains a gap in comprehensively addressing their security features. In this work, we aim to present a comprehensive study aimed at precisely evaluating and enhancing the security aspects of code LLMs. To support our research, we introduce CodeSecEval, a meticulously curated dataset designed to address 44 critical vulnerability types with 180 distinct samples. CodeSecEval serves as the foundation for the automatic evaluation of code models in two crucial tasks: code generation and code repair, with a strong emphasis on security. Our experimental results reveal that current models frequently overlook security issues during both code generation and repair processes, resulting in the creation of vulnerable code. In response, we propose different strategies that leverage vulnerability-aware information and insecure code explanations to mitigate these security vulnerabilities. Furthermore, our findings highlight that certain vulnerability types particularly challenge model performance, influencing their effectiveness in real-world applications. Based on these findings, we believe our study will have a positive impact on the software engineering community, inspiring the development of improved methods for training and utilizing LLMs, thereby leading to safer and more trustworthy model deployment.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01942",
        "title": "Certainly Uncertain: A Benchmark and Metric for Multimodal Epistemic and Aleatoric Awareness",
        "abstract": "The ability to acknowledge the inevitable uncertainty in their knowledge and reasoning is a prerequisite for AI systems to be truly truthful and reliable. In this paper, we present a taxonomy of uncertainty specific to vision-language AI systems, distinguishing between epistemic uncertainty (arising from a lack of information) and aleatoric uncertainty (due to inherent unpredictability), and further explore finer categories within. Based on this taxonomy, we synthesize a benchmark dataset, CertainlyUncertain, featuring 178K visual question answering (VQA) samples as contrastive pairs. This is achieved by 1) inpainting images to make previously answerable questions into unanswerable ones; and 2) using image captions to prompt large language models for both answerable and unanswerable questions. Additionally, we introduce a new metric confidence-weighted accuracy, that is well correlated with both accuracy and calibration error, to address the shortcomings of existing metrics.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.02067",
        "title": "Crossroads of Continents: Automated Artifact Extraction for Cultural Adaptation with Large Multimodal Models",
        "abstract": "In this work, we present a comprehensive three-phase study to examine (1) the effectiveness of large multimodal models (LMMs) in recognizing cultural contexts; (2) the accuracy of their representations of diverse cultures; and (3) their ability to adapt content across cultural boundaries. We first introduce Dalle Street, a large-scale dataset generated by DALL-E 3 and validated by humans, containing 9,935 images of 67 countries and 10 concept classes. We reveal disparities in cultural understanding at the sub-region level with both open-weight (LLaVA) and closed-source (GPT-4V) models on Dalle Street and other existing benchmarks. Next, we assess models' deeper culture understanding by an artifact extraction task and identify over 18,000 artifacts associated with different countries. Finally, we propose a highly composable pipeline, CultureAdapt, to adapt images from culture to culture. Our findings reveal a nuanced picture of the cultural competence of LMMs, highlighting the need to develop culture-aware systems. Dataset and code are available at this https URL",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01992",
        "title": "Is Your Large Language Model Knowledgeable or a Choices-Only Cheater?",
        "abstract": "Recent work shows that large language models (LLMs) can answer multiple-choice questions using only the choices, but does this mean that MCQA leaderboard rankings of LLMs are largely influenced by abilities in choices-only settings? To answer this, we use a contrast set that probes if LLMs over-rely on choices-only shortcuts in MCQA. While previous works build contrast sets via expensive human annotations or model-generated data which can be biased, we employ graph mining to extract contrast sets from existing MCQA datasets. We use our method on UnifiedQA, a group of six commonsense reasoning datasets with high choices-only accuracy, to build an 820-question contrast set. After validating our contrast set, we test 12 LLMs, finding that these models do not exhibit reliance on choice-only shortcuts when given both the question and choices. Thus, despite the susceptibility~of MCQA to high choices-only accuracy, we argue that LLMs are not obtaining high ranks on MCQA leaderboards just due to their ability to exploit choices-only shortcuts.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01976",
        "title": "A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding",
        "abstract": "Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce Interleaving Layout and Text in a Large Language Model (LayTextLLM)} for document understanding. In particular, LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA). Comprehensive benchmark evaluations reveal significant improvements, with a 27.0% increase on KIE tasks and 24.1% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.5% improvement over other SOTA OCR-based LLMs on KIE tasks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2407.01965",
        "title": "AdaCQR: Enhancing Query Reformulation for Conversational Search via Sparse and Dense Retrieval Alignment",
        "abstract": "Conversational Query Reformulation (CQR) has significantly advanced in addressing the challenges of conversational search, particularly those stemming from the latent user intent and the need for historical context. Recent works aimed to boost the performance of CRQ through alignment. However, they are designed for one specific retrieval system, which potentially results in poor generalization. To overcome this limitation, we present a novel framework AdaCQR. By aligning reformulation models with both term-based and semantic-based retrieval systems, AdaCQR enhances the generalizability of information-seeking queries across diverse retrieval environments through a dual-phase training strategy. We also developed two effective approaches for acquiring superior labels and diverse input candidates, boosting the efficiency and robustness of the framework. Experimental evaluations on the TopiOCQA and QReCC datasets demonstrate that AdaCQR significantly outperforms existing methods, offering both quantitative and qualitative improvements in conversational query reformulation.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2407.01948",
        "title": "Extracting and Encoding: Leveraging Large Language Models and Medical Knowledge to Enhance Radiological Text Representation",
        "abstract": "Advancing representation learning in specialized fields like medicine remains challenging due to the scarcity of expert annotations for text and images. To tackle this issue, we present a novel two-stage framework designed to extract high-quality factual statements from free-text radiology reports in order to improve the representations of text encoders and, consequently, their performance on various downstream tasks. In the first stage, we propose a \\textit{Fact Extractor} that leverages large language models (LLMs) to identify factual statements from well-curated domain-specific datasets. In the second stage, we introduce a \\textit{Fact Encoder} (CXRFE) based on a BERT model fine-tuned with objective functions designed to improve its representations using the extracted factual data. Our framework also includes a new embedding-based metric (CXRFEScore) for evaluating chest X-ray text generation systems, leveraging both stages of our approach. Extensive evaluations show that our fact extractor and encoder outperform current state-of-the-art methods in tasks such as sentence ranking, natural language inference, and label extraction from radiology reports. Additionally, our metric proves to be more robust and effective than existing metrics commonly used in the radiology report generation literature. The code of this project is available at \\url{this https URL}.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2407.02209",
        "title": "Generative Monoculture in Large Language Models",
        "abstract": "We introduce {\\em generative monoculture}, a behavior observed in large language models (LLMs) characterized by a significant narrowing of model output diversity relative to available training data for a given task: for example, generating only positive book reviews for books with a mixed reception. While in some cases, generative monoculture enhances performance (e.g., LLMs more often produce efficient code), the dangers are exacerbated in others (e.g., LLMs refuse to share diverse opinions). As LLMs are increasingly used in high-impact settings such as education and web search, careful maintenance of LLM output diversity is essential to ensure a variety of facts and perspectives are preserved over time. We experimentally demonstrate the prevalence of generative monoculture through analysis of book review and code generation tasks, and find that simple countermeasures such as altering sampling or prompting strategies are insufficient to mitigate the behavior. Moreover, our results suggest that the root causes of generative monoculture are likely embedded within the LLM's alignment processes, suggesting a need for developing fine-tuning paradigms that preserve or promote diversity.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2407.01906",
        "title": "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models",
        "abstract": "Parameter-efficient fine-tuning (PEFT) is crucial for customizing Large Language Models (LLMs) with constrained resources. Although there have been various PEFT methods for dense-architecture LLMs, PEFT for sparse-architecture LLMs is still underexplored. In this work, we study the PEFT method for LLMs with the Mixture-of-Experts (MoE) architecture and the contents of this work are mainly threefold: (1) We investigate the dispersion degree of the activated experts in customized tasks, and found that the routing distribution for a specific task tends to be highly concentrated, while the distribution of activated experts varies significantly across different tasks. (2) We propose Expert-Specialized Fine-Tuning, or ESFT, which tunes the experts most relevant to downstream tasks while freezing the other experts and modules; experimental results demonstrate that our method not only improves the tuning efficiency, but also matches or even surpasses the performance of full-parameter fine-tuning. (3) We further analyze the impact of the MoE architecture on expert-specialized fine-tuning. We find that MoE models with finer-grained experts are more advantageous in selecting the combination of experts that are most relevant to downstream tasks, thereby enhancing both the training efficiency and effectiveness.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2407.01899",
        "title": "Scope-enhanced Compositional Semantic Parsing for DRT",
        "abstract": "Discourse Representation Theory (DRT) distinguishes itself from other semantic representation frameworks by its ability to model complex semantic and discourse phenomena through structural nesting and variable binding. While seq2seq models hold the state of the art on DRT parsing, their accuracy degrades with the complexity of the sentence, and they sometimes struggle to produce well-formed DRT representations. We introduce the AMS parser, a compositional, neurosymbolic semantic parser for DRT. It rests on a novel mechanism for predicting quantifier scope. We show that the AMS parser reliably produces well-formed outputs and performs well on DRT parsing, especially on complex sentences.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2407.02351",
        "title": "Generative Large Language Models in Automated Fact-Checking: A Survey",
        "abstract": "The dissemination of false information across online platforms poses a serious societal challenge, necessitating robust measures for information verification. While manual fact-checking efforts are still instrumental, the growing volume of false information requires automated methods. Large language models (LLMs) offer promising opportunities to assist fact-checkers, leveraging LLM's extensive knowledge and robust reasoning capabilities. In this survey paper, we investigate the utilization of generative LLMs in the realm of fact-checking, illustrating various approaches that have been employed and techniques for prompting or fine-tuning LLMs. By providing an overview of existing approaches, this survey aims to improve the understanding of utilizing LLMs in fact-checking and to facilitate further progress in LLMs' involvement in this process.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2407.02345",
        "title": "MORPHEUS: Modeling Role from Personalized Dialogue History by Exploring and Utilizing Latent Space",
        "abstract": "Personalized Dialogue Generation (PDG) aims to create coherent responses according to roles or personas. Traditional PDG relies on external role data, which can be scarce and raise privacy concerns. Approaches address these issues by extracting role information from dialogue history, which often fail to generically model roles in continuous space. To overcome these limitations, we introduce a novel framework \\textbf{MO}dels \\textbf{R}oles from \\textbf{P}ersonalized Dialogue \\textbf{H}istory by \\textbf{E}xploring and \\textbf{U}tilizing Latent \\textbf{S}pace (MORPHEUS) through a three-stage training process. Specifically, we create a persona codebook to represent roles in latent space compactly, and this codebook is used to construct a posterior distribution of role information. This method enables the model to generalize across roles, allowing the generation of personalized dialogues even for unseen roles. Experiments on both Chinese and English datasets demonstrate that MORPHEUS enhances the extraction of role information, and improves response generation without external role data. Additionally, MORPHEUS can be considered an efficient fine-tuning for large language models.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2407.02333",
        "title": "Why do LLaVA Vision-Language Models Reply to Images in English?",
        "abstract": "We uncover a surprising multilingual bias occurring in a popular class of multimodal vision-language models (VLMs). Including an image in the query to a LLaVA-style VLM significantly increases the likelihood of the model returning an English response, regardless of the language of the query. This paper investigates the causes of this loss with a two-pronged approach that combines extensive ablation of the design space with a mechanistic analysis of the models' internal representations of image and text inputs. Both approaches indicate that the issue stems in the language modelling component of the LLaVA model. Statistically, we find that switching the language backbone for a bilingual language model has the strongest effect on reducing this error. Mechanistically, we provide compelling evidence that visual inputs are not mapped to a similar space as text ones, and that intervening on intermediary attention layers can reduce this bias. Our findings provide important insights to researchers and engineers seeking to understand the crossover between multimodal and multilingual spaces, and contribute to the goal of developing capable and inclusive VLMs for non-English contexts.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2407.02310",
        "title": "Evaluating the Ability of LLMs to Solve Semantics-Aware Process Mining Tasks",
        "abstract": "The process mining community has recently recognized the potential of large language models (LLMs) for tackling various process mining tasks. Initial studies report the capability of LLMs to support process analysis and even, to some extent, that they are able to reason about how processes work. This latter property suggests that LLMs could also be used to tackle process mining tasks that benefit from an understanding of process behavior. Examples of such tasks include (semantic) anomaly detection and next activity prediction, which both involve considerations of the meaning of activities and their inter-relations. In this paper, we investigate the capabilities of LLMs to tackle such semantics-aware process mining tasks. Furthermore, whereas most works on the intersection of LLMs and process mining only focus on testing these models out of the box, we provide a more principled investigation of the utility of LLMs for process mining, including their ability to obtain process mining knowledge post-hoc by means of in-context learning and supervised fine-tuning. Concretely, we define three process mining tasks that benefit from an understanding of process semantics and provide extensive benchmarking datasets for each of them. Our evaluation experiments reveal that (1) LLMs fail to solve challenging process mining tasks out of the box and when provided only a handful of in-context examples, (2) but they yield strong performance when fine-tuned for these tasks, consistently surpassing smaller, encoder-based language models.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/abs/2407.02301",
        "title": "CFinBench: A Comprehensive Chinese Financial Benchmark for Large Language Models",
        "abstract": "Large language models (LLMs) have achieved remarkable performance on various NLP tasks, yet their potential in more challenging and domain-specific task, such as finance, has not been fully explored. In this paper, we present CFinBench: a meticulously crafted, the most comprehensive evaluation benchmark to date, for assessing the financial knowledge of LLMs under Chinese context. In practice, to better align with the career trajectory of Chinese financial practitioners, we build a systematic evaluation from 4 first-level categories: (1) Financial Subject: whether LLMs can memorize the necessary basic knowledge of financial subjects, such as economics, statistics and auditing. (2) Financial Qualification: whether LLMs can obtain the needed financial qualified certifications, such as certified public accountant, securities qualification and banking qualification. (3) Financial Practice: whether LLMs can fulfill the practical financial jobs, such as tax consultant, junior accountant and securities analyst. (4) Financial Law: whether LLMs can meet the requirement of financial laws and regulations, such as tax law, insurance law and economic law. CFinBench comprises 99,100 questions spanning 43 second-level categories with 3 question types: single-choice, multiple-choice and judgment. We conduct extensive experiments of 50 representative LLMs with various model size on CFinBench. The results show that GPT4 and some Chinese-oriented models lead the benchmark, with the highest average accuracy being 60.16%, highlighting the challenge presented by CFinBench. The dataset and evaluation code are available at this https URL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.01909",
        "title": "Pinyin Regularization in Error Correction for Chinese Speech Recognition with Large Language Models",
        "abstract": "Recent studies have demonstrated the efficacy of large language models (LLMs) in error correction for automatic speech recognition (ASR). However, much of the research focuses on the English language. This paper redirects the attention to Chinese. Firstly, we construct a specialized benchmark dataset aimed at error correction for Chinese ASR with 724K hypotheses-transcription pairs, named the Chinese Hypotheses Paradise dataset (ChineseHP), which contains a wide range of scenarios and presents significant challenges. Subsequently, we conduct a preliminary evaluation using the dataset for both direct-prompting and fine-tuning pre-trained LLMs. Furthermore, we propose a straightforward method of Pinyin regularization for prompts, which involves the transcription of Pinyin directly from text hypotheses. The experimental results reveal that Pinyin regularization consistently enhances the error-correcting ability of LLMs when compared with those without regularization. The dataset is available on the website.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.02408",
        "title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models",
        "abstract": "As Large Language Models (LLMs) are increasingly deployed to handle various natural language processing (NLP) tasks, concerns regarding the potential negative societal impacts of LLM-generated content have also arisen. To evaluate the biases exhibited by LLMs, researchers have recently proposed a variety of datasets. However, existing bias evaluation efforts often focus on only a particular type of bias and employ inconsistent evaluation metrics, leading to difficulties in comparison across different datasets and LLMs. To address these limitations, we collect a variety of datasets designed for the bias evaluation of LLMs, and further propose CEB, a Compositional Evaluation Benchmark that covers different types of bias across different social groups and tasks. The curation of CEB is based on our newly proposed compositional taxonomy, which characterizes each dataset from three dimensions: bias types, social groups, and tasks. By combining the three dimensions, we develop a comprehensive evaluation strategy for the bias in LLMs. Our experiments demonstrate that the levels of bias vary across these dimensions, thereby providing guidance for the development of specific bias mitigation methods.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.02936",
        "title": "GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large Language Models",
        "abstract": "Evaluating the graph comprehension and reasoning abilities of Large Language Models (LLMs) is challenging and often incomplete. Existing benchmarks focus primarily on pure graph understanding, lacking a comprehensive evaluation across all graph types and detailed capability definitions. This paper presents GraCoRe, a benchmark for systematically assessing LLMs' graph comprehension and reasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and test models on pure graph and heterogeneous graphs, subdividing capabilities into 10 distinct areas tested through 19 tasks. Our benchmark includes 11 datasets with 5,140 graphs of varying complexity. We evaluated three closed-source and seven open-source LLMs, conducting thorough analyses from both ability and task perspectives. Key findings reveal that semantic enrichment enhances reasoning performance, node ordering impacts task success, and the ability to process longer texts does not necessarily improve graph comprehension or reasoning. GraCoRe is open-sourced at this https URL",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2106.09670",
        "title": "Indian Masked Faces in the Wild Dataset",
        "abstract": "Due to the COVID-19 pandemic, wearing face masks has become a mandate in\n public places worldwide. Face masks occlude a significant portion of the facial\n region. Additionally, people wear different types of masks, from simple ones to\n ones with graphics and prints. These pose new challenges to face recognition\n algorithms. Researchers have recently proposed a few masked face datasets for\n designing algorithms to overcome the challenges of masked face recognition.\n However, existing datasets lack the cultural diversity and collection in the\n unrestricted settings. Country like India with attire diversity, people are not\n limited to wearing traditional masks but also clothing like a thin cotton\n printed towel (locally called as ``gamcha''), ``stoles'', and ``handkerchiefs''\n to cover their faces. In this paper, we present a novel \\textbf{Indian Masked\n Faces in the Wild (IMFW)} dataset which contains images with variations in\n pose, illumination, resolution, and the variety of masks worn by the subjects.\n We have also benchmarked the performance of existing face recognition models on\n the proposed IMFW dataset. Experimental results demonstrate the limitations of\n existing algorithms in presence of diverse conditions.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1907.08480",
        "title": "Microscopic reweighting for non-equilibrium steady states dynamics",
        "abstract": "Computer simulations generate trajectories at a single, well-defined\n thermodynamic state point. Statistical reweighting offers the means to reweight\n static and dynamical properties to different equilibrium state points by means\n of analytic relations. We extend these ideas to non-equilibrium steady states\n by relying on a maximum path entropy formalism subject to physical constraints.\n Stochastic thermodynamics analytically relates the forward and backward\n probabilities of any pathway through the external non-conservative force,\n enabling reweighting both in and out of equilibrium. We avoid the combinatorial\n explosion of microtrajectories by systematically constructing pathways through\n Markovian transitions. We further identify a quantity that is invariant to\n dynamical reweighting, analogous to the density of states in equilibrium\n reweighting.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.00493",
        "title": "Gleason's theorem for composite systems",
        "abstract": "Gleason's theorem [A. Gleason, J. Math. Mech., \\textbf{6}, 885 (1957)] is an\n important result in the foundations of quantum mechanics, where it justifies\n the Born rule as a mathematical consequence of the quantum formalism. Formally,\n it presents a key insight into the projective geometry of Hilbert spaces,\n showing that finitely additive measures on the projection lattice $\\PH$ extend\n to positive linear functionals on the algebra of bounded operators $\\BH$. Over\n many years, and by the effort of various authors, the theorem has been\n broadened in its scope from type I to arbitrary von Neumann algebras (without\n type $\\text{I}_2$ factors). Here, we prove a generalisation of Gleason's\n theorem to composite systems. To this end, we strengthen the original result in\n two ways: first, we extend its scope to dilations in the sense of Naimark [M.\n A. Naimark, C. R. (Dokl.) Acad. Sci. URSS, n. Ser., \\textbf{41}, 359 (1943)]\n and Stinespring [W. F. Stinespring, Proc. Am. Math. Soc., \\textbf{6}, 211\n (1955)] and second, we require consistency with respect to dynamical\n correspondences on the respective (local) algebras in the composition [E. M.\n Alfsen and F. W. Shultz, Commun. Math. Phys., \\textbf{194}, 87 (1998)]. We show\n that neither of these conditions changes the result in the single system case,\n yet both are necessary to obtain a generalisation to bipartite systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0809.0095",
        "title": "Dualizing complex of a toric face ring",
        "abstract": "A \"toric face ring\", which generalizes both Stanley-Reisner rings and affine\n semigroup rings, is studied by Bruns, Roemer and their coauthors recently. In\n this paper, under the \"normality\" assumption, we describe a dualizing complex\n of a toric face ring $R$ in a very concise way. Since $R$ is not a graded ring\n in general, the proof is not straightforward. We also develop the squarefree\n module theory over $R$, and show that the Buchsbaum property and the\n Gorenstein* property of $R$ are topological properties of its associated cell\n complex.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1302.4212",
        "title": "Bosonic Part of 4d N=1 Supersymmetric Gauge Theory with General\n  Couplings: Local Existence",
        "abstract": "In this paper, we prove the local existence of the bosonic part of N=1\n supersymmetric gauge theory in four dimensions with general couplings. We start\n with the Lagrangian of the vector and chiral multiplets with general couplings\n and scalar potential turned on. Then, for the sake of simplicity, we set all\n fermions vanish at the level of equations of motions, so we only have the\n bosonic parts of the theory. We apply Segal's general theory to show the local\n existence of solutions of equations of motions by taking Kahler potential to be\n bounded above by U(n) symmetric Kahler potential and the first derivative of\n gauge couplings to be at most linear growth functions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1603.00545",
        "title": "The NANOGrav Nine-year Data Set: Mass and Geometric Measurements of\n  Binary Millisecond Pulsars",
        "abstract": "We analyze 24 binary radio pulsars in the North American Nanohertz\n Observatory for Gravitational Waves (NANOGrav) nine-year data set. We make\n fourteen significant measurements of Shapiro delay, including new detections in\n four pulsar-binary systems (PSRs J0613$-$0200, J2017+0603, J2302+4442, and\n J2317+1439), and derive estimates of the binary-component masses and orbital\n inclination for these MSP-binary systems. We find a wide range of binary pulsar\n masses, with values as low as $m_{\\rm p} =\n 1.18^{+0.10}_{-0.09}\\text{M}_{\\odot}$ for PSR J1918$-$0642 and as high as\n $m_{\\rm p} = 1.928^{+0.017}_{-0.017}\\text{M}_{\\odot}$ for PSR J1614$-$2230\n (both 68.3\\% credibility). We make an improved measurement of the Shapiro\n timing delay in the PSR J1918$-$0642 and J2043+1711 systems, measuring the\n pulsar mass in the latter system to be $m_{\\rm p} =\n 1.41^{+0.21}_{-0.18}\\text{M}_{\\odot}$ (68.3\\% credibility) for the first time.\n We measure secular variations of one or more orbital elements in many systems,\n and use these measurements to further constrain our estimates of the pulsar and\n companion masses whenever possible. In particular, we used the observed Shapiro\n delay and periastron advance due to relativistic gravity in the PSR J1903+0327\n system to derive a pulsar mass of $m_{\\rm p} =\n 1.65^{+0.02}_{-0.02}\\text{M}_{\\odot}$ (68.3\\% credibility). We discuss the\n implications that our mass measurements have on the overall neutron-star mass\n distribution, and on the \"mass/orbital-period\" correlation due to extended mass\n transfer.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1311.1523",
        "title": "A simple proof of Zeeman's theorem",
        "abstract": "It is discussed that Zeeman's theorem can be directly obtained from\n Liouville's theorem if we assume sufficient differentiability.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2002.08765",
        "title": "Two More Algorithms for Randomized Signature-Free Asynchronous Binary\n  Byzantine Consensus with $t < n/3$ and $O(n^2)$ Messages and $O(1)$ Round\n  Expected Termination",
        "abstract": "This work describes two randomized, asynchronous, round based, Binary\n Byzantine faulty tolerant consensus algorithms based on the algorithms of [25]\n and [26]. Like the algorithms of [25] and [26] they do not use signatures, use\n $O(n^2)$ messages per round (where each message is composed of a round number\n and a constant number of bits), tolerate up to one third failures, and have\n expected termination in constant number of rounds.\n  The first, like [26], uses a weak common coin (i.e. one that can return\n different values at different processes with a constant probability) to ensure\n termination. The algorithm consists of $5$ to $7$ message broadcasts per round.\n An optimization is described that reduces this to $4$ to $5$ broadcasts per\n round for rounds following the first round. Comparatively, [26] consists of $8$\n to $12$ message broadcasts per round.\n  The second algorithm, like [25], uses a perfect common coin (i.e. one that\n returns the same value at all non-faulty processes) for both termination and\n correctness. Unlike [25], it does not require a fair scheduler to ensure\n termination. Furthermore, the algorithm consists of $2$ to $3$ message\n broadcasts for the first round and $1$ to $2$ broadcasts for the following\n rounds, while [29] consists of $2$ to $3$ broadcasts per round.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/gr-qc/0602074",
        "title": "On the Analytic Structure of a Family of Hyperboloidal Beams of\n  Potential Interest for Advanced LIGO",
        "abstract": "For the baseline design of the advanced Laser Interferometer\n Gravitational-wave Observatory (LIGO), use of optical cavities with\n non-spherical mirrors supporting flat-top (\"mesa\") beams, potentially capable\n of mitigating the thermal noise of the mirrors, has recently drawn a\n considerable attention. To reduce the severe tilt-instability problems\n affecting the originally conceived nearly-flat, \"Mexican-hat-shaped\" mirror\n configuration, K. S. Thorne proposed a nearly-concentric mirror configuration\n capable of producing the same mesa beam profile on the mirror surfaces.\n Subsequently, Bondarescu and Thorne introduced a generalized construction that\n leads to a one-parameter family of \"hyperboloidal\" beams which allows\n continuous spanning from the nearly-flat to the nearly-concentric mesa beam\n configurations. This paper is concerned with a study of the analytic structure\n of the above family of hyperboloidal beams. Capitalizing on certain results\n from the applied optics literature on flat-top beams, a physically-insightful\n and computationally-effective representation is derived in terms of\n rapidly-converging Gauss-Laguerre expansions. Moreover, the functional relation\n between two generic hyperboloidal beams is investigated. This leads to a\n generalization (involving fractional Fourier transform operators of complex\n order) of some recently discovered duality relations between the nearly-flat\n and nearly-concentric mesa configurations. Possible implications and\n perspectives for the advanced LIGO optical cavity design are discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1808.03115",
        "title": "Supercharacter theories for Sylow $p$-subgroups of Lie type $G_2$",
        "abstract": "We construct a supercharacter theory, and establish the supercharacter table\n for Sylow $p$-subgroups $G_2^{syl}(q)$ of the Chevalley groups $G_2(q)$ of Lie\n type $G_2$ when $p>2$. Then we calculate the conjugacy classes, determine the\n complex irreducible characters by Clifford theory, and obtain the character\n tables for $G_2^{syl}(q)$ when $p>3$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2003.02879",
        "title": "The planar limit of $\\mathcal{N}=2$ superconformal field theories",
        "abstract": "We obtain the perturbative expansion of the free energy on $S^4$ for four\n dimensional Lagrangian ${\\cal N}=2$ superconformal field theories, to all\n orders in the 't Hooft coupling, in the planar limit. We do so by using\n supersymmetric localization, after rewriting the 1-loop factor as an effective\n action involving an infinite number of single and double trace terms. The\n answer we obtain is purely combinatorial, and involves a sum over tree graphs.\n We also apply these methods to the perturbative expansion of the free energy at\n finite $N$, and to the computation of the vacuum expectation value of the 1/2\n BPS circular Wilson loop, which in the planar limit involves a sum over rooted\n tree graphs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/physics/0701347",
        "title": "Statistical Geometry of Gravitation",
        "abstract": "General Relativity explains with precision the anomalous advance of the\n perihelion of Mercury, discovered by Le Verrier in 1859. Otherwise, diverse\n post-Newtonian proposals trying to solve this anomaly, introduce mathematical\n potentials focused on a finite propagation speed.\n  This paper tries to set some properties that should have any hypothetical\n gravitational potential suitable with material objects inside a physical\n universe. If a propagation speed is admitted, this assumption must link its\n origin, the continuous update, the trajectory, the retarded action and impulse\n mechanism (transit action) in the final target.\n  This gravitational potential, tries to explain the anomalous shift of the\n perihelion of Mercury. Otherwise, applied as a potential of force added to a\n mechanical momentum, could give a partial explanation of the anomalous\n acceleration of Pioneer 10/11. Anyway, it is an hypothetical conjecture without\n any proof of its physical reality",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0301065",
        "title": "Magnetic Field Geometry of the Broad Line Radio Galaxy 3C111",
        "abstract": "Very Long Baseline Polarimetric observations of the Broad Line Radio galaxy\n 3C111 performed in July and September of 1996 at 8 and 43 GHz reveal rapidly\n evolving parsec-scale radio structure after a large millimetre outburst. The\n B-field geometry is not simple. We present a first analysis of possible Faraday\n and optical depth effects based on a comparison of the polarization images for\n the two frequencies.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1905.12801",
        "title": "Reducing Gender Bias in Word-Level Language Models with a\n  Gender-Equalizing Loss Function",
        "abstract": "Gender bias exists in natural language datasets which neural language models\n tend to learn, resulting in biased text generation. In this research, we\n propose a debiasing approach based on the loss function modification. We\n introduce a new term to the loss function which attempts to equalize the\n probabilities of male and female words in the output. Using an array of bias\n evaluation metrics, we provide empirical evidence that our approach\n successfully mitigates gender bias in language models without increasing\n perplexity. In comparison to existing debiasing strategies, data augmentation,\n and word embedding debiasing, our method performs better in several aspects,\n especially in reducing gender bias in occupation words. Finally, we introduce a\n combination of data augmentation and our approach, and show that it outperforms\n existing strategies in all bias evaluation metrics.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1308.1438",
        "title": "Profiling hot and dense nuclear medium with high transverse momentum\n  hadrons produced in d+Au and Au+Au collisions by the PHENIX experiment at\n  RHIC",
        "abstract": "PHENIX measurements of high transverse momentum ($p_T$) identified hadrons in\n $d$+Au and Au+Au collisions are presented. The nuclear modification factors\n ($R_{d{\\rm A}}$ and $R_{\\rm AA}$) for $\\pi^0$ and $\\eta$ are found to be very\n consistent in both collision systems, respectively. Using large amount of $p+p$\n and Au+Au datasets, the fractional momentum loss ($S_{\\rm loss}$) and the\n path-length dependent yield of $\\pi^0$ in Au+Au collisions are obtained. The\n hadron spectra in the most central $d$+Au and the most peripheral Au+Au\n collisions are studied. The spectra shapes are found to be similar in both\n systems, but the yield is suppressed in the most peripheral Au+Au collisions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2305.08772",
        "title": "Partial slice regularity and Fueter's Theorem in several quaternionic\n  variables",
        "abstract": "We extend some definitions and give new results about the theory of slice\n analysis in several quaternionic variables. The sets of slice functions which\n are respectively slice, slice regular and circular w.r.t. given variables are\n characterized. We introduce new notions of partial spherical values and\n derivative for functions of several variables that extend those of one\n variable. We recover some of their properties and find new ones. Finally, we\n prove a generalization of Fueter's Theorem in this context.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1412.4056",
        "title": "Blind system identification using kernel-based methods",
        "abstract": "We propose a new method for blind system identification. Resorting to a\n Gaussian regression framework, we model the impulse response of the unknown\n linear system as a realization of a Gaussian process. The structure of the\n covariance matrix (or kernel) of such a process is given by the stable spline\n kernel, which has been recently introduced for system identification purposes\n and depends on an unknown hyperparameter. We assume that the input can be\n linearly described by few parameters. We estimate these parameters, together\n with the kernel hyperparameter and the noise variance, using an empirical Bayes\n approach. The related optimization problem is efficiently solved with a novel\n iterative scheme based on the Expectation-Maximization method. In particular,\n we show that each iteration consists of a set of simple update rules. We show,\n through some numerical experiments, very promising performance of the proposed\n method.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ph/0007077",
        "title": "Microscopic Theory of the Standard Model",
        "abstract": "The operator manifold formalism (part I) enables the unification of the\n geometry and the field theory, and yields the quantization of geometry. This is\n the mathematical framework for our physical outlook that the geometry and\n fields, with the internal symmetries and all interactions, as well the four\n major principles of relativity (special and general), quantum, gauge and colour\n confinement, are derivative, and come into being simultaneously in the stable\n system of the underlying ``primordial structures''. In part II we attempt to\n develop, further, the microscopic approach to the Standard Model of particle\n physics, which enables an insight to the key problems of particle\n phenomenology. We suggest the microscopic theory of the unified electroweak\n interactions. The Higgs bosons have arisen on an analogy of the Cooper pairs in\n superconductivity. Besides of microscopic interpretation of all physical\n parameters the resulting theory also makes plausible following testable\n implications for the current experiments: 1. The Higgs bosons never could\n emerge in spacetime continuum, thus, they cannot be discovered in these\n experiments nor at any energy range. 2. For each of the three SM families of\n quarks and leptons there are corresponding heavy family partners with the same\n quantum numbers lying far above the electroweak scale, respectively, at the\n energy threshold values: $E_{1}>(419.6 \\pm 12.0)GeV, \\quad E_{2}= (457.6 \\pm\n 13.2)GeV$ and $E_{3}=(521.4 \\pm 15.0)GeV.$",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2310.20198",
        "title": "Structured Two-Stage True-Time-Delay Array Codebook Design for\n  Multi-User Data Communication",
        "abstract": "Wideband millimeter-wave and terahertz (THz) systems can facilitate\n simultaneous data communication with multiple spatially separated users. It is\n desirable to orthogonalize users across sub-bands by deploying\n frequency-dependent beams with a sub-band-specific spatial response.\n True-Time-Delay (TTD) antenna arrays are a promising wideband architecture to\n implement sub-band-specific dispersion of beams across space using a single\n radio frequency (RF) chain. This paper proposes a structured design of analog\n TTD codebooks to generate beams that exhibit quantized sub-band-to-angle\n mapping. We introduce a structured Staircase TTD codebook and analyze the\n frequency-spatial behaviour of the resulting beam patterns. We develop the\n closed-form two-stage design of the proposed codebook to achieve the desired\n sub-band-specific beams and evaluate their performance in multi-user\n communication networks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0405566",
        "title": "The statistical mechanics of networks",
        "abstract": "We study the family of network models derived by requiring the expected\n properties of a graph ensemble to match a given set of measurements of a\n real-world network, while maximizing the entropy of the ensemble. Models of\n this type play the same role in the study of networks as is played by the\n Boltzmann distribution in classical statistical mechanics; they offer the best\n prediction of network properties subject to the constraints imposed by a given\n set of observations. We give exact solutions of models within this class that\n incorporate arbitrary degree distributions and arbitrary but independent edge\n probabilities. We also discuss some more complex examples with correlated edges\n that can be solved approximately or exactly by adapting various familiar\n methods, including mean-field theory, perturbation theory, and saddle-point\n expansions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0902.1056",
        "title": "New constraints for CP-violating forces between nucleons in the range\n  10-4 cm - 1 cm",
        "abstract": "The range of nucleon interaction 10-4 cm - 1 cm is interesting because it\n corresponds to the mass range of a intermediate particle inside so named \"axion\n window\" that is not closed yet by experiment. Depolarization of ultracold\n neutrons (UCN) during their storage in material traps can be caused by\n CP-violating pseudo-magnetic precession of the neutron spin in the vicinity of\n the unpolarized substance surface. Using the experimental limits for UCN\n depolarization new constraints were set for the product of the scalar,\n pseudo-scalar dimensionless constants gS*gP and the parameter lam_PS,\n determining the Yukawa-type of the nucleon interaction potential via new\n pseudo-scalar boson (axion-like particle) with a mass of m_PS: gS*gP*lam_PS^2\n <= 2.96*10-21 [cm^2] for 10-3 cm < lam_PS < 1 cm; gS*gP*lam_PS^2 <= 3.9*10-22\n [cm^2] for 10-4 cm < lam_PS < 10-3 cm. Improvement of the limit for gS*gP in\n the area of lam_PS from 0.1 cm to 1 cm accounts for 4-5 orders of magnitude in\n comparision with previous limit. The prospects of increasing in accuracy search\n for CP-violating pseudo-magnetic precession are considered. The estimations of\n the possible effects of pseudo-magnetic precession in the frame of the\n theoretical models with CP-violation are discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1112.2409",
        "title": "Medium Access Control Protocols for Wireless Sensor Networks with Energy\n  Harvesting",
        "abstract": "The design of Medium Access Control (MAC) protocols for wireless sensor\n networks (WSNs) has been conventionally tackled by assuming battery-powered\n devices and by adopting the network lifetime as the main performance criterion.\n While WSNs operated by energy-harvesting (EH) devices are not limited by\n network lifetime, they pose new design challenges due to the uncertain amount\n of harvestable energy. Novel design criteria are thus required to capture the\n trade-offs between the potentially infinite network lifetime and the uncertain\n energy availability. This paper addresses the analysis and design of WSNs with\n EH devices by focusing on conventional MAC protocols, namely TDMA, Framed-ALOHA\n (FA) and Dynamic-FA (DFA), and by accounting for the performance trade-offs and\n design issues arising due to EH. A novel metric, referred to as delivery\n probability, is introduced to measure the capability of a MAC protocol to\n deliver the measure of any sensor in the network to the intended destination\n (or fusion center, FC). The interplay between delivery efficiency and time\n efficiency (i.e., the data collection rate at the FC), is investigated\n analytically using Markov models. Numerical results validate the analysis and\n emphasize the critical importance of accounting for both delivery probability\n and time efficiency in the design of EH-WSNs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1507.04805",
        "title": "Pair correlations and structure factor of the $J_1$-$J_2$ square lattice\n  Ising model in an external field within the Cluster Variation Method",
        "abstract": "We compute the structure factor of the $J_1$-$J_2$ Ising model in an external\n field on the square lattice within the Cluster Variation Method. We use a four\n point plaquette approximation, which is the minimal one able to capture phases\n with broken orientational order in real space, like the recently reported\n Ising-nematic phase in the model. The analysis of different local maxima in the\n structure factor allows us to track the different phases and phase transitions\n against temperature and external field. Although the nematic susceptibility is\n not directly related to the structure factor, we show that because of the close\n relationship between the nematic order parameter and the structure factor, the\n latter shows unambiguous signatures of the presence of a nematic phase, in\n agreement with results from direct minimization of a variational free energy.\n The disorder variety of the model is identified and the possibility that the\n CVM four point approximation be exact on the disorder variety is discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.05679",
        "title": "Good-looking but Lacking Faithfulness: Understanding Local Explanation\n  Methods through Trend-based Testing",
        "abstract": "While enjoying the great achievements brought by deep learning (DL), people\n are also worried about the decision made by DL models, since the high degree of\n non-linearity of DL models makes the decision extremely difficult to\n understand. Consequently, attacks such as adversarial attacks are easy to carry\n out, but difficult to detect and explain, which has led to a boom in the\n research on local explanation methods for explaining model decisions. In this\n paper, we evaluate the faithfulness of explanation methods and find that\n traditional tests on faithfulness encounter the random dominance problem, \\ie,\n the random selection performs the best, especially for complex data. To further\n solve this problem, we propose three trend-based faithfulness tests and\n empirically demonstrate that the new trend tests can better assess faithfulness\n than traditional tests on image, natural language and security tasks. We\n implement the assessment system and evaluate ten popular explanation methods.\n Benefiting from the trend tests, we successfully assess the explanation methods\n on complex data for the first time, bringing unprecedented discoveries and\n inspiring future research. Downstream tasks also greatly benefit from the\n tests. For example, model debugging equipped with faithful explanation methods\n performs much better for detecting and correcting accuracy and security\n problems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1804.08118",
        "title": "On the insecurity of quantum Bitcoin mining",
        "abstract": "Grover's algorithm confers on quantum computers a quadratic advantage over\n classical computers for searching in an arbitrary data set, a scenario that\n describes Bitcoin mining. It has previously been argued that the only\n side-effect of quantum mining would be an increased difficulty.\n  In this work, we argue that a crucial argument in the analysis of Bitcoin\n security breaks down when quantum mining is performed.\n  Classically, a Bitcoin fork occurs rarely, i.e., when two miners find a block\n almost simultaneously, due to propagation time effects. The situation differs\n dramatically when quantum miners use Grover's algorithm, which repeatedly\n applies a procedure called a Grover iteration. The chances of finding a block\n grow quadratically with the number of Grover iterations applied. Crucially, a\n miner does not have to choose how many iterations to apply in advance. Suppose\n Alice receives Bob's new block. To maximize her revenue, she should stop and\n measure her state immediately in the hopes that her block (rather than Bob's)\n will become part of the longest chain. The strong correlation between the\n miners' actions and the fact that they all measure their states at the same\n time may lead to more forks -- which is known to be a security risk for\n Bitcoin. We propose a mechanism that, we conjecture, will prevent this form of\n quantum mining, thereby circumventing the high rate of forks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0804.0507",
        "title": "Extreme Horizontal Branch Stars",
        "abstract": "A review is presented on the properties, origin and evolutionary links of hot\n subluminous stars which are generally believed to be extreme Horizontal Branch\n stars or closely related objects. Amongst the field stars a large fraction of\n sdBs are found to reside in close binaries. The companions are predominantly\n white dwarfs, or low mass main sequence stars. Systems with sufficiently\n massive WD companions may qualify as SN Ia progenitors. Recently evidence has\n been found that the masses of some unseen companions might exceed the\n Chandrasekhar mass, hence they must be neutron stars or black holes. Even a\n planet has recently been detected orbiting the pulsating sdB star V391 Peg.\n Quite to the opposite, in globular clusters, only very few sdB binaries are\n found indicating that the dominant sdB formation processes is different in a\n dense environment. Binary population synthesis models identify three formation\n channels, (i) stable Roche lobe overflow, (ii) one or two common envelope\n ejection phases and (iii) the merger of two helium white dwarfs. The latter\n channel may explain the properties of the He-enriched sdO stars because their\n binary fraction is lower than that of the sdBs by a factor of ten or more.\n Pulsating subluminous B (sdB) stars play an important role for asteroseismology\n as this technique has already led to mass determinations for a handful of\n stars. A unique hyper-velocity sdO star moving so fast that it is unbound to\n the Galaxy has probably been ejected by the super-massive black hole in the\n Galactic centre. (abridged)",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0704.1775",
        "title": "Origin of Discrepancies in Inelastic Electron Tunneling Spectra of\n  Molecular Junctions",
        "abstract": "We report inelastic electron tunneling spectroscopy (IETS) of multilayer\n molecular junctions with and without incorporated metal nano-particles. The\n incorporation of metal nanoparticles into our devices leads to enhanced IET\n intensity and a modified line-shape for some vibrational modes. The enhancement\n and line-shape modification are both the result of a low lying hybrid metal\n nanoparticle-molecule electronic level. These observations explain the apparent\n discrepancy between earlier IETS measurements of alkane thiolate junctions by\n Kushmerick \\emph{et al.} [Nano Lett. \\textbf{4}, 639 (2004)] and Wang \\emph{et\n al.} [Nano Lett. \\textbf{4}, 643 (2004)].",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1011.4700",
        "title": "Industrially Scalable Process for Silicon Nanowires for Seebeck\n  Generators",
        "abstract": "The observation that the thermal conductivity of single-crystalline silicon\n nanowires with diameter on the length scale of 25 nm is lower than that of bulk\n material by two orders of magnitude has attracted the interest onto silicon as\n a potentially effective thermoelectric material. However, the potential\n interest has a hope of transforming in a practical interest only if\n poly-crystalline silicon can replace single crystalline silicon and the\n preparation of nanowires does not involve any advanced photolithography. In\n this work we show that a technique, based on the controlled etching and filling\n of recessed regions and employing standard photolithography and\n deposition-etching methods, succeeds in the preparation of poly-crystalline\n silicon nanowires (with diameter of 25 nm and length on the centimetre scale)\n at a linear density of 3E6 cm^-1.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1310.0431",
        "title": "QCD Sum Rule Analysis of Heavy Quarkonium Hybrids",
        "abstract": "We have studied the charmonium and bottomonium hybrid states with various\n $J^{PC}$ quantum numbers in QCD sum rules. At leading order in $\\alpha_s$, the\n two-point correlation functions have been calculated up to dimension six\n including the tri-gluon condensate and four-quark condensate. After performing\n the QCD sum rule analysis, we have confirmed that the dimension six condensates\n can stabilize the hybrid sum rules and allow the reliable mass predictions. We\n have updated the mass spectra of the charmonium and bottomonium hybrid states\n and identified that the negative-parity states with $J^{PC}=(0, 1, 2)^{-+},\n 1^{--}$ form the lightest hybrid supermultiplet while the positive-parity\n states with $J^{PC}=(0, 1)^{+-}, (0, 1, 2)^{++}$ belong to a heavier hybrid\n supermultiplet.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1203.0274",
        "title": "A Hybrid Scenario for the Formation of Brown Dwarfs and Very Low Mass\n  Stars",
        "abstract": "We present a calculation of protostellar disk formation and evolution in\n which gaseous clumps (essentially, the first Larson cores formed via disk\n fragmentation) are ejected from the disk during the early stage of evolution.\n This is a universal process related to the phenomenon of ejection in multiple\n systems of point masses. However, it occurs in our model entirely due to the\n interaction of compact, gravitationally-bound gaseous clumps and is free from\n the smoothing-length uncertainty that is characteristic of models using sink\n particles. Clumps that survive ejection span a mass range of 0.08--0.35\n $M_\\odot$, and have ejection velocities $0.8 \\pm 0.35$ km s$^{-1}$, which are\n several times greater than the escape speed. We suggest that, upon contraction,\n these clumps can form substellar or low-mass stellar objects with notable\n disks, or even close-separation very-low-mass binaries. In this hybrid\n scenario, allowing for ejection of clumps rather than finished\n protostars/proto--brown-dwarfs, disk formation and the low velocity dispersion\n of low-mass objects are naturally explained, while it is also consistent with\n the observation of isolated low-mass clumps that are ejection products. We\n conclude that clump ejection and the formation of isolated low mass stellar and\n substellar objects is a common occurrence, with important implications for\n understanding the initial mass function, the brown dwarf desert, and the\n formation of stars in all environments and epochs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0706.0754",
        "title": "Metal Cooling in Simulations of Cosmic Structure Formation",
        "abstract": "The addition of metals to any gas can significantly alter its evolution by\n increasing the rate of radiative cooling. In star-forming environments,\n enhanced cooling can potentially lead to fragmentation and the formation of\n low-mass stars, where metal-free gas-clouds have been shown not to fragment.\n Adding metal cooling to numerical simulations has traditionally required a\n choice between speed and accuracy. We introduce a method that uses the\n sophisticated chemical network of the photoionization software, Cloudy, to\n include radiative cooling from a complete set of metals up to atomic number 30\n (Zn) that can be used with large-scale three-dimensional hydrodynamic\n simulations. Our method is valid over an extremely large temperature range (10\n K < T < 10^8 K), up to hydrogen number densities of 10^12 cm^-3. At this\n density, a sphere of 1 Msun has a radius of roughly 40 AU. We implement our\n method in the adaptive mesh refinement (AMR) hydrodynamic/N-body code, Enzo.\n Using cooling rates generated with this method, we study the physical\n conditions that led to the transition from Population III to Population II star\n formation. While C, O, Fe, and Si have been previously shown to make the\n strongest contribution to the cooling in low-metallicity gas, we find that up\n to 40% of the metal cooling comes from fine-structure emission by S, when solar\n abundance patterns are present. At metallicities, Z > 10^-4 Zsun, regions of\n density and temperature exist where gas is both thermally unstable and has a\n cooling time less than its dynamical time. We identify these doubly unstable\n regions as the most inducive to fragmentation. At high redshifts, the CMB\n inhibits efficient cooling at low temperatures and, thus, reduces the size of\n the doubly unstable regions, making fragmentation more difficult.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1711.10661",
        "title": "Inner Product and Set Disjointness: Beyond Logarithmically Many Parties",
        "abstract": "A basic goal in complexity theory is to understand the communication\n complexity of number-on-the-forehead problems\n $f\\colon(\\{0,1\\}^n)^{k}\\to\\{0,1\\}$ with $k\\gg\\log n$ parties. We study the\n problems of inner product and set disjointness and determine their randomized\n communication complexity for every $k\\geq\\log n$, showing in both cases that\n $\\Theta(1+\\lceil\\log n\\rceil/\\log\\lceil1+k/\\log n\\rceil)$ bits are necessary\n and sufficient. In particular, these problems admit constant-cost protocols if\n and only if the number of parties is $k\\geq n^{\\epsilon}$ for some constant\n $\\epsilon>0.$",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1002.0863",
        "title": "Evolution of Low- and Intermediate-Mass Stars with [Fe/H] <= -2.5",
        "abstract": "We present extensive sets of stellar models for 0.8-9.0Msun in mass and -5 <=\n [Fe/H] <= -2 and Z = 0 in metallicity. The present work focuses on the\n evolutionary characteristics of hydrogen mixing into the He-flash convective\n zones during the core and shell He flashes which occurs for the models with\n [Fe/H] <~ -2.5. Evolution is followed from the zero age MS to the TPAGB phase\n including the hydrogen engulfment by the He-flash convection during the RGB or\n AGB phase. There exist various types of mixing episodes of how the H mixing\n sets in and how it affects the final abundances at the surface. In particular,\n we find H ingestion events without dredge-ups that enables repeated\n neutron-capture nucleosynthesis in the He flash convective zones with 13\n C(a,n)16 O as neutron source. For Z = 0, the mixing and dredge-up processes\n vary with the initial mass, which results in different final abundances in the\n surface. We investigate the occurrence of these events for various initial mass\n and metallicity to find the metallicity dependence for the He-flash driven deep\n mixing (He-FDDM) and also for the third dredge-up (TDU) events. In our models,\n we find He-FDDM for M <= 3Msun for Z = 0 and for M <~ 2Msun for -5 <~ [Fe/H] <~\n -3. On the other hand, the occurrence of the TDU is limited to the mass range\n of ~1.5Msun to ~5Msun for [Fe/H] = -3, which narrows with decreasing\n metallicity. The paper also discusses the implications of the results of model\n computations for observations. We compared the abundance pattern of CNO\n abundances with observed metal-poor stars. The origins of most iron-deficient\n stars are discussed by assuming that these stars are affected by binary mass\n transfer. We also point out the existence of a blue horizontal branch for -4 <~\n [Fe/H] <~ -2.5.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2302.09432",
        "title": "BBT-Fin: Comprehensive Construction of Chinese Financial Domain\nPre-trained Language Model, Corpus and Benchmark",
        "abstract": "To advance Chinese financial natural language\nprocessing (NLP), we introduce BBT-FinT5,\na new Chinese financial pre-training language\nmodel based on the T5 model. To support this\neffort, we have built BBT-FinCorpus, a largescale financial corpus with approximately\n300GB of raw text from four different sources.\nIn general domain NLP, comprehensive benchmarks like GLUE and SuperGLUE have\ndriven significant advancements in language\nmodel pre-training by enabling head-to-head\ncomparisons among models. Drawing inspiration from these benchmarks, we propose BBTCFLEB, a Chinese Financial Language understanding and generation Evaluation Benchmark, which includes six datasets covering\nboth understanding and generation tasks. Our\naim is to facilitate research in the development\nof NLP within the Chinese financial domain.\nOur model, corpus and benchmark are released at https://github.com/ssymmetry/\nBBT-FinCUGE-Applications. Our work belongs to the Big Bang Transformer (BBT),\na large-scale pre-trained language model\nproject.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.08688",
        "title": "TigerBot: An Open Multilingual Multitask LLM",
        "abstract": "We release and introduce the TigerBot family of large language models (LLMs) 1, consisting of base and chat models, sized from 7, 13, 70 and 180 billion parameters.We develop our models embarking from Llama-2 and BLOOM, and push the\nboundary further in data, training algorithm, infrastructure, and application tools.\nOur models yield meaningful performance gain over SOTA open-source models,\ne.g., Llama-2, specifically 6% gain in English and 20% gain in Chinese. TigerBot\nmodel family also achieves leading performance in major academic and industrial\nbenchmarks and leaderboards 2. We believe that TigerBot represents just a snapshot\nof lightning-fast progression in LLM open-source community. Therefore, we are\nthrilled to give back by publicly releasing our models and reporting our approach\nbehind, with additional emphases on building SOTA LLMs in a democratized way\nand making LLMs of use in real-world applications.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.05640",
        "title": "FinGPT: Large Generative Models for a Small Language",
        "abstract": "Large language models (LLMs) excel in many\ntasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the\nchallenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world\npopulation. We compile an extensive dataset\nof Finnish combining web crawls, news, social\nmedia and eBooks. We pursue two approaches\nto pretrain models: 1) we train seven monolingual models from scratch (186M to 13B\nparameters) dubbed FinGPT, 2) we continue\nthe pretraining of the multilingual BLOOM\nmodel on a mix of its original training data\nand Finnish, resulting in a 176 billion parameter model we call BLUUMI. For model evaluation, we introduce FIN-bench, a version of\nBIG-bench with Finnish tasks. We also assess\nother model qualities such as toxicity and bias.\nOur models and tools are openly available at\nhttps://turkunlp.org/gpt3-finnish.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2405.01249",
        "title": "Prompt engineering paradigms for medical applications: scoping review and recommendations for better practices",
        "abstract": "Prompt engineering is crucial for harnessing the potential of large language models (LLMs), especially in\nthe medical domain where specialized terminology and phrasing is used. However, the efficacy of prompt\nengineering in the medical domain remains to be explored. In this work, 114 recent studies (2022-2024)\napplying prompt engineering in medicine, covering prompt learning (PL), prompt tuning (PT), and prompt\ndesign (PD) are reviewed. PD is the most prevalent (78 articles). In 12 papers, PD, PL, and PT terms were\nused interchangeably. ChatGPT is the most used LLM, with seven papers using it for processing sensitive\nclinical data. Chain-of-Thought emerges as the most common prompt engineering technique. While PL and\nPT articles typically provide a baseline for evaluating prompt-based approaches, 64% of PD studies lack\nnon-prompt-related baselines. We provide tables and figures summarizing existing work and reporting\nrecommendations to guide future research contributions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2310.10631",
        "title": "LLEMMA: AN OPEN LANGUAGE MODEL FOR MATHEMATICS",
        "abstract": "We present LLEMMA, a large language model for mathematics. We continue\npretraining Code Llama on Proof-Pile-2, a mixture of scientific papers, web data\ncontaining mathematics, and mathematical code, yielding LLEMMA. On the MATH\nbenchmark LLEMMA outperforms all known open base models, as well as the\nunreleased Minerva model suite on an equi-parameter basis. Moreover, LLEMMA\nis capable of tool use and formal theorem proving without any further finetuning.\nWe openly release all artifacts, including 7 billion and 34 billion parameter models,\nthe Proof-Pile-2, and code to replicate our experiments.1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2211.15533",
        "title": "The Stack: 3 TB of permissively licensed source code",
        "abstract": "ferent Python subsets. We find that (1) near-deduplicating the data significantly boosts\nperformance across all experiments, and (2) it is possible to match previously reported\nHumanEval and MBPP performance using only permissively licensed data. We make the\ndataset available at https://hf.co/BigCode, provide a tool called \"Am I in The Stack\"\n(https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for\ncopies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2101.00027",
        "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
        "abstract": "Recent work has demonstrated that increased\ntraining dataset diversity improves general\ncross-domain knowledge and downstream generalization capability for large-scale language\nmodels. With this in mind, we present the\nPile: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse\nhigh-quality subsets\u2014both existing and newly\nconstructed\u2014many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and\nGPT-3 on the Pile shows that these models\nstruggle on many of its components, such as\nacademic writing. Conversely, models trained\non the Pile improve significantly over both\nRaw CC and CC-100 on all components of the\nPile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially\nconcerning aspects of the data for prospective\nusers. We make publicly available the code\nused in its construction.1",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2105.05241",
        "title": "Addressing \"Documentation Debt\" in Machine Learning Research: A Retrospective Datasheet for BookCorpus",
        "abstract": "Recent literature has underscored the importance of dataset documentation work for machine learning, and\npart of this work involves addressing \u201cdocumentation debt\u201d for datasets that have been used widely but\ndocumented sparsely. This paper aims to help address documentation debt for BookCorpus, a popular text\ndataset for training large language models. Notably, researchers have used BookCorpus to train OpenAI\u2019s\nGPT-N models and Google\u2019s BERT models, even though little to no documentation exists about the dataset\u2019s\nmotivation, composition, collection process, etc. We offer a preliminary datasheet that provides key context\nand information about BookCorpus, highlighting several notable deficiencies. In particular, we find evidence\nthat (1) BookCorpus likely violates copyright restrictions for many books, (2) BookCorpus contains thousands\nof duplicated books, and (3) BookCorpus exhibits significant skews in genre representation. We also find hints\nof other potential deficiencies that call for future research, including problematic content, potential skews\nin religious representation, and lopsided author contributions. While more work remains, this initial effort\nto provide a datasheet for BookCorpus adds to growing literature that urges more careful and systematic\ndocumentation for machine learning datasets.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.00964",
        "title": "A deep learning-based approach for identifying unresolved questions on Stack Exchange Q&A communities through graph-based communication modelling",
        "abstract": "In recent years, online question-answering (Q&A) platforms, such as Stack Exchange (SE), have\nbecome increasingly popular for information and knowledge sharing. Despite the vast amount of information available on these platforms, many questions remain unresolved. In this work, we aim to\naddress this issue by proposing a novel approach to identify unresolved questions in SE Q&A communities. Our approach utilises the graph structure of communication formed around a question by\nusers to model the communication network surrounding it. We employ a property graph model and\ngraph neural networks (GNNs), which can effectively capture both the structure of communication\nand the content of messages exchanged among users. By leveraging the power of graph representation and GNNs, our approach can effectively identify unresolved questions in SE communities.\nExperimental results on the complete historical data from three distinct Q&A communities demonstrate the superiority of our proposed approach over baseline methods that only consider the content\nof questions. Finally, our work represents a first but important step towards better understanding\nthe factors that can affect questions becoming and remaining unresolved in SE communities.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1911.02782",
        "title": "S2ORC: The Semantic Scholar Open Research Corpus",
        "abstract": "We introduce S2ORC,1\na large corpus of\n81.1M English-language academic papers\nspanning many academic disciplines. The corpus consists of rich metadata, paper abstracts,\nresolved bibliographic references, as well as\nstructured full text for 8.1M open access papers. Full text is annotated with automaticallydetected inline mentions of citations, figures,\nand tables, each linked to their corresponding paper objects. In S2ORC, we aggregate\npapers from hundreds of academic publishers\nand digital archives into a unified source, and\ncreate the largest publicly-available collection\nof machine-readable academic text to date. We\nhope this resource will facilitate research and\ndevelopment of tools and tasks for text mining\nover academic text.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2302.14035",
        "title": "The ROOTS Search Tool: Data Transparency for LLMs",
        "abstract": "ROOTS is a 1.6TB multilingual text corpus developed for the training of BLOOM, currently\nthe largest language model explicitly accompanied by commensurate data governance efforts.\nIn continuation of these efforts, we present the\nROOTS Search Tool: a search engine over the\nentire ROOTS corpus offering both fuzzy and\nexact search capabilities. ROOTS is the largest\ncorpus to date that can be investigated this way.\nThe ROOTS Search Tool is open-sourced and\navailable on Hugging Face Spaces. We describe our implementation and the possible use\ncases of our tool.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2003.08284",
        "title": "Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of\nUrban Roadways",
        "abstract": "Semantic segmentation of large-scale outdoor point\nclouds is essential for urban scene understanding in various applications, especially autonomous driving and urban\nhigh-definition (HD) mapping. With rapid developments of\nmobile laser scanning (MLS) systems, massive point clouds\nare available for scene understanding, but publicly accessible large-scale labeled datasets, which are essential for\ndeveloping learning-based methods, are still limited. This\npaper introduces Toronto-3D, a large-scale urban outdoor\npoint cloud dataset acquired by a MLS system in Toronto,\nCanada for semantic segmentation. This dataset covers approximately 1 km of point clouds and consists of about 78.3\nmillion points with 8 labeled object classes. Baseline experiments for semantic segmentation were conducted and\nthe results confirmed the capability of this dataset to train\ndeep learning models effectively. Toronto-3D is released 1\nto encourage new research, and the labels will be improved\nand updated with feedback from the research community.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1506.06724",
        "title": "Aligning Books and Movies: Towards Story-like Visual Explanations by\nWatching Movies and Reading Books",
        "abstract": "Books are a rich source of both fine-grained information,\nhow a character, an object or a scene looks like, as well as\nhigh-level semantics, what someone is thinking, feeling and\nhow these states evolve through a story. This paper aims to\nalign books to their movie releases in order to provide rich\ndescriptive explanations for visual content that go semantically far beyond the captions available in current datasets.\nTo align movies and books we exploit a neural sentence\nembedding that is trained in an unsupervised way from a\nlarge corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and\nsentences in the book. We propose a context-aware CNN\nto combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase\nthe diversity of tasks our model can be used for.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.08418",
        "title": "OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text",
        "abstract": "Image-text interleaved data, consisting of multiple images and texts arranged in a natural document format, aligns with the presentation paradigm of internet data and closely resembles human reading habits. Recent studies have shown that such data aids multimodal in-context learning and maintains the capabilities of large language models during multimodal fine-tuning. However, the limited scale and diversity of current image-text interleaved data restrict the development of multimodal large language models. In this paper, we introduce OmniCorpus, a 10 billion-level image-text interleaved dataset. Using an efficient data engine, we filter and extract large-scale high-quality documents, which contain 8.6 billion images and 1,696 billion text tokens. Compared to counterparts (e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales while maintaining good data quality; 2) features more diverse sources, including both English and non-English websites as well as video-centric websites; 3) is more flexible, easily degradable from an image-text interleaved format to pure text corpus and image-text pairs. Through comprehensive analysis and experiments, we validate the quality, usability, and effectiveness of the proposed dataset. We hope this could provide a solid data foundation for future multimodal model research. Code and data are released at https://github.com/OpenGVLab/OmniCorpus.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2303.03915",
        "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset",
        "abstract": "As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM)(BigScience Workshop, 2022) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2403.11793",
        "title": "Reasoning Abilities of Large Language Models: In-Depth\nAnalysis on the Abstraction and Reasoning Corpus",
        "abstract": "The existing methods for evaluating the inference abilities of Large Language Models (LLMs) have been resultscentric, making it difficult to assess the inference process. We introduce a new approach using the Abstract\nand Reasoning Corpus (ARC) dataset to evaluate the inference and contextual understanding abilities of large\nlanguage models in a process-centric manner. ARC demands rigorous logical structures for problem-solving,\nmaking it a benchmark that facilitates the comparison of model inference abilities with humans. Experimental\nresults confirm that while large language models possess weak inference abilities, they still lag in terms of\nlogical coherence, compositionality, and productivity. Our experiments highlight the reasoning capabilities of\nLLMs, proposing development paths for achieving human-level reasoning.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1805.07697",
        "title": "The UN Parallel Corpus Annotated for Translation Direction",
        "abstract": "This work distinguishes between translated and original text in the UN protocol corpus. By modeling the problem as classification problem, we can achieve up to 95% classification accuracy. We begin by deriving a parallel corpus for different language-pairs annotated for translation direction, and then classify the data by using various feature extraction methods. We compare the different methods as well as the ability to distinguish between translated and original texts in the different languages. The annotated corpus is publicly available.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2402.18041",
        "title": "Datasets for Large Language Models: A Comprehensive Survey",
        "abstract": "This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains. Information from 20 dimensions is incorporated into the dataset statistics. The total data size surveyed surpasses 774.5 TB for pretraining corpora and 700M instances for other datasets. We aim to present the entire landscape of LLM text datasets, serving as a comprehensive reference for researchers in this field and contributing to future studies. Related resources are available at: https://github.com/lmmlzn/Awesome-LLMs-Datasets.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2402.19282",
        "title": "WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset",
        "abstract": "This paper presents WanJuan-CC, a safe and high-quality open-sourced English\nwebtext dataset derived from Common Crawl data. The study addresses the challenges of constructing large-scale pre-training datasets for language models, which\nrequire vast amounts of high-quality data. A comprehensive process was designed\nto handle Common Crawl data, including extraction, heuristic rule filtering, fuzzy\ndeduplication, content safety filtering, and data quality filtering. From approximately 68 billion original English documents, we obtained 2.22T Tokens of safe\ndata and selected 1.0T Tokens of high-quality data as part of WanJuan-CC. We\nhave open-sourced 100B Tokens from this dataset. The paper also provides statistical information related to data quality, enabling users to select appropriate data\naccording to their needs. To evaluate the quality and utility of the dataset, we\ntrained 1B-parameter and 3B-parameter models using WanJuan-CC and another\ndataset, RefinedWeb. Results show that WanJuan-CC performs better on validation\ndatasets and downstream tasks.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2104.08758",
        "title": "Documenting Large Webtext Corpora:\nA Case Study on the Colossal Clean Crawled Corpus",
        "abstract": "Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora\navailable are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In\nthis work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created\nby applying a set of filters to a single snapshot\nof Common Crawl. We begin by investigating\nwhere the data came from, and find a significant amount of text from unexpected sources\nlike patents and US military websites. Then\nwe explore the content of the text itself, and\nfind machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets.\nTo understand the impact of the filters applied\nto create this dataset, we evaluate the text that\nwas removed, and show that blocklist filtering disproportionately removes text from and\nabout minority individuals. Finally, we conclude with some recommendations for how to\ncreated and document web-scale datasets from\na scrape of the internet.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2304.06939",
        "title": "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text",
        "abstract": "In-context vision and language models like Flamingo [2] support arbitrarily interleaved sequences of images and text as input. This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g., \u201cWhat do image A and image B have in common?\u201d To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text. To date, however, large-scale data of this form have not been publicly available. We release Multimodal C4 (mmc4), an augmentation of the popular text-only c4 corpus2 with images interleaved. We use a linear assignment algorithm to place images into longer bodies of text using CLIP features [24], a process that we show outperforms alternatives. mmc4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (88%) of images are topically relevant, and that linear assignment frequently selects individual sentences specifically well-aligned with each image (80%). After filtering NSFW images, ads, etc., the resulting mmc4 corpus consists of 101.2M documents with 571M images interleaved in 43B English tokens.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.04594",
        "title": "Boosting Large-scale Parallel Training Efficiency with C4 : A Communication-Driven Approach",
        "abstract": "The emergence of Large Language Models (LLMs) has necessitated the adoption of parallel training techniques, involving the deployment of thousands of GPUs to train a single model. Unfortunately, we have found that the efficiency of current parallel training is often suboptimal, largely due to the following two main issues. Firstly, hardware failures are inevitable, leading to interruptions in the training tasks. The inability to quickly identify the faulty components results in a substantial waste of GPU resources. Secondly, since GPUs must wait for parameter synchronization to complete before proceeding to the next round of computation, network congestions can greatly increase the waiting time for GPUs. To address these challenges, this paper introduces a communication-driven solution, namely the C4 . The key insights of C4 are two folds. First, in parallel training, collective communication exhibits periodic and homogeneous characteristics, so any anomalies are certainly due to some form of hardware malfunction. By leveraging this feature, C4 can rapidly identify the faulty components, swiftly isolate the anomaly, and restart the task, thereby avoiding resource wastage caused by delays in anomaly detection. Second, the predictable communication model of collective communication, involving few large flows, allows C4 to efficiently execute traffic planning, substantially reducing network congestion. C4 has been extensively implemented across our production systems, cutting error-induced overhead by roughly 30% and enhancing runtime performance by about 15% for certain applications with moderate communication costs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2402.00159",
        "title": "an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research",
        "abstract": "Information about pretraining corpora used to train the current best-performing language models is seldom discussed: commercial models rarely detail their data, and even open models are often released without accompanying training data or recipes to reproduce them. As a result, it is challenging to conduct and advance scientific research on language modeling, such as understanding how training data impacts model capabilities and limitations. To facilitate scientific research on language model pretraining, we curate and release Dolma, a three-trillion-token English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. We extensively document Dolma, including its design principles, details about its construction, and a summary of its contents. We present analyses and experimental results on intermediate states of Dolma to share what we have learned about important data curation practices. Finally, we open-source our data curation toolkit to enable reproduction of our work as well as support further research in large-scale data curation.1 hf.co/datasets/allenai/dolma github.com/allenai/dolma",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2401.16583",
        "title": "Data-Oblivious ML Accelerators using Hardware Security Extensions",
        "abstract": "Outsourced computation can put client data confidentiality at risk. Existing solutions are either inefficient or insufficiently secure: cryptographic techniques like fully-homomorphic\nencryption incur significant overheads, even with hardware\nassistance, while the complexity of hardware-assisted trusted\nexecution environments has been exploited to leak secret data.\nRecent proposals such as BliMe and OISA show how dynamic\ninformation flow tracking (DIFT) enforced in hardware can protect client data efficiently. They are designed to protect CPU-only\nworkloads. However, many outsourced computing applications,\nlike machine learning, make extensive use of accelerators.\nWe address this gap with Dolma, which applies DIFT to the\nGemmini matrix multiplication accelerator, efficiently guaranteeing client data confidentiality, even in the presence of malicious/vulnerable software and side channel attacks on the server.\nWe show that accelerators can allow DIFT logic optimizations\nthat significantly reduce area overhead compared with generalpurpose processor architectures. Dolma is integrated with the\nBliMe framework to achieve end-to-end security guarantees. We\nevaluate Dolma on an FPGA using a ResNet-50 DNN model\nand show that it incurs low overheads for large configurations\n(4.4%, 16.7%, 16.5% for performance, resource usage and power,\nrespectively, with a 32x32 configuration).",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2402.00838",
        "title": "Accelerating the Science of Language Models",
        "abstract": "Language models (LMs) have become ubiquitous in both NLP research and in commercial\nproduct offerings. As their commercial importance has surged, the most powerful models\nhave become closed off, gated behind proprietary interfaces, with important details of their\ntraining data, architectures, and development\nundisclosed. Given the importance of these\ndetails in scientifically studying these models,\nincluding their biases and potential risks, we\nbelieve it is essential for the research community to have access to powerful, truly open LMs.\nTo this end, we have built OLMo, a competitive, truly Open Language Model, to enable\nthe scientific study of language models. Unlike most prior efforts that have only released\nmodel weights and inference code, we release\nOLMo alongside open training data and training and evaluation code. We hope this release\nwill empower the open research community\nand inspire a new wave of innovation.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.01981",
        "title": "Zyda: A 1.3T Dataset for Open Language Modeling",
        "abstract": "The size of large language models (LLMs) has scaled dramatically in recent\nyears and their computational and data requirements have surged correspondingly.\nState-of-the-art language models, even at relatively smaller sizes, typically require\ntraining on at least a trillion tokens. This rapid advancement has eclipsed the\ngrowth of open-source datasets available for large-scale LLM pretraining. In this\npaper, we introduce Zyda (Zyphra Dataset), a dataset under a permissive license\ncomprising 1.3 trillion tokens, assembled by integrating several major respected\nopen-source datasets into a single, high-quality corpus. We apply rigorous filtering\nand deduplication processes, both within and across datasets, to maintain and\nenhance the quality derived from the original datasets. Our evaluations show that\nZyda not only competes favorably with other open datasets like Dolma, FineWeb,\nand RefinedWeb, but also substantially improves the performance of comparable\nmodels from the Pythia suite. Our rigorous data processing methods significantly\nenhance Zyda\u2019s effectiveness, outperforming even the best of its constituent datasets\nwhen used independently",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2103.12028",
        "title": "Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets",
        "abstract": "With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50% sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2306.13651v2",
        "title": "Bring Your Own Data! Self-Supervised Evaluation of Large Language Models",
        "abstract": "With the rise of Large Language Models (LLMs) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imper- ative. For example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set, which can lead to misleading evaluations. To bypass these drawbacks, we propose a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text. Self-supervised evaluation can directly monitor LLM behavior on datasets collected in the wild or streamed during live model deployment. We demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, toxicity, and long-range context dependence, in addition to sensitivity to grammatical structure and tokenization errors. When comparisons to similar human-labeled benchmarks are available, we find strong correlations between self-supervised and human-supervised evaluations. The self-supervised paradigm complements current evaluation strategies that rely on labeled data. Code is available at https://github.com/neelsjain/BYOD.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.16583v6",
        "title": "GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond",
        "abstract": "With the rapid advancement of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals1 . We systematically evaluate 10+ lead- ing LLMs as well as OpenAI\u2019s legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retro- spective study on OpenAI\u2019s earlier models of- fers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the commu- nity is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM\u2019s reasoning capability, which aspects of LLM ca- pability can be improved by SFT and RLHF, how much is the alignment tax, etc. Our analy- sis sheds light on many of these questions, aim- ing to improve the transparency of advanced LLMs.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2004.07347v3",
        "title": "HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data",
        "abstract": "Existing question answering datasets focus on dealing with homogeneous information, based either only on text or KB/Table information alone. However, as human knowledge is dis- tributed over heterogeneous forms, using ho- mogeneous information alone might lead to severe coverage problems. To fill in the gap, we present HybridQA1, a new large-scale question-answering dataset that requires rea- soning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the en- tities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would ren- der the question unanswerable. We test with three different models: 1) a table-only model. 2) text-only model. 3) a hybrid model that combines heterogeneous information to find the answer. The experimental results show that the EM scores obtained by two baselines are below 20%, while the hybrid model can achieve an EM over 40%. This gap suggests the necessity to aggregate heterogeneous in- formation in HybridQA. However, the hybrid model\u2019s score is still far behind human perfor- mance. Hence, HybridQA can serve as a chal- lenging benchmark to study question answer- ing with heterogeneous information.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2007.15207v2",
        "title": "MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering",
        "abstract": "Progress in cross-lingual modeling depends on challenging, realistic, and diverse evalua- tion sets. We introduce Multilingual Knowl- edge Questions and Answers (MKQA), an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse lan- guages (260k question-answer pairs in to- tal). Answers are based on heavily cu- rated, language-independent data representa- tion, making results comparable across lan- guages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to- date for evaluating question answering. We benchmark a variety of state-of-the-art meth- ods and baselines for generative and extrac- tive question answering, trained on Natural Questions, in zero shot and translation set- tings. Results indicate this dataset is chal- lenging even in English, but especially in low-resource languages.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2007.08124v1",
        "title": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning",
        "abstract": "Machine reading is a fundamental task for test- ing the capability of natural language understand- ing, which is closely related to human cognition in many aspects. With the rising of deep learning techniques, algorithmic models rival human perfor- mances on simple QA, and thus increasingly chal- lenging machine reading datasets have been pro- posed. Though various challenges such as ev- idence integration and commonsense knowledge have been integrated, one of the fundamental ca- pabilities in human reading, namely logical reason- ing, is not fully investigated. We build a compre- hensive dataset, named LogiQA, which is sourced from expert-written questions for testing human Logical reasoning. It consists of 8,678 QA in- stances, covering multiple types of deductive rea- soning. Results show that state-of-the-art neural models perform by far worse than human ceiling. Our dataset can also serve as a benchmark for re- investigating logical AI under the deep learning NLP setting. The dataset is freely available at https://github.com/lgw863/LogiQA-dataset.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1907.09190",
        "title": "ELI5: Long Form Question Answering",
        "abstract": "We introduce the first large-scale corpus for long-form question answering, a task requir- ing elaborate and in-depth answers to open- ended questions. The dataset comprises 270K threads from the Reddit forum \u201cExplain Like I\u2019m Five\u201d (ELI5) where an online community provides answers to questions which are com- prehensible by five year olds. Compared to ex- isting datasets, ELI5 comprises diverse ques- tions requiring multi-sentence answers. We provide a large set of web documents to help answer the question. Automatic and human evaluations show that an abstractive model trained with a multi-task objective outper- forms conventional Seq2Seq, language mod- eling, as well as a strong extractive baseline. However, our best model is still far from hu- man performance since raters prefer gold re- sponses in over 86% of cases, leaving ample opportunity for future improvement.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2109.13880v1",
        "title": "Single-dataset Experts for Multi-dataset Question Answering",
        "abstract": "Many datasets have been created for train- ing reading comprehension models, and a natural question is whether we can combine them to build models that (1) perform better on all of the training datasets and (2) gen- eralize and transfer better to new datasets. Prior work has addressed this goal by train- ing one network simultaneously on multiple datasets, which works well on average but is prone to over- or under-fitting different sub- distributions and might transfer worse com- pared to source models with more overlap with the target dataset. Our approach is to model multi-dataset question answering with a collection of single-dataset experts, by train- ing a collection of lightweight, dataset-specific adapter modules (Houlsby et al., 2019) that share an underlying Transformer model. We find that these Multi-Adapter Dataset Experts (MADE) outperform all our baselines in terms of in-distribution accuracy, and simple meth- ods based on parameter-averaging lead to bet- ter zero-shot generalization and few-shot trans- fer performance, offering a strong and ver- satile starting point for building new reading comprehension systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2101.00294v3",
        "title": "RIDER: Reader-Guided Passage Reranking for Open-Domain Question Answering",
        "abstract": "Current open-domain question answering sys- tems often follow a Retriever-Reader architec- ture, where the retriever first retrieves rele- vant passages and the reader then reads the retrieved passages to form an answer. In this paper, we propose a simple and effec- tive passage reranking method, named Reader- guIDEd Reranker (RIDER), which does not in- volve training and reranks the retrieved pas- sages solely based on the top predictions of the reader before reranking. We show that RIDER, despite its simplicity, achieves 10 to 20 abso- lute gains in top-1 retrieval accuracy and 1 to 4 Exact Match (EM) gains without refining the retriever or reader. In addition, RIDER, with- out any training, outperforms state-of-the-art transformer-based supervised rerankers. Re- markably, RIDER achieves 48.3 EM on the Natural Questions dataset and 66.4 EM on the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are used as the reader in- put after passage reranking.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2210.00063",
        "title": "DECAF: JOINT DECODING OF ANSWERS AND LOGICAL FORMS FOR QUESTION ANSWERING OVER KNOWLEDGE BASES",
        "abstract": "Question answering over knowledge bases (KBs) aims to answer natural language questions with factual information such as entities and relations in KBs. Previous methods either generate logical forms that can be executed over KBs to obtain final answers or predict answers directly. Empirical results show that the former often produces more accurate answers, but it suffers from non-execution issues due to potential syntactic and semantic errors in the generated logical forms. In this work, we propose a novel framework DECAF that jointly generates both logi- cal forms and direct answers, and then combines the merits of them to get the final answers. Moreover, different from most of the previous methods, DECAF is based on simple free-text retrieval without relying on any entity linking tools \u2014 this sim- plification eases its adaptation to different datasets. DECAF achieves new state- of-the-art accuracy on WebQSP, FreebaseQA, and GrailQA benchmarks, while getting competitive results on the ComplexWebQuestions benchmark",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2008.02637v1",
        "title": "Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets",
        "abstract": "Ideally Open-Domain Question Answering models should exhibit a number of competen- cies, ranging from simply memorizing ques- tions seen at training time, to answering novel question formulations with answers seen dur- ing training, to generalizing to completely novel questions with novel answers. However, single aggregated test set scores do not show the full picture of what capabilities models truly have. In this work, we perform a de- tailed study of the test sets of three popular open-domain benchmark datasets with respect to these competencies. We find that 60-70% of test-time answers are also present somewhere in the training sets. We also find that 30% of test-set questions have a near-duplicate para- phrase in their corresponding training sets. Us- ing these findings, we evaluate a variety of pop- ular open-domain models to obtain greater in- sight into what extent they can actually gen- eralize, and what drives their overall perfor- mance. We find that all models perform dra- matically worse on questions that cannot be memorized from training sets, with a mean ab- solute performance difference of 63% between repeated and non-repeated data. Finally we show that simple nearest-neighbor models out- perform a BART closed-book QA model, fur- ther highlighting the role that training set mem- orization plays in these benchmarks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2012.14610v3",
        "title": "UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering",
        "abstract": "We study open-domain question answer- ing with structured, unstructured and semi- structured knowledge sources, including text, tables, lists and knowledge bases. Depart- ing from prior work, we propose a unifying approach that homogenizes all sources by re- ducing them to text and applies the retriever- reader model which has so far been limited to text sources only. Our approach greatly improves the results on knowledge-base QA tasks by 11 points, compared to latest graph- based methods. More importantly, we demon- strate that our unified knowledge (UniK-QA1) model is a simple and yet effective way to combine heterogeneous sources of knowledge, advancing the state-of-the-art results on two popular question answering benchmarks, Nat- uralQuestions and WebQuestions, by 3.5 and 2.6 points, respectively.\n",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1707.03904v2",
        "title": "QUASAR: DATASETS FOR QUESTION ANSWERING BY SEARCH AND READING",
        "abstract": "We present two new large-scale datasets aimed at evaluating systems designed to comprehend a natural language query and extract its answer from a large corpus of text. The QUASAR-S dataset consists of 37000 cloze-style (fill-in-the-gap) queries constructed from definitions of software entity tags on the popular website Stack Overflow. The posts and comments on the website serve as the background cor- pus for answering the cloze questions. The QUASAR-T dataset consists of 43000 open-domain trivia questions and their answers obtained from various internet sources. ClueWeb09 (Callan et al., 2009) serves as the background corpus for ex- tracting these answers. We pose these datasets as a challenge for two related sub- tasks of factoid Question Answering: (1) searching for relevant pieces of text that include the correct answer to a query, and (2) reading the retrieved text to answer the query. We also describe a retrieval system for extracting relevant sentences and doc- uments from the corpus given a query, and include these in the release for researchers wishing to only focus on (2). We evaluate several baselines on both datasets, ranging from simple heuristics to powerful neu- ral models, and show that these lag be- hind human performance by 16.4% and 32.1% for QUASAR-S and -T respectively. The datasets are available at https:// github.com/bdhingra/quasar.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2108.06314v5",
        "title": "A Dataset for Answering Time-Sensitive Questions",
        "abstract": "Time is an important dimension in our physical world. Lots of facts can evolve with respect to time. For example, the U.S. President might change every four years. Therefore, it is important to consider the time dimension and empower the existing QA models to reason over time. However, the existing QA datasets contain rather few time-sensitive questions, hence not suitable for diagnosing or benchmarking the model\u2019s temporal reasoning capability. In order to promote research in this direction, we propose to construct a time-sensitive QA dataset. The dataset is constructed by 1) mining time-evolving facts from WikiData and aligning them to their corresponding Wikipedia page, 2) employing crowd workers to verify and calibrate these noisy facts, 3) generating question-answer pairs based on the annotated time-sensitive facts. Our dataset poses challenges in the aspect of both temporal understanding and temporal reasoning. We evaluate different SoTA long-document QA systems like BigBird and FiD on our dataset. The best- performing model FiD can only achieve 46% accuracy, still far behind the human performance of 87%. We demonstrate that these models are still lacking the ability to perform consistent temporal reasoning. Therefore, we believe that our dataset could serve as a benchmark to develop NLP models more sensitive to temporal shifts. The dataset and code are released in https://github.com/wenhuchen/ Time-Sensitive-QA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2005.10659v1",
        "title": "RuBQ: A Russian Dataset for Question Answering over Wikidata",
        "abstract": "The paper presents RuBQ, the first Russian knowledge base question answering (KBQA) dataset. The high-quality dataset consists of 1,500 Russian questions of varying complexity, their English machine translations, SPARQL queries to Wikidata, reference answers, as well as a Wikidata sample of triples containing entities with Russian labels. The dataset creation started with a large collection of question-answer pairs from online quizzes. The data underwent automatic filtering, crowd- assisted entity linking, automatic generation of SPARQL queries, and their subsequent in-house verification. The freely available dataset will be of interest for a wide community of researchers and practitioners in the areas of Semantic Web, NLP, and IR, especially for those working on multilingual question answering. The proposed dataset generation pipeline proved to be efficient and can be employed in other data anno- tation projects.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.10890v1",
        "title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models",
        "abstract": "Large language models (LLMs) inevitably memorize sensitive, copyrighted, and harmful knowledge from the training corpus; therefore, it is crucial to erase this knowledge from the models. Machine unlearning is a promising solution for effi- ciently removing specific knowledge by post hoc modifying models. In this paper,\nwe propose a Real-World Knowledge Unlearning benchmark ( RWKU) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the task setting, we consider a more practical and challenging unlearning setting, where neither the forget corpus nor the retain corpus is accessible. (2) For the knowledge source, we choose 200 real-world famous people as the unlearning targets and show that such popular knowledge is widely present in various LLMs. (3) For the evaluation framework, we design the forget set and the retain set to evaluate the model\u2019s capabilities across various real-world applications. Regard- ing the forget set, we provide four membership inference attack (MIA) methods and nine kinds of adversarial attack probes to rigorously test unlearning efficacy. Regarding the retain set, we assess locality and utility in terms of neighbor pertur- bation, general ability, reasoning ability, truthfulness, factuality, and fluency. We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods and obtain some meaningful findings. We release our benchmark and code publicly at http://rwku-bench.github.io for future work.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2110.07731v2",
        "title": "CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training",
        "abstract": "We propose a novel open-domain question- answering dataset based on the Common Crawl project. With a previously unseen number of around 130 million multilingual question-answer pairs (including about 60 mil- lion English data-points), we use our large- scale, natural, diverse and high-quality corpus to in-domain pre-train popular language mod- els for the task of question-answering. In our experiments, we find that our Common Crawl Question Answering dataset (CCQA) achieves promising results in zero-shot, low resource and fine-tuned settings across multiple tasks, models and benchmarks",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2005.02507v1",
        "title": "MultiReQA: A Cross-Domain Evaluation for Retrieval Question Answering Models",
        "abstract": "Retrieval question answering (ReQA) is the task of retrieving a sentence-level answer to a question from an open corpus (Ahmad et al., 2019). This paper presents MultiReQA, a new multi-domain ReQA evaluation suite com- posed of eight retrieval QA tasks drawn from publicly available QA datasets1 . We provide the first systematic retrieval based evaluation over these datasets using two supervised neu- ral models, based on fine-tuning BERT and USE-QA models respectively, as well as a sur- prisingly strong information retrieval baseline, BM25. Five of these tasks contain both train- ing and test data, while three contain test data only. Performance on the five tasks with train- ing data shows that while a general model cov- ering all domains is achievable, the best perfor- mance is often obtained by training exclusively on in-domain data.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.14393v1",
        "title": "Merging Generated and Retrieved Knowledge for Open-Domain QA",
        "abstract": "Open-domain question answering (QA) sys- tems are often built with retrieval modules. However, retrieving passages from a given source is known to suffer from insufficient knowledge coverage. Alternatively, prompt- ing large language models (LLMs) to gener- ate contextual passages based on their para- metric knowledge has been shown to improve QA performance. Yet, LLMs tend to \u201chal- lucinate\u201d content that conflicts with the re- trieved knowledge. Based on the intuition that answers supported by both sources are more likely to be correct, we propose COMBO, a Compatibility-Oriented knowledge Merging for Better Open-domain QA framework, to effec- tively leverage the two sources of informa- tion. Concretely, we match LLM-generated passages with retrieved counterparts into com- patible pairs, based on discriminators trained with silver compatibility labels. Then a Fusion- in-Decoder-based (Izacard and Grave, 2021b) reader model handles passage pairs to arrive at the final answer. Experiments show that COMBO outperforms competitive baselines on three out of four tested open-domain QA benchmarks. Further analysis reveals that our proposed framework demonstrates greater effi- cacy in scenarios with a higher degree of knowl- edge conflicts.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2108.08614v8",
        "title": "UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text",
        "abstract": "Question answering over RDF data like knowledge graphs has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents a method for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph typically contains all question-relevant evidences but also a lot of noise. UNIQORN copes with this input by a graph algorithm for Group Steiner Trees, that identifies the best answer candidates in the context graph. Experimental results on several benchmarks of complex questions with multiple entities and relations, show that UNIQORN significantly outperforms state-of-the-art methods for heterogeneous QA \u2013 in a full training mode, as well as in zero-shot settings. The graph-based methodology provides user-interpretable evidence for the complete answering process.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2305.12421v4",
        "title": "Evaluating Open-QA Evaluation",
        "abstract": "This study focuses on the evaluation of the Open Question Answering (Open- QA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes human- annotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this area. All resources are available at https://github.com/wangcunxiang/QA-Eval and it is under the Apache-2.0 License.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2112.01922v4",
        "title": "MetaQA: Combining Expert Agents for Multi-Skill Question Answering",
        "abstract": "The recent explosion of question-answering (QA) datasets and models has increased the in- terest in the generalization of models across multiple domains and formats by either train- ing on multiple datasets or combining multi- ple models. Despite the promising results of multi-dataset models, some domains or QA formats may require specific architectures, and thus the adaptability of these models might be limited. In addition, current approaches for combining models disregard cues such as question-answer compatibility. In this work, we propose to combine expert agents with a novel, flexible, and training-efficient architec- ture that considers questions, answer predic- tions, and answer-prediction confidence scores to select the best answer among a list of answer predictions. Through quantitative and quali- tative experiments, we show that our model i) creates a collaboration between agents that outperforms previous multi-agent and multi- dataset approaches, ii) is highly data-efficient to train, and iii) can be adapted to any QA for- mat. We release our code and a dataset of answer predictions from expert agents for 16 QA datasets to foster future research of multi- agent systems",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2112.09332v3",
        "title": "WebGPT: Browser-assisted question-answering with human feedback",
        "abstract": "We fine-tune GPT-3 to answer long-form questions using a text-based web- browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model\u2019s answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2112.09924v2",
        "title": "The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus",
        "abstract": "In order to address increasing demands of real-world applications, the research for knowledge-intensive NLP (KI-NLP) should advance by capturing the challenges of a truly open-domain environment: web-scale knowl- edge, lack of structure, inconsistent quality and noise. To this end, we propose a new setup for evaluating existing knowledge inten- sive tasks in which we generalize the back- ground corpus to a universal web snapshot. We investigate a slate of NLP tasks which rely on knowledge - either factual or com- mon sense, and ask systems to use a subset of CCNet\u2014the SPHERE corpus\u2014as a knowl- edge source. In contrast to Wikipedia, other- wise a common background corpus in KI-NLP, SPHERE is orders of magnitude larger and bet- ter reflects the full diversity of knowledge on the web. Despite potential gaps in coverage, challenges of scale, lack of structure and lower quality, we find that retrieval from SPHERE en- ables a state of the art system to match and even outperform Wikipedia-based models on several tasks. We also observe that while a dense index can outperform a sparse BM25 baseline on Wikipedia, on SPHERE this is not yet possible. To facilitate further research and minimise the community\u2019s reliance on propri- etary, black-box search engines, we share our indices, evaluation metrics and infrastructure.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.17012v1",
        "title": "BENCHMARKING COGNITIVE BIASES IN LARGE LANGUAGE MODELS AS EVALUATORS",
        "abstract": "Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the COgnitive Bias Benchmark for LLMs as EvaluatoRs (COBBLER)1, a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the EGOCENTRIC bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 49.6%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.07138v2",
        "title": "WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models",
        "abstract": "To mitigate the potential misuse of large lan- guage models (LLMs), recent research has de- veloped watermarking algorithms, which re- strict the generation process to leave an invisi- ble trace for watermark detection. Due to the two-stage nature of the task, most studies eval- uate the generation and detection separately, thereby presenting a challenge in unbiased, thorough, and applicable evaluations. In this paper, we introduce WaterBench, the first com- prehensive benchmark for LLM watermarks, in which we design three crucial factors: (1) For benchmarking procedure, to ensure an apples- to-apples comparison, we first adjust each wa- termarking method\u2019s hyper-parameter to reach the same watermarking strength, then jointly evaluate their generation and detection perfor- mance. (2) For task selection, we diversify the input and output length to form a five-category taxonomy, covering 9 tasks. (3) For evaluation metric, we adopt the GPT4-Judge for auto- matically evaluating the decline of instruction- following abilities after watermarking. We eval- uate 4 open-source watermarks on 2 LLMs under 2 watermarking strengths and observe the common struggles for current methods on maintaining the generation quality. The code and data are available at https://github. com/THU-KEG/WaterBench.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2402.15089v1",
        "title": "AttributionBench: How Hard is Automatic Attribution Evaluation?",
        "abstract": "Modern generative search engines enhance the reliability of large language model (LLM) re- sponses by providing cited evidence. How- ever, evaluating the answer\u2019s attribution, i.e., whether every claim within the generated re- sponses is fully supported by its cited evi- dence, remains an open problem. This veri- fication, traditionally dependent on costly hu- man evaluation, underscores the urgent need for automatic attribution evaluation methods. To bridge the gap in the absence of standard- ized benchmarks for these methods, we present AttributionBench, a comprehensive bench- mark compiled from various existing attri- bution datasets. Our extensive experiments on AttributionBench reveal the challenges of automatic attribution evaluation, even for state-of-the-art LLMs. Specifically, our find- ings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error cases indicates that a majority of failures stem from the model\u2019s in- ability to process nuanced information, and the discrepancy between the information the model has access to and that human annotators do. ",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.19740v1",
        "title": "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation",
        "abstract": "Humans are widely involved in the evalua- tion of open-ended natural language genera- tion tasks (NLG) that demand creativity, as automatic metrics often exhibit weak correla- tions with human judgments. Large language models (LLMs) recently have emerged as a scalable and cost-effective alternative to hu- man evaluations. However, both humans and LLMs have limitations, i.e., inherent subjectiv- ity and unreliable judgments, particularly for open-ended tasks that require adaptable met- rics tailored to diverse task requirements. To explore the synergy between humans and LLM- based evaluators and address the challenges of existing inconsistent evaluation criteria in open- ended NLG tasks, we propose a Collaborative Evaluation pipeline COEVAL, involving the design of a checklist of task-specific criteria and the detailed evaluation of texts, in which LLM generates initial ideation, and then hu- mans engage in scrutiny. We conducted a series of experiments to investigate the mutual effects between LLMs and humans in COEVAL. Re- sults show that, by utilizing LLMs, COEVAL effectively evaluates lengthy texts, saving sig- nificant time and reducing human evaluation outliers. Human scrutiny still plays a role, re- vising around 20% of LLM evaluation scores for ultimate reliability.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1806.00920",
        "title": "DRCD: a Chinese Machine Reading Comprehension Dataset",
        "abstract": "In this paper, we introduce DRCD (Delta\nReading Comprehension Dataset), an open\ndomain traditional Chinese machine\nreading comprehension (MRC) dataset.\nThis dataset aimed to be a standard\nChinese machine reading comprehension\ndataset, which can be a source dataset in\ntransfer learning. The dataset contains\n10,014 paragraphs from 2,108 Wikipedia\narticles and 30,000+ questions generated\nby annotators. We build a baseline model\nthat achieves an F1 score of 89.59%. F1\nscore of Human performance is 93.30%.\nThe dataset is available at\nhttps://github.com/DRCKnowledgeTeam/\nDRCD.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.08448v2",
        "title": "Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite",
        "abstract": "The evaluation of large language models is an essential task in the field of language understanding and generation. As language models continue to advance, the need for effective benchmarks to assess their performance has become imperative. In the context of Traditional Chinese, there is a scarcity of comprehensive and diverse benchmarks to evaluate the capabilities of language models, despite the existence of certain benchmarks such as DRCD, TTQA, CMDQA, and FGC dataset. To address this gap, we propose a novel set of benchmarks that leverage existing English datasets and are tailored to evaluate language models in Traditional Chinese. These benchmarks encompass a wide range of tasks, including contextual question-answering, summarization, classification, and table understanding. The proposed benchmarks offer a comprehensive evaluation framework, enabling the assessment of language models\u2019 capabilities across different tasks. In this paper, we evaluate the performance of GPT-3.5, Taiwan-LLaMa-v1.0, and Model 7-C, our proprietary model series, on these benchmarks. The evaluation results highlight that Model 7-C achieves performance comparable to GPT-3.5 with respect to a part of the evaluated capabilities. In an effort to advance the evaluation of language models in Traditional Chinese and stimulate further research in this field, we have open-sourced our benchmark and opened the model for trial.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1808.02280v1",
        "title": "ODSQA: OPEN-DOMAIN SPOKEN QUESTION ANSWERING DATASET",
        "abstract": "ODSQA: OPEN-DOMAIN SPOKEN QUESTION ANSWERING DATASET",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2006.12146v1",
        "title": "ReCO: A Large Scale Chinese Reading Comprehension Dataset on Opinion",
        "abstract": "This paper presents the ReCO, a human-curated Chinese Reading Comprehension dataset on Opinion. The questions in ReCO are opinion based queries issued to commercial search engine. The passages are provided by the crowdwork- ers who extract the support snippet from the retrieved doc- uments. Finally, an abstractive yes/no/uncertain answer was given by the crowdworkers. The release of ReCO consists of 300k questions that to our knowledge is the largest in Chi- nese reading comprehension. A prominent characteristic of ReCO is that in addition to the original context paragraph, we also provided the support evidence that could be directly used to answer the question. Quality analysis demonstrates the challenge of ReCO that it requires various types of rea- soning skills such as causal inference, logical reasoning, etc. Current QA models that perform very well on many ques- tion answering problems, such as BERT (Devlin et al. 2018), only achieves 77% accuracy on this dataset, a large margin behind humans nearly 92% performance, indicating ReCO present a good challenge for machine reading comprehen- sion. The codes, dataset and leaderboard will be freely avail- able at https://github.com/benywon/ReCO.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2403.16008",
        "title": "CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering",
        "abstract": "The recent advancements in artificial intelligence highlight the potential of language models in psychological health support. While models trained on data from mental health service platform have achieved preliminary success, challenges persist in areas such as data scarcity, quality, and ensuring a solid foundation in psychological techniques. To address these challenges, this study introduces a novel approach to enhance the precision and efficacy of psychological support through large language models. Specifically, we design a specific prompt derived from principles of Cognitive Behavioral Therapy (CBT) and have generated the CBT QA dataset, specifically for Chinese psychological health Q&A based on CBT structured intervention strategies. Unlike previous methods, our dataset emphasizes professional and structured response. Utilizing this dataset, we fine-tuned the large language model, giving birth to CBT-LLM, the large-scale language model specifically designed for Cognitive Behavioral Therapy techniques. Empirical evaluations demonstrate that CBT-LLM excels in generating structured, professional, and highly relevant responses in psychological health support tasks, showcasing its practicality and quality. The model is available on Hugging Face: https://huggingface.co/Hongbin37/CBT-LLM.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1611.09268",
        "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset",
        "abstract": "We introduce a large scale MAchine Reading COmprehension dataset, which we name MS MARCO. The dataset comprises of 1,010,916 anonymized questions\u2014 sampled from Bing\u2019s search query logs\u2014each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages\u2014extracted from 3,563,535 web documents retrieved by Bing\u2014that provide the information necessary for curating the natural language answers. A question in the MS MARCO dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes MS MARCO from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2108.13897",
        "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
        "abstract": "The MS MARCO ranking dataset has been widely used for training deep learning models for IR tasks, achieving considerable effective- ness on diverse zero-shot scenarios. However, this type of resource is scarce in languages other than English. In this work, we present mMARCO, a multilingual version of the MS MARCO passage ranking dataset comprising 13 languages that was created using machine translation. We evaluated mMARCO by fine- tuning monolingual and multilingual rerank- ing models, as well as a multilingual dense retrieval model on this dataset. We also eval- uated models finetuned using the mMARCO dataset in a zero-shot scenario on Mr. TyDi dataset, demonstrating that multilingual mod- els finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone. Our ex- periments also show that a distilled multilin- gual reranker is competitive with non-distilled models while having 5.4 times fewer param- eters. Lastly, we show a positive correlation between translation quality and retrieval ef- fectiveness, providing evidence that improve- ments in translation methods might lead to improvements in multilingual information re- trieval. The translated datasets and finetuned models are available at https://github. com/unicamp-dl/mMARCO.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.11694v2",
        "title": "QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations",
        "abstract": "Formulating selective information needs results in queries that implicitly specify set operations, such as intersection, union, and difference. For instance, one might search for \"shorebirds that are not sandpipers\" or \"science-fiction films shot in England\". To study the ability of re- trieval systems to meet such information needs, we construct QUEST, a dataset of 3357 natu- ral language queries with implicit set opera- tions, that map to a set of entities correspond- ing to Wikipedia documents. The dataset chal- lenges models to match multiple constraints mentioned in queries with corresponding evi- dence in documents and correctly perform vari- ous set operations. The dataset is constructed semi-automatically using Wikipedia category names. Queries are automatically composed from individual categories, then paraphrased and further validated for naturalness and flu- ency by crowdworkers. Crowdworkers also assess the relevance of entities based on their documents and highlight attribution of query constraints to spans of document text. We an- alyze several modern retrieval systems, find- ing that they often struggle on such queries. Queries involving negation and conjunction are particularly challenging and systems are further challenged with combinations of these opera- tions.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2104.08663v4",
        "title": "BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models",
        "abstract": "Existing neural information retrieval (IR) models have often been studied in ho- mogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facili- tate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the- art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction based models on average achieve the best zero- shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2203.10232v4",
        "title": "DuReaderretrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine",
        "abstract": "In this paper, we present DuReaderretrieval, a large-scale Chinese dataset for passage re- trieval. DuReaderretrieval contains more than 90K queries and over 8M unique passages from a commercial search engine. To alle- viate the shortcomings of other datasets and ensure the quality of our benchmark, we (1) reduce the false negatives in development and test sets by manually annotating results pooled from multiple retrievers, and (2) re- move the training queries that are semanti- cally similar to the development and testing queries. Additionally, we provide two out- of-domain testing sets for cross-domain eval- uation, as well as a set of human translated queries for for cross-lingual retrieval eval- uation. The experiments demonstrate that DuReaderretrieval is challenging and a num- ber of problems remain unsolved, such as the salient phrase mismatch and the syntactic mis- match between queries and paragraphs. These experiments also show that dense retrievers do not generalize well across domains, and cross-lingual retrieval is essentially challeng- ing. DuReaderretrieval is publicly available at https://github.com/baidu/DuReader/ tree/master/DuReader-Retrieval.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.02867v1",
        "title": "Benchmarking Middle-Trained Language Models for Neural Search",
        "abstract": "Middle training methods aim to bridge the gap between the Masked Language Model (MLM) pre-training and the final finetuning for retrieval. Recent models such as CoCondenser, RetroMAE, and LexMAE argue that the MLM task is not sufficient enough to pre- train a transformer network for retrieval and hence propose vari- ous tasks to do so. Intrigued by those novel methods, we noticed that all these models used different finetuning protocols, making it hard to assess the benefits of middle training. We propose in this paper a benchmark of CoCondenser, RetroMAE, and LexMAE, under the same finetuning conditions. We compare both dense and sparse approaches under various finetuning protocols and middle training on different collections (MS MARCO, Wikipedia or Tripclick). We use additional middle training baselines, such as a standard MLM finetuning on the retrieval collection, option- ally augmented by a CLS predicting the passage term frequency. For the sparse approach, our study reveals that there is almost no statistical difference between those methods: the more effec- tive the finetuning procedure is, the less difference there is be- tween those models. For the dense approach, RetroMAE using MS MARCO as middle-training collection shows excellent results in almost all the settings. Finally, we show that middle training on the retrieval collection, thus adapting the language model to it, is a critical factor. Overall, a better experimental setup should be adopted to evaluate middle training methods. Code available at https://github.com/naver/splade/tree/benchmarch-SIGIR23",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2206.06588v1",
        "title": "Shopping Queries Dataset: A Large-Scale ESCI Benchmark for Improving Product Search",
        "abstract": "Improving the quality of search results can significantly enhance users experience and engagement with search engines. In spite of several recent advancements in the fields of machine learning and data mining, correctly classifying items for a particular user search query has been a long standing challenge, which still has a large room for improvement. This paper introduces the \u201cShop- ping Queries Dataset\u201d, a large dataset of difficult Amazon search queries and results, publicly released with the aim of fostering research in improving the quality of search results. The dataset contains around 130 thousand unique queries and 2.6 million man- ually labeled (query,product) relevance judgements. The dataset is multilingual with queries in English, Japanese, and Spanish. The Shopping Queries Dataset is being used in one of the KDDCup\u201922 challenges. In this paper, we describe the dataset and present three evaluation tasks along with baseline results: (i) ranking the results list, (ii) classifying product results into relevance categories, and (iii) identifying substitute products for a given query. We anticipate that this data will become the gold standard for future research in the topic of product search.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1704.05179v3",
        "title": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine",
        "abstract": "We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article and generate a question-answer pair, but start from an ex- isting question-answer pair, crawled from J! Archive, and augment it with text snip- pets retrieved by Google. Following this approach, we built SearchQA, which con- sists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with ad- ditional meta-data such as the snippet\u2019s URL, which we believe will be valuable resources for future research. We conduct human evaluation as well as test two base- line methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a mean- ingful gap between the human and ma- chine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1905.10044v1",
        "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions",
        "abstract": "In this paper we study yes/no questions that are naturally occurring \u2014 meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid informa- tion, and require difficult entailment-like infer- ence to solve. We also explore the effective- ness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from para- phrase or extractive QA data, and that it, sur- prisingly, continues to be very beneficial even when starting from massive pre-trained lan- guage models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human anno- tators (and 62% majority-baseline), leaving a significant gap for future work.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1908.04364v2",
        "title": "AmazonQA: A Review-Based Question Answering Task",
        "abstract": "Every day, thousands of customers post questions\non Amazon product pages. After some time, if they\nare fortunate, a knowledgeable customer might an-\nswer their question. Observing that many questions\ncan be answered based upon the available prod-\nuct reviews, we propose the task of review-based\nQA. Given a corpus of reviews and a question, the\nQA system synthesizes an answer. To this end,\nwe introduce a new dataset and propose a method\nthat combines information retrieval techniques for\nselecting relevant reviews (given a question) and\n\u201creading comprehension\u201d models for synthesizing\nan answer (given a question and review). Our\nthe answers posted to specific questions; and (iii) follow- dataset consists of 923k questions, 3.6M answers\ning human-in-the-loop experiments, we discovered that more and 14M reviews across 156k products. Build-\nthan half of the questions can (at least partially) be answered ing on the well-known Amazon dataset, we col-\nlect additional annotations, marking each question\nas either answerable or unanswerable based on the\navailable reviews. A deployed system could first\nclassify a question as answerable and then attempt\nto generate an answer. Notably, unlike many popular QA datasets, here the questions, passages, and\nanswers are all extracted from real human interac-\ntions. We evaluate numerous models for answer\ngeneration and propose strong baselines, demon-\nstrating the challenging nature of this new task.\n",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2108.08787v2",
        "title": "Mr. TYDI: A Multi-lingual Benchmark for Dense Retrieval",
        "abstract": "We present Mr. TYDI, a multi-lingual bench- mark dataset for mono-lingual retrieval in eleven typologically diverse languages, de- signed to evaluate ranking with learned dense representations. The goal of this resource is to spur research in dense retrieval techniques in non-English languages, motivated by recent observations that existing techniques for rep- resentation learning perform poorly when ap- plied to out-of-distribution data. As a start- ing point, we provide zero-shot baselines for this new dataset based on a multi-lingual adap- tation of DPR that we call \u201cmDPR\u201d. Experi- ments show that although the effectiveness of mDPR is much lower than BM25, dense repre- sentations nevertheless appear to provide valu- able relevance signals, improving BM25 re- sults in sparse\u2013dense hybrids. In addition to analyses of our results, we also discuss future challenges and present a research agenda in multi-lingual dense retrieval. Mr. TYDI can be downloaded at https://github.com/ castorini/mr.tydi.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2105.07975v1",
        "title": "How Deep is your Learning: the DL-HARD Annotated Deep Learning Dataset",
        "abstract": "Deep Learning Hard (DL-HARD) is a new annotated dataset designed to more effectively evaluate neural ranking models on complex top- ics. It builds on TREC Deep Learning (DL) topics by extensively annotating them with question intent categories, answer types, wikified entities, topic categories, and result type metadata from a commercial web search engine. Based on this data, we introduce a framework for identifying challenging queries. DL-HARD contains fifty topics from the official DL 2019/2020 evaluation benchmark, half of which are newly and independently assessed. We perform experiments using the official submitted runs to DL on DL-HARD and find substantial differences in metrics and the ranking of partic- ipating systems. Overall, DL-HARD is a new resource that promotes research on neural ranking methods by focusing on challenging and complex topics.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2109.08365v1",
        "title": "CodeQA: A Question Answering Dataset for Source Code Comprehension",
        "abstract": "We propose CodeQA, a free-form question an- swering dataset for the purpose of source code comprehension: given a code snippet and a question, a textual answer is required to be gen- erated. CodeQA contains a Java dataset with 119,778 question-answer pairs and a Python dataset with 70,085 question-answer pairs. To obtain natural and faithful questions and an- swers, we implement syntactic rules and se- mantic analysis to transform code comments into question-answer pairs. We present the construction process and conduct systematic analysis of our dataset. Experiment results achieved by several neural baselines on our dataset are shown and discussed. While re- search on question-answering and machine reading comprehension develops rapidly, few prior work has drawn attention to code ques- tion answering. This new dataset can serve as a useful research benchmark for source code comprehension.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.13915v4",
        "title": "DAPR: A Benchmark on Document-Aware Passage Retrieval",
        "abstract": "The work of neural retrieval so far focuses on ranking short texts and is challenged with long documents. There are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. Wikipedia articles, research papers, etc. We propose and name this task Document-Aware Passage Retrieval (DAPR). While analyzing the errors of the State-of-The-Art (SoTA) passage retrievers, we find the major errors (53.5%) are due to missing document context. This drives us to build a benchmark for this task including multiple datasets from heterogeneous domains. In the experiments, we extend the SoTA passage retrievers with document context via (1) hybrid retrieval with BM25 and (2) con- textualized passage representations, which in- form the passage representation with document context. We find despite that hybrid retrieval performs the strongest on the mixture of the easy and the hard queries, it completely fails on the hard queries that require document-context understanding. On the other hand, contextual- ized passage representations (e.g. prepending document titles) achieve good improvement on these hard queries, but overall they also per- form rather poorly. Our created benchmark enables future research on developing and com- paring retrieval systems for the new task. The code and the data are available",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2105.12936v1",
        "title": "Corpus-Level Evaluation for Event QA:\nThe IndiaPoliceEvents Corpus Covering the 2002 Gujarat Violence",
        "abstract": "Automated event extraction in social science applications often requires corpus-level evalu- ations: for example, aggregating text predic- tions across metadata and unbiased estimates of recall. We combine corpus-level evalu- ation requirements with a real-world, social science setting and introduce the INDIAPO- LICEEVENTS corpus\u2014all 21,391 sentences from 1,257 English-language Times of India articles about events in the state of Gujarat during March 2002. Our trained annotators read and label every document for mentions of police activity events, allowing for unbi- ased recall evaluations. In contrast to other datasets with structured event representations, we gather annotations by posing natural ques- tions, and evaluate off-the-shelf models for three different tasks: sentence classification, document ranking, and temporal aggregation of target events. We present baseline results from zero-shot BERT-based models fine-tuned on natural language inference and passage re- trieval tasks. Our novel corpus-level eval- uations and annotation approach can guide creation of similar social-science-oriented re- sources in the future.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2402.12177",
        "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
        "abstract": "Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre- trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses sce- narios where the embeddings are only available from a black-box model. We in- troduce Model augmented fine-tuning (Mafin) \u2013 a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, illustrating its broad applicability and efficiency.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2005.02507v1",
        "title": "MultiReQA: A Cross-Domain Evaluation for Retrieval Question Answering Models",
        "abstract": "Retrieval question answering (ReQA) is the task of retrieving a sentence-level answer to a question from an open corpus (Ahmad et al., 2019). This paper presents MultiReQA, a new multi-domain ReQA evaluation suite com- posed of eight retrieval QA tasks drawn from publicly available QA datasets1 . We provide the first systematic retrieval based evaluation over these datasets using two supervised neu- ral models, based on fine-tuning BERT and USE-QA models respectively, as well as a sur- prisingly strong information retrieval baseline, BM25. Five of these tasks contain both train- ing and test data, while three contain test data only. Performance on the five tasks with train- ing data shows that while a general model cov- ering all domains is achievable, the best perfor- mance is often obtained by training exclusively on in-domain data.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2107.04374v1",
        "title": "Benchmarking for Biomedical Natural Language Processing Tasks with a Domain Specific ALBERT",
        "abstract": "The availability of biomedical text data and advances in natural language processing (NLP) have made new applications in biomedical NLP possible. Language models trained or fine-tuned using domain-specific corpora can outperform general models, but work to date in biomedical NLP has been limited in terms of corpora and tasks. We present BioALBERT, a domain-specific adaptation of A Lite Bidirectional Encoder Representations from Transformers (ALBERT), trained on biomedical (PubMed and PubMed Central) and clinical (MIMIC-III) corpora and fine-tuned for 6 different tasks across 20 benchmark datasets. Experiments show that BioALBERT outperforms the state-of-the-art on named-entity recognition (+11.09% BLURB score improvement), relation extraction (+0.80% BLURB score), sentence similarity (+1.05% BLURB score), document classification (+0.62% F1-score), and question answering (+2.83% BLURB score). It represents a new state-of-the-art in 17 out of 20 benchmark datasets. By making BioALBERT models and data available, our aim is to help the biomedical NLP community avoid computational costs of training and establish a new set of baselines for future efforts across a broad range of biomedical NLP tasks.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1909.06146v1",
        "title": "PubMedQA: A Dataset for Biomedical Research Question Answering",
        "abstract": "We introduce PubMedQA, a novel biomedi- cal question answering (QA) dataset collected from PubMed abstracts. The task of Pub- MedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding ab- stracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially gen- erated QA instances. Each PubMedQA in- stance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the re- search question, and (4) a yes/no/maybe an- swer which summarizes the conclusion. Pub- MedQA is the first QA dataset where rea- soning over biomedical research texts, espe- cially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1% accu- racy, compared to single human performance of 78.0% accuracy and majority-baseline of 55.2% accuracy, leaving much room for im- provement. PubMedQA is publicly available at https://pubmedqa.github.io.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2005.06376v1",
        "title": "BIOMRC: A Dataset for Biomedical Machine Reading Comprehension",
        "abstract": "\nWe introduce BIOMRC, a large-scale cloze- style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018). Ex- periments show that simple heuristics do not perform well on the new dataset, and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible. Non-expert human performance is also higher on the new dataset compared to BIOREAD, and biomedical experts perform even better. We also introduce a new BERT-based MRC model, the best version of which substantially outper- forms all other methods tested, reaching or sur- passing the accuracy of biomedical experts in some experiments. We make the new dataset available in three different sizes, also releasing our code, and providing a leaderboard.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2402.15838",
        "title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval",
        "abstract": "We propose LISTT5, a novel reranking ap- proach based on Fusion-in-Decoder (FiD) that handles multiple candidate passages at both train and inference time. We also introduce an efficient inference framework for listwise rank- ing based on m-ary tournament sort with output caching. We evaluate and compare our model on the BEIR benchmark for zero-shot retrieval task, demonstrating that LISTT5 (1) outper- forms the state-of-the-art RankT5 baseline with a notable +1.3 gain in the average NDCG@10 score, (2) has an efficiency comparable to pointwise ranking models and surpasses the efficiency of previous listwise ranking mod- els, and (3) overcomes the lost-in-the-middle problem of previous listwise rerankers. Our code, model checkpoints, and the evaluation framework are fully open-sourced at https: //github.com/soyoung97/ListT5.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2404.05825",
        "title": "LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language Models and Doc-Level Embedding",
        "abstract": "Recently embedding-based retrieval or dense retrieval have shown state of the art results, compared with traditional sparse or bag-of- words based approaches. This paper introduces a model-agnostic doc-level embedding frame- work through large language model (LLM) augmentation. In addition, it also improves some important components in the retrieval model training process, such as negative sam- pling, loss function, etc. By implementing this LLM-augmented retrieval framework, we have been able to significantly improve the effec- tiveness of widely-used retriever models such as Bi-encoders (Contriever, DRAGON) and late-interaction models (ColBERTv2), thereby achieving state-of-the-art results on LoTTE datasets and BEIR datasets.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2112.01488",
        "title": "ColBERTv2:\nEffective and Efficient Retrieval via Lightweight Late Interaction",
        "abstract": "Neural information retrieval (IR) has greatly advanced search and other knowledge- intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector repre- sentations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compres- sion mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6\u201310\u00d7.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2209.13738",
        "title": "mRobust04: A Multilingual Version of the TREC Robust 2004 Benchmark",
        "abstract": "Robust 2004 is an information retrieval benchmark whose large number of judgments per query make it a reliable evaluation dataset. In this paper, we present mRobust04, a multilingual version of Robust04 that was trans- lated to 8 languages using Google Translate. We also provide results of three different multilingual retrievers on this dataset. The dataset is available at https://huggingface.co/datasets/unicamp-dl/mrobust",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1812.10315",
        "title": "DBpedia NIF: Open, Large-Scale and Multilingual Knowledge Extraction Corpus",
        "abstract": "In the past decade, the DBpedia community has put significant amount of effort on developing technical infrastructure and methods for efficient extraction of structured information from Wikipedia. These efforts have been primarily focused on harvesting, refinement and publishing semi- structured information found in Wikipedia articles, such as information from infoboxes, categorization information, images, wikilinks and cita- tions. Nevertheless, still vast amount of valuable information is contained in the unstructured Wikipedia article texts. In this paper, we present DBpedia NIF - a large-scale and multilingual knowledge extraction cor- pus. The aim of the dataset is two-fold: to dramatically broaden and deepen the amount of structured information in DBpedia, and to provide large-scale and multilingual language resource for development of various NLP and IR task. The dataset provides the content of all articles for 128 Wikipedia languages. We describe the dataset creation process and the NLP Interchange Format (NIF) used to model the content, links and the structure the information of the Wikipedia articles. The dataset has been further enriched with about 25% more links and selected partitions published as Linked Data. Finally, we describe the maintenance and sus- tainability plans, and selected use cases of the dataset from the TextExt knowledge extraction challenge.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2004.07180",
        "title": "SPECTER: Document-level Representation Learning using Citation-informed Transformers",
        "abstract": "Representation learning is a critical ingre- dient for natural language processing sys- tems. Recent Transformer language mod- els like BERT learn powerful textual repre- sentations, but these models are targeted to- wards token- and sentence-level training ob- jectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For ap- plications on scientific documents, such as classification and recommendation, the em- beddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scien- tific documents based on pretraining a Trans- former language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we intro- duce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks rang- ing from citation prediction, to document clas- sification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2211.13308v4",
        "title": "SciRepEval: A Multi-Format Benchmark for Scientific Document Representations",
        "abstract": "Learned representations of scientific docu- ments can serve as valuable input features for downstream tasks without further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diver- sity of relevant tasks. In response, we in- troduce SciRepEval, the first comprehensive benchmark for training and evaluating scien- tific document representations. It includes 24 challenging and realistic tasks, 8 of which are new, across four formats: classification, regres- sion, ranking and search. We then use this benchmark to study and improve the general- ization ability of scientific document represen- tation models. We show how state-of-the-art models like SPECTER and SciNCL struggle to generalize across the task formats, and that sim- ple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters and find they outperform the existing single-embedding state-of-the-art by over 2 points absolute. We release the re- sulting family of multi-format models, called SPECTER2, for the community to use and build on.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1803.05355",
        "title": "FEVER: a large-scale dataset for Fact Extraction and VERification",
        "abstract": "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences ex- tracted from Wikipedia and subsequently verified without knowledge of the sen- tence they were derived from. The claims are classified as SUPPORTED, RE- FUTED or NOTENOUGHINFO by annota- tors achieving 0.6841 in Fleiss \u03ba. For the first two classes, the annotators also recorded the sentence(s) forming the nec- essary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed ora- cles. The best accuracy we achieve on la- beling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we be- lieve that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2012.00614",
        "title": "climate-fever: A Dataset for Verification of Real-World Climate Claims",
        "abstract": "We introduce climate-fever, a new publicly available dataset for verification of climate change-related claims. By providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. We adapt the methodology of fever [1], the largest dataset of artificially designed claims, to real- life claims collected from the Internet. While during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. We discuss the surprising, subtle complexity of modeling real-world climate-related claims within the fever framework, which we believe provides a valuable challenge for general natural language understanding. We hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and ai community.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.09818v1",
        "title": "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures",
        "abstract": "To handle the vast amounts of qualitative data produced in corporate climate communica- tion, stakeholders increasingly rely on Retrieval Augmented Generation (RAG) systems. How- ever, a significant gap remains in evaluating domain-specific information retrieval \u2013 the ba- sis for answer generation. To address this chal- lenge, this work simulates the typical tasks of a sustainability analyst by examining 30 sustain- ability reports with 16 detailed climate-related questions. As a result, we obtain a dataset with over 8.5K unique question-source-answer pairs labeled by different levels of relevance. Furthermore, we develop a use case with the dataset to investigate the integration of expert knowledge into information retrieval with em- beddings. Although we show that incorpo- rating expert knowledge works, we also out- line the critical limitations of embeddings in knowledge-intensive downstream domains like climate change communication.\n",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2004.14974",
        "title": "Fact or Fiction: Verifying Scientific Claims",
        "abstract": "We introduce scientific claim verification, a new task to select abstracts from the re- search literature containing evidence that SUP- PORTS or REFUTES a given scientific claim, and to identify rationales justifying each de- cision. To study this task, we construct SCI- FACT, a dataset of 1.4K expert-written scien- tific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SCIFACT, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 cor- pus. Our experiments indicate that SCIFACT will provide a challenging testbed for the de- velopment of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https:// github.com/allenai/scifact. A leader- board and COVID-19 fact-checking demo are available at https://scifact.apps. allenai.org.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1906.06127v3",
        "title": "DocRED: A Large-Scale Document-Level Relation Extraction Dataset",
        "abstract": "Multiple entities in a document generally ex- hibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically fo- cus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we in- troduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three fea- tures: (1) DocRED annotates both named en- tities and relations, and is the largest human- annotated dataset for document-level RE from plain text; (2) DocRED requires reading mul- tiple sentences in a document to extract en- tities and infer their relations by synthesiz- ing all information of the document; (3) along with the human-annotated data, we also of- fer large-scale distantly supervised data, which enables DocRED to be adopted for both su- pervised and weakly supervised scenarios. In order to verify the challenges of document- level RE, we implement recent state-of-the-art methods for RE and conduct a thorough eval- uation of these methods on DocRED. Empir- ical results show that DocRED is challeng- ing for existing RE methods, which indicates that document-level RE remains an open prob- lem and requires further efforts. Based on the detailed analysis on the experiments, we dis- cuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https: //github.com/thunlp/DocRED.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1902.09476v1",
        "title": "MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts",
        "abstract": "This paper presents the formal release of MedMentions, a new manually annotated resource for the recognition of biomedical concepts. What distinguishes MedMentions from other annotated biomedical corpora is its size (over 4,000 abstracts and over 350,000 linked mentions), as well as the size of the concept ontology (over 3 million concepts from UMLS 2017) and its broad coverage of biomedical disciplines. In addition to the full corpus, a sub-corpus of MedMentions is also presented, comprising annotations for a subset of UMLS 2017 targeted towards document retrieval. To encourage research in Biomedical Named Entity Recognition and Linking, data splits for training and testing are included in the release, and a baseline model and its metrics for entity linking are also described.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2109.11377v2",
        "title": "WRENCH: A Comprehensive Benchmark for Weak Supervision",
        "abstract": "Recent Weak Supervision (WS) approaches have had widespread success in easing the bottleneck of labeling training data for machine learning by synthesizing labels from multiple potentially noisy supervision sources. However, proper measurement and analysis of these approaches remain a challenge. First, datasets used in existing works are often private and/or custom, limiting standardization. Second, WS datasets with the same name and base data often vary in terms of the labels and weak supervision sources used, a significant \"hidden\" source of evaluation variance. Finally, WS studies often diverge in terms of the evaluation protocol and ablations used. To address these problems, we introduce a benchmark platform, WRENCH, for thorough and standardized evaluation of WS approaches. It consists of 22 varied real-world datasets for classification and sequence tagging; a range of real, synthetic, and procedurally-generated weak supervision sources; and a modular, extensible framework for WS evaluation, including implementations for popular WS methods. We use WRENCH to conduct extensive comparisons over more than 120 method variants to demonstrate its efficacy as a benchmark platform. The code is available at https://github.com/JieyuZ2/wrench.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2112.08808v4",
        "title": "Simple Questions Generate Named Entity Recognition Datasets",
        "abstract": "Recent named entity recognition (NER) mod- els often rely on human-annotated datasets, re- quiring the significant engagement of profes- sional knowledge on the target domain and entities. This research introduces an ask- to-generate approach that automatically gen- erates NER datasets by asking questions in simple natural language to an open-domain question answering system (e.g., \u201cWhich dis- ease?\u201d). Despite using fewer in-domain re- sources, our models, solely trained on the generated datasets, largely outperform strong low-resource models by an average F1 score of 19.4 for six popular NER benchmarks. Furthermore, our models provide competi- tive performance with rich-resource models that additionally leverage in-domain dictio- naries provided by domain experts. In few- shot NER, we outperform the previous best model by an F1 score of 5.2 on three bench- marks and achieve new state-of-the-art perfor- mance. The code and datasets are available at https://github.com/dmis-lab/GeNER.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2107.04374v1",
        "title": "Benchmarking for Biomedical Natural Language Processing Tasks with a Domain Specific ALBERT",
        "abstract": "The availability of biomedical text data and advances in natural language processing (NLP) have made new applications in biomedical NLP possible. Language models trained or fine-tuned using domain-specific corpora can outperform general models, but work to date in biomedical NLP has been limited in terms of corpora and tasks. We present BioALBERT, a domain-specific adaptation of A Lite Bidirectional Encoder Representations from Transformers (ALBERT), trained on biomedical (PubMed and PubMed Central) and clinical (MIMIC-III) corpora and fine-tuned for 6 different tasks across 20 benchmark datasets. Experiments show that BioALBERT outperforms the state-of-the-art on named-entity recognition (+11.09% BLURB score improvement), relation extraction (+0.80% BLURB score), sentence similarity (+1.05% BLURB score), document classification (+0.62% F1-score), and question answering (+2.83% BLURB score). It represents a new state-of-the-art in 17 out of 20 benchmark datasets. By making BioALBERT models and data available, our aim is to help the biomedical NLP community avoid computational costs of training and establish a new set of baselines for future efforts across a broad range of biomedical NLP tasks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2204.04263v2",
        "title": "BioRED: A Rich Biomedical Relation Extraction Dataset",
        "abstract": "Automated relation extraction (RE) from biomedical literature is critical for many downstream text mining appli-\ncations in both research and real-world settings. However, most existing benchmarking datasets for biomedical\nRE only focus on relations of a single type (e.g., protein-protein interactions) at the sentence level, greatly lim-\niting the development of RE systems in biomedicine. In this work, we first review commonly used named entity\nrecognition (NER) and RE datasets. Then we present BioRED, a first-of-its-kind biomedical RE corpus with\nmultiple entity types (e.g., gene/protein, disease, chemical) and relation pairs (e.g., gene-disease; chemical-\nchemical) at the document level, on a set of 600 PubMed abstracts. Further, we label each relation as describing\neither a novel finding or previously known background knowledge, enabling automated algorithms to differenti-\nate between novel and background information. We assess the utility of BioRED by benchmarking several ex-\nisting state-of-the-art methods, including BERT-based models, on the NER and RE tasks. Our results show that\nwhile existing approaches can reach high performance on the NER task (F-score of 89.3%), there is much room\nfor improvement for the RE task, especially when extracting novel relations (F-score of 47.7%). Our experiments\nalso demonstrate that such a rich dataset can successfully facilitate the development of more accurate, efficient,\nand robust RE systems for biomedicine.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.03736v1",
        "title": "FinRED: A Dataset for Relation Extraction in Financial Domain",
        "abstract": "FinRED: A Dataset for Relation Extraction in Financial Domain",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2103.06927",
        "title": "Linnaeus: A highly reusable and adaptable ML based log classification pipeline",
        "abstract": "Logs are a common way to record detailed run- time information in software. As modern software systems evolve in scale and complexity, logs have become indispensable to understanding the internal states of the system. At the same time however, manually inspecting logs has become impractical. In recent times, there has been more emphasis on statistical and machine learning (ML) based methods for analyzing logs. While the results have shown promise, most of the literature focuses on algorithms and state-of-the-art (SOTA), while largely ignoring the practical aspects. In this paper we demonstrate our end- to-end log classification pipeline, Linnaeus. Besides showing the more traditional ML flow, we also demonstrate our solutions for adaptability and re-use, integration towards large scale software development processes, and how we cope with lack of labelled data. We hope Linnaeus can serve as a blueprint for, and inspire the integration of, various ML based solutions in other large scale industrial settings.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1901.10219",
        "title": "Revised JNLPBA Corpus:\nA Revised Version of Biomedical NER\nCorpus for Relation Extraction Task",
        "abstract": "The advancement of biomedical named entity recognition (BNER) and biomedical\nrelation extraction (BRE) researches promotes the development of text mining in\nbiological domains. As a cornerstone of BRE, robust BNER system is required to\nidentify the mentioned NEs in plain texts for further relation extraction stage. However,\nthe current BNER corpora, which play important roles in these tasks, paid less attention\nto achieve the criteria for BRE task. In this study, we present Revised JNLPBA corpus,\nthe revision of JNLPBA corpus, to broaden the applicability of a NER corpus from\nBNER to BRE task. We preserve the original entity types including protein, DNA,\nRNA, cell line and cell type while all the abstracts in JNLPBA corpus are manually\ncurated by domain experts again basis on the new annotation guideline focusing on the\nspecific NEs instead of general terms. Simultaneously, several imperfection issues in\nJNLPBA are pointed out and made up in the new corpus. To compare the adaptability\nof different NER systems in Revised JNLPBA and JNLPBA corpora, the F1-measure\nwas measured in three open sources NER systems including BANNER, Gimli and\nNERSuite. In the same circumstance, all the systems perform average 10% better in\nRevised JNLPBA than in JNLPBA. Moreover, the cross-validation test is carried out\nwhich we train the NER systems on JNLPBA/Revised JNLPBA corpora and access the\nperformance in both protein-protein interaction extraction (PPIE) and biomedical event\nextraction (BEE) corpora to confirm that the newly refined Revised JNLPBA is a\ncompetent NER corpus in biomedical relation application. The revised JNLPBA corpus\nis freely available at iasl-btm.iis.sinica.edu.tw/BNER/Content/Revised_JNLPBA.zip.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.20440",
        "title": "The SourceData-NLP dataset: integrating curation\ninto scientific publishing for training large\nlanguage models",
        "abstract": "Introduction: The scientific publishing landscape is expanding rapidly, creating challenges for researchers to stay up-to-date with the evolution of the literature. Natural Language Processing (NLP) has emerged as a potent approach to automating knowledge extraction from this vast amount of publications and preprints. Tasks such as Named-Entity Recognition (NER) and Named-Entity Linking (NEL), in conjunction with context-dependent semantic interpretation, offer promising and complementary approaches to extracting structured information and revealing key concepts.\nResults: We present the SourceData-NLP dataset produced through the routine curation of papers during the publication process. A unique feature of this dataset is its emphasis on the annotation of bioentities in figure legends. We annotate eight classes of biomedical entities (small molecules, gene products, subcellular components, cell lines, cell types, tissues, organisms, and diseases), their role in the experimental design, and the nature of the experimental method as an additional class. SourceData- NLP contains more than 620,000 annotated biomedical entities, curated from 18,689 figures in 3,223 papers in molecular and cell biology. We illustrate the dataset\u2019s usefulness by assessing BioLinkBERT and PubmedBERT, two transformers-based models, fine-tuned on the SourceData-NLP dataset for NER. We also introduce a novel context-dependent semantic task that infers whether an entity is the target of a controlled intervention or the object of measurement.\nConclusions: SourceData-NLP\u2019s scale highlights the value of integrating curation into publishing. Models trained with SourceData-NLP will furthermore enable the development of tools able to extract causal hypotheses from the literature and assemble them into knowledge graphs.\nAvailability: To facilitate researchers\u2019 access to SourceData-NLP, we provide open-source access to the source code for the raw data (https://github.com/source-data/soda-data), machine- learning-ready data (https://huggingface.co/datasets/EMBO/SourceData), and generated models (https://github.com/source-data/soda-model).",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1808.09397",
        "title": "MedSTS: A Resource for Clinical Semantic Textual Similarity",
        "abstract": "The wide adoption of electronic health records (EHRs) has enabled a wide range of applications leveraging EHR data. However, the meaningful use of EHR data largely depends on our ability to efficiently extract and consolidate information embedded in clinical text where natural language processing (NLP) techniques are essential. Semantic textual similarity (STS) that measures the semantic similarity between text snippets plays a significant role in many NLP applications. In the general NLP domain, STS shared tasks have made available a huge collection of text snippet pairs with manual annotations in various domains. In the clinical domain, STS can enable us to detect and eliminate redundant information that may lead to a reduction in cognitive burden and an improvement in the clinical decision-making process. This paper elaborates our efforts to assemble a resource for STS in the medical domain, MedSTS. It consists of a total of 174,629 sentence pairs gathered from a clinical corpus at Mayo Clinic. A subset of MedSTS (MedSTS_ann) containing 1,068 sentence pairs was annotated by two medical experts with semantic similarity scores of 0-5 (low to high similarity). We further analyzed the medical concepts in the MedSTS corpus, and tested four STS systems on the MedSTS_ann corpus. In the future, we will organize a shared task by releasing the MedSTS_ann corpus to motivate the community to tackle the real world clinical problems.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1808.06752",
        "title": "Lessons from Natural Language Inference in the Clinical Domain",
        "abstract": "\nState of the art models using deep neural networks have become very good in learn- ing an accurate mapping from inputs to out- puts. However, they still lack generalization capabilities in conditions that differ from the ones encountered during training. This is even more challenging in specialized, and knowl- edge intensive domains, where training data is limited. To address this gap, we introduce MedNLI1 \u2013 a dataset annotated by doctors, performing a natural language inference task (NLI), grounded in the medical history of pa- tients. We present strategies to: 1) leverage transfer learning using datasets from the open domain, (e.g. SNLI) and 2) incorporate do- main knowledge from external data and lexi- cal sources (e.g. medical terminologies). Our results demonstrate performance gains using both strategies.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.14353",
        "title": "RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",
        "abstract": "We introduce RoMQA, the first benchmark for robust, multi-evidence, multi-answer ques- tion answering (QA). RoMQA contains clus- ters of questions that are derived from re- lated constraints mined from the Wikidata knowledge graph. RoMQA evaluates robust- ness of QA models to varying constraints by measuring worst-case performance within each question cluster. Compared to prior QA datasets, RoMQA has more human-written ques- tions that require reasoning over more evidence text and have, on average, many more cor- rect answers. In addition, human annotators rate RoMQA questions as more natural or likely to be asked by people. We evaluate state-of-the- art large language models in zero-shot, few-shot, and fine-tuning settings, and find that RoMQA is challenging: zero-shot and few-shot models per- form similarly to naive baselines, while super- vised retrieval methods perform well below gold evidence upper bounds. Moreover, existing mod- els are not robust to variations in question con- straints, but can be made more robust by tun- ing on clusters of related questions. Our results show that RoMQA is a challenging benchmark for large language models, and provides a quan- tifiable test to build more robust QA methods.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2004.07347v3",
        "title": "HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data",
        "abstract": "Existing question answering datasets focus on dealing with homogeneous information, based either only on text or KB/Table information alone. However, as human knowledge is dis- tributed over heterogeneous forms, using ho- mogeneous information alone might lead to severe coverage problems. To fill in the gap, we present HybridQA1, a new large-scale question-answering dataset that requires rea- soning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the en- tities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would ren- der the question unanswerable. We test with three different models: 1) a table-only model. 2) text-only model. 3) a hybrid model that combines heterogeneous information to find the answer. The experimental results show that the EM scores obtained by two baselines are below 20%, while the hybrid model can achieve an EM over 40%. This gap suggests the necessity to aggregate heterogeneous in- formation in HybridQA. However, the hybrid model\u2019s score is still far behind human perfor- mance. Hence, HybridQA can serve as a chal- lenging benchmark to study question answer- ing with heterogeneous information.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.01613v1",
        "title": "Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",
        "abstract": "We introduce MINTAKA, a complex, natural, and multilingual dataset designed for experi- menting with end-to-end question-answering models. Mintaka is composed of 20,000 question-answer pairs collected in English, an- notated with Wikidata entities, and translated into Arabic, French, German, Hindi, Italian, Japanese, Portuguese, and Spanish for a total of 180,000 samples. Mintaka includes 8 types of complex questions, including superlative, intersection, and multi-hop questions, which were naturally elicited from crowd workers. We run baselines over Mintaka, the best of which achieves 38% hits@1 in English and 31% hits@1 multilingually, showing that ex- isting models have room for improvement. We release Mintaka at https://github. com/amazon-research/mintaka.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1906.03008v2",
        "title": "RankQA: Neural Question Answering with Answer Re-Ranking",
        "abstract": "The conventional paradigm in neural question answering (QA) for narrative content is limited to a two-stage process: first, relevant text pas- sages are retrieved and, subsequently, a neural network for machine comprehension extracts the likeliest answer. However, both stages are largely isolated in the status quo and, hence, information from the two phases is never prop- erly fused. In contrast, this work proposes RankQA1: RankQA extends the conventional two-stage process in neural QA with a third stage that performs an additional answer re- ranking. The re-ranking leverages different features that are directly extracted from the QA pipeline, i. e., a combination of retrieval and comprehension features. While our inten- tionally simple design allows for an efficient, data-sparse estimation, it nevertheless outper- forms more complex QA systems by a signif- icant margin: in fact, RankQA achieves state- of-the-art performance on 3 out of 4 bench- mark datasets. Furthermore, its performance is especially superior in settings where the size of the corpus is dynamic. Here the answer re- ranking provides an effective remedy against the underlying noise-information trade-off due to a variable corpus size. As a consequence, RankQA represents a novel, powerful, and thus challenging baseline for future research in content-based QA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2107.07566",
        "title": "Internet-Augmented Dialogue Generation",
        "abstract": "The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giv- ing access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to fi- nally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversa- tions whereby one of the speakers is given access to internet search during knowledge- driven discussions in order to ground their re- sponses. We find that search-query based ac- cess of the internet in conversation provides superior performance compared to existing ap- proaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020).",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.07849",
        "title": "ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human",
        "abstract": "In this paper, we present ChatPLUG, a Chi- nese open-domain dialogue system for digi- tal human applications that instruction fine- tunes on a wide range of dialogue tasks in a unified internet-augmented format. Differ- ent from other open-domain dialogue mod- els that focus on large-scale pre-training and scaling up model size or dialogue corpus, we aim to build a powerful and practical dia- logue system for digital human with diverse skills and good multi-task generalization by internet-augmented instruction tuning. To this end, we first conduct large-scale pre-training on both common document corpus and dia- logue data with curriculum learning, so as to inject various world knowledge and dialogue abilities into ChatPLUG. Then, we collect a wide range of dialogue tasks spanning diverse features of knowledge, personality, multi-turn memory, and empathy, on which we further instruction tune ChatPLUG via unified nat- ural language instruction templates. Exter- nal knowledge from an internet search is also used during instruction finetuning for alleviat- ing the problem of knowledge hallucinations. We show that ChatPLUG outperforms state-of- the-art Chinese dialogue systems on both auto- matic and human evaluation, and demonstrates strong multi-task generalization on a variety of text understanding and generation tasks. In ad- dition, we deploy ChatPLUG to real-world ap- plications such as Smart Speaker and Instant Message applications with fast inference. Our models and code will be made publicly avail- able on ModelScope 1 and Github 2 .",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1910.11473v2",
        "title": "QASC: A Dataset for Question Answering via Sentence Composition",
        "abstract": "Composing knowledge from multiple pieces of texts is a key challenge in multi-hop question answering. We present a multi-hop reasoning dataset, Question Answering via Sentence Composition (QASC), that requires retrieving facts from a large corpus and composing them to answer a multiple-choice question. QASC is the first dataset to offer two desirable properties: (a) the facts to be composed are an- notated in a large corpus, and (b) the decomposition into these facts is not evident from the question itself. The latter makes retrieval challenging as the system must introduce new con- cepts or relations in order to discover potential decomposi- tions. Further, the reasoning model must then learn to identify valid compositions of these retrieved facts using common- sense reasoning. To help address these challenges, we provide annotation for supporting facts as well as their composition. Guided by these annotations, we present a two-step approach to mitigate the retrieval challenges. We use other multiple- choice datasets as additional training data to strengthen the reasoning model. Our proposed approach improves over cur- rent state-of-the-art language models by 11% (absolute). The reasoning and retrieval problems, however, remain unsolved as this model still lags by 20% behind human performance.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2005.00242",
        "title": "TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions",
        "abstract": "A critical part of reading is being able to un- derstand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have practically no questions that test temporal phenomena, so systems trained on these benchmarks have no capacity to answer questions such as \u201cwhat happened before/after [some event]?\u201d We introduce TORQUE, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships. Results show that RoBERTa-large achieves an exact-match score of 51% on the test set of TORQUE, about 30% behind human performance.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2402.15131",
        "title": "Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models",
        "abstract": "This study explores the realm of knowledge- base question answering (KBQA). KBQA is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing (SP)-based methods require extensive data an- notations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models (LLMs), has showcased promising capabili- ties. Yet, fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive- KBQA, a framework designed to generate logical forms through direct interaction with knowledge bases (KBs). Within this frame- work, we have developed three generic APIs for KB interaction. For each category of com- plex question, we devised exemplars to guide LLMs through the reasoning processes. Our method achieves competitive results on the We- bQuestionsSP, ComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal num- ber of examples (shots). Importantly, our ap- proach supports manual intervention, allowing for the iterative refinement of LLM outputs. By annotating a dataset with step-wise reasoning processes, we showcase our model\u2019s adaptabil- ity and highlight its potential for contributing significant enhancements to the field.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2310.08975",
        "title": "ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models",
        "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language questions over large-scale knowledge bases (KBs), which can be summarized into two crucial steps: knowledge retrieval and semantic parsing. However, three core challenges remain: in- efficient knowledge retrieval, mistakes of re- trieval adversely impacting semantic parsing, and the complexity of previous KBQA meth- ods. To tackle these challenges, we introduce ChatKBQA, a novel and simple generate-then- retrieve KBQA framework, which proposes first generating the logical form with fine- tuned LLMs, then retrieving and replacing en- tities and relations with an unsupervised re- trieval method, to improve both generation and retrieval more directly. Experimental re- sults show that ChatKBQA achieves new state- of-the-art performance on standard KBQA datasets, WebQSP, and CWQ. This work can also be regarded as a new paradigm for combin- ing LLMs with knowledge graphs (KGs) for in- terpretable and knowledge-required question answering. Our code is publicly available1.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2308.16884",
        "title": "The BELEBELE Benchmark:\na Parallel Reading Comprehension Dataset in 122 Language Variants",
        "abstract": "We present BELEBELE, a multiple-choice ma- chine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural lan- guage understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the FLORES-200 dataset and has four multiple- choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehen- sion. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model per- formance across all languages. We use this dataset to evaluate the capabilities of multi- lingual masked language models (MLMs) and large language models (LLMs). We present ex- tensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more lan- guages. We also observe that larger vocabulary size and conscious vocabulary construction cor- relate with better performance on low-resource languages. Overall, BELEBELE opens up new avenues for evaluating and analyzing the multi- lingual capabilities of NLP systems.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2202.00120",
        "title": "QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers",
        "abstract": "The ability to have the same experience for different user groups (i.e., accessibility) is one of the most important characteristics of Web-based systems. The same is true for Knowledge Graph Question Answering (KGQA) systems that provide the access to Semantic Web data via natural language interface. While following our research agenda on the multi- lingual aspect of accessibility of KGQA systems, we identified several ongoing challenges. One of them is the lack of multilingual KGQA benchmarks. In this work, we extend one of the most popular KGQA benchmarks \u2013 QALD-9 by introducing high- quality questions\u2019 translations to 8 languages provided by native speakers, and transferring the SPARQL queries of QALD-9 from DBpedia to Wikidata, s.t., the usability and relevance of the dataset is strongly increased. Five of the languages \u2013 Armenian, Ukrainian, Lithuanian, Bashkir and Belarusian \u2013 to our best knowledge were never considered in KGQA research community before. The latter two of the languages are considered as \u201cendangered\u201d by UNESCO. We call the extended dataset QALD-9-plus and made it available online",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2402.15862v2",
        "title": "SportQA: A Benchmark for Sports Understanding in Large Language Models",
        "abstract": "A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for ad- vancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in spe- cialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifi- cally designed for evaluating LLMs in the con- text of sports understanding. SportQA encom- passes over 70,000 multiple-choice questions across three distinct difficulty levels, each tar- geting different aspects of sports knowledge from basic historical facts to intricate, scenario- based reasoning tasks. We conducted a thor- ough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supple- mented by chain-of-thought (CoT) prompt- ing. Our results reveal that while LLMs ex- hibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports reasoning, lagging be- hind human expertise. The introduction of SportQA marks a significant step forward in NLP, offering a tool for assessing and en- hancing sports understanding in LLMs. The dataset is available at https://github.com/ haotianxia/SportQA",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2104.08727",
        "title": "GOOAQ\n: Open Question Answering with Diverse Answer Types",
        "abstract": "While day-to-day questions come with a vari- ety of answer types, the current open question- answering (QA) literature represents isolated efforts on niche response types, with a heavy focus on specific kinds of short responses (people, places, etc.). To address this gap, we present GOOAQ, a large-scale dataset col- lected from Google questions and answers, containing 3 million questions with diverse an- swer types ranging from factual short answers to snippets to collections. Our human evalu- ation shows that 94% of the mined answers are accurate, enabling fine-tuning a pre-trained language model for answering GOOAQ ques- tions. We use this dataset to study inher- ent differences between models producing dif- ferent answer types, and observe interesting trends. For example, in line with recent work, LM\u2019s strong performance on GOOAQ\u2019s short- answer questions heavily benefits from anno- tated data. However, their surprisingly high quality in generating coherent and accurate an- swers for questions requiring long responses (such as \u2018how\u2019 and \u2018why\u2019 questions) is less re- liant on observing annotated data and mainly supported by their pre-training. Moreover, we show that GOOAQ is a valuable training re- source, resulting in strong performance on the recent ELI5 long-answers dataset. We release GOOAQ to facilitate further research on im- proving QA with diverse response types",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2010.00526",
        "title": "LiveQA: A Question Answering Dataset over Sports Live\n",
        "abstract": "In this paper, we introduce LiveQA, a new question answering dataset constructed from play- by-play live broadcast. It contains 117k multiple-choice questions written by human commen- tators for over 1,670 NBA games, which are collected from the Chinese Hupu1 website. De- rived from the characteristics of sports games, LiveQA can potentially test the reasoning ability across timeline-based live broadcasts, which is challenging compared to the existing datasets. In LiveQA, the questions require understanding the timeline, tracking events or doing mathemati- cal computations. Our preliminary experiments show that the dataset introduces a challenging problem for question answering models, and a strong baseline model only achieves the accuracy of 53.1% and cannot beat the dominant option rule. We release the code and data of this paper for future research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1711.05789",
        "title": "CMU LiveMedQA at TREC 2017 LiveQA: A Consumer Health Question Answering System",
        "abstract": "In this paper, we present LiveMedQA, a question answering system that is opti- mized for consumer health question. On top of the general QA system pipeline, we introduce several new features that aim to exploit domain-specific knowledge and entity structures for better performance. This includes a question type/focus analyzer based on deep text classification model, a tree-based knowledge graph for answer generation and a complementary structure-aware searcher for answer retrieval. LiveMedQA system is evaluated in the TREC 2017 LiveQA medical subtask, where it received an average score of 0.356 on a 3 point scale. Evaluation results revealed 3 substantial drawbacks in current LiveMedQA system, based on which we provide a detailed discussion and propose a few solutions that constitute the main focus of our subsequent work.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1804.07927",
        "title": "DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension",
        "abstract": "We propose DuoRC, a novel dataset for Reading Comprehension (RC) that moti- vates several new challenges for neural approaches in language understanding be- yond those offered by existing RC datasets. DuoRC contains 186,089 unique question- answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie - one from Wikipedia and the other from IMDb - written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other ver- sion. This unique characteristic of DuoRC where questions and answers are created from different versions of a document nar- rating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different levels of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language under- standing and incorporating external back- ground knowledge. Additionally, the nar- rative style of passages arising from movie plots (as opposed to typical descriptive pas- sages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we ob- serve that state-of-the-art neural RC models which have achieved near human perfor- mance on the SQuAD dataset (Rajpurkar et al., 2016b), even when coupled with tra-\nditional NLP techniques to address the chal- lenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research av- enues wherein DuoRC could complement other RC datasets to explore novel neural approaches for studying language under- standing.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1711.05885",
        "title": "Crowdsourcing Question-Answer Meaning Representations",
        "abstract": "We introduce Question-Answer Meaning Rep- resentations (QAMRs), which represent the predicate-argument structure of a sentence as a set of question-answer pairs. We also develop a crowdsourcing scheme to show that QAMRs can be labeled with very little training, and gather a dataset with over 5,000 sentences and 100,000 questions. A detailed qualitative anal- ysis demonstrates that the crowd-generated question-answer pairs cover the vast majority of predicate-argument relationships in existing datasets (including PropBank, NomBank, QA- SRL, and AMR) along with many previously under-resourced ones, including implicit argu- ments and relations. The QAMR data and an- notation code is made publicly available1 to enable future work on how best to model these complex phenomena.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1905.10044",
        "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions",
        "abstract": "In this paper we study yes/no questions that are naturally occurring \u2014 meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid informa- tion, and require difficult entailment-like infer- ence to solve. We also explore the effective- ness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from para- phrase or extractive QA data, and that it, sur- prisingly, continues to be very beneficial even when starting from massive pre-trained lan- guage models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human anno- tators (and 62% majority-baseline), leaving a significant gap for future work.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.04215",
        "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
        "abstract": "It is very challenging to curate a dataset for language-specific knowledge and common sense in order to evaluate natural language un- derstanding capabilities of language models. Due to the limitation in the availability of an- notators, most current multilingual datasets are created through translation, which cannot eval- uate such language-specific aspects. Therefore, we propose Multilingual CommonsenseQA (mCSQA) based on the construction process of CSQA but leveraging language models for a more efficient construction, e.g., by asking LM to generate questions/answers, refine answers and verify QAs followed by reduced human efforts for verification. Constructed dataset is a benchmark for cross-lingual language- transfer capabilities of multilingual LMs, and experimental results showed high language- transfer capabilities for questions that LMs could easily solve, but lower transfer capabil- ities for questions requiring deep knowledge or commonsense. This highlights the neces- sity of language-specific datasets for evalua- tion and training. Finally, our method demon- strated that multilingual LMs could create QA including language-specific knowledge, sig- nificantly reducing the dataset creation cost compared to manual creation. The datasets are available at https://huggingface.co/ datasets/yusuke1997/mCSQA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1905.07830",
        "title": "HellaSwag: Can a Machine Really Finish Your Sentence?",
        "abstract": "Recent work by Zellers et al. (2018) intro- duced a new task of commonsense natural lan- guage inference: given an event description such as \u201cA woman sits at a piano,\u201d a machine must select the most likely followup: \u201cShe sets her fingers on the keys.\u201d With the intro- duction of BERT (Devlin et al., 2018), near human-level performance was reached. Does this mean that machines can perform human level commonsense inference?\nIn this paper, we show that commonsense in- ference still proves difficult for even state- of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its ques- tions are trivial for humans (\u010595% accuracy), state-of-the-art models struggle (\u010348%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key in- sight is to scale up the length and complex- ity of the dataset examples towards a critical \u2018Goldilocks\u2019 zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models.\nOur construction of HellaSwag, and its result- ing difficulty, sheds light on the inner work- ings of deep pretrained models. More broadly, it suggests a new path forward for NLP re- search, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1704.04683",
        "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations",
        "abstract": "We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE con- sists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and cov- ers a variety of topics which are care- fully designed for evaluating the students\u2019 ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art mod- els (43%) and the ceiling human perfor- mance (95%). We hope this new dataset can serve as a valuable resource for re- search and evaluation in machine com- prehension. The dataset is freely avail- able at http://www.cs.cmu.edu/  \u0303glai1/data/race/ and the code is available at https://github.com/ qizhex/RACE_AR_baselines",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1904.09728",
        "title": "SOCIAL IQA: Commonsense Reasoning about Social Interactions",
        "abstract": "We introduce SOCIALIQA, the first large- scale benchmark for commonsense reasoning about social situations. SOCIAL IQA contains 38,000 multiple choice questions for prob- ing emotional and social intelligence in a va- riety of everyday situations (e.g., Q: \u201cJor- dan wanted to tell Tracy a secret, so Jor- dan leaned towards Tracy. Why did Jordan do this?\u201d A: \u201cMake sure no one else could hear\u201d). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interac- tions, using a new framework that mitigates stylistic artifacts in incorrect answers by ask- ing workers to provide the right answer to a different but related question. Empirical re- sults show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance (>20% gap). Notably, we further establish SOCIALIQA as a re- source for transfer learning of commonsense knowledge, achieving state-of-the-art perfor- mance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2402.19450v1",
        "title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
        "abstract": "We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the rele- vant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of- the-art models over snapshots of MATH(), we find a reasoning gap\u2014the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35% to 80.31% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building \u201cgap 0\u201d mod- els. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at https://github.com/consequentai/fneval/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1911.11641",
        "title": "PIQA: Reasoning about Physical Commonsense in Natural Language",
        "abstract": "To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of phys- ical commonsense pose a challenge to today\u2019s natural lan- guage understanding systems. While recent pretrained mod- els (such as BERT) have made progress on question answer- ing over more abstract domains \u2013 such as news articles and encyclopedia entries, where text is plentiful \u2013 in more physi- cal domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical common- sense questions without experiencing the physical world?\nIn this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA . Though hu- mans find the dataset easy (95% accuracy), large pretrained models struggle (\u223c77%). We provide analysis about the di- mensions of knowledge that existing models lack, which of- fers significant opportunities for future research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1907.10641",
        "title": "WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale",
        "abstract": "The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense rea- soning, is a set of 273 expert-crafted pronoun resolution prob- lems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. How- ever, recent advances in neural language models have already reached around 90% accuracy on variants of WSC. This raises an important question whether these models have truly ac- quired robust commonsense capabilities or whether they rely on spurious biases in the datasets that lead to an overestimation of the true capabilities of machine commonsense.\nTo investigate this question, we introduce WINOGRANDE, a large-scale dataset of 44k problems, inspired by the origi- nal WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset con- struction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AFLITE algorithm that generalizes human-detectable word associations to machine-detectable embedding associ- ations. The best state-of-the-art methods on WINOGRANDE achieve 59.4 \u2013 79.1%, which are \u223c15-35% (absolute) below human performance of 94.0%, depending on the amount of the training data allowed (2% \u2013 100% respectively).\nFurthermore, we establish new state-of-the-art results on five related benchmarks \u2014 WSC (\u2192 90.1%), DPR (\u2192 93.1%), COPA(\u2192 90.6%), KnowRef (\u2192 85.6%), and Winogender (\u2192 97.1%). These results have dual implications: on one hand, they demonstrate the effectiveness of WINOGRANDE when used as a resource for transfer learning. On the other hand, they raise a concern that we are likely to be overestimating the true capabilities of machine commonsense across all these benchmarks. We emphasize the importance of algorithmic bias reduction in existing and future benchmarks to mitigate such overestimation.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2111.11520",
        "title": "Zero-Shot Open-Book Question Answering",
        "abstract": "Open book question answering is a subset of question answering tasks where the system aims to find answers in a given set of documents (open-book) and common knowledge about a topic. This article proposes a solution for answering natural language questions from a corpus of Amazon Web Services (AWS) technical documents with no domain-specific la- beled data (zero-shot). These questions can have yes-no-none answers, short answers, long answers, or any combination of the above. This solution comprises a two-step architecture in which a retriever finds the right document and an extractor finds the answers in the re- trieved document. We are introducing a new test dataset for open-book QA based on real customer questions on AWS technical documentation. After experimenting with several in- formation retrieval systems and extractor models based on extractive language models, the solution attempts to find the yes-no-none answers and text answers in the same pass. The model is trained on the The Stanford Question Answering Dataset - SQuAD (Rajpurkar et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) datasets. We were able to achieve 49% F1 and 39% exact match score (EM) end-to-end with no domain-specific training.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.05131",
        "title": "Overview of BioASQ 2023: The eleventh BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering",
        "abstract": "This is an overview of the eleventh edition of the BioASQ\nchallenge in the context of the Conference and Labs of the Evaluation\nForum (CLEF) 2023. BioASQ is a series of international challenges promoting advances in large-scale biomedical semantic indexing and question answering. This year, BioASQ consisted of new editions of the two\nestablished tasks b and Synergy, and a new task (MedProcNER) on\nsemantic annotation of clinical content in Spanish with medical procedures, which have a critical role in medical practice. In this edition of\nBioASQ, 28 competing teams submitted the results of more than 150 distinct systems in total for the three different shared tasks of the challenge.\nSimilarly to previous editions, most of the participating systems achieved\ncompetitive performance, suggesting the continuous advancement of the\nstate-of-the-art in the field.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2405.10974",
        "title": "Bottleneck-Minimal Indexing for Generative Document Retrieval",
        "abstract": "We apply an information-theoretic perspective to reconsider generative document retrieval (GDR), in which a document x \u2208 X is indexed by t \u2208 T , and a neural autoregressive model is trained to map queries Q to T . GDR can be considered to involve information transmission from documents X to queries Q, with the requirement to transmit more bits via the indexes T . By applying Shannon\u2019s rate-distortion theory, the optimality of indexing can be analyzed in terms of the mutual information, and the design of the indexes T can then be regarded as a bottleneck in GDR. After reformulating GDR from this perspective, we empirically quantify the bottleneck underlying GDR. Finally, using the NQ320K and MARCO datasets, we evaluate our proposed bottleneckminimal indexing method in comparison with various previous indexing methods, and we show that it outperforms those methods.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2206.02743",
        "title": "A Neural Corpus Indexer for Document Retrieval",
        "abstract": "Current state-of-the-art document retrieval solutions mainly follow an indexretrieve paradigm, where the index is hard to be directly optimized for the final\nretrieval target. In this paper, we aim to show that an end-to-end deep neural\nnetwork unifying training and indexing stages can significantly improve the recall\nperformance of traditional methods. To this end, we propose Neural Corpus Indexer (NCI), a sequence-to-sequence network that generates relevant document\nidentifiers directly for a designated query. To optimize the recall performance of\nNCI, we invent a prefix-aware weight-adaptive decoder architecture, and leverage\ntailored techniques including query generation, semantic document identifiers, and\nconsistency-based regularization. Empirical studies demonstrated the superiority\nof NCI on two commonly used academic benchmarks, achieving +21.4% and\n+16.8% relative enhancement for Recall@1 on NQ320k dataset and R-Precision\non TriviaQA dataset, respectively, compared to the best baseline method.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2304.04171",
        "title": "Learning to Tokenize for Generative Retrieval",
        "abstract": "Conventional document retrieval techniques are mainly based on the index-retrieve paradigm. It is challenging to optimize pipelines based on this paradigm in an end-to-end manner. As an alternative, generative retrieval represents documents as identifiers (docid) and retrieves documents by generating docids, enabling end-toend modeling of document retrieval tasks. However, it is an open question how one should define the document identifiers. Current approaches to the task of defining document identifiers rely on fixed rule-based docids, such as the title of a document or the result of clustering BERT embeddings, which often fail to capture the complete semantic information of a document. We propose GenRet, a document tokenization learning method to address the challenge of defining document identifiers for generative retrieval. GenRet learns to tokenize documents into short discrete representations (i.e., docids) via a discrete auto-encoding approach. Three components are included in GenRet: (i) a tokenization model that produces docids for documents; (ii) a reconstruction model that learns to reconstruct a document based on a docid; and (iii) a sequence-to-sequence retrieval model that generates relevant document identifiers directly for a designated query. By using an auto-encoding framework, GenRet learns semantic docids in a fully end-to-end manner, where the produced docids can be reconstructed back to the original documents to ensure their semantics. We also develop a progressive training scheme to capture the autoregressive nature of docids and to stabilize training. We conduct experiments on the NQ320K, MS MARCO, and BEIR datasets to assess the effectiveness of GenRet. GenRet establishes the new state-of-the-art on the NQ320K dataset. Especially, compared to generative retrieval baselines, GenRet can achieve significant improvements on the unseen documents (e.g., at least +14% relative improvements in terms of R@1). Furthermore, GenRet can better represent and retrieve documents that have not been seen during the training phase compared to previous rule-based tokenization methods. GenRet also outperforms comparable baselines on MS MARCO and BEIR, demonstrating the method\u2019s generalizability.1",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1903.00161",
        "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
        "abstract": "Reading comprehension has recently seen\nrapid progress, with systems matching humans\non the most popular datasets for the task. However, a large body of work has highlighted\nthe brittleness of these systems, showing that\nthere is much work left to be done. We introduce a new English reading comprehension\nbenchmark, DROP, which requires Discrete\nReasoning Over the content of Paragraphs. In\nthis crowdsourced, adversarially-created, 96kquestion benchmark, a system must resolve\nreferences in a question, perhaps to multiple input positions, and perform discrete operations\nover them (such as addition, counting, or sorting). These operations require a much more\ncomprehensive understanding of the content of\nparagraphs than what was necessary for prior\ndatasets. We apply state-of-the-art methods\nfrom both the reading comprehension and semantic parsing literatures on this dataset and\nshow that the best systems only achieve 32.7%\nF1 on our generalized accuracy metric, while\nexpert human performance is 96.4%. We additionally present a new model that combines\nreading comprehension methods with simple\nnumerical reasoning to achieve 47.0% F1.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2109.08207",
        "title": "Numerical reasoning in machine reading comprehension tasks: are we there yet?",
        "abstract": "Numerical reasoning based machine reading\ncomprehension is a task that involves reading\ncomprehension along with using arithmetic operations such as addition, subtraction, sorting,\nand counting. The DROP benchmark (Dua\net al., 2019) is a recent dataset that has inspired\nthe design of NLP models aimed at solving\nthis task. The current standings of these models in the DROP leaderboard, over standard\nmetrics, suggest that the models have achieved\nnear-human performance. However, does this\nmean that these models have learned to reason? In this paper, we present a controlled\nstudy on some of the top-performing model\narchitectures for the task of numerical reasoning. Our observations suggest that the standard\nmetrics are incapable of measuring progress towards such tasks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2307.06435",
        "title": "A Comprehensive Overview of Large Language Models",
        "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2403.13430",
        "title": "MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining",
        "abstract": "Foundation models have reshaped the landscape of Remote Sensing (RS) by enhancing various image interpretation tasks. Pretraining is an active research topic, encompassing supervised and self-supervised learning methods to initialize model weights effectively. However, transferring the pretrained models to downstream tasks may encounter task discrepancy due to their formulation of pretraining as image classification or object discrimination tasks. In this study, we explore the Multi-Task Pretraining (MTP) paradigm for RS foundation models to address this issue. Using a shared encoder and task-specific decoder architecture, we conduct multi-task supervised pretraining on the SAMRS dataset, encompassing semantic segmentation, instance segmentation, and rotated object detection. MTP supports both convolutional neural networks and vision transformer foundation models with over 300 million parameters. The pretrained models are finetuned on various RS downstream tasks, such as scene classification, horizontal and rotated object detection, semantic segmentation, and change detection. Extensive experiments across 14 datasets demonstrate the superiority of our models over existing ones of similar size and their competitive performance compared to larger state-of-the-art models, thus validating the effectiveness of MTP. The codes and pretrained models will be released at https://github.com/ViTAE-Transformer/MTP.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2407.03937",
        "title": "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models",
        "abstract": "Classical Chinese is a gateway to the rich heritage and wisdom of ancient China, yet its complexities pose formidable comprehension barriers for most modern people without specialized knowledge. While Large Language Models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), they struggle with Classical Chinese Understanding (CCU), especially in data-demanding and knowledge-intensive tasks. In response to this dilemma, we propose TongGu (mean understanding ancient and modern), the first CCU-specific LLM, underpinned by three core contributions. First, we construct a two-stage instruction-tuning dataset ACCN-INS derived from rich classical Chinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we propose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting, enabling TongGu to acquire new capabilities while preserving its foundational knowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG) technique to reduce hallucinations based on knowledge-grounding. Extensive experiments across 24 diverse CCU tasks validate TongGu\u2019s superior ability, underscoring the effectiveness of RAT and CCURAG. The model and dataset will be public available.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2405.19327",
        "title": "MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series",
        "abstract": "Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model\u2019s weights are provided with most details undisclosed (e.g., intermediate checkpoints, pre-training corpus, and training code, etc). To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs are still inferior to existing state-of-the-art LLMs with similar model sizes on reasoning, knowledge, and coding tasks. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework1 are provided. Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.11644",
        "title": "Textbooks Are All You Need",
        "abstract": "We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of \u201ctextbook quality\u201d data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2311.09732",
        "title": "Source Prompt: Coordinated Pre-training of Language Models on Diverse Corpora from Multiple Sources",
        "abstract": "Pre-trained language models (PLMs) have established the new paradigm in the field of NLP. For more powerful PLMs, one of the most popular and successful way is to continuously scale up sizes of the models and the pre-training corpora. These large corpora are generally obtained by converging smaller ones from multiple sources, they are thus growing increasingly diverse. However, the side-effects of these colossal converged corpora remain understudied. In this paper, we identify the disadvantage of heterogeneous corpora from multiple sources for pre-training PLMs. Towards coordinated pre-training on diverse corpora, we further propose source prompts (SP), which explicitly prompt the model of the data source at the pre-training and fine-tuning stages. Results of extensive experiments demonstrate that PLMs pretrained with SP on diverse corpora gain significant improvement in various downstream tasks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2203.08913",
        "title": "MEMORIZING TRANSFORMERS",
        "abstract": "Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2401.16380",
        "title": "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling",
        "abstract": "Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. In this work, we propose Web Rephrase Augmented Pre-training (WRAP) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as \u201clike Wikipedia\u201d or in \u201cquestion-answer format\u201d to jointly pre-train LLMs on real and synthetic rephrases. First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by \u223c 3\u00d7. At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher \u2018quality\u2019 than web-scraped data.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2210.00157",
        "title": "Deconstructing experimental decay energy spectra: the 26O case",
        "abstract": "In nuclear reaction experiments, the measured decay energy spectra can give insights into the shell structure of decaying systems. However, extracting the underlying physics from the measurements is challenging due to detector resolution and acceptance effects. The Richardson-Lucy (RL) algorithm, a deblurring method that is commonly used in optics and has proven to be a successful technique for restoring images, was applied to our experimental nuclear physics data. The only inputs to the method are the observed energy spectrum and the detector\u2019s response matrix also known as the transfer matrix. We demonstrate that the technique can help access information about the shell structure of particle-unbound systems from the measured decay energy spectrum that isn\u2019t immediately accessible via traditional approaches such as chi-square fitting. For a similar purpose, we developed a machine learning model that uses a deep neural network (DNN) classifier to identify resonance states from the measured decay energy spectrum. We tested the performance of both methods on simulated data and experimental measurements. Then, we applied both algorithms to the decay energy spectrum of 26O \u219224 O + n + n measured via invariant mass spectroscopy. The resonance states restored using the RL algorithm to deblur the measured decay energy spectrum agree with those found by the DNN classifier. Both deblurring and DNN approaches suggest that the raw decay energy spectrum of 26O exhibits three peaks at approximately 0.15 MeV, 1.50 MeV, and 5.00 MeV, with half-widths of 0.29 MeV, 0.80 MeV, and 1.85 MeV, respectively",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2402.18060",
        "title": "Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions",
        "abstract": "LLMs have demonstrated impressive performance in answering medical questions, such as achieving passing scores on medical licensing examinations. However, medical board exam or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets.1 JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises simulated clinical questions. Both datasets are structured as multiple-choice question-answering tasks, accompanied by expert-written explanations. We evaluate seven LLMs on the two datasets using various prompts. Experiments demonstrate that our datasets are harder than previous benchmarks. Human and automatic evaluations of model-generated explanations provide insights into the promise and deficiency of LLMs for explainable medical QA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1603.08680",
        "title": "Electron phase shift at the zero-bias anomaly of quantum point contacts",
        "abstract": "The Kondo effect is the many-body screening of a local spin by a cloud of\n electrons at very low temperature. It has been proposed as an explanation of\n the zero-bias anomaly in quantum point contacts where interactions drive a\n spontaneous charge localization. However, the Kondo origin of this anomaly\n remains under debate, and additional experimental evidence is necessary. Here\n we report on the first phase-sensitive measurement of the zero-bias anomaly in\n quantum point contacts using a scanning gate microscope to create an electronic\n interferometer. We observe an abrupt shift of the interference fringes by half\n a period in the bias range of the zero-bias anomaly, a behavior which cannot be\n reproduced by single-particle models. We instead relate it to the phase shift\n experienced by electrons scattering off a Kondo system. Our experiment\n therefore provides new evidence of this many-body effect in quantum point\n contacts.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2002.08079",
        "title": "History-dependent percolation on multiplex networks",
        "abstract": "The structure of interconnected systems and its impact on the system dynamics\n is a much-studied cross-disciplinary topic. Although various critical phenomena\n have been found in different models, the study on the connections between\n different percolation transitions is still lacking. Here we propose a unified\n framework to study the origins of the discontinuous transitions of the\n percolation process on interacting networks. The model evolves in generations\n with the result of the present percolation depending on the previous state and\n thus is history-dependent. Both theoretical analysis and Monte Carlo\n simulations reveal that the nature of the transition remains the same at finite\n generations but exhibits an abrupt change for the infinite generation. We use\n brain functional correlation and morphological similarity data to show that our\n model also provides a general method to explore the network structure and can\n contribute to many practical applications, such as detecting the abnormal\n structures of human brain networks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2404.10530",
        "title": "JCGM 101-compliant uncertainty evaluation using virtual experiments",
        "abstract": "Virtual experiments (VEs), a modern tool in metrology, can be used to help\n perform an uncertainty evaluation for the measurand. Current guidelines in\n metrology do not cover the many possibilities to incorporate VEs into an\n uncertainty evaluation, and it is often difficult to assess if the intended use\n of a VE complies with said guidelines. In recent work, it was shown that a VE\n can be used in conjunction with real measurement data and a Monte Carlo\n procedure to produce equal results to a supplement of the Guide to the\n Expression of Uncertainty in Measurement. However, this was shown only for\n linear measurement models. In this work, we extend this Monte Carlo approach to\n a common class of non-linear measurement models and more complex VEs, providing\n a reference approach for suitable uncertainty evaluations involving VEs.\n Numerical examples are given to show that the theoretical derivations hold in a\n practical scenario.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2211.14866",
        "title": "Spatially Sparse Precoding in Wideband Hybrid Terahertz Massive MIMO\n  Systems",
        "abstract": "In terahertz (THz) massive multiple-input multiple-output (MIMO) systems, the\n combination of huge bandwidth and massive antennas results in severe beam\n split, thus making the conventional phase-shifter based hybrid precoding\n architecture ineffective. With the incorporation of true-time-delay (TTD) lines\n in the hardware implementation of the analog precoders, delay-phase precoding\n (DPP) emerges as a promising architecture to effectively overcome beam split.\n However, existing DPP approaches suffer from poor performance, high complexity,\n and weak robustness in practical THz channels. In this paper, we propose a\n novel DPP approach in wideband THz massive MIMO systems. First, the\n optimization problem is converted into a compressive sensing (CS) form, which\n can be solved by the extended spatially sparse precoding (SSP) algorithm. To\n compensate for beam split, frequency-dependent measurement matrices are\n introduced, which can be approximately realized by feasible phase and delay\n codebooks. Then, several efficient atom selection techniques are developed to\n further reduce the complexity of extended SSP. In simulation, the proposed DPP\n approach achieves superior performance, complexity, and robustness by using it\n alone or in combination with existing DPP approaches.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2307.03217",
        "title": "Quantification of Uncertainty with Adversarial Models",
        "abstract": "Quantifying uncertainty is important for actionable predictions in real-world\n applications. A crucial part of predictive uncertainty quantification is the\n estimation of epistemic uncertainty, which is defined as an integral of the\n product between a divergence function and the posterior. Current methods such\n as Deep Ensembles or MC dropout underperform at estimating the epistemic\n uncertainty, since they primarily consider the posterior when sampling models.\n We suggest Quantification of Uncertainty with Adversarial Models (QUAM) to\n better estimate the epistemic uncertainty. QUAM identifies regions where the\n whole product under the integral is large, not just the posterior.\n Consequently, QUAM has lower approximation error of the epistemic uncertainty\n compared to previous methods. Models for which the product is large correspond\n to adversarial models (not adversarial examples!). Adversarial models have both\n a high posterior as well as a high divergence between their predictions and\n that of a reference model. Our experiments show that QUAM excels in capturing\n epistemic uncertainty for deep learning models and outperforms previous methods\n on challenging tasks in the vision domain.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1811.05318",
        "title": "MMALFM: Explainable Recommendation by Leveraging Reviews and Images",
        "abstract": "Although the latent factor model achieves good accuracy in rating prediction,\n it suffers from many problems including cold-start, non-transparency, and\n suboptimal results for individual user-item pairs. In this paper, we exploit\n textual reviews and item images together with ratings to tackle these\n limitations. Specifically, we first apply a proposed multi-modal aspect-aware\n topic model (MATM) on text reviews and item images to model users' preferences\n and items' features from different aspects, and also estimate the aspect\n importance of a user towards an item. Then the aspect importance is integrated\n into a novel aspect-aware latent factor model (ALFM), which learns user's and\n item's latent factors based on ratings. In particular, ALFM introduces a weight\n matrix to associate those latent factors with the same set of aspects in MATM,\n such that the latent factors could be used to estimate aspect ratings. Finally,\n the overall rating is computed via a linear combination of the aspect ratings,\n which are weighted by the corresponding aspect importance. To this end, our\n model could alleviate the data sparsity problem and gain good interpretability\n for recommendation. Besides, every aspect rating is weighted by its aspect\n importance, which is dependent on the targeted user's preferences and the\n targeted item's features. Therefore, it is expected that the proposed method\n can model a user's preferences on an item more accurately for each user-item\n pair. Comprehensive experimental studies have been conducted on the Yelp 2017\n Challenge dataset and Amazon product datasets to demonstrate the effectiveness\n of our method.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1512.00990",
        "title": "Quantum simulation of the dynamical Casimir effect with trapped ions",
        "abstract": "Quantum vacuum fluctuations are a direct manifestation of Heisenberg's\n uncertainty principle. The dynamical Casimir effect allows for the observation\n of these vacuum fluctuations by turning them into real, observable photons.\n However, the observation of this effect in a cavity QED experiment would\n require the rapid variation of the length of a cavity with relativistic\n velocities, a daunting challenge. Here, we propose a quantum simulation of the\n dynamical Casimir effect using an ion chain confined in a segmented ion trap.\n We derive a discrete model that enables us to map the dynamics of the multimode\n radiation field inside a variable-length cavity to radial phonons of the ion\n crystal. We perform a numerical study comparing the ion-chain quantum\n simulation under realistic experimental parameters to an ideal Fabry-Perot\n cavity, demonstrating the viability of the mapping. The proposed quantum\n simulator, therefore, allows for probing the photon (respectively phonon)\n production caused by the dynamical Casimir effect on the single photon level.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1308.2233",
        "title": "Frame Transformations for Fermions",
        "abstract": "The analog to the Legendre addition theorem is found for half-integral\n angular momentum using frame transformations for rotor states.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2303.18248",
        "title": "Towards Flexible Multi-modal Document Models",
        "abstract": "Creative workflows for generating graphical documents involve complex\n inter-related tasks, such as aligning elements, choosing appropriate fonts, or\n employing aesthetically harmonious colors. In this work, we attempt at building\n a holistic model that can jointly solve many different design tasks. Our model,\n which we denote by FlexDM, treats vector graphic documents as a set of\n multi-modal elements, and learns to predict masked fields such as element type,\n position, styling attributes, image, or text, using a unified architecture.\n Through the use of explicit multi-task learning and in-domain pre-training, our\n model can better capture the multi-modal relationships among the different\n document fields. Experimental results corroborate that our single FlexDM is\n able to successfully solve a multitude of different design tasks, while\n achieving performance that is competitive with task-specific and costly\n baselines.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.02911",
        "title": "Improving In-Context Learning with Prediction Feedback for Sentiment\n  Analysis",
        "abstract": "Large language models (LLMs) have achieved promising results in sentiment\n analysis through the in-context learning (ICL) paradigm. However, their ability\n to distinguish subtle sentiments still remains a challenge. Inspired by the\n human ability to adjust understanding via feedback, this paper enhances ICL by\n incorporating prior predictions and feedback, aiming to rectify sentiment\n misinterpretation of LLMs. Specifically, the proposed framework consists of\n three steps: (1) acquiring prior predictions of LLMs, (2) devising predictive\n feedback based on correctness, and (3) leveraging a feedback-driven prompt to\n refine sentiment understanding. Experimental results across nine sentiment\n analysis datasets demonstrate the superiority of our framework over\n conventional ICL methods, with an average F1 improvement of 5.95%.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1608.04336",
        "title": "The Scalability-Efficiency/Maintainability-Portability Trade-off in\n  Simulation Software Engineering: Examples and a Preliminary Systematic\n  Literature Review",
        "abstract": "Large-scale simulations play a central role in science and the industry.\n Several challenges occur when building simulation software, because simulations\n require complex software developed in a dynamic construction process. That is\n why simulation software engineering (SSE) is emerging lately as a research\n focus. The dichotomous trade-off between scalability and efficiency (SE) on the\n one hand and maintainability and portability (MP) on the other hand is one of\n the core challenges. We report on the SE/MP trade-off in the context of an\n ongoing systematic literature review (SLR). After characterizing the issue of\n the SE/MP trade-off using two examples from our own research, we (1) review the\n 33 identified articles that assess the trade-off, (2) summarize the proposed\n solutions for the trade-off, and (3) discuss the findings for SSE and future\n work. Overall, we see evidence for the SE/MP trade-off and first solution\n approaches. However, a strong empirical foundation has yet to be established;\n general quantitative metrics and methods supporting software developers in\n addressing the trade-off have to be developed. We foresee considerable future\n work in SSE across scientific communities.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2212.02178",
        "title": "A Data Fusion Approach for Ride-sourcing Demand Estimation: A Discrete\n  Choice Model with Sampling and Endogeneity Corrections",
        "abstract": "Ride-sourcing services offered by companies like Uber and Didi have grown\n rapidly in the last decade. Understanding the demand for these services is\n essential for planning and managing modern transportation systems. Existing\n studies develop statistical models for ride-sourcing demand estimation at an\n aggregate level due to limited data availability. These models lack foundations\n in microeconomic theory, ignore competition of ride-sourcing with other travel\n modes, and cannot be seamlessly integrated into existing individual-level\n (disaggregate) activity-based models to evaluate system-level impacts of\n ride-sourcing services. In this paper, we present and apply an approach for\n estimating ride-sourcing demand at a disaggregate level using discrete choice\n models and multiple data sources. We first construct a sample of trip-based\n mode choices in Chicago, USA by enriching household travel survey with publicly\n available ride-sourcing and taxi trip records. We then formulate a multivariate\n extreme value-based discrete choice with sampling and endogeneity corrections\n to account for the construction of the estimation sample from multiple data\n sources and endogeneity biases arising from supply-side constraints and surge\n pricing mechanisms in ride-sourcing systems. Our analysis of the constructed\n dataset reveals insights into the influence of various socio-economic, land use\n and built environment features on ride-sourcing demand. We also derive\n elasticities of ride-sourcing demand relative to travel cost and time. Finally,\n we illustrate how the developed model can be employed to quantify the welfare\n implications of ride-sourcing policies and regulations such as terminating\n certain types of services and introducing ride-sourcing taxes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0508361",
        "title": "Negative values of truncations to $L(1,\\chi)$",
        "abstract": "For a given $x$ we consider the minimum of $\\sum_{n\\le x} \\chi(n)/n$ as\n $\\chi$ ranges over all quadratic Dirichlet characters. For all large $x$, this\n minimum is negative and we give upper and lower bounds for it.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1706.03093",
        "title": "Dislocations, disclinations, and metric anomalies as sources of global\n  strain incompatibility in thin shells",
        "abstract": "The strain incompatibility equations are discussed for nonlinear\n Kirchhoff-Love shells with sources of inhomogeneity arising due to a\n distribution of topological defects, such as dislocations and disclinations,\n and metric anomalies, such as point defects, thermal strains, and biological\n growth. The incompatibility equations are given for all topological surfaces,\n with or without boundary, which are isometrically embeddable in a 3-dimensional\n Euclidean space.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2305.16597",
        "title": "Neural Architecture Search for Parameter-Efficient Fine-tuning of Large\n  Pre-trained Language Models",
        "abstract": "Parameter-efficient tuning (PET) methods fit pre-trained language models\n (PLMs) to downstream tasks by either computing a small compressed update for a\n subset of model parameters, or appending and fine-tuning a small number of new\n model parameters to the pre-trained network. Hand-designed PET architectures\n from the literature perform well in practice, but have the potential to be\n improved via automated neural architecture search (NAS). We propose an\n efficient NAS method for learning PET architectures via structured and\n unstructured pruning. We present experiments on GLUE demonstrating the\n effectiveness of our algorithm and discuss how PET architectural design choices\n affect performance in practice.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.13583",
        "title": "Low-Rank Mixture-of-Experts for Continual Medical Image Segmentation",
        "abstract": "The primary goal of continual learning (CL) task in medical image\n segmentation field is to solve the \"catastrophic forgetting\" problem, where the\n model totally forgets previously learned features when it is extended to new\n categories (class-level) or tasks (task-level). Due to the privacy protection,\n the historical data labels are inaccessible. Prevalent continual learning\n methods primarily focus on generating pseudo-labels for old datasets to force\n the model to memorize the learned features. However, the incorrect\n pseudo-labels may corrupt the learned feature and lead to a new problem that\n the better the model is trained on the old task, the poorer the model performs\n on the new tasks. To avoid this problem, we propose a network by introducing\n the data-specific Mixture of Experts (MoE) structure to handle the new tasks or\n categories, ensuring that the network parameters of previous tasks are\n unaffected or only minimally impacted. To further overcome the tremendous\n memory costs caused by introducing additional structures, we propose a Low-Rank\n strategy which significantly reduces memory cost. We validate our method on\n both class-level and task-level continual learning challenges. Extensive\n experiments on multiple datasets show our model outperforms all other methods.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0505351",
        "title": "Immersion microscopy based on photonic crystal materials",
        "abstract": "Theoretical model of the enhanced optical resolution of the surface plasmon\n immersion microscope is developed, which is based on the optics of surface\n plasmon Bloch waves in the tightly bound approximation. It is shown that a\n similar resolution enhancement may occur in a more general case of an immersion\n microscope based on photonic crystal materials with either positive or negative\n effective refractive index. Both signs of the effective refractive index have\n been observed in our experiments with surface plasmon immersion microscope,\n which is also shown to be capable of individual virus imaging.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2208.03323",
        "title": "DeepWSD: Projecting Degradations in Perceptual Space to Wasserstein\n  Distance in Deep Feature Space",
        "abstract": "Existing deep learning-based full-reference IQA (FR-IQA) models usually\n predict the image quality in a deterministic way by explicitly comparing the\n features, gauging how severely distorted an image is by how far the\n corresponding feature lies from the space of the reference images. Herein, we\n look at this problem from a different viewpoint and propose to model the\n quality degradation in perceptual space from a statistical distribution\n perspective. As such, the quality is measured based upon the Wasserstein\n distance in the deep feature domain. More specifically, the 1DWasserstein\n distance at each stage of the pre-trained VGG network is measured, based on\n which the final quality score is performed. The deep Wasserstein distance\n (DeepWSD) performed on features from neural networks enjoys better\n interpretability of the quality contamination caused by various types of\n distortions and presents an advanced quality prediction capability. Extensive\n experiments and theoretical analysis show the superiority of the proposed\n DeepWSD in terms of both quality prediction and optimization.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1612.01937",
        "title": "Next-to-leading order corrections to deeply virtual production of\n  pseudoscalar mesons",
        "abstract": "We complete the perturbative next-to-leading order corrections to the hard\n scattering amplitudes of deeply virtual meson leptoproduction processes at\n leading twist-two level by presenting the results for the production of flavor\n singlet pseudoscalar mesons. The new results are given in the common momentum\n fraction representation and in terms of conformal moments. We also comment on\n the flavor singlet results for deeply virtual vector meson production.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2107.06550",
        "title": "FAPR: Fast and Accurate Program Repair for Introductory Programming\n  Courses",
        "abstract": "In introductory programming courses, it is challenging for instructors to\n provide debugging feedback on students' incorrect programs. Some recent tools\n automatically offer program repair feedback by identifying any differences\n between incorrect and correct programs, but suffer from issues related to\n scalability, accuracy, and cross-language portability. This paper presents FAPR\n -- our novel approach that suggests repairs based on program differences in a\n fast and accurate manner. FAPR is different from current tools in three\n aspects. First, it encodes syntactic information into token sequences to enable\n high-speed comparison between incorrect and correct programs. Second, to\n accurately extract program differences, FAPR adopts a novel matching algorithm\n that maximizes token-level matches and minimizes statement-level differences.\n Third, FAPR relies on testing instead of static/dynamic analysis to validate\n and refine candidate repairs, so it eliminates the language dependency or high\n runtime overhead incurred by complex program analysis. We implemented FAPR to\n suggest repairs for both C and C++ programs; our experience shows the great\n cross-language portability of FAPR. More importantly, we empirically compared\n FAPR with a state-of-the-art tool Clara. FAPR suggested repairs for over 95.5%\n of incorrect solutions. We sampled 250 repairs among FAPR's suggestions, and\n found 89.6% of the samples to be minimal and correct. FAPR outperformed Clara\n by suggesting repairs for more cases, creating smaller repairs, producing\n higher-quality fixes, and causing lower runtime overheads. Our results imply\n that FAPR can potentially help instructors or TAs to effectively locate bugs in\n incorrect code, and to provide debugging hints/guidelines based on those\n generated repairs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1907.02971",
        "title": "Higher-Order Sudakov Resummation in Coupled Gauge Theories",
        "abstract": "We consider the higher-order resummation of Sudakov double logarithms in the\n presence of multiple coupled gauge interactions. The associated evolution\n equations depend on the coupled $\\beta$ functions of two (or more) coupling\n constants $\\alpha_a$ and $\\alpha_b$, as well as anomalous dimensions that have\n joint perturbative series in $\\alpha_a$ and $\\alpha_b$. We discuss possible\n strategies for solving the system of evolution equations that arises. As an\n example, we obtain the complete three-loop (NNLL) QCD$\\otimes$QED Sudakov\n evolution factor. Our results also readily apply to the joint higher-order\n resummation of electroweak and QCD Sudakov logarithms.\n  As part of our analysis we also revisit the case of a single gauge\n interaction (pure QCD), and study the numerical differences and reliability of\n various methods for evaluating the Sudakov evolution factor at higher orders.\n We find that the approximations involved in deriving commonly used analytic\n expressions for the evolution kernel can induce noticeable numerical\n differences of several percent or more at low scales, exceeding the\n perturbative precision at N$^3$LL and in some cases even NNLL. Therefore, one\n should be cautious when using approximate analytic evolution kernels for\n high-precision analyses.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0603632",
        "title": "Discrepancy principle for DSM",
        "abstract": "Let $Ay=f$, $A$ is a linear operator in a Hilbert space $H$, $y\\perp\n N(A):=\\{u:Au=0\\}$, $R(A):=\\{h:h=Au,u\\in D(A)\\}$ is not closed,\n $\\|f_\\delta-f\\|\\leq\\delta$. Given $f_\\delta$, one wants to construct $u_\\delta$\n such that $\\lim_{\\delta\\to 0}\\|u_\\delta-y\\|=0$. A version of the DSM (dynamical\n systems method) for finding $u_\\delta$ consists of solving the problem \\bee\n  \\dotu_\\delta(t)=-u_\\delta(t)+T^{-1}_{a(t)} A^\\ast f_\\delta, \\quad u(0)=u_0,\n \\eqno{(\\ast)}\\eee where $T:=A^\\ast A$, $T_a:=T+aI$, and $a=a(t)>0$,\n $a(t)\\searrow 0$ as $t\\to\\infty$ is suitably chosen. It is proved that\n $u_\\delta:=u_\\delta(t_\\delta)$ has the property $\\lim_{\\delta\\to\n 0}\\|u_\\delta-y\\|=0$. Here the stopping time $t_\\delta$ is defined by the\n discrepancy principle: \\bee\n  \\eqno{(\\ast\\ast)}\\eee $c\\in(1,2)$ is a constant. Equation $(\\ast)$ defines\n $t_\\delta$ uniquely and $\\lim_{\\delta\\to 0}t_\\delta=\\infty$. Another version of\n the discrepancy principle is also proved in this paper.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1407.0540",
        "title": "Summary of the ACAT Round Table Discussion: Open-source, knowledge\n  sharing and scientific collaboration",
        "abstract": "Round table discussions are in the tradition of ACAT. This year's plenary\n round table discussion was devoted to questions related to the use of\n scientific software in High Energy Physics and beyond. The 90 minutes of\n discussion were lively, and quite a lot of diverse opinions were spelled out.\n Although the discussion was, in part, controversial, the participants agreed\n unanimously on several basic issues in software sharing: (i) The importance of\n having various licensing models in academic research; (ii) The basic value of\n proper recognition and attribution of intellectual property, including\n scientific software; (iii) The user respect for the conditions of use,\n including licence statements, as formulated by the author. The need of a\n similar discussion on the issues of data sharing was emphasized and it was\n recommended to cover this subject at the conference round table discussion of\n next ACAT. In this contribution, we summarise selected topics that were covered\n in the introductory talks and in the following discussion.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1811.12479",
        "title": "Potential and Issues for Future Accelerators and Ultimate Colliders",
        "abstract": "Particle colliders have been remarkably successful tools in particle and\n nuclear physics. What are the future trends and limitations of accelerators as\n they currently exist, and are there possible alternative approaches? What would\n the ultimate collider look like? This talk examines some challenges and\n possible solutions. Accelerating a single particle rather than a thermal\n distribution may allow exploration of more controlled interactions without\n background. Also, cost drivers are possibly the most important limiting factor\n for large accelerators in the foreseeable future so emerging technologies to\n reduce cost are highlighted.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2311.10825",
        "title": "Pudding: Private User Discovery in Anonymity Networks",
        "abstract": "Anonymity networks allow messaging with metadata privacy, providing better\n privacy than popular encrypted messaging applications. However, contacting a\n user on an anonymity network currently requires knowing their public key or\n similar high-entropy information, as these systems lack a privacy-preserving\n mechanism for contacting a user via a short, human-readable username. Previous\n research suggests that this is a barrier to widespread adoption.\n  In this paper we propose Pudding, a novel private user discovery protocol\n that allows a user to be contacted on an anonymity network knowing only their\n email address. Our protocol hides contact relationships between users, prevents\n impersonation, and conceals which usernames are registered on the network.\n Pudding is Byzantine fault tolerant, remaining available and secure as long as\n less than one third of servers are crashed, unavailable, or malicious. It can\n be deployed on Loopix and Nym without changes to the underlying anonymity\n network protocol, and it supports mobile devices with intermittent network\n connectivity. We demonstrate the practicality of Pudding with a prototype using\n the Nym anonymity network. We also formally define the security and privacy\n goals of our protocol and conduct a thorough analysis to assess its compliance\n with these definitions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2207.10150",
        "title": "Tackling Long-Tailed Category Distribution Under Domain Shifts",
        "abstract": "Machine learning models fail to perform well on real-world applications when\n 1) the category distribution P(Y) of the training dataset suffers from\n long-tailed distribution and 2) the test data is drawn from different\n conditional distributions P(X|Y). Existing approaches cannot handle the\n scenario where both issues exist, which however is common for real-world\n applications. In this study, we took a step forward and looked into the problem\n of long-tailed classification under domain shifts. We designed three novel core\n functional blocks including Distribution Calibrated Classification Loss,\n Visual-Semantic Mapping and Semantic-Similarity Guided Augmentation.\n Furthermore, we adopted a meta-learning framework which integrates these three\n blocks to improve domain generalization on unseen target domains. Two new\n datasets were proposed for this problem, named AWA2-LTS and ImageNet-LTS. We\n evaluated our method on the two datasets and extensive experimental results\n demonstrate that our proposed method can achieve superior performance over\n state-of-the-art long-tailed/domain generalization approaches and the\n combinations. Source codes and datasets can be found at our project page\n https://xiaogu.site/LTDS.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ph/9910422",
        "title": "Momentum expansion of massive two-loop Feynman graphs around a finite\n  value",
        "abstract": "We give an algorithm for obtaining expansions of massive two-loop Feynman\n graphs in powers of the external momentum around a finite, nonzero value of the\n momentum. This is based on our general two-loop formalism to reduce massive\n two-loop graphs with renormalizable interactions into a standard set of special\n functions. After the algebraic reduction, the final results are obtained by\n numerical integration. We apply the expansion algorithm to treat the\n top-dependent corrections of O(g^2 alpha_s) to the b quark self-energy and\n extract its momentum expansion on-shell.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2112.07631",
        "title": "Quantum memory at an eigenstate phase transition in a weakly chaotic\n  model",
        "abstract": "We study a fully connected quantum spin model resonantly coupled to a small\n environment of non-interacting spins, and investigate how initial state\n properties are remembered at long times. We find memory of initial state\n properties, in addition to the total energy, that are not conserved by the\n dynamics. This memory occurs in the middle of the spectrum where an eigenstate\n quantum phase transition (ESQPT) occurs as a function of energy. The memory\n effect at that energy in the spectrum is robust to system-environment coupling\n until the coupling changes the energy of the ESQPT. This work demonstrates the\n effect of ESQPT memory as independent of integrability and suggests a wider\n generality of this mechanism for preventing thermalization at ESQPTs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2308.08349",
        "title": "Kropina metrics with isotropic scalar curvature",
        "abstract": "In this paper, we study Kropina metrics with isotropic scalar curvature.\n First, we obtain the expressions of Ricci curvature tensor and scalar\n curvature. Then, we characterize the Kropina metrics with isotropic scalar\n curvature on by tensor analysis.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1310.5055",
        "title": "Pathologies of the Brauer-Manin obstruction",
        "abstract": "We construct a conic bundle over an elliptic curve over a real quadratic\n field that is a counterexample to the Hasse principle not explained by the\n \\'etale Brauer-Manin obstruction. We also give simple examples of threefolds\n with the same property that are families of 2-dimensional quadrics, and discuss\n some other examples and general properties of the Brauer-Manin obstruction.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2304.14566",
        "title": "Interacting galaxies in the IllustrisTNG simulations -- V: Comparing the\n  influence of star-forming vs. passive companions",
        "abstract": "We study interacting galaxy pairs in the TNG100-1 and TNG300-1 cosmological\n simulations using previously generated closest companion samples. We study the\n specific star formation rates (sSFR) of massive ($10^{10} M_{\\odot} < M_* <\n 10^{12} M_{\\odot}$) galaxies at $z \\leq 0.2$ as a function of separation from\n the closest companion galaxy. We split our sample based on whether the\n companion galaxy is star-forming or passive. We find that galaxies with close\n star-forming companions have sSFRs that are enhanced (on average) by a factor\n of $2.9 \\pm 0.3$ in TNG100-1 and $2.27 \\pm 0.06$ in TNG300-1 compared to\n controls, with enhancements present out to separations of $\\sim 300$ kpc.\n Galaxies with passive companions in TNG300-1 exhibit mild sSFR suppression\n ($\\sim12$ percent) at 100-300 kpc and small sSFR enhancements at separations\n below 50 kpc. sSFR suppression is strongest in pairs where the galaxy's stellar\n mass is more than 2 times that of its passive companion. By generating a\n stellar mass-matched (\"twinned\") sample in TNG300-1, we show that differences\n in sSFR trends between companion types are not a result of intrinsic stellar\n mass differences in star-forming vs. passive galaxies. We compare with an\n analogous sample of galaxy pairs from SDSS, finding consistent results between\n observations and simulations. Overall, we find that star-forming galaxies show\n enhanced sSFRs regardless of companion type, but that galaxies with close\n passive companions are more likely to be passive themselves.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2402.05541",
        "title": "Reinforcement Learning as a Catalyst for Robust and Fair Federated\n  Learning: Deciphering the Dynamics of Client Contributions",
        "abstract": "Recent advancements in federated learning (FL) have produced models that\n retain user privacy by training across multiple decentralized devices or\n systems holding local data samples. However, these strategies often neglect the\n inherent challenges of statistical heterogeneity and vulnerability to\n adversarial attacks, which can degrade model robustness and fairness.\n Personalized FL strategies offer some respite by adjusting models to fit\n individual client profiles, yet they tend to neglect server-side aggregation\n vulnerabilities. To address these issues, we propose Reinforcement Federated\n Learning (RFL), a novel framework that leverages deep reinforcement learning to\n adaptively optimize client contribution during aggregation, thereby enhancing\n both model robustness against malicious clients and fairness across\n participants under non-identically distributed settings. To achieve this goal,\n we propose a meticulous approach involving a Deep Deterministic Policy\n Gradient-based algorithm for continuous control of aggregation weights, an\n innovative client selection method based on model parameter distances, and a\n reward mechanism guided by validation set performance. Empirically, extensive\n experiments demonstrate that, in terms of robustness, RFL outperforms the\n state-of-the-art methods, while maintaining comparable levels of fairness,\n offering a promising solution to build resilient and fair federated systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2403.04332",
        "title": "A new metric for the comparison of permittivity models in terahertz\n  time-domain spectroscopy",
        "abstract": "We present a robust method, as well as a new metric, for the comparison of\n permittivity models in terahertz timedomain spectroscopy (THz-TDS). In this\n work, we perform an extensive noise analysis of a THz-TDS system, we remove and\n model the unwanted deterministic noises and implement them into our fitting\n process. This is done using our open-source software, Fit@TDS, available at :\n https://github.com/THzbiophotonics/Fit-TDS. This work is the first step towards\n the derivation of uncertainties, and therefore the use of error bars. We hope\n that this will lead to performing analytical analysis with THz-TDS, as results\n obtained from different setups will be comparable. Finally, we apply this\n protocol to the study of a $\\alpha$-lactose monohydrate pellet in order to give\n more insight on the molecular dynamics behind the absorption peaks. The\n comparison with simulation results is made easier thanks to the probabilities\n derived from the metric.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2108.01307",
        "title": "Shock wave quantum memory in shocked detector",
        "abstract": "Usual uniformly accelerated frame, in Dray-'t Hooft spacetime, does not see\n the any quantum imprint on Unruh effect due to localised shock wave in\n Minkowski spacetime. Here we argue that such non-appearance of quantum memory\n is specific to those particular observers which do not incorporate the presence\n of wave in their trajectory. In fact a detector, associated with a frame which\n is affected by the shock, can not be trivial in terms of its response. We\n explicitly show that the later type of frame detects particle in the shock wave\n Minkowski vacuum which is the effect of shock. Therefore this quantum memory is\n very special to specific class of observers as far as Dray-'t Hooft spacetime\n is concerned. We analyse for a null like trajectory along which the detector is\n moving.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1104.4094",
        "title": "Constraining Decaying Dark Matter",
        "abstract": "We revisited the decaying dark matter (DDM) model, in which one collisionless\n particle decays early into two collisionless particles, that are potentially\n dark matter particles today. The effect of DDM will be manifested in the cosmic\n microwave background (CMB) and structure formation. With a systematic\n modification of CMB calculation tool \\texttt{camb}, we can numerically\n calculated this effect, and compare it to observations. Further Markov Chain\n Monte Carlo \\texttt{cosmomc} runnings update the constraints in that model: the\n free streaming length $\\lambda_{FS}\\lesssim0.5$Mpc for nonrelativistic decay,\n and $((M_{DDM}/keV) Y)^2 (T_d/yr)\\lesssim5\\times10^{-5}$ for relativistic\n decay.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1403.7128",
        "title": "An Early Look of Comet C/2013 A1 (Siding Spring): Breathtaker or\n  Nightmare?",
        "abstract": "The dynamically new comet, C/2013 A1 (Siding Spring), is to make a close\n approach to Mars on 2014 October 19 at 18:30 UT at a distance of 40+/-1 Martian\n radius. Such extremely rare event offers a precious opportunity for the\n spacecrafts on Mars to closely study a dynamically new comet itself as well as\n the planet-comet interaction. Meanwhile, the high speed meteoroids released\n from C/Siding Spring also pose a threat to physically damage the spacecrafts.\n Here we present our observations and modeling results of C/Siding Spring to\n characterize the comet and assess the risk posed to the spacecrafts on Mars. We\n find that the optical tail of C/Siding Spring is dominated by larger particles\n at the time of the observation. Synchrone simulation suggests that the comet\n was already active in late 2012 when it was more than 7 AU from the Sun. By\n parameterizing the dust activity with a semi-analytic model, we find that the\n ejection speed of C/Siding Spring is comparable to comets such as the target of\n the Rosetta mission, 67P/Churyumov-Gerasimenko. Under nominal situation, the\n simulated dust cone will miss the planet by about 20 Martian radius. At the\n extreme ends of uncertainties, the simulated dust cone will engulf Mars, but\n the meteoric influx at Mars is still comparable to the nominal sporadic influx,\n seemly indicating that intense and enduring meteoroid bombardment due to\n C/Siding Spring is unlikely. Further simulation also suggests that\n gravitational disruption of the dust tail may be significant enough to be\n observable at Earth.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1808.05141",
        "title": "Age of Information Minimization for an Energy Harvesting Source with\n  Updating Erasures: With and Without Feedback",
        "abstract": "Consider an energy harvesting (EH) sensor that continuously monitors a system\n and sends time-stamped status update to a destination. The sensor harvests\n energy from nature and uses it to power its updating operations. The\n destination keeps track of the system status through the successfully received\n updates. With the recently introduced information freshness metric \"Age of\n Information\" (AoI), our objective is to design optimal online status updating\n policy to minimize the long-term average AoI at the destination, subject to the\n energy causality constraint at the sensor. Due to the noisy channel between the\n sensor and the destination, each transmitted update may be erased with a fixed\n probability, and the AoI at the destination will be reset to zero only when an\n update is successfully received. We first consider status updating without\n feedback available to the sensor and show that the Best-effort Uniform updating\n (BU) policy is optimal. We then investigate status updating with perfect\n feedback to the sensor and prove the optimality of the Best-effort Uniform\n updating with Retransmission (BUR) policy. In order to prove the optimality of\n the proposed policies, for each case, we first identify a lower bound on the\n long-term average AoI among a broad class of online policies, and then\n construct a sequence of virtual policies to approach the lower bound\n asymptotically. Since those virtual policies are sub-optimal to the original\n policy, the original policy is thus optimal.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2004.01778",
        "title": "Elliptic equations with VMO a, b$\\,\\in L_{d}$, and c$\\,\\in L_{d/2}$",
        "abstract": "We consider elliptic equations with operators $L=a^{ij}D_{ij}+b^{i}D_{i}-c$\n with $a$ being almost in VMO, $b\\in L_{d}$ and $c\\in L_{q}$, $c\\geq0$, $d>q\\geq\n d/2$. We prove the solvability of $Lu=f\\in L_{p}$ in bounded $C^{1,1}$-domains,\n $1<p\\leq q$, and of $\\lambda u-Lu=f$ in the whole space for any $\\lambda>0$.\n Weak uniqueness of the martingale problem associated with such operators is\n also obtained.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2108.10917",
        "title": "Identifying the Dirac point composition in Bi1-xSbx alloys using the\n  temperature dependence of quantum oscillations",
        "abstract": "The thermal chiral anomaly is a new mechanism for thermal transport that\n occurs in Weyl semimetals (WSM). It is attributed to the generation and\n annihilation of energy at Weyl points of opposite chirality. The effect was\n observed in the Bi1-xSbx alloy system, at x=11% and 15%, which are topological\n insulators at zero field and driven into an ideal WSM phase by an external\n field. Given that the experimental uncertainty on x is of the order of 1%, any\n systematic study of the effect over a wider range of x requires precise\n knowledge of the transition composition xc at which the electronic bands at the\n L-point in these alloys have Dirac-like dispersions. At x>xc, the L-point bands\n are inverted and become topologically non-trivial. In the presence of a\n magnetic field along the trigonal direction, these alloys become WSMs. This\n paper describes how the temperature dependence of the frequency of the\n Shubnikov-de Haas oscillations F(x,T) at temperatures of the order of the\n cyclotron energy can be used to find xc and characterize the topology of the\n electronic Fermi surface. Alloys with topologically trivial bands have\n dF(x,T)/dT>0, those with Dirac/Weyl fermions display dF(x,T)/dT<0.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1911.07062",
        "title": "N-HANS: Introducing the Augsburg Neuro-Holistic Audio-eNhancement System",
        "abstract": "N-HANS is a Python toolkit for in-the-wild audio enhancement, including\n speech, music, and general audio denoising, separation, and selective noise or\n source suppression. The functionalities are realised based on two neural\n network models sharing the same architecture, but trained separately. The\n models are comprised of stacks of residual blocks, each conditioned on\n additional speech or environmental noise recordings for adapting to different\n unseen speakers or environments in real life. In addition to a Python API, a\n command line interface is provided to researchers and developers, both of which\n are documented at https://github.com/N-HANS/N-HANS. Experimental results\n indicate that N-HANS achieves outstanding performance, and ensure its reliable\n usage in real-life audio and speech-related tasks, reaching very high audio and\n speech quality.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/0502159",
        "title": "D-branes in Lorentzian AdS(3)",
        "abstract": "We study the exact construction of D-branes in Lorentzian AdS(3). We start by\n defining a family of conformal field theories that gives a natural Euclidean\n version of the SL(2,R) CFT and does not correspond to H(3)+, the analytic\n continuation of AdS(3). We argue that one can recuperate the exact CFT results\n of Lorentzian AdS(3), upon an analytic continuation in the moduli space of\n these conformal field theories. Then we construct exact boundary states for\n various symmetric and symmetry-breaking D-branes in AdS(3).",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1211.2310",
        "title": "$\\omega$-Operads of Coendomorphisms for Higher Structures",
        "abstract": "It is well known that strict $\\omega$-categories, strict $\\omega$-functors,\n strict natural $\\omega$-transformations, and so on, form a strict\n $\\omega$-category. A similar property for weak $\\omega$-categories is one of\n the main hypotheses in higher category theory in the globular setting. In this\n paper we show that there is a natural globular $\\omega$-operad which acts on\n the globular set of weak $\\omega$-categories, weak $\\omega$-functors, weak\n natural $\\omega$-transformations, and so on. Thus to prove the hypothesis it\n remains to prove that this $\\omega$-operad is contractible in Batanin's sense.\n To construct such an $\\omega$-operad we introduce more general technology and\n suggest a definition of $\\omega$-operad with the \\textit{fractal property}. If\n an $\\omega$-operad $B^{0}_{P}$ has this property then one can define a globular\n set of all higher $B^{0}_{P}$-transformations and, moreover, this globular set\n has a $B^{0}_{P}$-algebra structure.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1407.0036",
        "title": "Controlling systems that drift through a tipping point",
        "abstract": "Slow parameter drift is common in many systems (e.g., the amount of\n greenhouse gases in the terrestrial atmosphere is increasing). In such\n situations, the attractor on which the system trajectory lies can be destroyed,\n and the trajectory will then go to another attractor of the system. We consider\n the case where there are more than one of these possible final attractors, and\n we ask whether we can control the outcome (i.e., the attractor that ultimately\n captures the trajectory) using only small controlling perturbations.\n Specifically, we consider the problem of controlling a noisy system whose\n parameter slowly drifts through a saddle-node bifurcation taking place on a\n fractal boundary between the basins of multiple attractors. We show that, when\n the noise level is low, a small perturbation of size comparable to the noise\n amplitude applied at a single point in time can ensure that the system will\n evolve toward a target attracting state with high probability. For a range of\n noise levels, we find that the minimum size of perturbation required for\n control is much smaller within a time period that starts some time after the\n bifurcation, providing a \"window of opportunity\" for driving the system toward\n a desirable state. We refer to this procedure as tipping point control.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1301.6491",
        "title": "SINR-based k-coverage probability in cellular networks with arbitrary\n  shadowing",
        "abstract": "We give numerically tractable, explicit integral expressions for the\n distribution of the signal-to-interference-and-noise-ratio (SINR) experienced\n by a typical user in the down-link channel from the k-th strongest base\n stations of a cellular network modelled by Poisson point process on the plane.\n Our signal propagation-loss model comprises of a power-law path-loss function\n with arbitrarily distributed shadowing, independent across all base stations,\n with and without Rayleigh fading. Our results are valid in the whole domain of\n SINR, in particular for SINR<1, where one observes multiple coverage. In this\n latter aspect our paper complements previous studies reported in [Dhillon et\n al. JSAC 2012].",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1606.02219",
        "title": "A translation invariant bipolaron in the Holstein model and\n  superconductivity",
        "abstract": "Large-radius translation invariant (TI) bipolarons are considered in a\n one-dimensional Holstein molecular chain. Criteria of their stability are\n obtained. The energy of a translation invariant bipolaron is shown to be lower\n than that of a bipolaron with broken symmetry. The results obtained are applied\n to the problem of superconductivity in 1D-systems. It is shown that\n TI-bipolaron mechanism of Bose-Einstein condensation can support\n superconductivity even for infinite chain.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2009.14790",
        "title": "BERT for Monolingual and Cross-Lingual Reverse Dictionary",
        "abstract": "Reverse dictionary is the task to find the proper target word given the word\n description. In this paper, we tried to incorporate BERT into this task.\n However, since BERT is based on the byte-pair-encoding (BPE) subword encoding,\n it is nontrivial to make BERT generate a word given the description. We propose\n a simple but effective method to make BERT generate the target word for this\n specific task. Besides, the cross-lingual reverse dictionary is the task to\n find the proper target word described in another language. Previous models have\n to keep two different word embeddings and learn to align these embeddings.\n Nevertheless, by using the Multilingual BERT (mBERT), we can efficiently\n conduct the cross-lingual reverse dictionary with one subword embedding, and\n the alignment between languages is not necessary. More importantly, mBERT can\n achieve remarkable cross-lingual reverse dictionary performance even without\n the parallel corpus, which means it can conduct the cross-lingual reverse\n dictionary with only corresponding monolingual data. Code is publicly available\n at https://github.com/yhcc/BertForRD.git.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2209.11039",
        "title": "Constant-Time-Delay Interferences In Near-Field SAR: Analysis And\n  Suppression In Image Domain",
        "abstract": "Inevitable interferences exist for the SAR system, adversely affecting the\n imaging quality. However, current analysis and suppression methods mainly focus\n on the far-field situation. Due to different sources and characteristics of\n interferences, they are not applicable in the near field. To bridge this gap,\n in the first time, analysis and the suppression method of interferences in\n near-field SAR are presented in this work. We find that echoes from both the\n nadir points and the antenna coupling are the main causes, which have the\n constant-time-delay feature. To characterize this, we further establish an\n analytical model. It reveals that their patterns in 1D, 2D and 3D imaging\n results are all comb-like, while those of targets are point-like. Utilizing\n these features, a suppression method in image domain is proposed based on\n low-rank reconstruction. Measured data are used to validate the correctness of\n our analysis and the effectiveness of the suppression method.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1504.02338",
        "title": "Kernel Manifold Alignment",
        "abstract": "We introduce a kernel method for manifold alignment (KEMA) and domain\n adaptation that can match an arbitrary number of data sources without needing\n corresponding pairs, just few labeled examples in all domains. KEMA has\n interesting properties: 1) it generalizes other manifold alignment methods, 2)\n it can align manifolds of very different complexities, performing a sort of\n manifold unfolding plus alignment, 3) it can define a domain-specific metric to\n cope with multimodal specificities, 4) it can align data spaces of different\n dimensionality, 5) it is robust to strong nonlinear feature deformations, and\n 6) it is closed-form invertible which allows transfer across-domains and data\n synthesis. We also present a reduced-rank version for computational efficiency\n and discuss the generalization performance of KEMA under Rademacher principles\n of stability. KEMA exhibits very good performance over competing methods in\n synthetic examples, visual object recognition and recognition of facial\n expressions tasks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2001.08316",
        "title": "Constraining the $^{22}$Ne($\\alpha$,$\\gamma$)$^{26}$Mg and\n  $^{22}$Ne($\\alpha$,n)$^{25}$Mg reaction rates using sub-Coulomb\n  $\\alpha$-transfer reactions",
        "abstract": "The $^{22}$Ne($\\alpha$,$\\gamma$)$^{26}$Mg and $^{22}$Ne($\\alpha$,n)$^{25}$Mg\n reactions play an important role in astrophysics because they have significant\n influence on the neutron flux during the weak branch of the s-process. We\n constrain the astrophysical rates for these reactions by measuring partial\n $\\alpha$-widths of resonances in $^{26}$Mg located in the Gamow window for the\n $^{22}$Ne+$\\alpha$ capture. These resonances were populated using\n $^{22}$Ne($^6$Li,d)$^{26}$Mg and $^{22}$Ne($^7$Li,t)$^{26}$Mg reactions at\n energies near the Coulomb barrier. At these low energies $\\alpha$-transfer\n reactions favor population of low spin states and the extracted partial\n $\\alpha$-widths for the observed resonances exhibit only minor dependence on\n the model parameters. The astrophysical rates for both the\n $^{22}$Ne($\\alpha$,$\\gamma$)$^{26}$Mg and the $^{22}$Ne($\\alpha$,n)$^{25}$Mg\n reactions are shown to be significantly different than the previously suggested\n values.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1102.4148",
        "title": "On cluster theory and quantum dilogarithm identities",
        "abstract": "These are expanded notes from three survey lectures given at the 14th\n International Conference on Representations of Algebras (ICRA XIV) held in\n Tokyo in August 2010. We first study identities between products of quantum\n dilogarithm series associated with Dynkin quivers following Reineke. We then\n examine similar identities for quivers with potential and link them to\n Fomin-Zelevinsky's theory of cluster algebras. Here we mainly follow ideas due\n to Bridgeland, Fock-Goncharov, Kontsevich-Soibelman and Nagao.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1709.09468",
        "title": "Dual structure in the charge excitation spectrum of electron-doped\n  cuprates",
        "abstract": "Motivated by the recent resonant x-ray scattering (RXS) and resonant\n inelastic x-ray scattering (RIXS) experiments in electron-doped cuprates, we\n study the charge excitation spectrum in a layered t-J model with the long-range\n Coulomb interaction. We show that the spectrum is not dominated by a specific\n type of charge excitations, but by different kinds of charge fluctuations, and\n is characterized by a dual structure in the energy space. Low-energy charge\n excitations correspond to various types of bond-charge fluctuations driven by\n the exchange term (J-term) whereas high-energy charge excitations are due to\n usual on-site charge fluctuations and correspond to plasmon excitations above\n the particle-hole continuum. The interlayer coupling, which is frequently\n neglected in many theoretical studies, is particularly important to the\n high-energy charge excitations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2007.07767",
        "title": "Bi-objective facility location under uncertainty with an application in\n  last-mile disaster relief",
        "abstract": "Multiple and usually conflicting objectives subject to data uncertainty are\n main features in many real-world problems. Consequently, in practice,\n decision-makers need to understand the trade-off between the objectives,\n considering different levels of uncertainty in order to choose a suitable\n solution. In this paper, we consider a two-stage bi-objective single source\n capacitated model as a base formulation for designing a last-mile network in\n disaster relief where one of the objectives is subject to demand uncertainty.\n We analyze scenario-based two-stage risk-neutral stochastic programming,\n adaptive (two-stage) robust optimization, and a two-stage risk-averse\n stochastic approach using conditional value-at-risk (CVaR). To cope with the\n bi-objective nature of the problem, we embed these concepts into two criterion\n space search frameworks, the $\\epsilon$-constraint method and the balanced box\n method, to determine the Pareto frontier. Additionally, a matheuristic\n technique is developed to obtain high-quality approximations of the Pareto\n frontier for large-size instances. In an extensive computational experiment, we\n evaluate and compare the performance of the applied approaches based on\n real-world data from a Thies drought case, Senegal.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2311.01307",
        "title": "The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",
        "abstract": "Large Language Models (LLMs) make natural\ninterfaces to factual knowledge, but their usefulness is limited by their tendency to deliver\ninconsistent answers to semantically equivalent\nquestions. For example, a model might predict\nboth \u201cAnne Redpath passed away in Edinburgh.\u201d\nand \u201cAnne Redpath\u2019s life ended in London.\u201d In\nthis work, we identify potential causes of inconsistency and evaluate the effectiveness of\ntwo mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus. Our\nresults on the LLaMA and Atlas models show\nthat both strategies reduce inconsistency while\nretrieval augmentation is considerably more efficient. We further consider and disentangle the\nconsistency contributions of different components of Atlas. For all LMs evaluated we find\nthat syntactical form and other evaluation task\nartifacts impact consistency. Taken together,\nour results provide a better understanding of\nthe factors affecting the factual consistency of\nlanguage models.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2306.01116",
        "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",
        "abstract": "Large language models are commonly trained on a mixture of filtered web data and curated \u201chigh-quality\u201d corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our REFINEDWEB dataset, and 1.3/7.5B parameters language models trained on it*.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2001.08435",
        "title": "The Pushshift Reddit Dataset",
        "abstract": "Social media data has become crucial to the advancement of scientific understanding. However, even though it has become ubiquitous, just collecting large-scale social media data involves a high degree of engineering skill set and computational resources. In fact, research is often times gated by data engineering problems that must be overcome before analysis can proceed. This has resulted recognition of datasets as meaningful research contributions in and of themselves. Reddit, the so called \u201cfront page of the Internet,\u201d in particular has been the subject of numerous scientific studies. Although Reddit is relatively open to data acquisition compared to social media platforms like Facebook and Twitter, the technical barriers to acquisition still remain. Thus, Reddit\u2019s millions of subreddits, hundreds of millions of users, and hundreds of billions of comments are at the same time relatively accessible, but time consuming to collect and analyze systematically. In this paper, we present the Pushshift Reddit dataset. Pushshift is a social media data collection, analysis, and archiving platform that since 2015 has collected Reddit data and made it available to researchers. Pushshift\u2019s Reddit dataset is updated in real-time, and includes historical data back to Reddit\u2019s inception. In addition to monthly dumps, Pushshift provides computational tools to aid in searching, aggregating, and performing exploratory analysis on the entirety of the dataset. The Pushshift Reddit dataset makes it possible for social media researchers to reduce time spent in the data collection, cleaning, and storage phases of their projects.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2001.08438",
        "title": "The Pushshift Telegram Dataset",
        "abstract": "Messaging platforms, especially those with a mobile focus, have become increasingly ubiquitous in society. These mobile messaging platforms can have deceivingly large user bases, and in addition to being a way for people to stay in touch, are often used to organize social movements, as well as a place for extremists and other ne\u2019er-do-well to congregate. In this paper, we present a dataset from one such mobile messaging platform: Telegram. Our dataset is made up of over 27.8K channels and 317M messages from 2.2M unique users. To the best of our knowledge, our dataset comprises the largest and most complete of its kind. In addition to the raw data, we also provide the source code used to collect it, allowing researchers to run their own data collection instance. We believe the Pushshift Telegram dataset can help researchers from a variety of disciplines interested in studying online social movements, protests, political extremism, and disinformation.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2404.17855",
        "title": "Recontextualized Knowledge and Narrative Coalitions on Telegram",
        "abstract": "A defining characteristic of conspiracy texts is that they negotiate power and identity by recontextualizing prior knowledge. This dynamic has been shown to intensify on social media, where knowledge sources can readily be integrated into antagonistic narratives through hyperlinks. The objective of the present chapter is to further our understanding of this dynamic by surfacing and examining 1) how online conspiracy narratives recontextualize prior knowledge by coupling it with heterogeneous antagonistic elements, and 2) how such recontextualizing narratives operate as connectors around which diverse actors might form narrative coalitions. To this end, the chapter offers an empirical analysis of links to prior knowledge in public messaging channels from the Pushshift Telegram dataset. Using transferable methods from the field of bibliometrics, we find that politically extreme Telegram channels engage with a variety of established knowledge sources, including scientific journals, scientific repositories and other sources associated with the system of scholarly communication. Channels engaging with shared knowledge sources thereby form narrative coalitions ranging from scientific and technological imaginaries to far-right extremist and antisemitic conspiracy theories. Our analysis of these coalitions reveals (i) linguistic, political, and thematic forces that shape conspiracy narratives, (ii) emerging ideological, epistemological and ontological positions associated with online conspiracism, and (iii) how references to shared knowledge contribute to the communicability of conspiracy narratives.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.12053",
        "title": "AceGPT, Localizing Large Language Models in Arabic",
        "abstract": "This paper is devoted to the development of a\nlocalized Large Language Model (LLM) specifically for Arabic, a language imbued with\nunique cultural characteristics inadequately addressed by current mainstream models. Significant concerns emerge when addressing cultural sensitivity and local values. To address\nthis, the paper proposes a comprehensive solution that includes further pre-training with\nArabic texts, Supervised Fine-Tuning (SFT)\nutilizing native Arabic instructions, and GPT-4\nresponses in Arabic, alongside Reinforcement\nLearning with AI Feedback (RLAIF) employing a reward model attuned to local culture and\nvalues. The goal is to cultivate culturally cognizant and value-aligned Arabic LLMs capable of accommodating the diverse, applicationspecific needs of Arabic-speaking communities. Comprehensive evaluations reveal that\nthe resulting model, dubbed \u2018AceGPT\u2019, sets\nthe state-of-the-art standard for open Arabic\nLLMs across various benchmarks. Codes,\ndata, and models are in https://github.com/\nFreedomIntelligence/AceGPT.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2308.16149",
        "title": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models",
        "abstract": "We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model \u2014the foundation Jais model, and an instruction-tuned Jais-chat variant\u2014 with the aim of promoting research on Arabic LLMs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2405.12496",
        "title": "A Survey of Integrating Wireless Technology into Active Noise Control",
        "abstract": "Active Noise Control (ANC) is a widely adopted technology for reducing environmental noise across various scenarios. This paper focuses on enhancing noise reduction performance, particularly through the refinement of signal quality fed into ANC systems. We discuss the main wireless technique integrated into the ANC system equipped with some innovative algorithms in diverse environments. Instead of using microphone arrays, which increase the computation complexity of the ANC system, to isolate multiple noise sources to improve noise reduction performance, the application of the wireless technique avoids extra computation demand. Wireless transmissions of reference, error, and control signals are also applied to improve the convergence performance of the ANC system. Furthermore, this paper lists some wireless ANC applications, such as earbuds, headphones, windows, and headrests, underscoring their adaptability and efficiency in various settings.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2211.09110",
        "title": "Holistic Evaluation of Language Models",
        "abstract": "Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what\u2019s missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios to the extent possible (87.5% of the time), ensuring that metrics beyond accuracy don\u2019t fall to the wayside, and that trade-offs across models and metrics are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to more deeply analyze specific aspects (e.g. knowledge, reasoning, memorization/copyright, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, including 21 scenarios that were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on a set of core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings concerning the interplay between different scenarios, metrics, and models. For full transparency, we release all raw model prompts and completions publicly1 for further analysis, as well as a general modular toolkit for easily adding new scenarios, models, metrics, and prompting strategies.2 We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2311.08896",
        "title": "HeLM: Highlighted Evidence augmented Language Model for Enhanced Table-to-Text Generation",
        "abstract": "Typically, Table-to-Text generation models directly expand tables line by line into a long string as input makes it difficult for generators (Language Models) to focus on saliency units or data units relevant to the query from the unstructured, lengthy strings. To address this issue, we propose a two-step solution called HeLM which consists of two modules: a table highlighter that identifies relevant row evidence, and a table summarizer that generates sentences based on the highlighted table. To facilitate this, we utilized the open-sourced large language model LLaMA2 as the backbone for these two modules and efficient finetuning on it. We also propose a searching algorithm along with label distillation to construct highlighting labels for obtaining the table highlighter. On both the FetaQA and QTSumm datasets, our approach achieved state-of-the-art results in ROUGE and BLEU scores. Additionally, experiment results show that highlighting the key evidence on input tables significantly enhances the model\u2019s performance and provides valuable interpretability.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2402.06052",
        "title": "High-quality Extragalactic Legacy-field Monitoring (HELM) with DECam",
        "abstract": "High-quality Extragalactic Legacy-field Monitoring (HELM) is a long-term observing program that photometrically monitors several well-studied extragalactic legacy fields with the Dark Energy Camera (DECam) imager on the CTIO 4m Blanco telescope. Since Feb 2019, HELM has been monitoring regions within COSMOS, XMM-LSS, CDF-S, S-CVZ, ELAIS-S1, and SDSS Stripe 82 with few-day cadences in the (u)gri(z) bands, over a collective sky area of \u223c38 deg2. The main science goal of HELM is to provide high-quality optical light curves for a large sample of active galactic nuclei (AGNs), and to build decades-long time baselines when combining past and future optical light curves in these legacy fields. These optical images and light curves will facilitate the measurements of AGN reverberation mapping lags, as well as studies of AGN variability and its dependences on accretion properties. In addition, the time-resolved and coadded DECam photometry will enable a broad range of science applications from galaxy evolution to time-domain science. We describe the design and implementation of the program and present the first data release that includes source catalogs and the first \u223c3.5 years of light curves during 2019A--2022A.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.09312",
        "title": "Semantic HELM: A Human-Readable Memory for Reinforcement Learning",
        "abstract": "Reinforcement learning agents deployed in the real world often have to cope with partially observable environments. Therefore, most agents employ memory mechanisms to approximate the state of the environment. Recently, there have been impressive success stories in mastering partially observable environments, mostly in the realm of computer games like Dota 2, StarCraft II, or MineCraft. However, existing methods lack interpretability in the sense that it is not comprehensible for humans what the agent stores in its memory. In this regard, we propose a novel memory mechanism that represents past events in human language. Our method uses CLIP to associate visual inputs with language tokens. Then we feed these tokens to a pretrained language model that serves the agent as memory and provides it with a coherent and human-readable representation of the past. We train our memory mechanism on a set of partially observable environments and find that it excels on tasks that require a memory component, while mostly attaining performance on-par with strong baselines on tasks that do not. On a challenging continuous recognition task, where memorizing the past is crucial, our memory mechanism converges two orders of magnitude faster than prior methods. Since our memory mechanism is human-readable, we can peek at an agent's memory and check whether crucial pieces of information have been stored. This significantly enhances troubleshooting and paves the way toward more interpretable agents.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2202.00120",
        "title": "QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers",
        "abstract": "The ability to have the same experience for different\nuser groups (i.e., accessibility) is one of the most important\ncharacteristics of Web-based systems. The same is true for\nKnowledge Graph Question Answering (KGQA) systems that\nprovide the access to Semantic Web data via natural language\ninterface. While following our research agenda on the multilingual aspect of accessibility of KGQA systems, we identified\nseveral ongoing challenges. One of them is the lack of multilingual\nKGQA benchmarks. In this work, we extend one of the most\npopular KGQA benchmarks \u2013 QALD-9 by introducing highquality questions\u2019 translations to 8 languages provided by native\nspeakers, and transferring the SPARQL queries of QALD-9\nfrom DBpedia to Wikidata, s.t., the usability and relevance\nof the dataset is strongly increased. Five of the languages \u2013\nArmenian, Ukrainian, Lithuanian, Bashkir and Belarusian \u2013 to\nour best knowledge were never considered in KGQA research\ncommunity before. The latter two of the languages are considered\nas \u201cendangered\u201d by UNESCO. We call the extended dataset\nQALD-9-plus and made it available online1.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.09841",
        "title": "Leveraging LLMs in Scholarly Knowledge Graph Question Answering",
        "abstract": "This paper presents a scholarly Knowledge Graph Question Answering (KGQA) that answers bibliographic natural language questions by leveraging a large language model (LLM) in a few-shot manner. The model initially identifies the top-n similar training questions related to a given test question via a BERT-based sentence encoder and retrieves their corresponding SPARQL. Using the top-n similar question-SPARQL pairs as an example and the test question creates a prompt. Then pass the prompt to the LLM and generate a SPARQL. Finally, runs the SPARQL against the underlying KG - ORKG (Open Research KG) endpoint and returns an answer. Our system achieves an F1 score of 99.0%, on SciQA - one of the Scholarly-QALD-23 challenge benchmarks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1809.09528",
        "title": "ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters",
        "abstract": "To bridge the gap between the capabilities of the state-of-the-art in factoid question answering (QA) and what users ask, we need large datasets of real user questions that capture the various question phenomena users are interested in, and the diverse ways in which these questions are formulated. We introduce ComQA, a large dataset of real user questions that exhibit different challenging aspects such as compositionality, temporal reasoning, and comparisons. ComQA questions come from the WikiAnswers community QA platform, which typically contains questions that are not satisfactorily answerable by existing search engine technology. Through a large crowdsourcing effort, we clean the question dataset, group questions into paraphrase clusters, and annotate clusters with their answers. ComQA contains 11, 214 questions grouped into 4,834 paraphrase clusters. We detail the process of constructing ComQA, including the measures taken to ensure its high quality while making effective use of crowdsourcing. We also present an extensive analysis of the dataset and the results achieved by state-of-the-art systems on ComQA, demonstrating that our dataset can be a driver of future research on QA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2101.06400",
        "title": "ComQA:Compositional Question Answering via Hierarchical Graph Neural Networks",
        "abstract": "With the development of deep learning techniques and large scale datasets, the question answering (QA) systems have been quickly improved, providing more accurate and satisfying answers. However, current QA systems either focus on the sentence-level answer, i.e., answer selection, or phrase-level answer, i.e., machine reading comprehension. How to produce compositional answers has not been throughout investigated. In compositional question answering, the systems should assemble several supporting evidence from the document to generate the final answer, which is more difficult than sentence-level or phrase-level QA. In this paper, we present a largescale compositional question answering dataset containing more than 120k human-labeled questions. The answer in this dataset is composed of discontiguous sentences in the corresponding document. To tackle the ComQA problem, we proposed a hierarchical graph neural networks, which represent the document from the low-level word to the high-level sentence. We also devise a question selection and node selection task for pre-training. Our proposed model achieves a significant improvement over previous machine reading comprehension methods and pre-training methods. Codes, dataset can be found at https://github.com/benywon/ComQA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2009.03300",
        "title": "MEASURING MASSIVE MULTITASK LANGUAGE UNDERSTANDING",
        "abstract": "We propose a new test to measure a text model\u2019s multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have nearrandom accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model\u2019s academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.17789",
        "title": "Spanish and LLM Benchmarks: is MMLU Lost in Translation?",
        "abstract": "The evaluation of Large Language Models (LLMs) is a key element in their continuous improvement process and many benchmarks have been developed to assess the performance of LLMs in different tasks and topics. As LLMs become adopted worldwide, evaluating them in languages other than English is increasingly important. However, most LLM benchmarks are simply translated using an automated tool and then run in the target language. This means that the results depend not only on the LLM performance in that language but also on the quality of the translation. In this paper, we consider the case of the well-known Massive Multitask Language Understanding (MMLU) benchmark. Selected categories of the benchmark are translated into Spanish using Azure Translator and ChatGPT4 and run on ChatGPT4. Next, the results are processed to identify the test items that produce different answers in Spanish and English. Those are then analyzed manually to understand if the automatic translation caused the change. The results show that a significant fraction of the failing items can be attributed to mistakes in the translation of the benchmark. These results make a strong case for improving benchmarks in languages other than English by at least revising the translations of the items and preferably by adapting the tests to the target language by experts.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2403.01858",
        "title": "An Improved Traditional Chinese Evaluation Suite for Foundation Model",
        "abstract": "We present TMMLU+, a comprehensive dataset designed for the Traditional Chinese massive multitask language understanding dataset. TMMLU+ is a multiple-choice question-answering dataset with 66 subjects from elementary to professional level. Compared to its predecessor, TMMLU, TMMLU+ is six times larger and boasts a more balanced subject distribution. We included benchmark results in TMMLU+ from closed-source models and 24 open-weight Chinese large language models of parameters ranging from 1.8B to 72B. Our findings reveal that Traditional Chinese models still trail behind their Simplified Chinese counterparts. Additionally, current large language models have yet to outperform human performance in average scores. We publicly release our dataset and the corresponding benchmark source code.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.11511",
        "title": "ComplexityNet: Increasing Language Model Inference Efficiency by Learning Task Complexity",
        "abstract": "We introduce ComplexityNet, a framework designed for the evaluation of task complexity and the allocation of tasks to Large Language Models (LLMs) of varying capabilities. This framework was applied to python code generation tasks, utilizing the Mostly Basic Python Problems (MBPP) dataset. To facilitate this, we first developed a set of labels to quantify task complexity accurately. Our methodology began with fine-tuning a small language model to predict the likelihood of generating accurate output across different LLMs. It achieved an accuracy of 79% in classifying task complexity, a significant increase from the 34% accuracy observed in the baseline model without fine-tuning. In the following step of allocating tasks, ComplexityNet reduced computational resource usage by 90% when compared to using the most complex model alone, while sustaining a high code generation accuracy of 86.7%. This study demonstrates that fine-tuning smaller models to categorize tasks based on their complexity can lead to a more balanced trade-off between accuracy and efficiency in the use of Large Language Models. Our findings suggest a promising direction for optimizing LLM applications, especially in resource-constrained environments.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2206.07682",
        "title": "Emergent Abilities of Large Language Models",
        "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2405.03548",
        "title": "MAmmoTH2: Scaling Instructions from the Web",
        "abstract": "Instruction tuning improves the reasoning abilities of large language models\n(LLMs), with data quality and scalability being the crucial factors. Most instruction\ntuning data come from human crowd-sourcing or GPT-4 distillation. We propose a\nparadigm to efficiently harvest 10 million naturally existing instruction data from\nthe pre-training web corpus to enhance LLM reasoning. Our approach involves\n(1) recalling relevant documents, (2) extracting instruction-response pairs, and (3)\nrefining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on\nthis dataset, we build MAmmoTH2 models, which significantly boost performance on\nreasoning benchmarks. Notably, MAmmoTH2-7B\u2019s (Mistral) performance increases\nfrom 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training\non any in-domain data. Further training MAmmoTH2 on public instruction tuning\ndatasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several\nreasoning and chatbot benchmarks. Our work demonstrates how to harvest largescale, high-quality instruction data without costly human annotation or GPT-4\ndistillation, providing a new paradigm for building better instruction tuning data.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2406.11409",
        "title": "CodeGemma: Open Code Models Based on Gemma",
        "abstract": "This paper introduces CodeGemma, a collection of specialized open code models built on top of Gemma, capable of a variety of code and natural language generation tasks. We release three model variants. CodeGemma 7B pretrained (PT) and instruction-tuned (IT) variants have remarkably resilient natural language understanding, excel in mathematical reasoning, and match code capabilities of other open models. CodeGemma 2B is a state-of-the-art code completion model designed for fast code infilling and open-ended generation in latency-sensitive settings.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1808.07042",
        "title": "CoQA: A Conversational Question Answering Challenge",
        "abstract": "Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems.1 Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp. github.io/coqa.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1910.00856",
        "title": "BookQA: Stories of Challenges and Opportunities",
        "abstract": "We present a system for answering questions based on the full text of books (BookQA), which first selects book passages given a question at hand, and then uses a memory network to reason and predict an answer. To improve generalization, we pretrain our memory network using artificial questions generated from book sentences. We experiment with the recently published NarrativeQA corpus, on the subset of Who questions, which expect book characters as answers. We experimentally show that BERT-based retrieval and pretraining improve over baseline results significantly. At the same time, we confirm that NarrativeQA is a highly challenging data set, and that there is need for novel research in order to achieve high-precision BookQA results. We analyze some of the bottlenecks of the current approach, and we argue that more research is needed on text representation, retrieval of relevant passages, and reasoning, including commonsense knowledge.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1712.07040",
        "title": "The NarrativeQA Reading Comprehension Challenge",
        "abstract": "Reading comprehension (RC)---in contrast to information retrieval---requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.09400",
        "title": "CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages",
        "abstract": "The driving factors behind the development of large language models (LLMs) with impressive learning capabilities are their colossal model sizes and extensive training datasets. Along with the progress in natural language processing, LLMs have been frequently made accessible to the public to foster deeper investigation and applications. However, when it comes to training datasets for these LLMs, especially the recent state-of-the-art models, they are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is fully released to the public in HuggingFace to facilitate research and advancements in multilingual LLMs: this https URL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2405.01623",
        "title": "The Binary Black Hole Merger Rate Deviates From the Cosmic Star Formation Rate: A Tug of War Between Metallicity and Delay Times",
        "abstract": "Gravitational-wave detectors are now making it possible to investigate how the merger rate of binary black holes (BBHs) evolves with redshift. In this study, we examine whether the BBH merger rate of isolated binaries deviates from a scaled star formation rate density (SFRD) -- a frequently used model in state-of-the-art research. To address this question, we conduct population synthesis simulations using COMPAS with a grid of stellar evolution models, calculate their cosmological merger rates, and compare them to a scaled SFRD. We find that our simulated rates deviate by factors up to 3.5\u00d7 at z\u223c0 and 5\u00d7 at z\u223c9 due to two main phenomena: (i) The formation efficiency of BBHs is an order of magnitude higher at low metallicities than at solar metallicity; and (ii) BBHs experience a wide range of delays (from a few Myr to many Gyr) between formation and merger. Deviations are similar when comparing to a delayed SFRD, and even larger (up to \u223c10\u00d7) when comparing to SFRD-based models scaled to the local merger rate. Interestingly, our simulations find that the BBH delay time distribution is redshift-dependent, increasing the complexity of the redshift distribution of mergers. We find similar results for simulated merger rates of BHNSs and BNSs. We conclude that the rate of BBH, BHNS, and BNS mergers from the isolated channel can significantly deviate from a scaled SFRD, and that future measurements of the merger rate will provide insights into the formation pathways of gravitational-wave sources.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2206.04615",
        "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
        "abstract": "Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit \"breakthrough\" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2402.15202",
        "title": "Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models",
        "abstract": "Impressive results have been achieved in natural language processing (NLP) tasks through the training of large language models (LLMs). However, these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility. To tackle this issue, various finetuning-based and decoding-based approaches have been utilized to mitigate toxicity. However, these methods typically necessitate additional costs such as high-quality training data or auxiliary models. In this paper, we propose fine-grained detoxification via instance-level prefixes (FGDILP) to mitigate toxic text without additional cost. Specifically, FGDILP contrasts the contextualized representation in attention space using a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level. This allows for constructing fine-grained subtoxicity vectors, which enables collaborative detoxification by fusing them to correct the normal generation process when provided with a raw prompt. We validate that FGDILP enables controlled text generation with regard to toxicity at both the utterance and context levels. Our method surpasses promptbased baselines in detoxification, although at a slight cost to generation fluency and diversity.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2107.07498v2",
        "title": "FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark",
        "abstract": "Pretrained Language Models (PLMs) have achieved tremendous success in natural lan- guage understanding tasks. While different learning schemes\u2014fine-tuning, zero-shot, and few-shot learning\u2014have been widely explored and compared for languages such as English, there is comparatively little work in Chinese to fairly and comprehensively evaluate and compare these methods and thus hinders cu- mulative progress. In this paper, we intro- duce the Chinese Few-shot Learning Evalu- ation Benchmark (FewCLUE), the first com- prehensive few-shot evaluation benchmark in Chinese. It includes nine tasks, ranging from single-sentence and sentence-pair clas- sification tasks to machine reading compre- hension tasks. We systematically evaluate five state-of-the-art (SOTA) few-shot learning methods (including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare their perfor- mance with fine-tuning and zero-shot learn- ing schemes on the newly constructed Few- CLUE benchmark. Experimental results re- veal that: 1) The effect of different few-shot learning methods is sensitive to the pre-trained model to which the methods are applied; 2) PET and P-tuning achieve the best overall per- formance with RoBERTa and ERNIE respec- tively. Our benchmark is used in the few- shot learning contest of NLPCC 20211. In ad- dition, we provide a user-friendly toolkit, as well as an online leaderboard to help facilitate further progress on Chinese few-shot learning. We provide a baseline performance on differ- ent learning methods, a reference for future re- search.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1811.00937v2",
        "title": "COMMONSENSEQA: A Question Answering Challenge Targeting Commonsense Knowledge",
        "abstract": "\nWhen answering a question, people often draw upon their rich world knowledge in addi- tion to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present COMMONSENSEQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from CON- CEPTNET (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discrim- inate in turn between each of the target con- cepts. This encourages workers to create ques- tions with complex semantics that often re- quire prior knowledge. We create 12,247 ques- tions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and ob- tains 56% accuracy, well below human perfor- mance, which is 89%.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1908.08025v3",
        "title": "WikiCREM: A Large Unsupervised Corpus for Coreference Resolution",
        "abstract": "Pronoun resolution is a major area of natural language understanding. However, large-scale training sets are still scarce, since manually la- belling data is costly. In this work, we intro- duce WIKICREM (Wikipedia CoREferences Masked) a large-scale, yet accurate dataset of pronoun disambiguation instances. We use a language-model-based approach for pro- noun resolution in combination with our WI- KICREM dataset. We compare a series of models on a collection of diverse and challeng- ing coreference resolution problems, where we match or outperform previous state-of-the- art approaches on 6 out of 7 datasets, such as GAP, DPR, WNLI, PDP, WINOBIAS, and WINOGENDER. We release our model to be used off-the-shelf for solving pronoun disam- biguation.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1906.02123v1",
        "title": "SP-10K: A Large-scale Evaluation Set for Selectional Preference Acquisition",
        "abstract": "Selectional Preference (SP) is a commonly ob- served language phenomenon and proved to be useful in many natural language processing tasks. To provide a better evaluation method for SP models, we introduce SP-10K, a large- scale evaluation set that provides human rat- ings for the plausibility of 10,000 SP pairs over five SP relations, covering 2,500 most fre- quent verbs, nouns, and adjectives in Ameri- can English. Three representative SP acquisi- tion methods based on pseudo-disambiguation are evaluated with SP-10K. To demonstrate the importance of our dataset, we investi- gate the relationship between SP-10K and the commonsense knowledge in ConceptNet5 and show the potential of using SP to represent the commonsense knowledge. We also use the Winograd Schema Challenge to prove that the proposed new SP relations are essential for the hard pronoun coreference resolution problem.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2301.12139v3",
        "title": "Bipol: Multi-axes Evaluation of Bias with Explainability in Benchmark Datasets",
        "abstract": "We investigate five English NLP benchmark datasets (on the superGLUE leaderboard) and two Swedish datasets for bias, along multiple axes. The datasets are the following: Boolean Question (Boolq), CommitmentBank (CB), Winograd Schema Challenge (WSC), Wino- gender diagnostic (AXg), Recognising Textual Entailment (RTE), Swedish CB, and SWEDN. Bias can be harmful and it is known to be com- mon in data, which ML models learn from. In order to mitigate bias in data, it is crucial to be able to estimate it objectively. We use bipol, a novel multi-axes bias metric with explainability, to estimate and explain how much bias exists in these datasets. Multilingual, multi-axes bias evaluation is not very common. Hence, we also contribute a new, large Swedish bias-labeled dataset (of 2 million samples), translated from the English version and train the SotA mT5 model on it. In addition, we contribute new multi-axes lexica for bias detection in Swedish. We make the codes, model, and new dataset publicly available.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1811.08048v1",
        "title": "QUAREL: A Dataset and Models for Answering Questions about Qualitative Relationships",
        "abstract": "Many natural language questions require recognizing and rea- soning with qualitative relationships (e.g., in science, eco- nomics, and medicine), but are challenging to answer with corpus-based methods. Qualitative modeling provides tools that support such reasoning, but the semantic parsing task of mapping questions into those models has formidable chal- lenges. We present QUAREL, a dataset of diverse story questions involving qualitative relationships that character- ize these challenges, and techniques that begin to address them. The dataset has 2771 questions relating 19 different types of quantities. For example, \u201cJenny observes that the robot vacuum cleaner moves slower on the living room car- pet than on the bedroom carpet. Which carpet has more friction?\u201d We contribute (1) a simple and flexible concep- tual framework for representing these kinds of questions; (2) the QUAREL dataset, including logical forms, exempli- fying the parsing challenges; and (3) two novel models for this task, built as extensions of type-constrained semantic parsing. The first of these models (called QUASP+) signif- icantly outperforms off-the-shelf tools on QUAREL. The sec- ond (QUASP+ZERO) demonstrates zero-shot capability, i.e., the ability to handle new qualitative relationships without re- quiring additional training data, something not possible with previous models. This work thus makes inroads into answer- ing complex, qualitative questions that require reasoning, and scaling to new relationships at low cost. The dataset and mod- els are available at http://data.allenai.org/quarel.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1804.09301",
        "title": "Gender Bias in Coreference Resolution",
        "abstract": "We present an empirical study of gender bias in coreference resolution systems. We first in- troduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pro- noun gender. With these Winogender schemas, we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2404.19234",
        "title": "MULTI-HOP QUESTION ANSWERING OVER KNOWLEDGE GRAPHS USING LARGE LANGUAGE MODELS",
        "abstract": "Knowledge graphs (KGs) are large datasets with specific structures representing large knowledge bases (KB) where each node represents a key entity and relations amongst them are typed edges. Natural language queries formed to extract information from a KB entail starting from specific nodes and reasoning over multiple edges of the corresponding KG to arrive at the correct set of answer nodes. Traditional approaches of question answering on KG are based on (a) semantic parsing (SP), where a logical form (e.g., S-expression, SPARQL query, etc.) is generated using node and edge embeddings and then reasoning over these representations or tuning language models to generate the final answer directly, or (b) information-retrieval based that works by extracting entities and relations sequentially. In this work, we evaluate the capability of (LLMs) to answer questions over KG that involve multiple hops. We show that depending upon the size and nature of the KG we need different approaches to extract and feed the relevant information to an LLM since every LLM comes with a fixed context window. We evaluate our approach on six KGs with and without the availability of example-specific sub-graphs and show that both the IR and SP-based methods can be adopted by LLMs resulting in an extremely competitive performance.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1811.01747",
        "title": "The KNOWREF Coreference Corpus: Removing Gender and Number Cues for Difficult Pronominal Anaphora Resolution",
        "abstract": "We introduce a new benchmark for corefer- ence resolution and NLI, KNOWREF, that tar- gets common-sense understanding and world knowledge. Previous coreference resolution tasks can largely be solved by exploiting the number and gender of the antecedents, or have been handcrafted and do not reflect the diver- sity of naturally occurring text. We present a corpus of over 8,000 annotated text pas- sages with ambiguous pronominal anaphora. These instances are both challenging and re- alistic. We show that various coreference systems, whether rule-based, feature-rich, or neural, perform significantly worse on the task than humans, who display high inter- annotator agreement. To explain this perfor- mance gap, we show empirically that state-of- the art models often fail to capture context, in- stead relying on the gender or number of can- didate antecedents to make a decision. We then use problem-specific insights to propose a data-augmentation trick called antecedent switching to alleviate this tendency in mod- els. Finally, we show that antecedent switch- ing yields promising results on other tasks as well: we use it to achieve state-of-the-art re- sults on the GAP coreference task.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1903.00161",
        "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
        "abstract": "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. How- ever, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We in- troduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96k- question benchmark, a system must resolve references in a question, perhaps to multiple in- put positions, and perform discrete operations over them (such as addition, counting, or sort- ing). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and se- mantic parsing literatures on this dataset and show that the best systems only achieve 32.7% F1 on our generalized accuracy metric, while expert human performance is 96.4%. We ad- ditionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F1.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1712.07040",
        "title": "The NarrativeQA Reading Comprehension Challenge",
        "abstract": "Reading comprehension (RC)\u2014in contrast to information retrieval\u2014requires integrating in- formation and reasoning about events, enti- ties, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by ques- tions that can be solved by selecting answers using superficial information (e.g., local con- text similarity or global term frequency); they thus fail to test for the essential integrative as- pect of RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underly- ing narrative rather than relying on shallow pattern matching or salience. We show that al- though humans solve the tasks easily, standard RC models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1809.09528",
        "title": "ComQA: A Community-sourced Dataset for\nComplex Factoid Question Answering with Paraphrase Clusters\n",
        "abstract": "To bridge the gap between the capabilities of the state-of-the-art in factoid question an- swering (QA) and what users ask, we need large datasets of real user questions that cap- ture the various question phenomena users are interested in, and the diverse ways in which these questions are formulated. We introduce ComQA, a large dataset of real user questions that exhibit different challenging aspects such as compositionality, temporal reasoning, and comparisons. ComQA questions come from the WikiAnswers community QA platform, which typically contains questions that are not satisfactorily answerable by existing search engine technology. Through a large crowd- sourcing effort, we clean the question dataset, group questions into paraphrase clusters, and annotate clusters with their answers. ComQA contains 11, 214 questions grouped into 4,834 paraphrase clusters. We detail the process of constructing ComQA, including the measures taken to ensure its high quality while mak- ing effective use of crowdsourcing. We also present an extensive analysis of the dataset and the results achieved by state-of-the-art systems on ComQA, demonstrating that our dataset can be a driver of future research on QA.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2204.05895",
        "title": "XQA-DST: Multi-Domain and Multi-Lingual Dialogue State Tracking",
        "abstract": "Dialogue State Tracking (DST), a crucial com- ponent of task-oriented dialogue (ToD) sys- tems, keeps track of all important informa- tion pertaining to dialogue history: filling slots with the most probable values throughout the conversation. Existing methods generally rely on a predefined set of values and struggle to generalise to previously unseen slots in new domains. To overcome these challenges, we propose a domain-agnostic extractive question answering (QA) approach with shared weights across domains. To disentangle the complex domain information in ToDs, we train our DST with a novel domain filtering strategy by excluding out-of-domain question samples. With an independent classifier that predicts the presence of multiple domains given the context, our model tackles DST by extract- ing spans in active domains. Empirical results demonstrate that our model can efficiently leverage domain-agnostic QA datasets by two- stage fine-tuning while being both domain- scalable and open-vocabulary in DST. It shows strong transferability by achieving zero-shot domain-adaptation results on MultiWOZ 2.1 with an average JGA of 36.7%. It further achieves cross-lingual transfer with state-of- the-art zero-shot results, 66.2% JGA from En- glish to German and 75.7% JGA from English to Italian on WOZ 2.0.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1910.07475",
        "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
        "abstract": "Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challeng- ing. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned ex- tractive QA evaluation benchmark intended to spur research in this area.1 MLQA contains QA instances in 7 languages, English, Ara- bic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K in- stances in English and 5K in each other lan- guage, with each instance parallel between 4 languages on average. We evaluate state- of-the-art cross-lingual models and machine- translation-based baselines on MLQA. In all cases, transfer results are significantly behind training-language performance.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.06364",
        "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models",
        "abstract": "Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial Gen- eral Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses aver- age human performance on SAT, LSAT, and math competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordi- nary performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks that require complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understand- ing, knowledge, reasoning, and calculation) reveal these models\u2019 strengths and limitations, providing valuable insights into future directions for enhancing their general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a more meaningful and robust evaluation of foundation models\u2019 performance in real-world scenarios",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2010.11856",
        "title": "XOR QA: Cross-lingual Open-Retrieval Question Answering",
        "abstract": "Multilingual question answering tasks typi- cally assume that answers exist in the same language as the question. Yet in prac- tice, many languages face both information scarcity\u2014where languages have few reference articles\u2014and information asymmetry\u2014where questions reference concepts from other cul- tures. This work extends open-retrieval ques- tion answering to a cross-lingual setting en- abling questions from one language to be an- swered via answer content from another lan- guage. We construct a large-scale dataset built on 40K information-seeking questions across 7 diverse non-English languages that TYDI QA could not find same-language an- swers for. Based on this dataset, we introduce a task framework, called Cross-lingual Open- Retrieval Question Answering (XOR QA), that consists of three new tasks involving cross- lingual document retrieval from multilingual and English resources. We establish baselines with state-of-the-art machine translation sys- tems and cross-lingual pretrained models. Ex- perimental results suggest that XOR QA is a challenging task that will facilitate the devel- opment of novel techniques for multilingual question answering. Our data and code are available at https://nlp.cs.washington. edu/xorqa/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1803.05457",
        "title": "think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",
        "abstract": "We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering. Together, these constitute the AI2 Reasoning Challenge (ARC), which requires far more powerful knowl- edge and reasoning than previous challenges such as SQuAD or SNLI. The ARC question set is partitioned into a Chal- lenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (au- thored for human tests), and is the largest public-domain set of this kind (7,787 questions). We test several baselines on the Challenge Set, including leading neural models from the SQuAD and SNLI tasks, and find that none are able to signif- icantly outperform a random baseline, reflecting the difficult nature of this task. We are also releasing the ARC Corpus, a corpus of 14M science sentences relevant to the task, and im- plementations of the three neural baseline models tested. Can your model perform better? We pose ARC as a challenge to the community.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1810.00278",
        "title": "MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling",
        "abstract": "Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available. To address this fun- damental obstacle, we introduce the Multi- Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human writ- ten conversations spanning over multiple do- mains and topics. At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora. The contribution of this work apart from the open-sourced dataset labelled with dialogue belief states and dialogue actions is two-fold: firstly, a detailed description of the data collec- tion procedure along with a summary of data structure and analysis is provided. The pro- posed data-collection pipeline is entirely based on crowd-sourcing without the need of hir- ing professional annotators; secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a base- line for future studies.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1606.06031",
        "title": "The LAMBADA dataset:\nWord prediction requiring a broad discourse context",
        "abstract": "We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages shar- ing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preced- ing the target word. To succeed on LAM- BADA, computational models cannot sim- ply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA ex- emplifies a wide range of linguistic phe- nomena, and that none of several state-of- the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the develop- ment of new models capable of genuine understanding of broad context in natural language text.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2312.10253v1",
        "title": "Catwalk: A Unified Language Model Evaluation Framework for Many Datasets",
        "abstract": "The success of large language models has shifted the evaluation paradigms in natural lan- guage processing (NLP). The community\u2019s in- terest has drifted towards comparing NLP mod- els across many tasks, domains, and datasets, often at an extreme scale. This imposes new engineering challenges: efforts in construct- ing datasets and models have been fragmented, and their formats and interfaces are incom- patible. As a result, it often takes extensive (re)implementation efforts to make fair and con- trolled comparisons at scale.\nCatwalk aims to address these issues. Catwalk provides a unified interface to a broad range of existing NLP datasets and models, ranging from both canonical supervised training and fine-tuning, to more modern paradigms like in-context learning. Its carefully-designed ab- stractions allow for easy extensions to many others. Catwalk substantially lowers the bar- riers to conducting controlled experiments at scale. For example, we finetuned and evalu- ated over 64 models on over 86 datasets with a single command, without writing any code. Maintained by the AllenNLP team at the Allen Institute for Artificial Intelligence (AI2), Cat- walk is an ongoing open-source effort: https: //github.com/allenai/catwalk.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2004.12934v1",
        "title": "SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations",
        "abstract": "We introduce SCDE, a dataset to evaluate the performance of computational models through sentence prediction. SCDE is a human- created sentence cloze dataset, collected from public school English examinations. Our task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by English teachers. Ex- perimental results demonstrate that this task requires the use of non-local, discourse-level context beyond the immediate sentence neigh- borhood. The blanks require joint solving and significantly impair each other\u2019s context. Fur- thermore, through ablations, we show that the distractors are of high quality and make the task more challenging. Our experiments show that there is a significant performance gap be- tween advanced models (72%) and humans (87%), encouraging future models to bridge this gap.\n",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2401.06947",
        "title": "Parameter-Efficient Detoxification with Contrastive Decoding",
        "abstract": "The field of natural language generation has witnessed significant advancements in recent years, including the development of controllable text generation techniques. However, controlling the attributes of the generated text remains a challenge, especially when aiming to avoid undesirable behavior such as toxicity. In this work, we introduce Detoxification Generator (DETOXIGEN), an inference-time algorithm that steers the generation away from unwanted styles. DETOXIGEN is an ensemble of a pre-trained language model (generator) and a detoxifier. The detoxifier is trained intentionally on the toxic data representative of the undesirable attribute, encouraging it to generate text in that style exclusively. During the actual generation, we use the trained detoxifier to produce undesirable tokens for the generator to contrast against at each decoding step. This approach directly informs the generator to avoid generating tokens that the detoxifier considers highly likely. We evaluate DETOXIGEN on the commonly used REALTOXICITYPROMPTS benchmark (Gehman et al., 2020) with various language models as generators. We find that it significantly outperforms previous approaches in detoxification metrics while not compromising on the generation quality. Moreover, the detoxifier is obtained by soft prompt-tuning using the same backbone language model as the generator. Hence, DETOXIGEN requires only a tiny amount of extra weights from the virtual tokens of the detoxifier to be loaded into GPU memory while decoding, making it a promising lightweight, practical, and parameter-efficient detoxification strategy",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2109.07958",
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
        "abstract": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.16583",
        "title": "GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond",
        "abstract": "With the rapid advancement of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals1. We systematically evaluate 10+ leading LLMs as well as OpenAI\u2019s legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retrospective study on OpenAI\u2019s earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM\u2019s reasoning capability, which aspects of LLM capability can be improved by SFT and RLHF, how much is the alignment tax, etc. Our analysis sheds light on many of these questions, aiming to improve the transparency of advanced LLMs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2403.10131",
        "title": "RAFT: Adapting Language Model to Domain Specific RAG",
        "abstract": "Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally incorporate new information into the pretrained model either through RAG-based-prompting, or finetuning. However, the best methodology to incorporate information remains an open question. In this paper, we present Retrieval Augmented Fine Tuning (RAFT), a training recipe which improves the model\u2019s ability to answer questions in \"open-book\" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don\u2019t help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document to help answer the question. This coupled with RAFT\u2019s chain-of-thought-style response helps improve the model\u2019s ability to reason. In domain specific RAG, RAFT consistently improves the model\u2019s performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2110.15943",
        "title": "MetaICL: Learning to Learn In Context",
        "abstract": "We introduce MetaICL (Meta-training for InContext Learning), a new meta-training framework for few-shot learning where a pretrained\nlanguage model is tuned to do in-context learning on a large set of training tasks. This metatraining enables the model to more effectively\nlearn a new task in context at test time, by simply conditioning on a few training examples\nwith no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets\nincluding classification, question answering,\nnatural language inference, paraphrase detection and more, across seven different metatraining/target splits. MetaICL outperforms a\nrange of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find\nthat the gains are particularly significant for\ntarget tasks that have domain shifts from the\nmeta-training tasks, and that using a diverse\nset of the meta-training tasks is key to improvements. We also show that MetaICL\napproaches (and sometimes beats) the performance of models fully finetuned on the\ntarget task, and outperforms much bigger\nmodels with nearly 8x parameters. Finally,\nwe show that MetaICL is complementary to\nhuman-written instructions, and the best performance can be achieved by combining both\napproaches.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2106.05784",
        "title": "Programming Puzzles",
        "abstract": "We introduce a new type of programming challenge called programming puzzles, as an objective and comprehensive evaluation of program synthesis, and release an open-source dataset of Python Programming Puzzles (P3).1 Each puzzle is defined by a short Python program f, and the goal is to find an input which makes f return True. The puzzles are objective in that each one is specified entirely by the source code of its verifier f, so evaluating f is all that is needed to test a candidate solution. They do not require an answer key or input/output examples, nor do they depend on natural language understanding. The dataset is comprehensive in that it spans problems of a range of difficulties and domains, ranging from trivial string manipulation problems, to classic programming puzzles (e.g., Tower of Hanoi), to interview/competitive-programming problems (e.g., dynamic programming), to longstanding open problems in algorithms and mathematics (e.g., factoring). We develop baseline enumerative program synthesis, GPT-3 and Codex solvers that are capable of solving puzzles\u2014even without access to any reference solutions\u2014by learning from their own past solutions. Codex performs best, solving up to 18% of 397 test problems with a single try and 80% of the problems with 1,000 tries per problem. In a small user study, we find a positive correlation between puzzlesolving performance and coding experience, and between the puzzle difficulty for humans and AI solvers. Therefore, further improvements on P3 could have a significant impact on many program synthesis areas.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2102.00913",
        "title": "Hydrogen-mediated CVD epitaxy of Graphene on SiC: growth mechanism and\n  atomic configuration",
        "abstract": "Despite the large literature focused on the growth of graphene (Gr) on\n 6H-SiC(0001) by chemical vapour deposition (CVD), some important issues have\n not been solved and full wafer scale epitaxy of Gr remains challenging,\n hampering applications in microelectronics. With this study we shed light on\n the generic mechanism which produces the coexistence of two different types of\n Gr domains, whose proportion can be carefully controlled by tuning the H2 flow\n rate. For the first time, we show that the growth of Gr using CVD under H2/Ar\n flow rate proceeds in two stages. Firstly, the nucleation of free-standing\n epitaxial graphene on hydrogen (H-Gr) occurs, then H-atoms eventually desorb\n from either step edges or defects. This gives rise, for H2 flow rate below a\n critical value, to the formation of (6x6)Gr domains on 6H-SiC(0001). The front\n of H-desorption progresses proportionally to the reduction of H2. Using a\n robust and generic X-ray photoelectron spectroscopy (XPS) analysis, we\n realistically quantify the proportions of H-Gr and (6x6)Gr domains of a Gr film\n synthetized in any experimental conditions. Scanning tunnelling microscopy\n supports the XPS measurements. From these results we can deduce that the H-\n assisted CVD growth of Gr developed here is a unique method to grow fully\n free-standing H-Gr on the contrary to the method consisting of H-intercalation\n below epitaxial Gr on buffer layer. These results are of crucial importance for\n future applications of Gr/SiC(0001) in nanoelectronics, providing the\n groundwork for the use of Gr as an optimal template layer for Van der Waals\n homo- and hetero-epitaxy.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1409.7186",
        "title": "Feature-based tuning of simulated annealing applied to the\n  curriculum-based course timetabling problem",
        "abstract": "We consider the university course timetabling problem, which is one of the\n most studied problems in educational timetabling. In particular, we focus our\n attention on the formulation known as the curriculum-based course timetabling\n problem, which has been tackled by many researchers and for which there are\n many available benchmarks.\n  The contribution of this paper is twofold. First, we propose an effective and\n robust single-stage simulated annealing method for solving the problem.\n Secondly, we design and apply an extensive and statistically-principled\n methodology for the parameter tuning procedure. The outcome of this analysis is\n a methodology for modeling the relationship between search method parameters\n and instance features that allows us to set the parameters for unseen instances\n on the basis of a simple inspection of the instance itself. Using this\n methodology, our algorithm, despite its apparent simplicity, has been able to\n achieve high quality results on a set of popular benchmarks.\n  A final contribution of the paper is a novel set of real-world instances,\n which could be used as a benchmark for future comparison.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1407.2895",
        "title": "Spin noise spectroscopy beyond thermal equilibrium and linear response",
        "abstract": "Per the fluctuation-dissipation theorem, the information obtained from spin\n fluctuation studies in thermal equilibrium is necessarily constrained by the\n system's linear response functions. However, by including weak radiofrequency\n magnetic fields, we demonstrate that intrinsic and random spin fluctuations\n even in strictly unpolarized ensembles \\emph{can} reveal underlying patterns of\n correlation and coupling beyond linear response, and can be used to study\n non-equilibrium and even multiphoton coherent spin phenomena. We demonstrate\n this capability in a classical vapor of $^{41}$K alkali atoms, where spin\n fluctuations alone directly reveal Rabi splittings, the formation of Mollow\n triplets and Autler-Townes doublets, ac Zeeman shifts, and even nonlinear\n multiphoton coherences.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1703.08549",
        "title": "The HI-to-H2 Transition in a Turbulent Medium",
        "abstract": "We study the effect of density fluctuations induced by turbulence on the\n HI/H$_2$ structure in photodissociation regions (PDRs) both analytically and\n numerically. We perform magnetohydrodynamic numerical simulations for both\n subsonic and supersonic turbulent gas, and chemical HI/H$_2$ balance\n calculations. We derive atomic-to-molecular density profiles and the HI column\n density probability density function (PDF) assuming chemical equilibrium. We\n find that while the HI/H$_2$ density profiles are strongly perturbed in\n turbulent gas, the mean HI column density is well approximated by the\n uniform-density analytic formula of Sternberg et al. (2014). The PDF width\n depends on (a) the radiation intensity to mean density ratio, (b) the sonic\n Mach number and (c) the turbulence decorrelation scale, or driving scale. We\n derive an analytic model for the HI PDF and demonstrate how our model, combined\n with 21 cm observations, can be used to constrain the Mach number and driving\n scale of turbulent gas. As an example, we apply our model to observations of HI\n in the Perseus molecular cloud. We show that a narrow observed HI PDF may imply\n small scale decorrelation, pointing to the potential importance of\n subcloud-scale turbulence driving.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1002.3450",
        "title": "Gating of high-mobility two-dimensional electron gases in GaAs/AlGaAs\n  heterostructures",
        "abstract": "We investigate high-mobility two-dimensional electron gases in AlGaAs\n heterostructures by employing Schottky-gate-dependent measurements of the\n samples' electron density and mobility. Surprisingly, we find that two\n different sample configurations can be set in situ with mobilities diering by a\n factor of more than two in a wide range of densities. This observation is\n discussed in view of charge redistributions between the doping layers and is\n relevant for the design of future gateable high-mobility electron gases.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1205.2982",
        "title": "Seshadri constants of $K3$ surfaces of degrees 6 and 8",
        "abstract": "We compute Seshadri constants $\\eps(X):= \\eps(\\O_X(1))$ on $K3$ surfaces $X$\n of degrees 6 and 8. Moreover, more generally, we prove that if $X$ is any\n embedded $K3$ surface of degree $2r-2 \\geq 8$ in $\\PP^r$ not containing lines,\n then $1 < \\eps(X) <2$ if and only if the homogeneous ideal of $X$ is not\n generated by only quadrics (in which case $\\eps(X)=3/2$).",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2209.02202",
        "title": "Efficient Extraction of Hot Carriers in Perovskite Quantum Dot through\n  Building State Coupled Complex",
        "abstract": "Utilizing hot carriers is the crucial approach for solar cell to exceed the\n thermodynamic detailed balance limit, yet effective extraction of hot carriers\n in absorber materials via most commonly used semiconductor acceptors has been a\n challenge in both materials and photophysics research for many years. Herein,\n we build series of CsPbI3 quantum dot and fullerene derivative systems to\n explore the decisive factors of this process and have for the first time\n realized efficient hot carrier extraction in these systems (maximum extraction\n efficiency ~ 84%). We find building the systems as state-coupled complexes\n creates new carrier transport channels at about 0.22 eV above CsPbI3 quantum\n dot bandgap, which facilitates highly efficient HC extraction. Our research\n directly visualizes the inner connection of molecule interaction and ultrafast\n hot carrier extraction. The knowledge and strategy gained here are of universal\n meaning, taking an important step forward true hot carrier photovoltaics.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/0302088",
        "title": "Constraint Superalgebras and Their Application to Gauge Field Theories\n  and String Theories",
        "abstract": "We show that with every classical system possessing first class constraints\n that form a natural Lie algebra, we can associate a superalgebra that admits\n the constraint Lie algebra as a subalgebra. An odd generator of this\n superalgebra that commutes with the constraints is shown to be the BRST\n operator whose form follows from a non linear coset representation of the\n superalgebra. We further show the existence of the superalgebra for all\n Yang-Mills theories and for 26-dimensional bosonic strings.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1505.07423",
        "title": "Halpha3: an Halpha imaging survey of HI selected galaxies from ALFALFA.\n  V: The Coma Supercluster survey completion",
        "abstract": "Neutral hydrogen represents the major observable baryonic constituent of\n galaxies that fuels the formation of stars through the transformation in\n molecular hydrogen. The emission of the hydrogen recombination line Halpha is\n the most direct tracer of the process that transforms gas (fuel) into stars. We\n continue to present Halpha3 (acronym for Halpha-alpha-alpha), an extensive\n Halpha+[NII] narrow-band imaging campaign of galaxies selected from the HI\n Arecibo Legacy Fast ALFA Survey (ALFALFA), using the instrumentation available\n at the San Pedro Martir observatory (Mexico). In only four years since 2011 we\n were able to complete in 48 nights the Halpha imaging observations of 724\n galaxies in the region of the Coma supercluster 10^h < R.A. <16^h; 24^o < Dec.\n <28^o and 3900<cz<9000 kms^{-1}. Of these, 603 are selected from the HI Arecibo\n Legacy Fast ALFA Survey (ALFALFA) and constitute a 97% complete sample. They\n provide for the first time a complete census of the massive star formation\n properties of local gas-rich galaxies belonging to different environments\n (cluster vs filaments), morphological type (spirals vs dwarf Irr), over a wide\n range of stellar mass (10^{8}-10^{11.5} Modot) in the Coma Supercluster. The\n present Paper V provides the Halpha data and the derived star formation rates\n for the observed galaxies.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1510.06643",
        "title": "A long-period Cepheid variable in the starburst cluster VdBH222",
        "abstract": "Galactic starburst clusters play a twin role in astrophysics, serving as\n laboratories for the study of stellar physics and also delineating the\n structure and recent star formation history of the Milky Way. In order to\n exploit these opportunities we have undertaken a multi-epoch spectroscopic\n survey of the red supergiant dominated young massive clusters thought to be\n present at both near and far ends of the Galactic Bar. Significant\n spectroscopic variability suggestive of radial pulsations was found for the\n yellow supergiant VdBH 222 #505. Follow-up photometric investigations revealed\n modulation with a period of ~23.325d; both timescale and pulsational profile\n are consistent with a Cepheid classification. As a consequence #505 may be\n recognised as one of the longest period Galactic cluster Cepheids identified to\n date and hence of considerable use in constraining the bright end of the\n period/luminosity relation at solar metallicities. In conjunction with extant\n photometry we infer a distance of ~6kpc for VdBH222 and an age of ~20Myr. This\n results in a moderate reduction in both integrated cluster mass (~2x10^4Msun)\n and the initial stellar masses of the evolved cluster members (~10Msun). As\n such, VdBH222 becomes an excellent test-bed for studying the properties of some\n of the lowest mass stars observed to undergo type-II supernovae. Moreover, the\n distance is in tension with a location of VdBH 222 at the far end of the\n Galactic Bar. Instead a birthsite in the near 3kpc arm is suggested; providing\n compelling evidence of extensive recent star formation in a region of the inner\n Milky Way which has hitherto been thought to be devoid of such activity.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1507.06510",
        "title": "Consistent estimation of the filtering and marginal smoothing\n  distributions in nonparametric hidden Markov models",
        "abstract": "In this paper, we consider the filtering and smoothing recursions in\n nonparametric finite state space hidden Markov models (HMMs) when the\n parameters of the model are unknown and replaced by estimators. We provide an\n explicit and time uniform control of the filtering and smoothing errors in\n total variation norm as a function of the parameter estimation errors. We prove\n that the risk for the filtering and smoothing errors may be uniformly upper\n bounded by the risk of the estimators. It has been proved very recently that\n statistical inference for finite state space nonparametric HMMs is possible. We\n study how the recent spectral methods developed in the parametric setting may\n be extended to the nonparametric framework and we give explicit upper bounds\n for the L2-risk of the nonparametric spectral estimators. When the observation\n space is compact, this provides explicit rates for the filtering and smoothing\n errors in total variation norm. The performance of the spectral method is\n assessed with simulated data for both the estimation of the (nonparametric)\n conditional distribution of the observations and the estimation of the marginal\n smoothing distributions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1906.09308",
        "title": "Approximating Interactive Human Evaluation with Self-Play for\n  Open-Domain Dialog Systems",
        "abstract": "Building an open-domain conversational agent is a challenging problem.\n Current evaluation methods, mostly post-hoc judgments of static conversation,\n do not capture conversation quality in a realistic interactive context. In this\n paper, we investigate interactive human evaluation and provide evidence for its\n necessity; we then introduce a novel, model-agnostic, and dataset-agnostic\n method to approximate it. In particular, we propose a self-play scenario where\n the dialog system talks to itself and we calculate a combination of proxies\n such as sentiment and semantic coherence on the conversation trajectory. We\n show that this metric is capable of capturing the human-rated quality of a\n dialog model better than any automated metric known to-date, achieving a\n significant Pearson correlation (r>.7, p<.05). To investigate the strengths of\n this novel metric and interactive evaluation in comparison to state-of-the-art\n metrics and human evaluation of static conversations, we perform extended\n experiments with a set of models, including several that make novel\n improvements to recent hierarchical dialog generation architectures through\n sentiment and semantic knowledge distillation on the utterance level. Finally,\n we open-source the interactive evaluation platform we built and the dataset we\n collected to allow researchers to efficiently deploy and evaluate dialog\n models.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2212.10513",
        "title": "ECoHeN: A Hypothesis Testing Framework for Extracting Communities from\n  Heterogeneous Networks",
        "abstract": "Community discovery is the general process of attaining assortative\n communities from a network: collections of nodes that are densely connected\n within yet sparsely connected to the rest of the network. While community\n discovery has been well studied, few such techniques exist for heterogeneous\n networks, which contain different types of nodes and possibly different\n connectivity patterns between the node types. In this paper, we introduce a\n framework called ECoHeN, which \\textbf{e}xtracts \\textbf{co}mmunities from a\n \\textbf{he}terogeneous \\textbf{n}etwork in a statistically meaningful way.\n Using a heterogeneous configuration model as a reference distribution, ECoHeN\n identifies communities that are significantly more densely connected than\n expected given the node types and connectivity of its membership. Specifically,\n the ECoHeN algorithm extracts communities one at a time through a dynamic set\n of iterative updating rules, is guaranteed to converge, and imposes no\n constraints on the type composition of extracted communities. To our knowledge\n this is the first discovery method that distinguishes and identifies both\n homogeneous and heterogeneous, possibly overlapping, community structure in a\n network. We demonstrate the performance of ECoHeN through simulation and in\n application to a political blogs network to identify collections of blogs which\n reference one another more than expected considering the ideology of its'\n members.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ph/9803328",
        "title": "Hadronic Phases and Isospin Amplitudes in $D(B) \\to \\pi \\pi$ and $D(B)\n  \\to K \\bar K$ Decays",
        "abstract": "Hadronic phase in $\\pi \\pi$ and $K \\bar K$ channels are calculated a la\n Regge. At the D mass one finds $\\delta_{\\pi \\pi} \\approx \\pi/3$ and $\\delta_{K\n \\bar K} \\approx -\\pi/6$ in good agreement with the CLEO data while at the B\n mass these angles are predicted to be, respectively, $11^\\circ$ and $-7^\\circ$.\n With the hadronic phase $e^{i \\delta_{K \\bar K}}$ taken into account, a quark\n diagram decomposition of the isospin invariant amplitudes in $D \\to K \\bar K$\n decays fits the data provided the exchange diagram contribution is about 1/3 of\n the tree level one.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/9310007",
        "title": "The Cosmological Constant and Volume-Preserving Diffeomorphism\n  Invariants",
        "abstract": "Observables in the quantum field theories of $(D-1)$-form fields, $\\ca$, on\n $D$-dimensional, compact and orientable manifolds, $M$, are computed.\n Computations of the vacuum value of $T_{ab}$ find it to be the metric times a\n function of the volume of spacetime, $\\Omega(M)$. Part of this function of\n $\\Omega$ is a finite zero-mode contribution. The correlation functions of\n another set of operators give intersection numbers on $M$. Furthermore, a\n similar computation for products of Wilson area operators results in a function\n of the volumes of the intersections of the submanifolds the operators are\n defined on. In addition, scalar field couplings are introduced and potentials\n are induced after integrating out the $\\ca$ field. Lastly, the thermodynamics\n of the pure theories is found to be analogous to the zero-point motion of a\n scalar particle. The coupling of a Gaussian scalar field to the $\\ca$ field is\n found to manifest itself on the free energy at high temperatures and/or small\n volumes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1206.6210",
        "title": "Parametrization of Crab pulsar spectrum",
        "abstract": "CRAB pulsar data has been parameterized by using exponential function or\n broken exponential function. we use non extensive form of exponential function\n to parameterize this data.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1404.2411",
        "title": "Approximation of a stochastic wave equation in dimension three, with\n  application to a support theorem in H\\\"older norm: The non-stationary case",
        "abstract": "This paper is a continuation of (Bernoulli 20 (2014) 2169-2216) where we\n prove a characterization of the support in H\\\"older norm of the law of the\n solution to a stochastic wave equation with three-dimensional space variable\n and null initial conditions. Here, we allow for non-null initial conditions\n and, therefore, the solution does not possess a stationary property in space.\n As in (Bernoulli 20 (2014) 2169-2216), the support theorem is a consequence of\n an approximation result, in the convergence of probability, of a sequence of\n evolution equations driven by a family of regularizations of the driving noise.\n However, the method of the proof differs from (Bernoulli 20 (2014) 2169-2216)\n since arguments based on the stationarity property of the solution cannot be\n used.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0505139",
        "title": "Counting Bitangents with Stable Maps",
        "abstract": "This paper is an elementary introduction to the theory of moduli spaces of\n curves and maps. As an application to enumerative geometry, we show how to\n count the number of bitangent lines to a projective plane curve of degree $d$\n by doing intersection theory on moduli spaces.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/9705228",
        "title": "Time evolution in general gauge theories",
        "abstract": "Talk at the International Workshop \"New Non Perturbative Methods and\n Quantization on the Light Cone\", Les Houches, France, Feb.24-March 7, 1997",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1812.08394",
        "title": "A thought on generalized Morrey spaces",
        "abstract": "Morrey spaces can complement the boundedness properties of operators that\n Lebesgue spaces can not handle. Morrey spaces which we have been handling are\n called classical Morrey spaces. However, classical Morrey spaces are not\n totally enough to describe the boundedness properties. To this end, we need to\n generalize parameters $p$ and $q$, among others $p$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1405.4322",
        "title": "Leveraging Evolutionary Search to Discover Self-Adaptive and\n  Self-Organizing Cellular Automata",
        "abstract": "Building self-adaptive and self-organizing (SASO) systems is a challenging\n problem, in part because SASO principles are not yet well understood and few\n platforms exist for exploring them. Cellular automata (CA) are a well-studied\n approach to exploring the principles underlying self-organization. A CA\n comprises a lattice of cells whose states change over time based on a discrete\n update function. One challenge to developing CA is that the relationship of an\n update function, which describes the local behavior of each cell, to the global\n behavior of the entire CA is often unclear. As a result, many researchers have\n used stochastic search techniques, such as evolutionary algorithms, to\n automatically discover update functions that produce a desired global behavior.\n However, these update functions are typically defined in a way that does not\n provide for self-adaptation. Here we describe an approach to discovering CA\n update functions that are both self-adaptive and self-organizing. Specifically,\n we use a novel evolutionary algorithm-based approach to discover finite state\n machines (FSMs) that implement update functions for CA. We show how this\n approach is able to evolve FSM-based update functions that perform well on the\n density classification task for 1-, 2-, and 3-dimensional CA. Moreover, we show\n that these FSMs are self-adaptive, self-organizing, and highly scalable, often\n performing well on CA that are orders of magnitude larger than those used to\n evaluate performance during the evolutionary search. These results demonstrate\n that CA are a viable platform for studying the integration of self-adaptation\n and self-organization, and strengthen the case for using evolutionary\n algorithms as a component of SASO systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1411.6709",
        "title": "On functional equations leading to exact solutions for standing internal\n  waves",
        "abstract": "The Dirichlet problem for the wave equation is a classical example of a\n problem which is not well-posed. Nevertheless, it has been used to model\n internal waves oscillating sinusoidally in time, in various situations,\n standing internal waves amongst them. We consider internal waves in\n two-dimensional domains bounded above by the plane z=0 and below by z=-d(x) for\n depth functions d. This paper draws attention to the Abel and Schr{\\\"o}der\n functional equations as a convenient way of organizing analytical solutions.\n Exact internal wave solutions are constructed for a selected number of simple\n depth functions d.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/0101153",
        "title": "Supersymmetry Transformation of Quantum Fields",
        "abstract": "In the Wess-Zumino gauge, supersymmetry transformations become non-linear and\n are usually incorporated together with BRS transformations in the form of\n Slavnov-Taylor identities, such that they appear at first sight to be even\n non-local. Furthermore, the gauge fixing term breaks supersymmetry. In the\n present paper, we clarify in which sense supersymmetry is still a symmetry of\n the system and how it is realized on the level of quantum fields.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1811.08445",
        "title": "The Caltech-NRAO Stripe 82 Survey (CNSS) Paper II: On-The-Fly Mosaicing\n  Methodology",
        "abstract": "Telescope slew and settle time markedly reduces the efficiency of wide-field\n multi-epoch surveys for sensitive interferometers with small fields of view.\n The overheads can be mitigated through the use of On-the-Fly Mosaicing (OTFM),\n where the the antennas are driven at a non-sidereal rate and visibilities are\n recorded continuously. Here we introduce the OTFM technique for the VLA, and\n describe its implementation for the Caltech-NRAO Stripe 82 Survey (CNSS), a\n dedicated 5-epoch survey for slow transients at S band (2-4 GHz). We also\n describe the OTFSim tool for planning dynamically-scheduled OTFM observations\n on the VLA, the latest imaging capabilities for OTFM in CASA, and present a\n comparison of OTFM observations with pointed observations. Using the subset of\n our observations from the CNSS pilot and final surveys, we demonstrate that the\n wide-band and wide-field OTFM observations with the VLA can be imaged\n accurately, and that this technique offers a more efficient alternative to\n standard mosaicing for multi-epoch shallow surveys such as the CNSS and the VLA\n Sky Survey (VLASS). We envisage that the new OTFM mode will facilitate new\n synoptic surveys and high-frequency mapping experiments on the VLA.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1907.05229",
        "title": "(Co)homology of Crossed Products by Weak Hopf Algebras",
        "abstract": "We obtain a mixed complex simpler than the canonical one the computes the\n type cyclic homologies of a crossed product with invertible cocycle\n $A\\times_{\\rho}^f H$, of a weak module algebra $A$ by a weak Hopf algebra $H$.\n This complex is provided with a filtration. The spectral sequence of this\n filtration generalizes the spectral sequence obtained in \\cite{CGG}. When $f$\n takes its values in a separable subalgebra of $A$ that satisfies suitable\n conditions, the above mentioned mixed complex is provided with another\n filtration, whose spectral sequence generalize the Feigin-Tsygan spectral\n sequence.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2305.18618",
        "title": "Chatbots put to the test in math and logic problems: A preliminary\n  comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard",
        "abstract": "A comparison between three chatbots which are based on large language models,\n namely ChatGPT-3.5, ChatGPT-4 and Google Bard is presented, focusing on their\n ability to give correct answers to mathematics and logic problems. In\n particular, we check their ability to Understand the problem at hand; Apply\n appropriate algorithms or methods for its solution; and Generate a coherent\n response and a correct answer. We use 30 questions that are clear, without any\n ambiguities, fully described with plain text only, and have a unique, well\n defined correct answer. The questions are divided into two sets of 15 each. The\n questions of Set A are 15 \"Original\" problems that cannot be found online,\n while Set B contains 15 \"Published\" problems that one can find online, usually\n with their solution. Each question is posed three times to each chatbot. The\n answers are recorded and discussed, highlighting their strengths and\n weaknesses. It has been found that for straightforward arithmetic, algebraic\n expressions, or basic logic puzzles, chatbots may provide accurate solutions,\n although not in every attempt. However, for more complex mathematical problems\n or advanced logic tasks, their answers, although written in a usually\n \"convincing\" way, may not be reliable. Consistency is also an issue, as many\n times a chatbot will provide conflicting answers when given the same question\n more than once. A comparative quantitative evaluation of the three chatbots is\n made through scoring their final answers based on correctness. It was found\n that ChatGPT-4 outperforms ChatGPT-3.5 in both sets of questions. Bard comes\n third in the original questions of Set A, behind the other two chatbots, while\n it has the best performance (first place) in the published questions of Set B.\n This is probably because Bard has direct access to the internet, in contrast to\n ChatGPT chatbots which do not have any communication with the outside world.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2005.06382",
        "title": "Super-Resolution Domain Adaptation Networks for Semantic Segmentation\n  via Pixel and Output Level Aligning",
        "abstract": "Recently, Unsupervised Domain Adaptation (UDA) has attracted increasing\n attention to address the domain shift problem in the semantic segmentation\n task. Although previous UDA methods have achieved promising performance, they\n still suffer from the distribution gaps between source and target domains,\n especially the resolution discrepany in the remote sensing images. To address\n this problem, this paper designs a novel end-to-end semantic segmentation\n network, namely Super-Resolution Domain Adaptation Network (SRDA-Net). SRDA-Net\n can simultaneously achieve the super-resolution task and the domain adaptation\n task, thus satisfying the requirement of semantic segmentation for remote\n sensing images which usually involve various resolution images. The proposed\n SRDA-Net includes three parts: a Super-Resolution and Segmentation (SRS) model\n which focuses on recovering high-resolution image and predicting segmentation\n map, a Pixel-level Domain Classifier (PDC) for determining which domain the\n pixel belongs to, and an Output-space Domain Classifier (ODC) for\n distinguishing which domain the pixel contribution is from. By jointly\n optimizing SRS with two classifiers, the proposed method can not only\n eliminates the resolution difference between source and target domains, but\n also improve the performance of the semantic segmentation task. Experimental\n results on two remote sensing datasets with different resolutions demonstrate\n that SRDA-Net performs favorably against some state-of-the-art methods in terms\n of accuracy and visual quality. Code and models are available at\n https://github.com/tangzhenjie/SRDA-Net.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2211.12062",
        "title": "NLS ground states on the half-line with point interactions",
        "abstract": "We investigate the existence and the uniqueness of NLS ground states of fixed\n mass on the half-line in the presence of a point interaction at the origin. The\n nonlinearity is of power type, and the regime is either $L^2$-subcritical or\n $L^{2}$-critical, while the point interaction is either attractive or\n repulsive. In the $L^{2}$-subcritical case, we prove that ground states exist\n for every mass value if the interaction is attractive, while ground states\n exist only for sufficiently large masses if the interaction is repulsive. In\n the latter case, if the power is less or equal to four, ground states coincide\n with the only bound state. If instead, the power is greater than four, then\n there are values of the mass for which two bound states exist, and neither of\n the two is a ground state, and values of the mass for which two bound states\n exist, and one of them is a ground state. In the $L^{2}$-critical case, we\n prove that ground states exist for masses strictly below a critical mass value\n in the attractive case, while ground states never exist in the repulsive case.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2001.09829",
        "title": "Common-Envelope Episodes that lead to Double Neutron Star formation",
        "abstract": "Close double neutron stars have been observed as Galactic radio pulsars,\n while their mergers have been detected as gamma-ray bursts and\n gravitational-wave sources. They are believed to have experienced at least one\n common-envelope episode during their evolution prior to double neutron star\n formation. In the last decades there have been numerous efforts to understand\n the details of the common-envelope phase, but its computational modelling\n remains challenging. We present and discuss the properties of the donor and the\n binary at the onset of the Roche-lobe overflow leading to these common-envelope\n episodes as predicted by rapid binary population synthesis models. These\n properties can be used as initial conditions for detailed simulations of the\n common-envelope phase. There are three distinctive populations, classified by\n the evolutionary stage of the donor at the moment of the onset of the\n Roche-lobe overflow: giant donors with fully-convective envelopes, cool donors\n with partially-convective envelopes, and hot donors with radiative envelopes.\n We also estimate that, for standard assumptions, tides would not circularise a\n large fraction of these systems by the onset of Roche-lobe overflow. This makes\n the study and understanding of eccentric mass-transferring systems relevant for\n double neutron star populations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1804.08846",
        "title": "Stable attosecond electron bunches from a nanofiber driven by\n  Laguerre-Gaussian lasers",
        "abstract": "Generation of attosecond bunches of energetic electrons offers significant\n potential from ultrafast physics to novel radiation sources. However, it is\n still a great challenge to stably produce such electron beams with lasers,\n since the typical sub-femtosecond electron bunches from laser-plasma\n interactions either carry low beam charge, or propagate for only several tens\n of femtoseconds. Here we propose an all-optical scheme for generating dense\n attosecond electron bunches via the interaction of an intense Laguerre-Gaussian\n (LG) laser pulse with a nanofiber. The stable bunch train results from the\n unique field structure of a circularly polarized LG laser pulse, enabling each\n bunch to be phase-locked and accelerated forward with low divergence, high beam\n charge and large beam-angular-momentum. This paves the way for wide\n applications in various fields, e.g., ultrabrilliant attosecond x/$\\gamma$-ray\n emission.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.08147",
        "title": "Gluing of Fourier-Mukai partners in a triangular spectrum and birational\n  geometry",
        "abstract": "Balmer defined the tensor triangulated spectrum $\\operatorname{Spec}_\\otimes\n \\mathcal{T}$ of a tensor triangulated category $(\\mathcal{T},\\otimes)$ and\n showed that for a variety $X$, we have the reconstruction $X \\cong\n \\operatorname{Spec}_{\\otimes_{\\mathscr{O}_X}^{\\mathbb{L}}}\\operatorname{Perf}\n X$. In the absence of the tensor structure, Matsui recently introduced the\n triangular spectrum $\\operatorname{Spec}_\\vartriangle \\mathcal{T}$ of a\n triangulated category $\\mathcal{T}$ and showed that there exists an immersion\n $X \\cong\n \\operatorname{Spec}_{\\otimes_{\\mathscr{O}_X}^{\\mathbb{L}}}\\operatorname{Perf} X\n \\subset \\operatorname{Spec}_\\vartriangle \\operatorname{Perf} X$. In this paper,\n we construct a scheme $\\operatorname{Spec}^{\\mathsf{FM}} \\mathcal{T} \\subset\n \\operatorname{Spec}_\\vartriangle \\mathcal{T}$, called the Fourier-Mukai (FM)\n locus, by gathering all varieties $X$ satisfying $\\operatorname{Perf} X \\simeq\n \\mathcal{T}$. Those varieties are called FM partners of $\\mathcal{T}$ and\n immersed into $\\operatorname{Spec}_\\vartriangle \\mathcal{T}$ as tensor\n triangulated spectra. We present a variety of examples illustrating how\n geometric and birational properties of FM partners are reflected in the way\n their tensor triangulated spectra are glued in the FM locus. Finally, we\n compare the FM locus with other loci within the triangular spectrum admitting\n categorical characterizations, and in particular, make a precise conjecture\n about the relation of the FM locus with the Serre invariant locus.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1712.02202",
        "title": "The Mellin-Barnes Approach to Hadronic Vacuum Polarization and\n  $g_{\\mu}-2$",
        "abstract": "It is shown that with a precise determination of a few derivatives of the\n hadronic vacuum polarization (HVP) self-energy function $\\Pi(Q^2)$ at $Q^2=0$,\n from lattice QCD (LQCD) or from a dedicated low-energy experiment, one can\n obtain an evaluation of the lowest order HVP contribution to the anomalous\n magnetic moment of the muon $a_{\\mu}^{\\rm HVP}$ with an accuracy comparable to\n the one reached using the $e^+ e^-$ annihilation cross-section into hadrons.\n The technique of Mellin-Barnes approximants (MBa) that we propose is\n illustrated in detail with the example of the two loop vacuum polarization\n function in QED. We then apply it to the first few moments of the hadronic\n spectral function obtained from experiment and show that the resulting MBa\n evaluations of $a_{\\mu}^{\\rm HVP}$ converge very quickly to the full\n experimental determination.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0205076",
        "title": "Sub-Milliarcsecond Imaging of Quasars and Active Galactic Nuclei. II.\n  Additional Sources",
        "abstract": "We report further results from our imaging survey at 15GHz (lambda=2 cm) with\n the Very Long Baseline Array. This paper presents single epoch images for 39\n sources, bringing the total number of objects in the sample to 171. Our sample\n is representative of a complete unbiased sample and it will be used for\n statistical analysis of source properties. We compare the observed brightness\n temperatures derived from our VLBA observations to those derived from total\n intensity variations at 22 and 37 GHz. These are consistent with intrinsic\n brightness temperatures in the range 10^10 to 10^12 K. We also present three\n new spectroscopic redshift values: z=0.517+/-0.001 for 0026+346,\n z=1.591+/-0.003 for 0727-115, and z=0.2016+/-0.0004 for 1155+251.\n  Images from this VLBA 2 cm survey are available on the Internet under\n http://www.cv.nrao.edu/2cmsurvey.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1412.4528",
        "title": "Properties of a Bose Gas in the Presence of Disorder (Laurea thesis)",
        "abstract": "The phenomenon of Bose-Einstein condensation and superfluidity in a Bose gas\n with disorder is investigated. Diffusion Monte Carlo (DMC) method is used to\n calculate superfluid and condensate fraction of the system as a function of\n density and strength of disorder at zero temperature. The algorithm and\n implementation of the Diffusion Monte Carlo method is explained in details.\n Bogoliubov theory is developed for the analytical description of the problem.\n Ground state energy, superfluid fraction and condensate fraction are\n calculated. It is shown that same results for the superfluid fraction can be\n obtained in a perturbative manner from Gross-Pitaevskii equation. Ground state\n energy, obtained from DMC calculations, is compared to predictions of\n Bogoliubov theory, which are found to be valid in the regime, when the strength\n of disorder is small. It is shown that \"unusual\" situation, when the superfluid\n fraction is smaller than the condensate fraction, can be realized in this\n system.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/9912179",
        "title": "Local symmetries in the Hamiltonian framework. 1. Hamiltonian form of\n  the symmetries and the Noether identities",
        "abstract": "We study in the Hamiltonian framework the local transformations\n $\\delta_\\epsilon q^A(\\tau)=\\sum^{[k]}_{k=0}\\partial^k_\\tau\\epsilon^a{}\n R_{(k)a}{}^A(q^B, \\dot q^C)$ which leave invariant the Lagrangian action:\n $\\delta_\\epsilon S=div$. Manifest form of the symmetry and the corresponding\n Noether identities is obtained in the first order formalism as well as in the\n Hamiltonian one. The identities has very simple form and interpretation in the\n Hamiltonian framework. Part of them allows one to express the symmetry\n generators which correspond to the primarily expressible velocities through the\n remaining one. Other part of the identities allows one to select subsystem of\n constraints with a special structure from the complete constraint system. It\n means, in particular, that the above written symmetry implies an appearance of\n the Hamiltonian constraints up to at least $([k]+1)$ stage. It is proved also\n that the Hamiltonian symmetries can always be presented in the form of\n canonical transformation for the phase space variables. Manifest form of the\n resulting generating function is obtained.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ph/0507100",
        "title": "Impact of tau polarization on the study of the MSSM charged Higgs bosons\n  in top quark decays at the ILC",
        "abstract": "The process of top quark pair production at the ILC with subsequent decays of\n one of the top quarks to a charged Higgs boson and b-quark is considered. The\n charged Higgs decays to tau leptons whose polarization is the opposite to those\n coming from W bosons. This difference is reflected in the energy distributions\n of the tau decay products in the top quark rest frame, which can be\n reconstructed at the ILC using the recoil mass technique. We present an\n analysis including spin correlations, backgrounds, ISR/FSR and beamstrahlung,\n and show that a fit of the shape of the pion energy spectrum yields the charged\n Higgs boson mass with an accuracy of about 1 GeV.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1808.05657",
        "title": "Maximal operators and decoupling for $\\Lambda(p)$ Cantor measures",
        "abstract": "For $2\\leq p<\\infty$, $\\alpha'>2/p$, and $\\delta>0$, we construct Cantor-type\n measures on $\\mathbb{R}$ supported on sets of Hausdorff dimension\n $\\alpha<\\alpha'$ for which the associated maximal operator is bounded from\n $L^p_\\delta (\\mathbb{R})$ to $L^p(\\mathbb{R})$. Maximal theorems for fractal\n measures on the line were previously obtained by Laba and Pramanik. The result\n here is weaker in that we are not able to obtain $L^p$ estimates; on the other\n hand, our approach allows Cantor measures that are self-similar, have\n arbitrarily low dimension $\\alpha>0$, and have no Fourier decay. The proof is\n based on a decoupling inequality similar to that of Laba and Wang.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/0301166",
        "title": "Graphical Representation of Supersymmetry",
        "abstract": "A graphical representation of supersymmetry is presented. It clearly\n expresses the chiral flow appearing in SUSY quantities, by representing spinors\n by {\\it directed lines} (arrows). The chiral suffixes are expressed by the\n directions (up, down, left, right) of the arrows. The SL(2,C) invariants are\n represented by {\\it wedges}. Both the Weyl spinor and the Majorana spinor are\n treated. We are free from the messy symbols of spinor suffixes. The method is\n applied to the 5D supersymmetry. Many applications are expected. The result is\n suitable for coding a computer program and is highly expected to be applicable\n to various SUSY theories (including Supergravity) in various dimensions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2002.06397",
        "title": "Open Knowledge Enrichment for Long-tail Entities",
        "abstract": "Knowledge bases (KBs) have gradually become a valuable asset for many AI\n applications. While many current KBs are quite large, they are widely\n acknowledged as incomplete, especially lacking facts of long-tail entities,\n e.g., less famous persons. Existing approaches enrich KBs mainly on completing\n missing links or filling missing values. However, they only tackle a part of\n the enrichment problem and lack specific considerations regarding long-tail\n entities. In this paper, we propose a full-fledged approach to knowledge\n enrichment, which predicts missing properties and infers true facts of\n long-tail entities from the open Web. Prior knowledge from popular entities is\n leveraged to improve every enrichment step. Our experiments on the synthetic\n and real-world datasets and comparison with related work demonstrate the\n feasibility and superiority of the approach.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0602652",
        "title": "A numerical method for calculating the Green's function arising from\n  electronic structure theory",
        "abstract": "We developed a fast numerical methodfor complex symmetric shifted linear\n systems, which is motivated by the quantum-mechanical (electronic-structure)\n theory in nanoscale materials. The method is named shifted Conjugate Orthogonal\n Conjugate Gradient (shifted COCG) method. The formulation is given and several\n numerical aspects are discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1210.5193",
        "title": "Early-stage young stellar objects in the Small Magellanic Cloud",
        "abstract": "We present new observations of 34 YSO candidates in the SMC. The anchor of\n the analysis is a set of Spitzer-IRS spectra, supplemented by groundbased 3-5\n micron spectra, Spitzer and NIR photometry, optical spectroscopy and radio\n data. The sources' SEDs and spectral indices are consistent with embedded YSOs;\n prominent silicate absorption is observed in the spectra of at least ten\n sources, silicate emission is observed towards four sources. PAH emission is\n detected towards all but two sources. Based on band ratios (in particular the\n strength of the 11.3 micron and the weakness of the 8.6 micron bands) PAH\n emission towards SMC YSOs is dominated by predominantly small neutral grains.\n Ice absorption is observed towards fourteen sources in the SMC. The comparison\n of H2O and CO2 ice column densities for SMC, LMC and Galactic samples suggests\n that there is a significant H2O column density threshold for the detection of\n CO2 ice. This supports the scenario proposed by Oliveira et al. (2011), where\n the reduced shielding in metal-poor environments depletes the H2O column\n density in the outer regions of the YSO envelopes. No CO ice is detected\n towards the SMC sources. Emission due to pure-rotational 0-0 transitions of H2\n is detected towards the majority of SMC sources, allowing us to estimate\n rotational temperatures and column densities. All but one source are\n spectroscopically confirmed as SMC YSOs. Of the 33 YSOs identified in the SMC,\n 30 sources populate different stages of massive stellar evolution. The\n remaining three sources are classified as intermediate-mass YSOs with a thick\n dusty disc and a tenuous envelope still present. We propose one of the sources\n is a D-type symbiotic system, based on the presence of Raman, H and He emission\n lines in the optical spectrum, and silicate emission in the IRS-spectrum. This\n would be the first dust-rich symbiotic system identified in the SMC. (abridged)",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1506.07731",
        "title": "A multidimensional maximum bisection problem",
        "abstract": "This work introduces a multidimensional generalization of the maximum\n bisection problem. A mixed integer linear programming formulation is proposed\n with the proof of its correctness. The numerical tests, made on the randomly\n generated graphs, indicates that the multidimensional generalization is more\n difficult to solve than the original problem.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/quant-ph/9911013",
        "title": "Qubit and Entanglement assisted Optimal Entanglement Concentration",
        "abstract": "We present two method for optimal entanglement concentration from pure\n entangled states by local actions only. However a prior knowledge of the\n Schmidt coefficients is required. The first method is optimally efficient only\n when a finite ensemble of pure entangled states are available whereas the\n second method realizes the single pair optimal concentration probability. We\n also propose an entanglement assisted method which is again optimally efficient\n even for a single pair. We also discuss concentrating entanglement from\n N-partite cat like states.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2406.04292",
        "title": "VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval",
        "abstract": "Multi-modal retrieval becomes increasingly popular in practice. However, the\n existing retrievers are mostly text-oriented, which lack the capability to\n process visual information. Despite the presence of vision-language models like\n CLIP, the current methods are severely limited in representing the text-only\n and image-only data. In this work, we present a new embedding model VISTA for\n universal multi-modal retrieval. Our work brings forth threefold technical\n contributions. Firstly, we introduce a flexible architecture which extends a\n powerful text encoder with the image understanding capability by introducing\n visual token embeddings. Secondly, we develop two data generation strategies,\n which bring high-quality composed image-text to facilitate the training of the\n embedding model. Thirdly, we introduce a multi-stage training algorithm, which\n first aligns the visual token embedding with the text encoder using massive\n weakly labeled data, and then develops multi-modal representation capability\n using the generated composed image-text data. In our experiments, VISTA\n achieves superior performances across a variety of multi-modal retrieval tasks\n in both zero-shot and supervised settings. Our model, data, and source code are\n available at https://github.com/FlagOpen/FlagEmbedding.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/0010083",
        "title": "Multi-Black Hole Sectors of AdS_3 Gravity",
        "abstract": "We construct and discuss solutions of SO(1,2) x SO(1,2) Chern-Simons theory\n which correspond to multiple BTZ black holes. These solutions typically have\n additional singularities, the simplest cases being special conical\n singularities with a 2 pi surplus angle. There are solutions with singularities\n inside a common outer horizon, and other solutions with naked conical\n singularities. Previously such singularities have been ruled out on physical\n grounds, because they do not obey the geodesic equation. We find however that\n the Chern-Simons gauge symmetry may be used to locate all such singularities to\n the horizons, where they necessarily follow geodesics. We are therefore led to\n conclude that these singular solutions correspond to physically sensible\n geometries.\n  Boundary charges at infinity are only sensitive to the total mass and spin of\n the black holes, and not to the distribution among the black holes. We\n therefore argue that a holographic description in terms of a boundary conformal\n field theory should represent both single and multiple BTZ solutions with the\n same asymptotic charges. Then sectors with multiple black holes would\n contribute to the black hole entropy calculated from a boundary CFT.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2308.06099",
        "title": "Direct determination of the spin-polarization at buried interfaces using\n  voltage dependent MOKE signals",
        "abstract": "Here, we report a novel and conceptually straightforward technique to detect\n the spin-polarization directly, with laser spot spatial resolution, at buried\n ferromagnet/insulator interfaces in ambient settings. This has been\n accomplished by monitoring the voltage-induced change of the longitudinal MOKE\n signal as a function of the applied magnetic field and applying an AC voltage\n across the interface. For the case where the spin polarization enters the VMOKE\n signal, a simple quantitative model is proposed. A distinct positive majority\n spin polarization has been found for Fe and Co, whereas Ni exhibits a negative\n minority spin polarization.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2312.10717",
        "title": "Pseudo-random Instance Generators in C++ for Deterministic and\n  Stochastic Multi-commodity Network Design Problems",
        "abstract": "Network design problems constitute an important family of combinatorial\n optimization problems for which numerous exact and heuristic algorithms have\n been developed over the last few decades. Two central problems in this family\n are the multi-commodity, capacitated, fixed charge network design problem\n (MCFNDP) and its stochastic counterpart, the two-stage MCFNDP with recourse.\n These are standard problems that often serve as work benches for devising and\n testing models and algorithms in stylized but close-to-realistic settings. The\n purpose of this paper is to introduce two flexible, high-speed generators\n capable of simulating a wide range of settings for both the deterministic and\n stochastic MCFNDPs. We hope that, by facilitating systematic experimentation\n with new and larger sets of instances, these generators will lead to a more\n thorough assessment of the performance achieved by exact and heuristic solution\n methods in both deterministic and stochastic settings. We also hope that making\n these generators available will promote the reproducibility and comparability\n of published research.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2007.09714",
        "title": "Probing Noncommutative Gravity with Gravitational Wave and Binary Pulsar\n  Observations",
        "abstract": "Noncommutative gravity is a natural method of quantizing spacetime by\n promoting the spacetime coordinates themselves to operators which do not\n commute. This approach is motivated, for example, from a quantum gravity\n perspective, among others. Noncommutative gravity has been tested against the\n binary black hole merger event GW150914. Here, we extend and improve upon such\n a previous analysis by (i) relaxing an assumption made on the preferred\n direction due to noncommutativity, (ii) using posterior samples produced by the\n LIGO/Virgo Collaborations, (iii) consider other gravitational wave events,\n namely GW151226, GW170608, GW170814 and GW170817, and (iv) consider binary\n pulsar observations. Using Kepler's law that contains the noncommutative effect\n at second post-Newtonian order, we derive corrections to the gravitational\n waveform phase and the pericenter precession. Using the gravitational wave and\n double pulsar binary observations, we find bounds on a space-time\n noncommutative tensor $\\theta^{0i}$ in terms of the preferred frame direction\n with respect to the orientation of each binary. We find that the gravitational\n wave bounds are stronger than the binary pulsar one by an order of magnitude\n and the noncommutative tensor normalized by the Planck length and time is\n constrained to be of order unity.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0804.0856",
        "title": "Data acquisition system for the TUNKA-133 array",
        "abstract": "The new EAS Cherenkov array TUNKA-133, with about 1 km**2 sensitive area, is\n being installed in the Tunka Valley. The investigated energy range is\n 10**15-10**18 eV. It will consist of 133 optical detectors based on EMI9350\n PMTs. Optical detectors are grouped into 19 clusters with 7 detectors each. The\n detectors are connected to the cluster box with RG-58 cables. Every PMT signal\n is digitized in the cluster box with 200 MHz FADC. The cluster boxes are\n connected to the data acquisition center with a 1 Gb/s optical link. A detailed\n description of the data acquisition system (DAQ) is presented.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ex/9806021",
        "title": "Total Cross Section Measurements in gamma-p and gammastar-p at HERA",
        "abstract": "Measurements of the total cross section for real and virtual photons on\n protons at center-of-mass energies in the range from 20 GeV to 270 GeV with\n photon virtualities up to 5000 GeV^2 are presented. For real photons this cross\n section can be described by Regge-motivated models while for virtual photons\n perturbative QCD can be applied. The measurements of the two HERA collider\n experiments ZEUS and H1 open the possibility to investigate the interplay\n between the two theoretical approaches in the transition region as well as the\n high-energy behavior of the cross sections. The results of total cross section\n measurements are discussed in the above context.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2106.08349",
        "title": "Presence of water on exomoons orbiting free-floating planets: a case\n  study",
        "abstract": "A free-floating planet is a planetary-mass object that orbits around a\n non-stellar massive object (e.g. a brown dwarf) or around the Galactic Center.\n The presence of exomoons orbiting free-floating planets has been theoretically\n predicted by several models. Under specific conditions, these moons are able to\n retain an atmosphere capable of ensuring the long-term thermal stability of\n liquid water on their surface. We model this environment with a one-dimensional\n radiative-convective code coupled to a gas-phase chemical network including\n cosmic rays and ion-neutral reactions. We find that, under specific conditions\n and assuming stable orbital parameters over time, liquid water can be formed on\n the surface of the exomoon. The final amount of water for an Earth-mass\n exomonoon is smaller than the amount of water in Earth oceans, but enough to\n host the potential development of primordial life. The chemical equilibrium\n time-scale is controlled by cosmic rays, the main ionization driver in our\n model of the exomoon atmosphere.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2112.04160",
        "title": "A Finitely Convergent Cutting Plane, and a Bender's Decomposition\n  Algorithm for Mixed-Integer Convex and Two-Stage Convex Programs using\n  Cutting Planes",
        "abstract": "We present a finitely convergent cutting-plane algorithm for solving a\n general mixed-integer convex programs given an oracle for solving general\n convex programs. This method is extended to solve a family of two-stage\n mixed-integer convex programs using cutting planes, with applications to\n solving distributionally-robust two-stage stochastic mixed-integer convex\n programs. Since algorithms purely using cutting planes are not very practical\n for implementation, we combined the cut generation with a branch-and-union\n scheme to develop a more practical algorithm. Analysis is also given for the\n case where convex programming oracle provides an $\\epsilon-$optimal solution.\n Computational results on generated test problems show the practicality of our\n algorithm. Specifically, results show that addition of cuts speed up solution\n times by nearly 10-fold on the largest test problems that are solved.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2208.13507",
        "title": "Confinement of Bose-Einstein magnon condensates in adjustable complex\n  magnetization landscapes",
        "abstract": "Coherent wave states such as Bose-Einstein condensates (BECs), which\n spontaneously form in an overpopulated magnon gas even at room temperature,\n have considerable potential for wave-based computing and information processing\n at microwave frequencies. The ability to control the transport properties of\n magnon BECs plays an essential role for their practical use. Here, we\n demonstrate spatio-temporal control of the BEC density distribution through the\n excitation of magnon supercurrents in an inhomogeneously magnetized yttrium\n iron garnet film. The BEC is created by microwave parametric pumping and probed\n by Brillouin light scattering spectroscopy. The desired magnetization profile\n is prepared by heating the film with optical patterns projected onto its\n surface using a phase-based wavefront modulation technique. Specifically, we\n observe a pronounced spatially localized magnon accumulation caused by magnon\n supercurrents flowing toward each other originating in two heated regions. This\n accumulation effect increases the BEC lifetime due to the constant influx of\n condensed magnons into the confinement region. The shown approach to manipulate\n coherent waves provides an opportunity to extend the lifetime of freely\n evolving magnon BECs, create dynamic magnon textures, and study the interaction\n of magnon condensates formed in different regions of the sample.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/9607041",
        "title": "Uncovering Spiral Structure in Flocculent Galaxies",
        "abstract": "We present K'(2.1 micron) observations of four nearby flocculent spirals,\n which clearly show low-level spiral structure and suggest that kiloparsec-scale\n spiral structure is more prevalent in flocculent spirals than previously\n supposed. In particular, the prototypical flocculent spiral NGC 5055 is shown\n to have regular, two-arm spiral structure to a radius of 4 kpc in the near\n infrared, with an arm-interarm contrast of 1.3. The spiral structure in all\n four galaxies is weaker than that in grand design galaxies. Taken in unbarred\n galaxies with no large, nearby companions, these data are consistent with the\n modal theory of spiral density waves, which maintains that density waves are\n intrinsic to the disk. As an alternative, mechanisms for driving spiral\n structure with non-axisymmetric perturbers are also discussed. These\n observations highlight the importance of near infrared imaging for exploring\n the range of physical environments in which large-scale dynamical processes,\n such as density waves, are important.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0111323",
        "title": "Human Sexual Contact Network as a Bipartite Graph",
        "abstract": "A simple model to encapsulate the essential growth properties of \\emph{the\n web of human sexual contacts} is presented. In the model only heterosexual\n connection is considered and represented by a random growing bipartite graph\n where both male-female contact networks grow simultaneously. The time evolution\n of the model is analysed by a rate equation approach leading to confirm that\n male and female sexual contact distributions decay as power laws with exponents\n depending on influx and charisma of the sexes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1802.04913",
        "title": "Active crystals on a sphere",
        "abstract": "Two-dimensional crystals on curved manifolds exhibit nontrivial defect\n structures. Here, we consider \"active crystals\" on a sphere, which are composed\n of self-propelled colloidal particles. Our work is based on a new\n phase-field-crystal-type model that involves a density and a polarization field\n on the sphere. Depending on the strength of the self-propulsion, three\n different types of crystals are found: a static crystal, a self-spinning\n \"vortex-vortex\" crystal containing two vortical poles of the local velocity,\n and a self-translating \"source-sink\" crystal with a source pole where\n crystallization occurs and a sink pole where the active crystal melts. These\n different crystalline states as well as their defects are studied theoretically\n here and can in principle be confirmed in experiments.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2003.04279",
        "title": "Restore from Restored: Video Restoration with Pseudo Clean Video",
        "abstract": "In this study, we propose a self-supervised video denoising method called\n \"restore-from-restored.\" This method fine-tunes a pre-trained network by using\n a pseudo clean video during the test phase. The pseudo clean video is obtained\n by applying a noisy video to the baseline network. By adopting a fully\n convolutional neural network (FCN) as the baseline, we can improve video\n denoising performance without accurate optical flow estimation and registration\n steps, in contrast to many conventional video restoration methods, due to the\n translation equivariant property of the FCN. Specifically, the proposed method\n can take advantage of plentiful similar patches existing across multiple\n consecutive frames (i.e., patch-recurrence); these patches can boost the\n performance of the baseline network by a large margin. We analyze the\n restoration performance of the fine-tuned video denoising networks with the\n proposed self-supervision-based learning algorithm, and demonstrate that the\n FCN can utilize recurring patches without requiring accurate registration among\n adjacent frames. In our experiments, we apply the proposed method to\n state-of-the-art denoisers and show that our fine-tuned networks achieve a\n considerable improvement in denoising performance.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2201.07432",
        "title": "Reconfigurable optical logic operations through scattering media with\n  wavefront shaping",
        "abstract": "Optical logic gates are fundamental blocks of optical computing to accelerate\n information processing. While significant progress has been achieved in recent\n years, existing implementations typically rely on dedicated structures that are\n predesigned to modulate the phases and intensities of optical beams accurately\n for specific logic functions. Thus, these optical gates usually lack\n reconfigurability and are incapable within or through dynamic complex\n media/environment, such as fog and turbid water. In this work, as a conceptual\n demonstration, we propose reconfigurable optical logic operations through\n scattering media with transmission matrix-based wavefront shaping. A light beam\n is reflected by a spatial light modulator divided into several subregions\n functioning as logic units, with each displayed with predetermined wavefronts\n via transmission matrix-based wavefront shaping. Each modulated wavefront\n transmits through the scattering medium to form a desired light field. The\n interference of these light fields generates bright optical focus at\n pre-assigned locations, representing different logic states. As a proof of\n concept, we experimentally demonstrate five basic logic functions (AND, OR,\n NOT, NAND, NOR). As the transmission matrix of the scattering medium/system can\n be measured instantly to adapt to environment perturbation, the method, if\n further engineered, opens new venues towards reconfigurable optical logic\n computing in a dynamically complex environment.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2112.10798",
        "title": "Gravitationally Mediated Entanglement: Newtonian Field vs. Gravitons",
        "abstract": "We argue that if the Newtonian gravitational field of a body can mediate\n entanglement with another body, then it should also be possible for the body\n producing the Newtonian field to entangle directly with on-shell gravitons. Our\n arguments are made by revisiting a gedankenexperiment previously analyzed by\n Belenchia et al., which showed that a quantum superposition of a massive body\n requires both quantized gravitational radiation and local vacuum fluctuations\n of the spacetime metric in order to avoid contradictions with complementarity\n and causality. We provide a precise and rigorous description of the\n entanglement and decoherence effects occurring in this gedankenexperiment,\n thereby significantly improving upon the back-of-the-envelope estimates given\n in the analysis of Belenchia et al. and also showing that their conclusions are\n valid in much more general circumstances. As a by-product of our analysis, we\n show that under the protocols of the gedankenexperiment, there is no clear\n distinction between entanglement mediated by the Newtonian gravitational field\n of a body and entanglement mediated by on-shell gravitons emitted by the body.\n This suggests that Newtonian entanglement implies the existence of graviton\n entanglement and supports the view that the experimental discovery of Newtonian\n entanglement may be viewed as implying the existence of the graviton.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1211.1336",
        "title": "Optical Signatures of the Tunable Band Gap and Valley-Spin Coupling in\n  Silicene",
        "abstract": "We investigate the optical response of the silicene and similar materials,\n such as germanene, in the presence of an electrically tunable band gap for\n variable doping. The interplay of spin orbit coupling, due to the buckled\n structure of these materials, and a perpendicular electric field gives rise to\n a rich variety of phases: a topological or quantum spin Hall insulator, a\n valley-spin-polarized metal and a band insulator. We show that the dynamical\n conductivity should reveal signatures of these different phases which would\n allow for their identification along with the determination of parameters such\n as the spin orbit energy gap. We find an interesting feature where the electric\n field tuning of the band gap might be used to switch on and off the Drude\n intraband response. Furthermore, in the presence of spin-valley coupling, the\n response to circularly polarized light as a function of frequency and electric\n field tuning of the band gap is examined. Using right- and left-handed circular\n polarization it is possible to select a particular combination of spin and\n valley index. The frequency for this effect can be varied by tuning the band\n gap.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0510661",
        "title": "The X-ray Jet in Centaurus A: Clues on the Jet Structure and Particle\n  Acceleration",
        "abstract": "We report detailed studies of the X-ray emission from the kpc scale jet in\n the nearest active galaxy, Cen A. 41 compact sources were found within the jet,\n 13 of which were newly identified. We construct the luminosity function for the\n detected jet-knots and argue that the remaining emission is most likely to be\n truly diffuse, rather than resulting from the pile-up of unresolved faint\n knots. The transverse jet profile reveals that the extended emission has the\n intensity peak at the jet boundaries. We note that limb-brightened jet\n morphologies have been observed previously at radio frequencies in some jet\n sources, but never so clearly at higher photon energies. Our result therefore\n supports a stratified jet model, consisting of a relativistic outflow including\n a boundary layer with a velocity shear. In addition, we found that the X-ray\n spectrum of the diffuse component is almost uniform across and along the jet.\n We discuss this spectral behavior within a framework of shock and stochastic\n particle acceleration processes. We note some evidence for a possible spectral\n hardening at the outer sheath of the jet. Due to the limited photon statistics\n of the present data, further deep observations of Cen A are required to\n determine the reality of this finding, however we note that the existence of\n the hard X-ray features at outer jet boundaries would provide an important\n challenge to theories for the evolution of ultra-relativistic particles within\n the jets.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2303.12751",
        "title": "A Unified Framework for Fast Large-Scale Portfolio Optimization",
        "abstract": "We introduce a unified framework for rapid, large-scale portfolio\n optimization that incorporates both shrinkage and regularization techniques.\n This framework addresses multiple objectives, including minimum variance,\n mean-variance, and the maximum Sharpe ratio, and also adapts to various\n portfolio weight constraints. For each optimization scenario, we detail the\n translation into the corresponding quadratic programming (QP) problem and then\n integrate these solutions into a new open-source Python library. Using 50 years\n of return data from US mid to large-sized companies, and 33 distinct\n firm-specific characteristics, we utilize our framework to assess the\n out-of-sample monthly rebalanced portfolio performance of widely-adopted\n covariance matrix estimators and factor models, examining both daily and\n monthly returns. These estimators include the sample covariance matrix, linear\n and nonlinear shrinkage estimators, and factor portfolios based on Asset\n Pricing (AP) Trees, Principal Component Analysis (PCA), Risk Premium PCA\n (RP-PCA), and Instrumented PCA (IPCA). Our findings emphasize that AP-Trees and\n PCA-based factor models consistently outperform all other approaches in\n out-of-sample portfolio performance. Finally, we develop new l1 and l2\n regularizations of factor portfolio norms which not only elevate the portfolio\n performance of AP-Trees and PCA-based factor models but they have a potential\n to reduce an excessive turnover and transaction costs often associated with\n these models.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1512.03396",
        "title": "Boosted Sparse Non-linear Distance Metric Learning",
        "abstract": "This paper proposes a boosting-based solution addressing metric learning\n problems for high-dimensional data. Distance measures have been used as natural\n measures of (dis)similarity and served as the foundation of various learning\n methods. The efficiency of distance-based learning methods heavily depends on\n the chosen distance metric. With increasing dimensionality and complexity of\n data, however, traditional metric learning methods suffer from poor scalability\n and the limitation due to linearity as the true signals are usually embedded\n within a low-dimensional nonlinear subspace. In this paper, we propose a\n nonlinear sparse metric learning algorithm via boosting. We restructure a\n global optimization problem into a forward stage-wise learning of weak learners\n based on a rank-one decomposition of the weight matrix in the Mahalanobis\n distance metric. A gradient boosting algorithm is devised to obtain a sparse\n rank-one update of the weight matrix at each step. Nonlinear features are\n learned by a hierarchical expansion of interactions incorporated within the\n boosting algorithm. Meanwhile, an early stopping rule is imposed to control the\n overall complexity of the learned metric. As a result, our approach guarantees\n three desirable properties of the final metric: positive semi-definiteness, low\n rank and element-wise sparsity. Numerical experiments show that our learning\n model compares favorably with the state-of-the-art methods in the current\n literature of metric learning.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1611.02164",
        "title": "Numerical methods for the deterministic second moment equation of\n  parabolic stochastic PDEs",
        "abstract": "Numerical methods for stochastic partial differential equations typically\n estimate moments of the solution from sampled paths. Instead, we shall directly\n target the deterministic equations satisfied by the first and second moments,\n as well as the covariance.\n  In the first part, we focus on stochastic ordinary differential equations.\n For the canonical examples with additive noise (Ornstein-Uhlenbeck process) or\n multiplicative noise (geometric Brownian motion) we derive these deterministic\n equations in variational form and discuss their well-posedness in detail.\n Notably, the second moment equation in the multiplicative case is naturally\n posed on projective-injective tensor product spaces as trial-test spaces. We\n construct Petrov-Galerkin discretizations based on tensor product piecewise\n polynomials and analyze their stability and convergence in these natural norms.\n  In the second part, we proceed with parabolic stochastic partial differential\n equations with affine multiplicative noise. We prove well-posedness of the\n deterministic variational problem for the second moment, improving an earlier\n result. We then propose conforming space-time Petrov-Galerkin discretizations,\n which we show to be stable and quasi-optimal.\n  In both parts, the outcomes are illustrated by numerical examples.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2101.05771",
        "title": "A search for runaway stars in twelve Galactic supernova remnants",
        "abstract": "Runaway stars can result from core-collapse supernovae in multiple stellar\n systems. If the supernova disrupts the system, the companion gets ejected with\n its former orbital velocity. A clear identification of a runaway star can yield\n the time and place of the explosion as well as orbital parameters of the\n pre-supernova binary system. Previous searches have mostly considered O- and\n B-type stars as runaway stars because they are always young in absolute terms\n (not much older than the lifetime of the progenitor) and can be detected up to\n larger distances. We present here a search for runaway stars of all spectral\n types. For late-type stars, a young age can be inferred from the lithium test.\n We used Gaia data to identify and characterise runaway star candidates in\n nearby supernova remnants, obtained spectra of 39 stars with UVES at the VLT\n and HDS at the Subaru telescope and found a significant amount of lithium in\n the spectra of six dwarf stars. We present the spectral analysis, including\n measurements of radial velocities, atmospheric parameters and lithium\n abundances. Then we estimate the ages of our targets from the\n Hertzsprung-Russell diagram and with the lithium test, present a selection of\n promising runaway star candidates and draw constraints on the number of ejected\n runaway stars compared to model expectations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1809.05641",
        "title": "Symmetric vs. bosonic extension for bipartite states",
        "abstract": "A bipartite state $\\rho^{AB}$ has a $k$-symmetric extension if there exists a\n $k+1$-partite state $\\rho^{AB_1B_2\\ldots B_k}$ with marginals\n $\\rho^{AB_i}=\\rho^{AB}, \\forall i$. The $k$-symmetric extension is called\n bosonic if $\\rho^{AB_1B_2\\ldots B_k}$ is supported on the symmetric subspace of\n $B_1B_2\\ldots B_k$. Understanding the structure of symmetric/bosonic extension\n has various applications in the theory of quantum entanglement, quantum key\n distribution and the quantum marginal problem. In particular, bosonic extension\n gives a tighter bound for the quantum marginal problem based on seperability.\n In general, it is known that a $\\rho^{AB}$ admitting symmetric extension may\n not have bosonic extension. In this work, we show that when the dimension of\n the subsystem $B$ is $2$ (i.e. a qubit), $\\rho^{AB}$ admits a $k$-symmetric\n extension if and only if it has a $k$-bosonic extension. Our result has an\n immediate application to the quantum marginal problem and indicates a special\n structure for qubit systems based on group representation theory.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1002.0424",
        "title": "Cooperative Algorithms for MIMO Interference Channels",
        "abstract": "Interference alignment is a transmission technique for exploiting all\n available degrees of freedom in the interference channel with an arbitrary\n number of users. Most prior work on interference alignment, however, neglects\n interference from other nodes in the network not participating in the alignment\n operation. This paper proposes three generalizations of interference alignment\n for the multiple-antenna interference channel with multiple users that account\n for colored noise, which models uncoordinated interference. First, a minimum\n interference-plus-noise leakage algorithm is presented, and shown to be\n equivalent to previous subspace methods when noise is spatially white or\n negligible. A joint minimum mean squared error design is then proposed that\n jointly optimizes the transmit precoders and receive spatial filters, whereas\n previous designs neglect the receive spatial filter. This algorithm is shown to\n be a generalization of previous joint MMSE designs for other system\n configurations such as the broadcast channel. Finally, a maximum\n signal-to-interference-plus-noise ratio algorithm is developed that is proven\n to converge, unlike previous maximum SINR algorithms. The latter two designs\n are shown to have increased complexity due to non-orthogonal precoders, more\n required iterations, or more channel state knowledge than the min INL or\n subspace methods. The sum throughput performance of these algorithms is\n simulated in the context of a network with uncoordinated co-channel interferers\n not participating in the alignment protocol. It is found that a network with\n cochannel interference can benefit from employing precoders designed to\n consider that interference, but in some cases, ignoring the co-channel\n interference is advantageous.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1709.00203",
        "title": "Two-dimensional Massless Dirac Fermions in Antiferromagnetic AFe2As2 (A\n  = Ba, Sr)",
        "abstract": "We report infrared studies of AFe$_{2}$As$_{2}$ (A = Ba, Sr), two\n representative parent compounds of iron-arsenide superconductors, at magnetic\n fields (B) up to 17.5 T. Optical transitions between Landau levels (LLs) were\n observed in the antiferromagnetic states of these two parent compounds. Our\n observation of a $\\sqrt{B}$ dependence of the LL transition energies, the\n zero-energy intercepts at B = 0 T under the linear extrapolations of the\n transition energies and the energy ratio ($\\sim$ 2.4) between the observed LL\n transitions, combined with the linear band dispersions in two-dimensional (2D)\n momentum space obtained by theoretical calculations, demonstrates the existence\n of massless Dirac fermions in antiferromagnetic BaFe$_{2}$As$_{2}$. More\n importantly, the observed dominance of the zeroth-LL-related absorption\n features and the calculated bands with extremely weak dispersions along the\n momentum direction $k_{z}$ indicate that massless Dirac fermions in\n BaFe$_{2}$As$_{2}$ are 2D. Furthermore, we find that the total substitution of\n the barium atoms in BaFe$_{2}$As$_{2}$ by strontium atoms not only maintains 2D\n massless Dirac fermions in this system, but also enhances their Fermi velocity,\n which supports that the Dirac points in iron-arsenide parent compounds are\n topologically protected.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0811.1134",
        "title": "Eruption of magnetic flux ropes during flux emergence",
        "abstract": "Aims: We investigate the formation of flux ropes in a flux emergence region\n and their rise into the outer atmosphere of the Sun.\n  Methods: We perform 3D numerical experiments solving the time-dependent and\n resistive MHD equations.\n  Results: A sub-photospheric twisted flux tube rises from the solar interior\n and expands into the corona. A flux rope is formed within the expanding field,\n due to shearing and reconnection of field lines at low atmospheric heights. If\n the tube emerges into a non-magnetized atmosphere, the flux rope rises, but\n remains confined inside the expanding magnetized volume. On the contrary, if\n the expanding tube is allowed to reconnect with a preexisting coronal field,\n the flux rope experiences a full eruption with a rise profile which is in\n qualitative agreement with erupting filaments and Coronal Mass Ejections.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0703408",
        "title": "Monotone and Boolean Convolutions for Non-compactly Supported\n  Probability Measures",
        "abstract": "The equivalence of the characteristic function approach and the probabilistic\n approach to monotone and boolean convolutions is proven for non-compactly\n supported probability measures. A probabilistically motivated definition of the\n multiplicative boolean convolution of probability measures on the positive\n half-line is proposed. Unlike Bercovici's multiplicative boolean convolution it\n is always defined, but it turns out to be neither commutative nor associative.\n Finally some relations between free, monotone, and boolean convolutions are\n discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2303.10421",
        "title": "Mutilmodal Feature Extraction and Attention-based Fusion for Emotion\n  Estimation in Videos",
        "abstract": "The continuous improvement of human-computer interaction technology makes it\n possible to compute emotions. In this paper, we introduce our submission to the\n CVPR 2023 Competition on Affective Behavior Analysis in-the-wild (ABAW).\n Sentiment analysis in human-computer interaction should, as far as possible\n Start with multiple dimensions, fill in the single imperfect emotion channel,\n and finally determine the emotion tendency by fitting multiple results.\n Therefore, We exploited multimodal features extracted from video of different\n lengths from the competition dataset, including audio, pose and images.\n Well-informed emotion representations drive us to propose a Attention-based\n multimodal framework for emotion estimation. Our system achieves the\n performance of 0.361 on the validation dataset. The code is available at\n [https://github.com/xkwangcn/ABAW-5th-RT-IAI].",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1308.4895",
        "title": "Optimizing Key Distribution in Peer to Peer Network Using B-Trees",
        "abstract": "Peer to peer network architecture introduces many desired features including\n self-scalability that led to achieving higher efficiency rate than the\n traditional server-client architecture. This was contributed to the highly\n distributed architecture of peer to peer network. Meanwhile, the lack of a\n centralized control unit in peer to peer network introduces some challenge. One\n of these challenges is key distribution and management in such an architecture.\n This research will explore the possibility of developing a novel scheme for\n distributing and managing keys in peer to peer network architecture\n efficiently.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0708.3474",
        "title": "Nonlinear control of chaotic walking of atoms in an optical lattice",
        "abstract": "Centre-of-mass atomic motion in an optical lattice near the resonance is\n shown to be a chaotic walking due to the interplay between coherent internal\n atomic dynamics and spontaneous emission. Statistical properties of chaotic\n atomic motion can be controlled by the single parameter, the detuning between\n the atomic transition frequency and the laser frequency. We derive a\n Fokker-Planck equation in the energetic space to describe the atomic transport\n near the resonance and demonstrate numerically how to manipulate the atomic\n motion varying the detuning.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0707.2539",
        "title": "Three-dimensional Monte Carlo simulations of the quantum linear\n  Boltzmann equation",
        "abstract": "Recently the general form of a translation-covariant quantum Boltzmann\n equation has been derived which describes the dynamics of a tracer particle in\n a quantum gas. We develop a stochastic wave function algorithm that enables\n full three-dimensional Monte Carlo simulations of this equation. The simulation\n method is used to study the approach to equilibrium for various scattering\n cross sections and to determine dynamical deviations from Gaussian statistics\n through an investigation of higher-order cumulants. Moreover, we examine the\n loss of coherence of superpositions of momentum eigenstates and determine the\n corresponding decoherence time scales to quantify the transition from quantum\n to classical behavior of the state of the test particle.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ph/0105164",
        "title": "A supersymmetric solution to the bottom-quark cross section anomaly",
        "abstract": "In this talk, I describe a supersymmetric solution to the long-standing\n discrepancy between the bottom-quark production cross section and predictions\n of perturbative quantum chromodynamics. Pair production of light gluinos, of\n mass 12-16 GeV, with two-body decays into bottom quarks and bottom squarks, of\n mass 2-5.5 GeV, yields the correct normalizations and shapes of the measured\n bottom-quark distributions. One prediction of this scenario is that like-sign B\n mesons, B+B+ and B-B-, should be produced with a measurable rate at the next\n run of the Fermilab Tevatron Collider.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2106.12087",
        "title": "Generalized eigenvalues of the Perron-Frobenius operators of symbolic\n  dynamical systems",
        "abstract": "The generalized spectral theory is an effective approach to analyze a linear\n operator on a Hilbert space $\\mathcal{H}$ with a continuous spectrum. The\n generalized spectrum is computed via analytic continuations of the resolvent\n operators using a dense locally convex subspace $X$ of $\\mathcal{H}$ and its\n dual space $X'$. The three topological spaces $X \\subset \\mathcal{H} \\subset\n X'$ is called the rigged Hilbert space or the Gelfand triplet. In this paper,\n the generalized spectra of the Perron-Frobenius operators of the one-sided and\n two-sided shifts of finite types (symbolic dynamical systems) are determined. A\n one-sided subshift of finite type which is conjugate to the multiplication with\n the golden ration on $[0,1]$ modulo $1$ is also considered. A new construction\n of the Gelfand triplet for the generalized spectrum of symbolic dynamical\n systems is proposed by means of an algebraic procedure. The asymptotic formula\n of the iteration of Perron-Frobenius operators is also given. The iteration\n converges to the mixing state whose rate of convergence is determined by the\n generalized spectrum.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1106.2486",
        "title": "Maximal CHSH violations with low efficiency photodetection and homodyne\n  measurements",
        "abstract": "We study nonlocality tests in which each party performs photodetection and\n homodyne measurements. The results of such measurements are dichotomized and a\n Clauser-Horne-Shimony-Holt (CHSH) inequality is used. We prove that in this\n scenario the maximal violation is attainable and fully characterize the set of\n maximally violating states. If we restrict our search to states composed by at\n most 2, 4, and 6 photons per mode, we find critical photodetection efficiencies\n of 0.48, 0.36, and 0.29. We also found an entangled variation of the famous cat\n states that has critical efficiency 0.32. These values are well within the\n limit of current photodetector technology, which suggests the present approach\n as a road for a loophole-free Bell experiment.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2005.02000",
        "title": "On Interpretability of Deep Learning based Skin Lesion Classifiers using\n  Concept Activation Vectors",
        "abstract": "Deep learning based medical image classifiers have shown remarkable prowess\n in various application areas like ophthalmology, dermatology, pathology, and\n radiology. However, the acceptance of these Computer-Aided Diagnosis (CAD)\n systems in real clinical setups is severely limited primarily because their\n decision-making process remains largely obscure. This work aims at elucidating\n a deep learning based medical image classifier by verifying that the model\n learns and utilizes similar disease-related concepts as described and employed\n by dermatologists. We used a well-trained and high performing neural network\n developed by REasoning for COmplex Data (RECOD) Lab for classification of three\n skin tumours, i.e. Melanocytic Naevi, Melanoma and Seborrheic Keratosis and\n performed a detailed analysis on its latent space. Two well established and\n publicly available skin disease datasets, PH2 and derm7pt, are used for\n experimentation. Human understandable concepts are mapped to RECOD image\n classification model with the help of Concept Activation Vectors (CAVs),\n introducing a novel training and significance testing paradigm for CAVs. Our\n results on an independent evaluation set clearly shows that the classifier\n learns and encodes human understandable concepts in its latent representation.\n Additionally, TCAV scores (Testing with CAVs) suggest that the neural network\n indeed makes use of disease-related concepts in the correct way when making\n predictions. We anticipate that this work can not only increase confidence of\n medical practitioners on CAD but also serve as a stepping stone for further\n development of CAV-based neural network interpretation methods.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2107.12754",
        "title": "Circumventing the Stability Problems of Graphene Nanoribbon Zigzag Edges",
        "abstract": "Carbon nanostructures with zigzag edges exhibit unique properties with\n exciting potential applications. Such nanostructures are generally synthesized\n under vacuum because their zigzag edges are unstable under ambient conditions:\n a barrier that must be surmounted to achieve their scalable exploitation. Here,\n we prove the viability of chemical protection/deprotection strategies for this\n aim, demonstrated on labile chiral graphene nanoribbons (chGNRs). Upon\n hydrogenation, the chGNRs survive an exposure to air, after which they are\n easily converted back to their original structure via annealing. We also\n approach the problem from another angle by synthesizing a chemically stable\n oxidized form of the chGNRs that can be converted to the pristine hydrocarbon\n form via hydrogenation and annealing. These findings may represent an important\n step toward the integration of zigzag-edged nanostructures in devices.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2008.10022",
        "title": "COVID-19 Pandemic: Identifying Key Issues using Social Media and Natural\n  Language Processing",
        "abstract": "The COVID-19 pandemic has affected people's lives in many ways. Social media\n data can reveal public perceptions and experience with respect to the pandemic,\n and also reveal factors that hamper or support efforts to curb global spread of\n the disease. In this paper, we analyzed COVID-19-related comments collected\n from six social media platforms using Natural Language Processing (NLP)\n techniques. We identified relevant opinionated keyphrases and their respective\n sentiment polarity (negative or positive) from over 1 million randomly selected\n comments, and then categorized them into broader themes using thematic\n analysis. Our results uncover 34 negative themes out of which 17 are economic,\n socio-political, educational, and political issues. 20 positive themes were\n also identified. We discuss the negative issues and suggest interventions to\n tackle them based on the positive themes and research evidence.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2209.00554",
        "title": "Operational Interpretation of the Sandwiched R\\'enyi Divergence of Order\n  1/2 to 1 as Strong Converse Exponents",
        "abstract": "We provide the sandwiched R\\'enyi divergence of order\n $\\alpha\\in(\\frac{1}{2},1)$, as well as its induced quantum information\n quantities, with an operational interpretation in the characterization of the\n exact strong converse exponents of quantum tasks. Specifically, we consider (a)\n smoothing of the max-relative entropy, (b) quantum privacy amplification, and\n (c) quantum information decoupling. We solve the problem of determining the\n exact strong converse exponents for these three tasks, with the performance\n being measured by the fidelity or purified distance. The results are given in\n terms of the sandwiched R\\'enyi divergence of order $\\alpha\\in(\\frac{1}{2},1)$,\n and its induced quantum R\\'enyi conditional entropy and quantum R\\'enyi mutual\n information. This is the first time to find the precise operational meaning for\n the sandwiched R\\'enyi divergence with R\\'enyi parameter in the interval\n $\\alpha\\in(\\frac{1}{2},1)$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0907.1114",
        "title": "Viable entanglement detection of unknown mixed states in low dimensions",
        "abstract": "We explore procedures to detect entanglement of unknown mixed states, which\n can be experimentally viable. The heart of the method is a hierarchy of simple\n feasibility problems, which provides sufficient conditions to entanglement. Our\n numerical investigations indicate that the entanglement is detected with a cost\n which is much lower than full state tomography. The procedure is applicable to\n both free and bound entanglement, and involves only single copy measurements.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1312.5400",
        "title": "Curvature identities on contact metric manifolds and their applications",
        "abstract": "We study curvature identities on contact metric manifolds on the geometry of\n the corresponding almost K\\\"aehler cones, and we provide applications of the\n derived curvature identities.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1601.03365",
        "title": "High photon energy spectroscopy of NiO: experiment and theory",
        "abstract": "We have revisited the valence band electronic structure of NiO by means of\n hard x-ray photoemission spectroscopy (HAXPES) together with theoretical\n calculations using both the GW method and the local density approximation +\n dynamical mean-field theory (LDA+DMFT) approaches. The effective impurity\n problem in DMFT is solved through the exact diagonalization (ED) method. We\n show that the LDA+DMFT method alone cannot explain all the observed structures\n in the HAXPES spectra. GW corrections are required for the O bands and Ni-s and\n p derived states to properly position their binding energies. Our results\n establish that a combination of the GW and DMFT methods is necessary for\n correctly describing the electronic structure of NiO in a proper ab-initio\n framework. We also demonstrate that the inclusion of photoionization cross\n section is crucial to interpret the HAXPES spectra of NiO.We argue that our\n conclusions are general and that the here suggested approach is appropriate for\n any complex transition metal oxide.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1201.5431",
        "title": "Component reduction in N=2 supergravity: the vector, tensor, and\n  vector-tensor multiplets",
        "abstract": "Recent advances in curved N=2 superspace methods have rendered the component\n reduction of superspace actions more feasible than in the past. In this paper,\n we consider models involving both vector and tensor multiplets coupled to\n supergravity and demonstrate explicitly how component actions may be\n efficiently obtained. In addition, tensor multiplets coupled to conformal\n supergravity are considered directly within projective superspace, where their\n formulation is most natural. We then demonstrate how the inverse procedure --\n the lifting of component results to superspace -- can simplify the analysis of\n complicated multiplets. We address the off-shell N=2 vector-tensor multiplet\n coupled to conformal supergravity with a central charge and demonstrate\n explicitly how its constraints and Lagrangian can be written in a simpler way\n using superfields.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.13383",
        "title": "The art of finding the optimal scattering center(s)",
        "abstract": "The efficient use of a multipole expansion of the far field for rapid\n numerical modeling and optimization of the optical response from ordered and\n disordered arrays of various structural elements is complicated by the\n ambiguity in choosing the ultimate expansion centers for individual scatterers.\n Since the multipolar decomposition depends on the position of the expansion\n center, the sets of multipoles are not unique. They may require constrained\n optimization to get the compact and most efficient spatial spectrum for each\n scatterer. We address this problem by finding {\\em the optimal scattering\n centers} for which the spatial multipolar spectra become unique. We separately\n derive these optimal positions for the electric and magnetic parts by\n minimizing the norm of the poloidal electric and magnetic quadrupoles.\n Employing the long-wave approximation (LWA) ansatz, we verify the approach with\n the theoretical discrete models and realistic scatterers. We show that the\n optimal electric and magnetic scattering centers, in all cases, are not\n co-local with the centers of mass. The optimal multipoles, including the\n toroidal terms, are calculated for several structurally distinct scattering\n cases, and their utility for low-cost numerical schemes, including the\n generalized T-matrix approach, is discussed. Expansion of the work beyond the\n LWA is possible, with a promise for faster and universal numerical schemes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1712.00065",
        "title": "Multiscale modeling of a rectifying bipolar nanopore: explicit-water\n  versus implicit-water simulations",
        "abstract": "In a multiscale modeling approach, we present computer simulation results for\n a rectifying bipolar nanopore on two modeling levels. In an all-atom model, we\n use explicit water to simulate ion transport directly with the molecular\n dynamics technique. In a reduced model, we use implicit water and apply the\n Local Equilibrium Monte Carlo method together with the Nernst-Planck transport\n equation. This hybrid method makes the fast calculation of ion transport\n possible at the price of lost details. We show that the implicit-water model is\n an appropriate representation of the explicit-water model when we look at the\n system at the device (i.e., input vs.\\ output) level. The two models produce\n qualitatively similar behavior of the electrical current for different voltages\n and model parameters. Looking at details of concentration and potential\n profiles, we find profound differences between the two models. These\n differences, however, do not influence the basic behavior of the model as a\n device because they do not influence the $z$-dependence of the concentration\n profiles which are the main determinants of current. These results then address\n an old paradox: how do reduced models, whose assumptions should break down in a\n nanoscale device, predict experimental data? Our simulations show that reduced\n models can still capture the overall device physics correctly, even though they\n get some important aspects of the molecular-scale physics quite wrong; reduced\n models work because they include the physics that is necessary from the point\n of view of device function. Therefore, reduced models can suffice for general\n device understanding and device design, but more detailed models might be\n needed for molecular level understanding.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2009.05672",
        "title": "Solar Analogs as a Tool to Understand the Sun",
        "abstract": "Solar analogs, broadly defined as stars similar to the Sun in mass or\n spectral type, provide a useful laboratory for exploring the range of Sun-like\n behaviors and exploring the physical mechanisms underlying some of the Sun's\n most elusive processes like coronal heating and the dynamo. We describe a\n series of heliophysics-motivated, but astrophysics-like studies of solar\n analogs. We argue for a range of stellar observations, including (a) the\n identification and fundamental parameter determination of new solar analogs,\n and (b) characterizing emergent properties like activity, magnetism, and\n granulation. These parameters should be considered in the framework of\n statistical studies of the dependences of these observables on fundamental\n stellar parameters like mass, metallicity, and rotation.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1706.08237",
        "title": "A gradient flow for the prescribed Gaussian curvature problem on a\n  closed Riemann surface with conical singularity",
        "abstract": "In this note, we prove that the abstract gradient flow introduced by\n Baird-Fardoun-Regbaoui \\cite{BFR}is well-posed on a closed Riemann surface with\n conical singularity. Long time existence and convergence of the flow are proved\n under certain assumptions. As an application, the prescribed Gaussian curvature\n problem is solved when the singular Euler characteristic of the conical surface\n is non-positive.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2308.01067",
        "title": "Influence of band occupation on electron-phonon coupling in gold",
        "abstract": "Electron-phonon coupling is a fundamental process that governs the energy\n relaxation dynamics of solids excited by ultrafast laser pulses. It has been\n found to strongly depend on electron temperature as well as on nonequilibrium\n effects. Recently, the effect of occupational nonequilibrium in noble metals,\n which outlasts the fully kinetic stage, has come into increased focus. In this\n work, we investigate the influence of nonequilibrium density distributions in\n gold on the electron-phonon coupling. We find a large effect on the coupling\n parameter which describes the energy exchange between the two subsystems. Our\n results challenge the conventional view that electron temperature alone is a\n sufficient predictor of electron-phonon coupling.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1609.08545",
        "title": "Squared Dehn twists and deformed symplectic invariants",
        "abstract": "We establish an infinitesimal version of fragility for squared Dehn twists\n around even dimensional Lagrangian spheres. The precise formulation involves\n twisting the Fukaya category by a closed two-form or bulk deforming it by a\n half-dimensional cycle. As our main application, we compute the twisted and\n bulk deformed symplectic cohomology of the subflexible Weinstein manifolds\n constructed in \\cite{murphysiegel}.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2405.10417",
        "title": "Cosmic rays for imaging cultural heritage objects",
        "abstract": "In cultural heritage conservation, it is increasingly common to rely on\n non-destructive imaging methods based on the absorption or scattering of\n photons ($X$ or $\\gamma$ rays) or neutrons. However, physical and practical\n issues limit these techniques: their penetration depth may be insufficient for\n large and dense objects, they require transporting the objects of interest to\n dedicated laboratories, artificial radiation is hazardous and may induce\n activation in the material under study. Muons are elementary particles\n abundantly and freely produced in cosmic-ray interactions in the atmosphere.\n Their absorption and scattering in matter are characteristically dependent on\n the density and elemental composition of the material that they traverse, which\n offers the possibility of exploiting them for sub-surface remote imaging. This\n novel technique, nicknamed \"muography\", has been applied in use cases ranging\n from geophysics to archaeology to nuclear safety, but it has been so far\n under-explored for a vast category of cultural heritage objects that are\n relatively large (from decimeters to human size) and dense (stone, metals). The\n development of portable muon detectors makes muography particularly competitive\n in cases where the items to be analysed are not transportable, or set up in a\n confined environment. This document reviews the relevant literature, presents\n some exemplary use cases, and critically assesses the strengths and weaknesses\n of muography in this context.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1403.2104",
        "title": "A close examination of cosmic microwave background mirror-parity after\n  Planck",
        "abstract": "Previous claims of significant evidence for mirror-parity in the large-scale\n cosmic microwave background (CMB) data from the Wilkinson Microwave Anisotropy\n Probe (WMAP) experiment have been recently echoed in the first study of\n isotropy and statistics of CMB data from Planck. We revisit these claims with a\n careful analysis of the latest data available. We construct statistical\n estimators in both harmonic and pixel space, test them on simulated data with\n and without mirror-parity symmetry, apply different Galactic masks, and study\n the dependence of the results on arbitrary choices of free parameters. We\n confirm that the data exhibit evidence for odd mirror-parity at a significance\n which reaches as high as ~ 99 per cent C.L., under some circumstances. However,\n given the inherent biases in the pixel-based statistic and the dependence of\n both pixel and harmonic space statistics on the particular form of Galactic\n masking and other a-posteriori choices, we conclude that these results are not\n in significant tension with the predictions of the concordance cosmological\n model.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/9903026",
        "title": "The random magnetic flux problem in a quantum wire",
        "abstract": "The random magnetic flux problem on a lattice and in a quasi one-dimensional\n (wire) geometry is studied both analytically and numerically. The first two\n moments of the conductance are obtained analytically. Numerical simulations for\n the average and variance of the conductance agree with the theory. We find that\n the center of the band $\\epsilon=0$ plays a special role. Away from\n $\\epsilon=0$, transport properties are those of a disordered quantum wire in\n the standard unitary symmetry class. At the band center $\\epsilon=0$, the\n dependence on the wire length of the conductance departs from the standard\n unitary symmetry class and is governed by a new universality class, the chiral\n unitary symmetry class. The most remarkable property of this new universality\n class is the existence of an even-odd effect in the localized regime:\n Exponential decay of the average conductance for an even number of channels is\n replaced by algebraic decay for an odd number of channels.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2104.13204",
        "title": "Some new results on generalized diagonally dominant matrices and matrix\n  eigenvalue inclusion regions",
        "abstract": "In matrix theory and numerical analysis there are two very famous and\n important results. One is Gersgorin circle theorem, the other is strictly\n diagonally dominant theorem. They have important application and research\n value, and have been widely used and studied. In this paper, we investigate\n generalized diagonally dominant matrices and matrix eigenvalue inclusion\n regions. A class of G-function pairs is proposed, which extends the concept of\n G-functions. Thirteen kind of G-function pairs are established. Their\n properties and characteristics are studied. By using these special G-function\n pairs, we construct a large number of sufficient and necessary conditions for\n strictly diagonally dominant matrices and matrix eigenvalue inclusion regions.\n These conditions and regions are composed of different combinations of\n G-function pairs, deleted absolute row sums and column sums of matrices. The\n results extend, include and are better than some classical results.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1406.3080",
        "title": "First detection and characterization of symbiotic stars in M31",
        "abstract": "Symbiotic binaries are putative progenitors of type Ia supernovae. The census\n of Galactic symbiotic binaries is so incomplete that we cannot reliably\n estimate the total population of these stars, and use it to check whether that\n number is consistent with the observed type Ia supernova rate in spiral\n galaxies. We have thus begun a survey of the nearest counterpart of our own\n Galaxy, namely M31, where a relatively complete census of symbiotic stars is\n achievable. We report the first detections and spectrographic characterizations\n of 35 symbiotic binaries in M31, and compare these stars with the symbiotic\n population in the Milky Way. These newly detected M31 symbiotic binaries are\n remarkably similar to galactic symbiotics, though we are clearly only sampling\n (in this feasibility study) the most luminous symbiotics in M31. We have also\n found, in M31, the symbiotic star (M31SyS J004233.17+412720.7) with the highest\n ionization level known amongst all symbiotics. An optical outburst of the M31\n symbiotic star M31SyS J004322.50+413940.9 was probably a nova-like outburst,\n the first symbiotic outburst detected outside the Milky Way and Magellanic\n Clouds.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2008.10585",
        "title": "Linear and superlinear spread for continuous-time frog model",
        "abstract": "Consider a stochastic growth model on $\\mathbb{Z} ^d$. Start with some active\n particle at the origin and sleeping particles elsewhere. The initial number of\n particles at $x \\in \\mathbb{Z} ^d$ is $\\eta(x)$, where $\\eta (x)$ are\n independent random variables distributed according to $\\mu$. Active particles\n perform a simple continuous-time random walk while sleeping particles stay put\n until the first arrival of an active particle to their location. Upon the\n arrival all sleeping particles at the site activate at once and start moving\n according to their own simple random walks. The aim of this paper is to give\n conditions on $\\mu$ under which the spread of the process is linear or faster\n than linear. The proofs rely on comparison to various percolation models.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2306.00399",
        "title": "Melting of Quarkonia in strong magnetic field",
        "abstract": "In this paper, spectra of the quarkonium states has been studied using the\n conditions temperature, chemical potential and the magnetic field. Here our\n main focus is to study the effect of strong magnetic field on the quarkonium\n properties. The binding energies and the dissociation temperature for the\n ground and the first excited states of the charmonium and bottomonium in the\n presence of strong magnetic field at chemical potential \\mu = 500 MeV has been\n studied. Here we use quasiparticle (QP) Debye mass depending upon temperature,\n magnetic field and chemical potential obtained from the quasiparticle approach.\n The Debye mass strongly increases at different values of temperature and\n magnetic field. The binding energy decreases with increase in the temperature\n at different magnetic field eB= 0.3, 0.5, and 0.7 GeV2 and also decreases with\n magnetic field at different at T=200,300 and 400 MeV for the J/\\psi, \\psi,\n \\upsilon, and \\upsilon prime states of the quarkonia. The dissociation\n temperature of the quarkonium states falls with the increasing values of the\n magnetic field at critical temperature Tc =197 MeV",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1403.6748",
        "title": "Photoconductivity and photo-detection response of multiferroic bismuth\n  iron oxide",
        "abstract": "We report visible light detection with in-plane BiFeO3 (BFO) thin films grown\n on pre-patterned inter-digital electrodes. In-plane configured BFO film\n displayed photocurrents with a 40:1 photo-to-dark-current ratio and improved\n photo-sensing ability for >15000 s (4 hrs) under small bias voltage (42V).\n Nearly sixty percent of the photo-induced charge carriers decay in 1.0 s and\n follow a double-exponential decay model. At 373 K the effect of light does not\n significantly increase the dark current, probably due to reduced mobility.\n Sub-bandgap weak monochromatic light (1 mw/cm2) shows one fold increase in\n photo-charge carriers.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1810.01312",
        "title": "Tunnel spectroscopy of localised electronic states in hexagonal boron\n  nitride",
        "abstract": "Hexagonal boron nitride (hBN) is a large band gap layered crystal, frequently\n incorporated in van der Waals (vdW) heterostructures as an insulating or tunnel\n barrier. Localised states with energies within its band gap can emit visible\n light, relevant to applications in nanophotonics and quantum information\n processing. However, they also give rise to conducting channels, which can\n induce electrical breakdown when a large voltage is applied. Here we use gated\n tunnel transistors to study resonant electron tunnelling through the localised\n states in few atomic-layer hBN barriers sandwiched between two monolayer\n graphene electrodes. The measurements are used to determine the energy,\n linewidth, tunnelling transmission probability, and depth within the barrier of\n more than 50 distinct localised states. A three-step process of electron\n percolation through two spatially separated localised states is also\n investigated.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2401.07224",
        "title": "Vehicle Selection for C-V2X Mode 4 Based Federated Edge Learning Systems",
        "abstract": "Federated learning (FL) is a promising technology for vehicular networks to\n protect vehicles' privacy in Internet of Vehicles (IoV). Vehicles with limited\n computation capacity may face a large computational burden associated with FL.\n Federated edge learning (FEEL) systems are introduced to solve such a problem.\n In FEEL systems, vehicles adopt the cellular-vehicle to everything (C-V2X) mode\n 4 to upload encrypted data to road side units' (RSUs)' cache queue. Then RSUs\n train the data transmitted by vehicles, update the locally model\n hyperparameters and send back results to vehicles, thus vehicles' computational\n burden can be released. However, each RSU has limited cache queue. To maintain\n the stability of cache queue and maximize the accuracy of model, it is\n essential to select appropriate vehicles to upload data. The vehicle selection\n method for FEEL systems faces challenges due to the random departure of data\n from the cache queue caused by the stochastic channel and the different system\n status of vehicles, such as remaining data amount, transmission delay, packet\n collision probability and survival ability. This paper proposes a vehicle\n selection method for FEEL systems that aims to maximize the accuracy of model\n while keeping the cache queue stable. Extensive simulation experiments\n demonstrate that our proposed method outperforms other baseline selection\n methods.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2211.02275",
        "title": "Homogeneous ACM bundles on exceptional Grassmannians",
        "abstract": "In this paper, we characterize homogeneous arithmetically Cohen-Macaulay\n (ACM) bundles over exceptional Grassmannians in terms of their associated data.\n We show that there are only finitely many irreducible homogeneous ACM bundles\n by twisting line bundles over exceptional Grassmannians. As a consequence, we\n prove that some exceptional Grassmannians are of wild representation type.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1506.08976",
        "title": "He-Accreting WDs: AM CVn stars with WD Donors",
        "abstract": "We study the physical and evolutionary properties of the \"WD family\" of AM\n CVn stars by computing realistic models of IDD systems. We evaluate\n self-consistently both the mass transfer rate from the donor, as determined by\n GW emission and interaction with the binary companion, and the thermal response\n of the accretor to mass deposition. We find that, after the onset of mass\n transfer, all the considered systems undergo a strong non-dynamical He-flash.\n However, due to the compactness of these systems, the expanding accretors fill\n their Roche lobe very soon, thus preventing the efficient heating of the\n external layers of the accreted CO WDs. Moreover, due to the loss of matter\n from the systems, the orbital separations enlarge and mass transfer comes to a\n halt. The further evolution depends on the value of $\\dot{M}$, after the donors\n fill again their lobe. On one hand, if the accretion rate, as determined by the\n actual value of (M$_{don}$,M$_{acc}$), is high enough, the accretors experience\n several He-flashes of decreasing strength and then quiescent He-burning sets\n in. Later on, since the mass transfer rate in IDD is a permanently decreasing\n function of time, accretors experience several recurrent strong flashes. On the\n other hand, for intermediate and low values of $\\dot{M}$, the accretors enter\n the strong flashes accretion regime. As expected, in all the considered systems\n the last He-flash is the strongest one, even if a dynamical event never occurs.\n When the mass accretion rate decreases below (2-3)$\\times 10^{-8} M_\\odot\n yr^{-1}$, the compressional heating of the He-shell becomes less efficient than\n the neutrino cooling, so that all the accretors in the considered systems\n evolve into massive degenerate objects. Our results suggest that SNe .Ia or\n type Ia Supernovae due to Edge-Lit Detonation in the WD family of AM CVn stars\n should be much more rare than previously expected.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1110.2051",
        "title": "The microwave sky after one year of Planck operations",
        "abstract": "The ESA Planck satellite, launched on May 14th, 2009, is the third generation\n space mission dedicated to the measurement of the Cosmic Microwave Background\n (CMB), the first light in the Universe. Planck observes the full sky in nine\n frequency bands from 30 to 857 GHz and is designed to measure the CMB\n anisotropies with an unprecedented combination of sensitivity, angular\n resolution and control of systematic effects. In this presentation we summarise\n the Planck instruments performance and discuss the main scientific results\n obtained after one year of operations in the fields of galactic and\n extragalactic astrophysics.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0801.1490",
        "title": "CP-Conserving Unparticle Phase Effects on the Unpolarized and Polarized\n  Direct CP Asymmetry in $b \\to d \\ell^+\\ell^-$ Transition",
        "abstract": "We examine the unparticle CP-conserving phase effects on the direct CP\n asymmetry for both polarized and unpolarized lepton in the inclusive $b\\to d\n \\ell^+ \\ell^-$ transition, where the flavor changing neutral currents are\n forbidden at tree level but are induced by one-loop penguin diagrams. The\n averaged polarized and unpolarized CP asymmetries depict strong dependency on\n the unparticle parameters. In particular, a sizable discrepancy corresponding\n to the standard model is achieved when the scale dimension value is $1<d_{\\UP}<\n 2$. We see that the unparticle stuff significantly enhances, suppresses or\n changes the sign of the CP asymmetry depending on the definite value of the\n scaling dimension $d_{\\UP}$. Especially, when $d_{\\UP}\\sim 1.1$ the CP\n asymmetries vanish.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2204.01476",
        "title": "$\\mathbb{C}P^2$ Skyrmion Crystals in an SU(3) Magnet with a Generalized\n  Dzyaloshinskii-Moriya Interaction",
        "abstract": "We study $\\mathbb{C}P^2$ Skyrmion crystals in the ferromagnetic SU(3)\n Heisenberg model with a generalization of the Dzyaloshinskii-Moriya interaction\n and the Zeeman term. The model possesses two different types of Skyrmion\n crystals with unit-Skyrmions that can be interpreted as bound states of two\n half-Skyrmions or four quarter-Skyrmions. Our study on $\\mathbb{C}P^2$ Skyrmion\n crystals opens up the possibility for useful future applications since\n $\\mathbb{C}P^2$ Skyrmions have more degrees of freedom than the usual\n $\\mathbb{C}P^1$ (magnetic) Skyrmions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2110.07112",
        "title": "On the Sample Complexity of Decentralized Linear Quadratic Regulator\n  with Partially Nested Information Structure",
        "abstract": "We study the problem of control policy design for decentralized\n state-feedback linear quadratic control with a partially nested information\n structure, when the system model is unknown. We propose a model-based learning\n solution, which consists of two steps. First, we estimate the unknown system\n model from a single system trajectory of finite length, using least squares\n estimation. Next, based on the estimated system model, we design a control\n policy that satisfies the desired information structure. We show that the\n suboptimality gap between our control policy and the optimal decentralized\n control policy (designed using accurate knowledge of the system model) scales\n linearly with the estimation error of the system model. Using this result, we\n provide an end-to-end sample complexity result for learning decentralized\n controllers for a linear quadratic control problem with a partially nested\n information structure.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/0506081",
        "title": "On Graviphoton F-terms of N=1 SU(N) SYM with Fundamental Matter",
        "abstract": "We consider N=1 SU(N_c) supersymmetric gauge theory with chiral matter\n multiplets in the fundamental representation of the gauge group. The general\n form of the meson correlation functions in the presence of graviphoton\n background with or without gravity is obtained. Finally, the perturbation\n theory scheme of computing these correlation functions is discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.08010",
        "title": "The e-value and the Full Bayesian Significance Test: Logical Properties\n  and Philosophical Consequences",
        "abstract": "This article gives a conceptual review of the e-value, ev(H|X) -- the\n epistemic value of hypothesis H given observations X. This statistical\n significance measure was developed in order to allow logically coherent and\n consistent tests of hypotheses, including sharp or precise hypotheses, via the\n Full Bayesian Significance Test (FBST). Arguments of analysis allow a full\n characterization of this statistical test by its logical or compositional\n properties, showing a mutual complementarity between results of mathematical\n statistics and the logical desiderata lying at the foundations of this theory.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1110.2445",
        "title": "Hubble and Shapley - Two Early Giants of Observational Cosmology",
        "abstract": "Observational cosmology of the first decades of the Twentieth Century was\n dominated by two giants: Edwin Hubble and Harlow Shapley. Hubble's major\n contributions were to the study and classification of individual galaxies with\n large telescopes, whereas Shapley is best remembered for his work on groups and\n clusters of galaxies using telescopes of more modest aperture.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1505.05859",
        "title": "Path algebras, wave-particle duality, and quantization of phase space",
        "abstract": "Semigroup algebras admit certain `coherent' deformations which, in the\n special case of a path algebra, may associate a periodic function to an\n evolving path; for a particle moving freely on a straight line after an initial\n impulse, the wave length is that hypothesized by de Broglie's wave-particle\n duality. This theory leads to a model of \"physical\" phase space of which\n mathematical phase space, the cotangent bundle of configuration space, is a\n projection. This space is singular, quantized at the Planck level, its\n structure implies the existence of spin, and the spread of a packet can be\n described as a random walk. The wavelength associated to a particle moving in\n this space need not be constant and its phase can change discontinuously.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1405.0881",
        "title": "On the Galois correspondence theorem in separable Hopf Galois theory",
        "abstract": "In this paper we present a reformulation of the Galois correspondence theorem\n of Hopf Galois theory in terms of groups carrying farther the description of\n Greither and Pareigis. We prove that the class of Hopf Galois extensions for\n which the Galois correspondence is bijective is larger than the class of almost\n classically Galois extensions but not equal to the whole class. We show as well\n that the image of the Galois correspondence does not determine the Hopf Galois\n structure.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/9912099",
        "title": "Complex extended line emission in the cD galaxy in Abell 2390",
        "abstract": "This paper reports maps of the cD galaxy in the rich z=0.23 cluster Abell\n 2390 at UV, [OII], L alpha and H alpha wavelengths. Spatially resolved UV and\n optical spectra were obtained with STIS on the Hubble Space Telescope; the 2\n arcsec wide slit was aligned close to the long axis and blue lane of the galaxy\n and includes all the inner bright features seen in WFPC2 images. The L alpha is\n seen in emission from several bright knots and over an extended region from 4\n arcsec NW to 2 arcsec SE of the nucleus. Three of these knots have detected UV\n continuum as well. H alpha images were obtained with OSIS at CFHT; both H alpha\n and [O II] show extended emission that roughly trace L alpha, primarily to the\n NW. Notable differences between the spatial distributions of H alpha, L alpha\n and [O II] emission and the blue knots in the galaxy may be the result of\n inhomogeneous dust extinction, or variations in ionisation. The L alpha\n emission velocities depend the uncertain identification with features in the\n undispersed images; there is strong evidence for resolved emission knots to\n have large velocities, >~1000 km/s, indicative of infall. The L alpha spectrum\n shows a sharp cutoff which may be due to absorption; we entertain the\n possibility that this edge is due to an shell of absorbing gas, outflowing at\n several thousand km/s.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1511.01796",
        "title": "Computing B-Stationary Points of Nonsmooth DC Programs",
        "abstract": "Motivated by a class of applied problems arising from physical layer based\n security in a digital communication system, in particular, by a secrecy\n sum-rate maximization problem, this paper studies a nonsmooth,\n difference-of-convex (dc) minimization problem. The contributions of this paper\n are: (i) clarify several kinds of stationary solutions and their relations;\n (ii) develop and establish the convergence of a novel algorithm for computing a\n d-stationary solution of a problem with a convex feasible set that is arguably\n the sharpest kind among the various stationary solutions; (iii) extend the\n algorithm in several directions including: a randomized choice of the\n subproblems that could help the practical convergence of the algorithm, a\n distributed penalty approach for problems whose objective functions are sums of\n dc functions, and problems with a specially structured (nonconvex) dc\n constraint. For the latter class of problems, a pointwise Slater constraint\n qualification is introduced that facilitates the verification and computation\n of a B(ouligand)-stationary point.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1012.2242",
        "title": "Are biological systems poised at criticality?",
        "abstract": "Many of life's most fascinating phenomena emerge from interactions among many\n elements--many amino acids determine the structure of a single protein, many\n genes determine the fate of a cell, many neurons are involved in shaping our\n thoughts and memories. Physicists have long hoped that these collective\n behaviors could be described using the ideas and methods of statistical\n mechanics. In the past few years, new, larger scale experiments have made it\n possible to construct statistical mechanics models of biological systems\n directly from real data. We review the surprising successes of this \"inverse\"\n approach, using examples form families of proteins, networks of neurons, and\n flocks of birds. Remarkably, in all these cases the models that emerge from the\n data are poised at a very special point in their parameter space--a critical\n point. This suggests there may be some deeper theoretical principle behind the\n behavior of these diverse systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1707.08268",
        "title": "Incidence Results and Bounds of Trilinear and Quadrilinear Exponential\n  Sums",
        "abstract": "We give a new bound on the number of collinear triples for two arbitrary\n subsets of a finite field. This improves on existing results which rely on the\n Cauchy inequality. We then us this to provide a new bound on trilinear and\n quadrilinear exponential sums.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2308.10852",
        "title": "Uncertainty benchmarks for time-dependent transport problems",
        "abstract": "Verification solutions for uncertainty quantification are presented for time\n dependent transport problems where $c$, the scattering ratio, is uncertain. The\n method of polynomial chaos expansions is employed for quick and accurate\n calculation of the quantities of interest and uncollided solutions are used to\n treat part of the uncertainty calculation analytically. We find that\n approximately six moments in the polynomial expansion are required to represent\n the solutions to these problems accurately. Additionally, the results show that\n if the uncertainty interval spans c=1, which means it is uncertain whether the\n system is multiplying or not, the confidence interval will grow in time.\n Finally, since the QoI is a strictly increasing function, the percentile values\n are known and can be used to verify the accuracy of the expansion. These\n results can be used to test UQ methods for time-dependent transport problems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0312108",
        "title": "Truncated-Determinant Diagrammatic Monte Carlo for Fermions with Contact\n  Interaction",
        "abstract": "For some models of interacting fermions the known solution to the notorious\n sign-problem in Monte Carlo (MC) simulations is to work with macroscopic\n fermionic determinants; the price, however, is a macroscopic scaling of the\n numerical effort spent on elementary local updates. We find that the {\\it\n ratio} of two macroscopic determinants can be found with any desired accuracy\n by considering truncated (local in space and time) matices. In this respect, MC\n for interacting fermionic systems becomes similar to that for the\n sign-problem-free bosonic systems with system-size independent update cost. We\n demonstrate the utility of the truncated-determinant method by simulating the\n attractive Hubbard model within the MC scheme based on partially summed Feynman\n diagrams. We conjecture that similar approach may be useful in other\n implementations of the sign-free determinant schemes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2302.13930",
        "title": "High-kinetic inductance NbN films for high-quality compact\n  superconducting resonators",
        "abstract": "Niobium nitride (NbN) is a particularly promising material for quantum\n technology applications, as entails the degree of reproducibility necessary for\n large-scale of superconducting circuits. We demonstrate that resonators based\n on NbN thin films present a one-photon internal quality factor above 10$^5$\n maintaining a high impedance (larger than 2k$\\Omega$), with a footprint of\n approximately 50x100 $\\mu$m$^2$ and a self-Kerr nonlinearity of few tenths of\n Hz. These quality factors, mostly limited by losses induced by the coupling to\n two-level systems, have been maintained for kinetic inductances ranging from\n tenths to hundreds of pH/square. We also demonstrate minimal variations in the\n performance of the resonators during multiple cooldowns over more than nine\n months. Our work proves the versatility of niobium nitride high-kinetic\n inductance resonators, opening perspectives towards the fabrication of compact,\n high-impedance and high-quality multimode circuits, with sizable interactions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0909.3981",
        "title": "Measurement of CP violation observables and parameters for the decays\n  $B^{\\pm}\\to D K^{*\\pm}$",
        "abstract": "We study the decay $B^-\\to DK^{*-}$ using a sample of 379$\\times 10^6$\n $\\Upsilon(4S)\\to B\\bar{B}$ events collected with the BABAR detector at the\n PEP-II $B$-factory. We perform a \"GLW\" analysis where the $D$ meson decays into\n either a CP-even ($CP+$) eigenstate ($K^+K^-$, $\\pi^+\\pi^-$), CP-odd ($CP-$)\n eigenstate ($K^0_S\\pi^0$, $K^0_S\\phi$, $K^0_S\\omega$) or a non-CP state\n ($K^-\\pi^+$). We also analyze $D$ meson decays into $K^+\\pi^-$ from a\n Cabibbo-favored $\\bar{D}^0$ decay or doubly suppressed $D^0$ decay (\"ADS\"\n analysis). We measure observables that are sensitive to the CKM angle $\\gamma$:\n the partial-rate charge asymmetries ${\\cal A}_{CP\\pm}$, the ratios ${\\cal\n R}_{CP\\pm}$ of the B-decay branching fractions in $CP\\pm$ and non-CP decay, the\n ratio ${\\cal R}_{ADS}$ of the charge-averaged branching fractions, and the\n charge asymmetry ${\\cal A}_{ADS}$ of the ADS decays: ${\\cal A}_{CP+}=\n 0.09\\pm0.13 \\pm 0.06$, ${\\cal A}_{CP-}=-0.23\\pm 0.21\\pm 0.07$, ${\\cal R}_{CP+}\n =2.17 \\pm 0.35 \\pm 0.09$,${\\cal R}_{CP-} =1.03\\pm 0.27\\pm 0.13$, ${\\cal\n R}_{ADS}=0.066\\pm0.031\\pm0.010$, and ${\\cal A}_{ADS}=-0.34\\pm0.43\\pm 0.16$,\n where the first uncertainty is statistical and the second is systematic.\n Combining all the measurements and using a frequentist approach yields the\n magnitude of the ratio between the Cabibbo-suppressed and favored amplitudes,\n $r_B$ = 0.31 with a one (two) sigma confidence level interval of [0.24, 0.38]\n ([0.17, 0.43]). The value $r_B=0$ is excluded at the 3.3 sigma level. A similar\n analysis excludes values of $\\gamma$ in the intervals $[0, 7]^{\\circ}, [55,\n 111]^{\\circ}$, and $[175, 180]^{\\circ}$ ([85, 99]$^{\\circ}$) at the one (two)\n sigma confidence level.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0902.3419",
        "title": "Delange's Tauberian theorem and asymptotic normality of random ordered\n  factorizations of integers",
        "abstract": "By a suitable shifting-the-mean parametrization at the Dirichlet series level\n and Delange's Tauberian theorems, we show that the number of factors in random\n ordered factorizations of integers is asymptotically normally distributed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1505.06529",
        "title": "An efficient dynamic programming algorithm for the generalized LCS\n  problem with multiple substring inclusive constraints",
        "abstract": "In this paper, we consider a generalized longest common subsequence problem\n with multiple substring inclusive constraints. For the two input sequences $X$\n and $Y$ of lengths $n$ and $m$, and a set of $d$ constraints\n $P=\\{P_1,\\cdots,P_d\\}$ of total length $r$, the problem is to find a common\n subsequence $Z$ of $X$ and $Y$ including each of constraint string in $P$ as a\n substring and the length of $Z$ is maximized. A new dynamic programming\n solution to this problem is presented in this paper. The correctness of the new\n algorithm is proved. The time complexity of our algorithm is $O(d2^dnmr)$. In\n the case of the number of constraint strings is fixed, our new algorithm for\n the generalized longest common subsequence problem with multiple substring\n inclusive constraints requires $O(nmr)$ time and space.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math-ph/0103048",
        "title": "Asymptotic Completeness for Rayleigh Scattering",
        "abstract": "It is expected that the state of an atom or molecule, initially put into an\n excited state with an energy below the ionization threshold, relaxes to a\n groundstate by spontaneous emission of photons which propagate to spatial\n infinity. In this paper, this picture is established for a large class of\n models of non-relativistic atoms and molecules coupled to the quantized\n radiation field, but with the simplifying feature that an (arbitrarily tiny,\n but positive) infrared cutoff is imposed on the interaction Hamiltonian.\n  This result relies on a proof of asymptotic completeness for Rayleigh\n scattering of light on an atom. We establish asymptotic completeness of\n Rayleigh scattering for a class of model Hamiltonians with the features that\n the atomic Hamiltonian has point spectrum coexisting with absolutely continuous\n spectrum, and that either an infrared cutoff is imposed on the interaction\n Hamiltonian or photons are treated as massive particles.\n  We show that, for models of massless photons, the spectrum of the Hamiltonian\n strictly below the ionization threshold is purely continuous, except for the\n groundstate energy.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0012221",
        "title": "Investigating ULIRGs in the Near-Infrared: Imaging & Spectroscopy",
        "abstract": "We present imaging and spectroscopic observations of 2 nearby (z<0.1) ULIRGs\n from a larger sample, and address the question of whether the JHK continuum\n colours and slope might be effective probes of the nuclear region in searches\n for AGN. Certainly there is evidence for significant quantities of hot dust\n emission at temperatures >1000K; but it may be that rather than pointing to an\n AGN, this instead tells us more about the environment and evolution of the star\n formation.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2403.14014",
        "title": "Crowdsourcing Task Traces for Service Robotics",
        "abstract": "Demonstration is an effective end-user development paradigm for teaching\n robots how to perform new tasks. In this paper, we posit that demonstration is\n useful not only as a teaching tool, but also as a way to understand and assist\n end-user developers in thinking about a task at hand. As a first step toward\n gaining this understanding, we constructed a lightweight web interface to\n crowdsource step-by-step instructions of common household tasks, leveraging the\n imaginations and past experiences of potential end-user developers. As evidence\n of the utility of our interface, we deployed the interface on Amazon Mechanical\n Turk and collected 207 task traces that span 18 different task categories. We\n describe our vision for how these task traces can be operationalized as task\n models within end-user development tools and provide a roadmap for future work.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2112.02107",
        "title": "Time- and Space-Varying Neutrino Mass Matrix from Soft Topological\n  Defects",
        "abstract": "We study the formation and evolution of topological defects that arise in the\n post-recombination phase transition predicted by the gravitational neutrino\n mass model in [Dvali, Funcke, Phys. Rev. D 93, 113002 (2016)]. In the\n transition, global skyrmions, monopoles, strings, and domain walls form due to\n the spontaneous breaking of the neutrino flavor symmetry. These defects are\n unique in their softness and origin; as they appear at a very low energy scale,\n they only require Standard Model particle content, and they differ\n fundamentally depending on the Majorana or Dirac nature of the neutrinos. One\n of the observational signatures is the time dependence and space dependence of\n the neutrino mass matrix, which could be observable in future neutrino\n experiments. Already existing data rule out parts of the parameter space in the\n Majorana case. The detection of this effect could shed light onto the open\n question of the Dirac versus Majorana neutrino nature.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1507.01524",
        "title": "A Complete Generalized Adjustment Criterion",
        "abstract": "Covariate adjustment is a widely used approach to estimate total causal\n effects from observational data. Several graphical criteria have been developed\n in recent years to identify valid covariates for adjustment from graphical\n causal models. These criteria can handle multiple causes, latent confounding,\n or partial knowledge of the causal structure; however, their diversity is\n confusing and some of them are only sufficient, but not necessary. In this\n paper, we present a criterion that is necessary and sufficient for four\n different classes of graphical causal models: directed acyclic graphs (DAGs),\n maximum ancestral graphs (MAGs), completed partially directed acyclic graphs\n (CPDAGs), and partial ancestral graphs (PAGs). Our criterion subsumes the\n existing ones and in this way unifies adjustment set construction for a large\n set of graph classes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1801.08595",
        "title": "Lattice self-similar sets on the real line are not Minkowski measurable",
        "abstract": "We show that any nontrivial self-similar subset of the real line that is\n invariant under a lattice iterated function system (IFS) satisfying the open\n set condition (OSC) is not Minkowski measurable. So far, this was only known\n for special classes of such sets. Thereby, we provide the last puzzle-piece in\n proving that under OSC a nontrivial self-similar subset of the real line is\n Minkowski measurable iff it is invariant under a nonlattice IFS, a 25-year-old\n conjecture.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1610.06132",
        "title": "Weak solutions of the Shigesaka-Kawasaki-Teramoto equations and their\n  attractors",
        "abstract": "We derive the global existence of weak solutions of the\n Shigesada-Kawasaki-Teramoto systems in space dimension less or equal to 4 with\n a rather general condition on the coefficients. The existence is established\n using finite differences in time with truncations and an argument of\n Stampachia's maximum principle to show the positivity of the solutions. We\n derive also the existence of a weak global attractor.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1412.4895",
        "title": "Reexamination of the role of the $\\Delta ^{\\ast}$ resonances in the\n  $pp\\rightarrow nK^{+}\\Sigma ^{+}$ reaction",
        "abstract": "In this work, the role of the $\\Delta ^{\\ast }$ resonances in the process of\n $pp\\rightarrow nK^{+}\\Sigma ^{+}$ are systematically investigated with the\n effective Lagrangian approach and the isobar model. We find that a $P_{31}$\n state, either $\\Delta ^{\\ast }(1750)$ or $\\Delta ^{\\ast }(1910)$, is favored by\n the data while the $P_{33}$ state, namely $\\Delta ^{\\ast }(1920)$, has small\n contribution. Besides, either sub-threshold $S_{31}$ $\\Delta ^{\\ast }(1620)$\n resonance or strong $n\\Sigma $ final state interaction or both have possible\n contribution at near threshold region, depending on the measured cross\n sections. We demonstrate the invariant mass distributions and the Dalitz Plots\n in order to investigate whether it is possible to distinguish the controversial\n $K\\Sigma $ production mechanism in these observables.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1905.00071",
        "title": "Harmonic cubic homogeneous polynomials such that the norm-squared of the\n  Hessian is a multiple of the Euclidean quadratic form",
        "abstract": "There is considered the problem of describing up to linear conformal\n equivalence those harmonic cubic homogeneous polynomials for which the\n squared-norm of the Hessian is a nonzero multiple of the quadratic form\n defining the Euclidean metric. Solutions are constructed in all dimensions and\n solutions are classified in dimension at most $4$. Techniques are given for\n determining when two solutions are linearly conformally inequivalent.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0805.3965",
        "title": "Ion beam induced enhanced diffusion from gold thin films in silicon",
        "abstract": "Enhanced diffusion of gold atoms into silicon substrate has been studied in\n Au thin films of various thicknesses (2.0, 5.3, 10.9 and 27.5 nm) deposited on\n Si(111) and followed by irradiation with 1.5 MeV Au2+ at a flux of 6.3x10^12\n ions cm-2 s-1 and fluence up to 1x10^15 ions cm-2. The high resolution\n transmission electron microscopy measurements showed the presence of gold\n silicide formation for the above-mentioned systems at fluence greater than\n equal to 1x1014 ions cm-2. The maximum depth to which the gold atoms have been\n diffused at a fluence of 1x10^14 ions cm-2 for the cases of 2.0, 5.3, 10.9 and\n 27.5 nm thick films has been found to be 60, 95, 160 and 13 nm respectively.\n Interestingly, at higher fluence of 1x1015 ions cm-2 in case of 27.5 nm thick\n film, gold atoms from the film transported to a maximum depth of 265 nm in the\n substrate. The substrate silicon is found to be amorphous at the above fluence\n values where unusually large mass transport occurred. Enhanced diffusion has\n been explained on the basis of ion beam induced, flux dependent amorphous\n nature of the substrate, and transient beam induced temperature effects. This\n work confirms the absence of confinement effects that arise from spatially\n confined structures and existence of thermal and chemical reactions during ion\n irradiation.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/9812313",
        "title": "Cosmological Tracking Solutions",
        "abstract": "A substantial fraction of the energy density of the universe may consist of\n quintessence in the form of a slowly-rolling scalar field. Since the energy\n density of the scalar field generally decreases more slowly than the matter\n energy density, it appears that the ratio of the two densities must be set to a\n special, infinitesimal value in the early universe in order to have the two\n densities nearly coincide today.\n  Recently, we introduced the notion of tracker fields to avoid this initial\n conditions problem. In the paper, we address the following questions: What is\n the general condition to have tracker fields? What is the relation between the\n matter energy density and the equation-of-state of the universe imposed by\n tracker solutions? And, can tracker solutions explain why quintessence is\n becoming important today rather than during the early universe?",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2112.06189",
        "title": "An original model for multi-target learning of logical rules for\n  knowledge graph reasoning",
        "abstract": "Large-scale knowledge graphs provide structured representations of human\n knowledge. However, as it is impossible to collect all knowledge, knowledge\n graphs are usually incomplete. Reasoning based on existing facts paves a way to\n discover missing facts. In this paper, we study the problem of learning logical\n rules for reasoning on knowledge graphs for completing missing factual\n triplets. Learning logical rules equips a model with strong interpretability as\n well as the ability to generalize to similar tasks. We propose a model able to\n fully use training data which also considers multi-target scenarios. In\n addition, considering the deficiency in evaluating the performance of models\n and the quality of mined rules, we further propose two novel indicators to help\n with the problem. Experimental results empirically demonstrate that our model\n outperforms state-of-the-art methods on five benchmark datasets. The results\n also prove the effectiveness of the indicators.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1509.04456",
        "title": "A note on summability of multilinear forms on classical sequence spaces",
        "abstract": "We obtain some optimal estimates for multilinear forms on $\\ell _{p}$ spaces.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2305.16385",
        "title": "Magic Angle Butterfly in Twisted Trilayer Graphene",
        "abstract": "We consider a configuration of three stacked graphene monolayers with\n commensurate twist angles $\\theta_{12}/\\theta_{23}=p/q$, where $p$ and $q$ are\n coprime integers with $0<p<|q|$ and $q$ can be positive or negative. We study\n this system using the continuum model in the chiral limit when interlayer\n coupling terms between $\\textrm{AA}_{12}$ and $\\textrm{AA}_{23}$ sites of the\n moir\\'{e} patterns $12$ and $23$ are neglected. There are only three\n inequivalent displacements between the moir\\'{e} patterns $12$ and $23$, at\n which the three monolayers' Dirac zero modes are protected. Remarkably, for\n these displacements and an arbitrary $p/q$ we discover exactly flat bands at an\n infinite set of twist angles (magic angles). We provide theoretical explanation\n and classification of all possible configurations and topologies of the flat\n bands.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1805.06646",
        "title": "Implementation of Memristor in Bessel filter with RLC components",
        "abstract": "The Bessel filters are optimized to collect competent transient response due\n to a linear phase in the passband. In other words, during the filtering\n process, there will be comparatively impoverished frequency response with lower\n amplitude inequity. Memristor is asserted as a passive, two-terminal essential\n component of the circuit and the use of such element in schemes as an\n adjustable resistance allows the realization of the memory resistor based\n analog circuits, which achieve the wide range of specific parameters. The\n application of RLC circuit for Bessel filter prototype is theoretically\n expected to behave in a positive way, however, the further simulations with\n software and analysis of the results will reveal the nature of the effect",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2101.12325",
        "title": "Hyperpolarisation of external nuclear spins using nitrogen-vacancy\n  centre ensembles",
        "abstract": "The nitrogen-vacancy (NV) centre in diamond has emerged as a candidate to\n non-invasively hyperpolarise nuclear spins in molecular systems to improve the\n sensitivity of nuclear magnetic resonance (NMR) experiments. Several promising\n proof of principle experiments have demonstrated small-scale polarisation\n transfer from single NVs to hydrogen spins outside the diamond. However, the\n scaling up of these results to the use of a dense NV ensemble, which is a\n necessary prerequisite for achieving realistic NMR sensitivity enhancement, has\n not yet been demonstrated. In this work, we present evidence for a polarising\n interaction between a shallow NV ensemble and external nuclear targets over a\n micrometre scale, and characterise the challenges in achieving useful\n polarisation enhancement. In the most favourable example of the interaction\n with hydrogen in a solid state target, a maximum polarisation transfer rate of\n $\\approx 7500$ spins per second per NV is measured, averaged over an area\n containing order $10^6$ NVs. Reduced levels of polarisation efficiency are\n found for liquid state targets, where molecular diffusion limits the transfer.\n Through analysis via a theoretical model, we find that our results suggest\n implementation of this technique for NMR sensitivity enhancement is feasible\n following realistic diamond material improvements.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2210.13537",
        "title": "Private Online Prediction from Experts: Separations and Faster Rates",
        "abstract": "Online prediction from experts is a fundamental problem in machine learning\n and several works have studied this problem under privacy constraints. We\n propose and analyze new algorithms for this problem that improve over the\n regret bounds of the best existing algorithms for non-adaptive adversaries. For\n approximate differential privacy, our algorithms achieve regret bounds of\n $\\tilde{O}(\\sqrt{T \\log d} + \\log d/\\varepsilon)$ for the stochastic setting\n and $\\tilde{O}(\\sqrt{T \\log d} + T^{1/3} \\log d/\\varepsilon)$ for oblivious\n adversaries (where $d$ is the number of experts). For pure DP, our algorithms\n are the first to obtain sub-linear regret for oblivious adversaries in the\n high-dimensional regime $d \\ge T$. Moreover, we prove new lower bounds for\n adaptive adversaries. Our results imply that unlike the non-private setting,\n there is a strong separation between the optimal regret for adaptive and\n non-adaptive adversaries for this problem. Our lower bounds also show a\n separation between pure and approximate differential privacy for adaptive\n adversaries where the latter is necessary to achieve the non-private\n $O(\\sqrt{T})$ regret.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1702.01501",
        "title": "Wide-Field $^{12}$CO ($J=2-1$) and $^{13}$CO ($J=2-1$) Observations\n  toward the Aquila Rift and Serpens Molecular Cloud Complexes. I. Molecular\n  Clouds and Their Physical Properties",
        "abstract": "We present results of wide-field $^{12}$CO ($J = 2 - 1$) and $^{13}$CO ($J =\n 2 - 1$) observations toward the Aquila Rift and Serpens molecular cloud\n complexes (25$^\\circ < l < 33^\\circ$ and $1^\\circ < b < 6^\\circ$) at an angular\n resolution of 3$'$.4 ($\\approx$ 0.25 pc) and at a velocity resolution of 0.079\n km s$^{-1}$ with the velocity coverage of $-5$ km s$^{-1} < V_{\\rm LSR} <$ 35\n km s$^{-1}$. We found that the $^{13}$CO emission better traces the structures\n seen in the extinction map and derived the $X_{\\rm ^{13}CO}$-factor of this\n region. Applying \\texttt{SCIMES} to the $^{13}$CO data cube, we identified 61\n clouds and derived their masses, radii, and line widths. The line-width-radius\n relation of the identified clouds basically follows those of nearby molecular\n clouds. Majority of the identified clouds are close to virial equilibrium\n although the dispersion is large. By inspecting the $^{12}$CO channel maps by\n eye, we found several arcs which are spatially extended to 0.2 $-$ 3 degree in\n length. In the longitude-velocity diagrams of $^{12}$CO, we also found the two\n spatially-extended components which appear to converge toward Serpens South and\n W40 region. The existence of two components with different velocities and arcs\n suggests that large-scale expanding bubbles and/or flows play a role in the\n formation and evolution of the Serpens South and W40 cloud.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1306.6506",
        "title": "The Galactic NH - AV Relation and its Application to Historical Galactic\n  SNRs",
        "abstract": "We refine a classic relation between the hydrogen column density (NH) and\n optical extinction (Av) by employing 39 Galactic Supernova Remnants (SNRs) with\n X-rays, optical and/or infra-red data available. We find NH =\n (1.69+/-0.07)*10^21 Av cm^(-2) mag^(-1) . Applying this relation to three\n Galactic SNRs with good historical records allows us to further constrain\n either their progenitor's distances or magnitudes, which is independent access\n to their distances.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/9901352",
        "title": "The Entropy Production Fluctuation Theorem and the Nonequilibrium Work\n  Relation for Free Energy Differences",
        "abstract": "There are only a very few known relations in statistical dynamics that are\n valid for systems driven arbitrarily far-from-equilibrium. One of these is the\n fluctuation theorem, which places conditions on the entropy production\n probability distribution of nonequilibrium systems. Another recently discovered\n far-from-equilibrium expression relates nonequilibrium measurements of the work\n done on a system to equilibrium free energy differences. In this paper, we\n derive a generalized version of the fluctuation theorem for stochastic,\n microscopically reversible dynamics. Invoking this generalized theorem provides\n a succinct proof of the nonequilibrium work relation.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2104.02098",
        "title": "Comprehensive Gas Characterization of a $z= 2.5$ Protocluster: A Cluster\n  Core Caught in the Beginning of Virialization?",
        "abstract": "In order to connect galaxy clusters to their progenitor protoclusters, we\n must constrain the star formation histories within their member galaxies and\n the timescale of virial collapse. In this paper we characterize the complex\n star-forming properties of a $z=2.5$ protocluster in the COSMOS field using\n ALMA dust continuum and new VLA CO(1-0) observations of two filaments\n associated with the structure, sometimes referred to as the \"Hyperion\"\n protocluster. We focus in particular on the protocluster \"core\" which has\n previously been suggested as the highest redshift bona fide galaxy cluster\n traced by extended X-ray emission in a stacked Chandra/XMM image. We re-analyze\n this data and refute these claims, finding that at least 40 $\\pm$ 17% of\n extended X-ray sources of similar luminosity and size at this redshift arise\n instead from Inverse Compton scattering off recently extinguished radio\n galaxies rather than intracluster medium. Using ancillary COSMOS data, we also\n constrain the SEDs of the two filaments' eight constituent galaxies from the\n rest-frame UV to radio. We do not find evidence for enhanced star formation\n efficiency in the core and conclude that the constituent galaxies are already\n massive (M$_{\\star} \\approx 10^{11} M_{\\odot}$), with molecular gas reservoirs\n $>10^{10} M_{\\odot}$ that will be depleted within 200-400 Myr. Finally, we\n calculate the halo mass of the nested core at $z=2.5$ and conclude that it will\n collapse into a cluster of 2-9 $\\times 10^{14} M_{\\odot}$, comparable to the\n size of the Coma cluster at $z=0$ and accounting for at least 50% of the total\n estimated halo mass of the extended \"Hyperion\" structure.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/nucl-ex/0105014",
        "title": "Neutral Pion Distributions in PHENIX at RHIC",
        "abstract": "Transverse momentum spectra for identified $\\pi^0$'s in the range 1 GeV/c $<\n p_T <$ 4 GeV/c have been measured by the PHENIX experiment in Au-Au collisions\n at $\\sqrt{s}=130$ GeV. The spectra from peripheral nuclear collisions are\n consistent with the simple expectation of scaling the spectra from p+p\n collisions by the average number of nucleon-nucleon binary collisions. The\n spectra from central collisions and the ratio of central/peripheral spectra are\n significantly suppressed when compared to point-like scaling.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1812.08061",
        "title": "Spontaneous center formation in Dictyostelium discoideum",
        "abstract": "Dictyostelium discoideum (D.d.) is a widely studied amoeba due to its\n capabilities of development, survival, and self-organization. During\n aggregation it produces and relays a chemical signal (cAMP) which shows spirals\n and target centers. Nevertheless, the natural emergence of these structures is\n still not well understood. We present a mechanism for creation of centers and\n target waves of cAMP in D.d. by adding cell inhomogeneity to a well known\n reaction-diffusion model of cAMP waves and we characterize its properties. We\n show how stable activity centers appear spontaneously in areas of higher cell\n density with the oscillation frequency of these centers depending on their\n density. The cAMP waves have the characteristic dispersion relation of trigger\n waves and a velocity which increases with cell density. Chemotactically\n competent cells react to these waves and create aggregation streams even with\n very simple movement rules. Finally we argue in favor of the existence of\n bounded phosphodiesterase to maintain the wave properties once small cell\n clusters appear.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0705.1262",
        "title": "Extremal first Dirichlet eigenvalue of doubly connected plane domains\n  and dihedral symmetry",
        "abstract": "We deal with the following eigenvalue optimization problem: Given a bounded\n domain $D\\subset \\R^2$, how to place an obstacle $B$ of fixed shape within $D$\n so as to maximize or minimize the fundamental eigenvalue $\\lambda_1$ of the\n Dirichlet Laplacian on $D\\setminus B$. This means that we want to extremize the\n function $\\rho\\mapsto \\lambda_1(D\\setminus \\rho (B))$, where $\\rho$ runs over\n the set of rigid motions such that $\\rho (B)\\subset D$. We answer this problem\n in the case where both $D$ and $B$ are invariant under the action of a dihedral\n group $\\mathbb{D}_n$, $n\\ge2$, and where the distance from the origin to the\n boundary is monotonous as a function of the argument between two axes of\n symmetry. The extremal configurations correspond to the cases where the axes of\n symmetry of $B$ coincide with those of $D$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1003.4757",
        "title": "Luminosity-variation independent location of the circum-nuclear, hot\n  dust in NGC 4151",
        "abstract": "After recent sensitivity upgrades at the Keck Interferometer (KI), systematic\n interferometric 2um studies of the innermost dust in nearby Seyfert nuclei are\n within observational reach. Here, we present the analysis of new\n interferometric data of NGC 4151, discussed in context of the results from\n recent dust reverberation, spectro-photometric and interferometric campaigns.\n The complete data set gives a complex picture, in particular the measured\n visibilities from now three different nights appear to be rather insensitive to\n the variation of the nuclear luminosity. KI data alone indicate two scenarios:\n the K-band emission is either dominated to ~90% by size scales smaller than\n 30mpc, which falls short of any dust reverberation measurement in NGC 4151 and\n of theoretical models of circum-nuclear dust distributions. Or contrary, and\n more likely, the K-band continuum emission is dominated by hot dust (>= 1300K)\n at linear scales of about 50mpc. The linear size estimate varies by a few tens\n of percent depending on the exact morphology observed. Our interferometric,\n deprojected centro-nuclear dust radius estimate of 55+-5mpc is roughly\n consistent with the earlier published expectations from circum-nuclear, dusty\n radiative transfer models, and spectro-photometric modeling. However, our data\n do not support the notion that the dust emission size scale follows the nuclear\n variability of NGC 4151 as a R_dust \\propto L_nuc^0.5 scaling relation. Instead\n variable nuclear activity, lagging, and variable dust response to illumination\n changes need to be combined to explain the observations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/quant-ph/0210183",
        "title": "Fixed point theorem for simple quantum strategies in quantum market\n  games",
        "abstract": "A simple but nontrivial class of the quantum strategies in buying-selling\n games is presented. The player moves are a rational buying and an unconditional\n selling. The possibility of gaining extremal profits in such the games is\n considered. The entangled merchants hypothesis is proposed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1805.10856",
        "title": "r-Instance Learning for Missing People Tweets Identification",
        "abstract": "The number of missing people (i.e., people who get lost) greatly increases in\n recent years. It is a serious worldwide problem, and finding the missing people\n consumes a large amount of social resources. In tracking and finding these\n missing people, timely data gathering and analysis actually play an important\n role. With the development of social media, information about missing people\n can get propagated through the web very quickly, which provides a promising way\n to solve the problem. The information in online social media is usually of\n heterogeneous categories, involving both complex social interactions and\n textual data of diverse structures. Effective fusion of these different types\n of information for addressing the missing people identification problem can be\n a great challenge. Motivated by the multi-instance learning problem and\n existing social science theory of \"homophily\", in this paper, we propose a\n novel r-instance (RI) learning model.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1802.01607",
        "title": "$hh+\\text{jet}$ production at 100 TeV",
        "abstract": "Higgs pair production is a crucial phenomenological process in deciphering\n the nature of the TeV scale and the mechanism underlying electroweak symmetry\n breaking. At the Large Hadron Collider, this process is statistically limited.\n Pushing the energy frontier beyond the LHC's reach will create new\n opportunities to exploit the rich phenomenology at higher centre-of-mass\n energies and luminosities. In this work, we perform a comparative analysis of\n the $hh+\\text{jet}$ channel at a future 100 TeV hadron collider. We focus on\n the $hh\\to b\\bar b b\\bar b$ and $hh \\to b\\bar b \\tau^+\\tau^-$ channels and\n employ a range of analysis techniques to estimate the sensitivity potential\n that can be gained by including this jet-associated Higgs pair production to\n the list of sensitive collider processes in such an environment. In particular,\n we observe that $hh \\to b\\bar b \\tau^+\\tau^-$ in the boosted regime exhibits a\n large sensitivity to the Higgs boson self-coupling and the Higgs self-coupling\n could be constrained at the 8\\% level in this channel alone.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0010177",
        "title": "A Classification and Examples of Four-Dimensional Nonisoclinic\n  Three-Webs",
        "abstract": "A classification and examples of four-dimensional isoclinic three-webs of\n codimension two are given. The examples considered prove the existence theorem\n for many classes of webs for which the general existence theorems are not\n proved yet.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0905.3556",
        "title": "Kinematic analysis of nuclear spirals: feeding the black hole in NGC1097",
        "abstract": "We present a harmonic expansion of the observed line-of-sight velocity field\n as a method to recover and investigate spiral structures in the nuclear regions\n of galaxies. We apply it to the emission-line velocity field within the\n circumnuclear starforming ring of NGC1097, obtained with the GMOS-IFU\n spectrograph. The radial variation of the third harmonic terms are well\n described by a logarithmic spiral, from which we interpret that the\n gravitational potential is weakly perturbed by a two-arm spiral density wave\n with inferred pitch angle of of 52+/-4 degrees. This interpretation predicts a\n two-arm spiral distortion in the surface brightness, as hinted by the dust\n structures in central images of NGC1097, and predicts a combined one-arm and\n three-arm spiral structure in the velocity field, as revealed in the\n non-circular motions of the ionised gas within the circumnuclear region of this\n galaxy. Next, we use a simple spiral perturbation model to constrain the\n fraction of the measured non-circular motions that is due to radial inflow. We\n combine the resulting inflow velocity with the gas density in the spiral arms,\n inferred from emission line ratios, to estimate the mass inflow rate as a\n function of radius, which reaches about 0.011 Msun/yr at a distance of 70 pc\n from the center. This value corresponds to a fraction of about 4.2 x 10^{-3} of\n the Eddington mass accretion rate onto the central black hole in this\n LINER/Seyfert1 galaxy. We conclude that the line-of-sight velocity not only can\n provide a cleaner view of nuclear spirals than the associated dust, but that\n the presented method also allows the quantitative study of these possibly\n important links in fueling the centers of galaxies, including providing a\n handle on the mass inflow rate as a function of radius.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.02092",
        "title": "Where are We in Event-centric Emotion Analysis? Bridging Emotion Role\n  Labeling and Appraisal-based Approaches",
        "abstract": "The term emotion analysis in text subsumes various natural language\n processing tasks which have in common the goal to enable computers to\n understand emotions. Most popular is emotion classification in which one or\n multiple emotions are assigned to a predefined textual unit. While such setting\n is appropriate for identifying the reader's or author's emotion, emotion role\n labeling adds the perspective of mentioned entities and extracts text spans\n that correspond to the emotion cause. The underlying emotion theories agree on\n one important point; that an emotion is caused by some internal or external\n event and comprises several subcomponents, including the subjective feeling and\n a cognitive evaluation. We therefore argue that emotions and events are related\n in two ways. (1) Emotions are events; and this perspective is the fundament in\n natural language processing for emotion role labeling. (2) Emotions are caused\n by events; a perspective that is made explicit with research how to incorporate\n psychological appraisal theories in NLP models to interpret events. These two\n research directions, role labeling and (event-focused) emotion classification,\n have by and large been tackled separately. In this paper, we contextualize both\n perspectives and discuss open research questions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1811.10738",
        "title": "Eco-friendly Power Cost Minimization for Geo-distributed Data Centers\n  Considering Workload Scheduling",
        "abstract": "The rapid development of renewable energy in the energy Internet is expected\n to alleviate the increasingly severe power problem in data centers, such as the\n huge power costs and pollution. This paper focuses on the eco-friendly power\n cost minimization for geo-distributed data centers supplied by multi-source\n power, where the geographical scheduling of workload and temporal scheduling of\n batteries' charging and discharging are both considered. Especially, we\n innovatively propose the Pollution Index Function to model the pollution of\n different kinds of power, which can encourage the use of cleaner power and\n improve power savings. We first formulate the eco-friendly power cost\n minimization problem as a multi-objective and mixed-integer programming\n problem, and then simplify it as a single-objective problem with integer\n constraints. Secondly, we propose a Sequential Convex Programming (SCP)\n algorithm to find the globally optimal non-integer solution of the simplified\n problem, which is non-convex, and then propose a low-complexity searching\n method to seek for the quasi-optimal mixed-integer solution of it. Finally,\n simulation results reveal that our method can improve the clean energy usage up\n to 50\\%--60\\% and achieve power cost savings up to 10\\%--30\\%, as well as\n reduce the delay of requests.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2403.07692",
        "title": "Masked AutoDecoder is Effective Multi-Task Vision Generalist",
        "abstract": "Inspired by the success of general-purpose models in NLP, recent studies\n attempt to unify different vision tasks in the same sequence format and employ\n autoregressive Transformers for sequence prediction. They apply uni-directional\n attention to capture sequential dependencies and generate task sequences\n recursively. However, such autoregressive Transformers may not fit vision tasks\n well, as vision task sequences usually lack the sequential dependencies\n typically observed in natural languages. In this work, we design Masked\n AutoDecoder~(MAD), an effective multi-task vision generalist. MAD consists of\n two core designs. First, we develop a parallel decoding framework that\n introduces bi-directional attention to capture contextual dependencies\n comprehensively and decode vision task sequences in parallel. Second, we design\n a masked sequence modeling approach that learns rich task contexts by masking\n and reconstructing task sequences. In this way, MAD handles all the tasks by a\n single network branch and a simple cross-entropy loss with minimal\n task-specific designs. Extensive experiments demonstrate the great potential of\n MAD as a new paradigm for unifying various vision tasks. MAD achieves superior\n performance and inference efficiency compared to autoregressive counterparts\n while obtaining competitive accuracy with task-specific models. Code will be\n released.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1411.3395",
        "title": "On the decomposition of a 2D-complex germ with non-isolated\n  singularities",
        "abstract": "The decomposition of a two dimensional complex germ with non-isolated\n singularity into semi-algebraic sets is given. This decomposition consists of\n four classes: Riemannian cones defined over a Seifert fibered manifold, a\n topological cone over thickened tori endowed with Cheeger-Nagase metric, a\n topological cone over mapping torus endowed with Hsiang-Pati metric and a\n topological cone over the tubular neighbourhoods of the link's singularities.\n In this decomposition there exist semi-algebraic sets that are metrically\n conical over the manifolds constituting the link. The germ is reconstituted up\n to bi-Lipschitz equivalence to a model describing its geometric behavior.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0104225",
        "title": "On certain higher dimensional analogues of vertex algebras",
        "abstract": "A higher dimensional analogue of the notion of vertex algebra is formulated\n in terms of formal variable language with Borcherds' notion of $G$-vertex\n algebra as a motivation. Some examples are given and certain analogous duality\n properties are proved. Furthermore, it is proved that for any vector space $W$,\n any set of mutually local multi-variable vertex operators on $W$ in a certain\n canonical way generates a vertex algebra with $W$ as a natural module.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2211.15606",
        "title": "Mixed-dimensional moir\\'e systems of graphitic thin films with a twisted\n  interface",
        "abstract": "Moir\\'e patterns formed by stacking atomically-thin van der Waals crystals\n with a relative twist angle can give rise to dramatic new physical properties.\n The study of moir\\'e materials has so far been limited to structures comprising\n no more than a few vdW sheets, since a moir\\'e pattern localized to a single\n two-dimensional interface is generally assumed to be incapable of appreciably\n modifying the properties of a bulk three-dimensional crystal. Layered\n semimetals such as graphite offer a unique platform to challenge this paradigm,\n owing to distinctive properties arising from their nearly-compensated electron\n and hole bulk doping. Here, we perform transport measurements of dual-gated\n devices constructed by slightly rotating a monolayer graphene sheet atop a thin\n bulk graphite crystal. We find that the moir\\'e potential transforms the\n electronic properties of the entire bulk graphitic thin film. At zero and small\n magnetic fields, transport is mediated by a combination of gate-tunable moir\\'e\n and graphite surface states, as well as coexisting semimetallic bulk states\n that do not respond to gating. At high field, the moir\\'e potential hybridizes\n with the graphitic bulk states owing to the unique properties of the two lowest\n Landau bands of graphite. These Landau bands facilitate the formation of a\n single quasi-two-dimensional hybrid structure in which the moir\\'e and bulk\n graphite states are inextricably mixed. Our results establish twisted\n graphene-graphite as the first in a new class of mixed-dimensional moir\\'e\n materials.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1904.13060",
        "title": "On the change of old neutron star masses with galactocentric distance",
        "abstract": "We show that the pulsar mass depends on the environment, and that it\n decreases going towards the center of the Milky Way. This is due to two\n combined effects, the capture and accumulation of self-interacting,\n non-annihilating dark matter by pulsars, and the increase of the dark matter\n density going towards the galactic center. We show that mass decrease depends\n both on the density profile of dark matter, steeper profiles producing a faster\n and larger decrease of the pulsar mass, and on the strength of\n self-interaction. Once future observations will provide the pulsar mass in a\n dark matter rich environment, close to the galactic center, the present result\n will be able to put constraints on the characteristics of our Galaxy halo dark\n matter profile, on the nature of dark matter, namely on its annihilating or\n non-annihilating nature, on its strength of self-interaction, and on the\n particle mass.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1907.05505",
        "title": "Artificial Intelligence as a Services (AI-aaS) on Software-Defined\n  Infrastructure",
        "abstract": "This paper investigates a paradigm for offering artificial intelligence as a\n service (AI-aaS) on software-defined infrastructures (SDIs). The increasing\n complexity of networking and computing infrastructures is already driving the\n introduction of automation in networking and cloud computing management\n systems. Here we consider how these automation mechanisms can be leveraged to\n offer AI-aaS. Use cases for AI-aaS are easily found in addressing smart\n applications in sectors such as transportation, manufacturing, energy, water,\n air quality, and emissions. We propose an architectural scheme based on SDIs\n where each AI-aaS application is comprised of a monitoring, analysis, policy,\n execution plus knowledge (MAPE-K) loop (MKL). Each application is composed as\n one or more specific service chains embedded in SDI, some of which will include\n a Machine Learning (ML) pipeline. Our model includes a new training plane and\n an AI-aaS plane to deal with the model-development and operational phases of AI\n applications. We also consider the role of an ML/MKL sandbox in ensuring\n coherency and consistency in the operation of multiple parallel MKL loops. We\n present experimental measurement results for three AI-aaS applications deployed\n on the SAVI testbed: 1. Compressing monitored data in SDI using autoencoders;\n 2. Traffic monitoring to allocate CPUs resources to VNFs; and 3. Highway\n segment classification in smart transportation.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1609.02315",
        "title": "Index of the critical catenoid",
        "abstract": "In this article, we show that the critical catenoid, as a free boundary\n minimal surface of the unit ball in $\\mathbb{R}^3$, has index $4$. We also\n prove that a free boundary minimal surface of the unit ball in $\\mathbb{R}^3$,\n that is not a flat disk, has index at least $4$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1308.5187",
        "title": "A criterion for asymptotic completeness in local relativistic QFT",
        "abstract": "We formulate a generalized concept of asymptotic completeness and show that\n it holds in any Haag-Kastler quantum field theory with an upper and lower mass\n gap. It remains valid in the presence of pairs of oppositely charged particles\n in the vacuum sector, which invalidate the conventional property of asymptotic\n completeness. Our result can be restated as a criterion characterizing a class\n of theories with complete particle interpretation in the conventional sense.\n This criterion is formulated in terms of certain asymptotic observables\n (Araki-Haag detectors) whose existence, as strong limits of their approximating\n sequences, is our main technical result. It is proven with the help of a novel\n propagation estimate, which is also relevant to scattering theory of quantum\n mechanical dispersive systems.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1805.01281",
        "title": "Multi-band high resolution spectroscopy rules out the hot Jupiter BD+20\n  1790b - First data from the GIARPS Commissioning",
        "abstract": "Context. Stellar activity is currently challenging the detection of young\n planets via the radial velocity (RV) technique. Aims. We attempt to\n definitively discriminate the nature of the RV variations for the young active\n K5 star BD+20 1790, for which visible (VIS) RV measurements show divergent\n results on the existence of a substellar companion. Methods. We compare VIS\n data with high precision RVs in the near infrared (NIR) range by using the\n GIANO - B and IGRINS spectrographs. In addition, we present for the first time\n simultaneous VIS-NIR observations obtained with GIARPS (GIANO - B and HARPS -\n N) at Telescopio Nazionale Galileo (TNG). Orbital RVs are achromatic, so the RV\n amplitude does not change at different wavelengths, while stellar activity\n induces wavelength-dependent RV variations, which are significantly reduced in\n the NIR range with respect to the VIS. Results. The NIR radial velocity\n measurements from GIANO - B and IGRINS show an average amplitude of about one\n quarter with respect to previously published VIS data, as expected when the RV\n jitter is due to stellar activity. Coeval multi-band photometry surprisingly\n shows larger amplitudes in the NIR range, explainable with a mixture of cool\n and hot spots in the same active region. Conclusions. In this work, the claimed\n massive planet around BD+20 1790 is ruled out by our data. We exploited the\n crucial role of multi- wavelength spectroscopy when observing young active\n stars: thanks to facilities like GIARPS that provide simultaneous observations,\n this method can reach its maximum potential.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0809.1809",
        "title": "Subdiffusive model of substance releasing from a thick membrane",
        "abstract": "We study both theoretically and experimentally the process of subdiffusive\n substance releasing from a thick membrane. The theoretical model uses the\n subdiffusion equation with fractional time derivative and specific boundary\n conditions at the membrane surfaces. Using a special \\textit{ansatz} we find\n analytical formulas describing the time evolution of concentration profiles and\n an amount of the substance remains in the membrane. Fitting the theoretical\n functions to the experimental results, we estimate the subdiffusion coefficient\n of polyethylene glycol 2000 in agarose hydrogel.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2308.01651",
        "title": "lifex-ep: a robust and efficient software for cardiac electrophysiology\n  simulations",
        "abstract": "Simulating the cardiac function requires the numerical solution of\n multi-physics and multi-scale mathematical models. This underscores the need\n for streamlined, accurate, and high-performance computational tools. Despite\n the dedicated endeavors of various research teams, comprehensive and\n user-friendly software programs for cardiac simulations are still in the\n process of achieving full maturity within the scientific community. This work\n introduces lifex-ep, a publicly available software for numerical simulations of\n the electrophysiology activity of the cardiac muscle, under both physiological\n and pathological conditions. lifex-ep employs the monodomain equation to model\n the heart's electrical activity. It incorporates both phenomenological and\n second-generation ionic models. These models are discretized using the Finite\n Element method on tetrahedral or hexahedral meshes. Additionally, lifex-ep\n integrates the generation of myocardial fibers based on Laplace-Dirichlet\n Rule-Based Methods, previously released in Africa et al., 2023, within\n lifex-fiber. This paper provides a concise overview of the mathematical models\n and numerical methods underlying lifex-ep, along with comprehensive\n implementation details and instructions for users. lifex-ep features\n exceptional parallel speedup, scaling efficiently when using up to thousands of\n cores, and its implementation has been verified against an established\n benchmark problem for computational electrophysiology. We showcase the key\n features of lifex-ep through various idealized and realistic simulations.\n lifex-ep offers a user-friendly and flexible interface. lifex-ep provides easy\n access to cardiac electrophysiology simulations for a wide user community. It\n offers a computational tool that integrates models and accurate methods for\n simulating cardiac electrophysiology within a high-performance framework, while\n maintaining a user-friendly interface.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0901.2694",
        "title": "Particle-unparticle duality in super-relativity",
        "abstract": "New method of shaping quantum \"particle - unparticle\" vacuum excitations has\n been proposed in the framework of unification of relativity and quantum theory.\n Such unification is based solely on the notion of generalized coherent state\n (GCS) of N-level system and the geometry of unitary group SU(N) acting in state\n space $C^N$. Initially, neither contradictable notion of quantum particle, nor\n space-time coordinates (that cannot be a priori attached to nothing) are used\n in this construction. Quantum measurement of local dynamical variables (LDV)\n leads to the emergence of 4D dynamical space-time (DST). Morphogenesis of the\n \"field shell\" of GCS and its dynamics have been studied for N=2 in DST.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0908.0091",
        "title": "Coleman Maps for Modular Forms at Supersingular Primes over Lubin-Tate\n  Extensions",
        "abstract": "Given an elliptic curve with supersingular reduction at an odd prime p,\n Iovita and Pollack have generalised results of Kobayashi to define even and odd\n Coleman maps at p over Lubin-Tate extensions given by a formal group of height\n 1. We generalise this construction to modular forms of higher weights.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1301.0034",
        "title": "Open quantum random walks with decoherence on coins with $n$ degrees of\n  freedom",
        "abstract": "In this paper, we define a new type of decoherent quantum random walks with\n parameter $0\\le p\\le 1$, which becomes a unitary quantum random walk (UQRW)\n when $p=0$ and an open quantum random walk (OPRW) when $p=1$ respectively. We\n call this process a partially open quantum random walk (POQRW). We study the\n limiting distribution of a POQRW on $Z^1$ subject to decoherence on coins with\n $n$ degrees of freedom, which converges to a convex combination of normal\n distributions if the superoperator $\\mathcal{L}_{kk}$ satisfies the eigenvalue\n condition, that is, 1 is an eigenvalue of $\\mathcal{L}_{kk}$ with multiplicity\n one and all other eigenvalues have absolute values less than 1. A\n Perron-Frobenius type of theorem is provided in determining whether or not the\n superoperator satisfies the eigenvalue condition. Moreover, we compute the\n limiting distributions of characteristic equations of the position probability\n functions for $n=2$ and 3.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/9906397",
        "title": "Delocalized Coulomb phase in two dimensions",
        "abstract": "Extending finite size scaling theory to the many body ground state, one finds\n that Coulomb repulsion can drive a system of spinless fermions in a random\n potential from the Anderson insulator (Fermi glass of localized states) towards\n a new extended phase in dimension $d=2$. The transition occurs at a Coulomb\n energy to Fermi energy ratio $r_s \\approx 4$, where a change in the nature of\n the persistent currents has been previously observed. Relevance to the recently\n observed $2d$ metallic phase is suggested.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/0103084",
        "title": "ADE-Quiver Theories and Mirror Symmetry",
        "abstract": "We show that the Higgs branch of a four-dimensional Yang-Mills theory, with\n gauge and matter content summarised by an ADE quiver diagram, is identical to\n the generalised Coulomb branch of a four-dimensional superconformal strongly\n coupled gauge theory with ADE global symmetry. This equivalence suggests the\n existence of a mirror symmetry between the quiver theories and the strongly\n coupled theories.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1605.06103",
        "title": "Observation of $\\pi^- K^+$ and $\\pi^+ K^-$ atoms",
        "abstract": "The observation of hydrogen-like $\\pi K$ atoms, consisting of $\\pi^- K^+$ or\n $\\pi^+ K^-$ mesons, is presented. The atoms have been produced by 24 GeV/$c$\n protons from the CERN PS accelerator, interacting with platinum or nickel foil\n targets. The breakup (ionisation) of $\\pi K$ atoms in the same targets yields\n characteristic $\\pi K$ pairs, called \"atomic pairs\", with small relative\n momenta in the pair centre-of-mass system. The upgraded DIRAC experiment has\n observed $349\\pm62$ such atomic $\\pi K$ pairs, corresponding to a signal of 5.6\n standard deviations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1902.05945",
        "title": "Types by Need (Extended Version)",
        "abstract": "A cornerstone of the theory of lambda-calculus is that intersection types\n characterise termination properties. They are a flexible tool that can be\n adapted to various notions of termination, and that also induces adequate\n denotational models.\n  Since the seminal work of de Carvalho in 2007, it is known that multi types\n (i.e. non-idempotent intersection types) refine intersection types with\n quantitative information and a strong connection to linear logic. Typically,\n type derivations provide bounds for evaluation lengths, and minimal type\n derivations provide exact bounds.\n  De Carvalho studied call-by-name evaluation, and Kesner used his system to\n show the termination equivalence of call-by-need and call-by-name. De\n Carvalho's system, however, cannot provide exact bounds on call-by-need\n evaluation lengths.\n  In this paper we develop a new multi type system for call-by-need. Our system\n produces exact bounds and induces a denotational model of call-by-need,\n providing the first tight quantitative semantics of call-by-need.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1801.00003",
        "title": "A non-oscillatory energy-splitting method for the computation of\n  compressible multi-fluid flows",
        "abstract": "This paper proposes a new non-oscillatory {\\em energy-splitting} conservative\n algorithm for computing multi-fluid flows in the Eulerian framework. In\n comparison with existing multi-fluid algorithms in literatures, it is shown\n that the mass fraction model with isobaric hypothesis is a plausible choice for\n designing numerical methods for multi-fluid flows. Then we construct a\n conservative Godunov-based scheme with the high order accurate extension by\n using the generalized Riemann problem (GRP) solver, through the detailed\n analysis of kinetic energy exchange when fluids are mixed under the hypothesis\n of isobaric equilibrium. Numerical experiments are carried out for the\n shock-interface interaction and shock-bubble interaction problems, which\n display the excellent performance of this type of schemes and demonstrate that\n nonphysical oscillations are suppressed around material interfaces\n substantially.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1707.04164",
        "title": "Deep-inelastic multinucleon transfer processes in the $^{16}$O+$^{27}$Al\n  reaction",
        "abstract": "The reaction mechanism of deep-inelastic multinucleon transfer processes in\n the $^{16}$O+$^{27}$Al reaction at an incident $^{16}$O energy ($E_{\\rm\n lab}=134$ MeV) substantially above the Coulomb barrier has been studied both\n experimentally and theoretically. Elastic-scattering angular distribution,\n total kinetic energy loss spectra and angular distributions for various\n transfer channels have been measured. The $Q$-value- and angle-integrated\n isotope production cross sections have been deduced. To obtain deeper insight\n into the underlying reaction mechanism, we have carried out a detailed analysis\n based on the time-dependent Hartree-Fock (TDHF) theory. A recently developed\n method, TDHF+GEMINI, has been applied to evaluate production cross sections for\n secondary products. From a comparison between the experimental and theoretical\n cross sections, we find that the theory qualitatively reproduces the\n experimental data. Significant effects of secondary light-particle emissions\n are demonstrated. Possible interplay between fusion-fission, deep-inelastic,\n multinucleon transfer and particle evaporation processes are discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1308.3849",
        "title": "The Effect of ISP Traffic Shaping on User-Perceived Performances in\n  Broadband Shared Access Networks",
        "abstract": "Recent studies on the practice of shaping subscribers' traffic by ISPs give a\n new insight into the actual performance of broadband access networks at a\n packet level. Unlike metro and backbone networks, however, access networks\n directly interface with end-users, so it is important to base the study and\n design of access networks on the behaviors of and the actual performance\n perceived by end-users. In this paper we study the effect of ISP traffic\n shaping using traffic models based on user behaviors and\n application/session-layer metrics providing quantifiable measures of\n user-perceived performance for HTTP, FTP, and streaming video traffic. To\n compare the user-perceived performance of shaped traffic flows with those of\n unshaped ones in an integrated way, we use a multivariate non-inferiority\n testing procedure. We first investigate the effect of the token generation rate\n and the token bucket size of a token bucket filter on user-perceived\n performance at a subscriber level with a single subscriber. Then we investigate\n their effect at an access level where shaped traffic flows from multiple\n subscribers interact with one another in a common shared access network. The\n simulation results show that for a given token generation rate, a larger token\n bucket provides better user-perceived performance at both subscriber and access\n levels. It is also shown that the loose burst control resulting from the large\n token bucket --- up to 100 MB for access line rate of 100 Mbit/s --- does not\n negatively affect user-perceived performance with multiple subscribers even in\n the presence of non-conformant subscribers; with a much larger token bucket,\n however, the negative effect of non-conformant subscribers on the\n user-perceived performance of conformant subscribers becomes clearly visible\n because the impact of token bucket size and that of token generation rate are\n virtually indistinguishable in this case.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2006.14467",
        "title": "Robust Relative Hand Placement For Bi-Manual Tasks",
        "abstract": "In many bi-manual robotic tasks, like peg-in-a-hole assembly, the success of\n the task execution depends on the error in achieving the desired relative pose\n between the peg and the hole in a pre-insertion configuration. Random actuation\n errors in the joint space usually prevent the two arms from reaching their\n desired task space poses, which in turn results in a random error in relative\n pose between the two hands. This random error varies from trial to trial, and\n thus depending on the tolerance between the peg and the hole, the outcome of\n the assembly task may be random (sometimes the task execution succeeds and\n sometimes it fails). In general, since the relative pose has $6$\n degrees-of-freedom, there are infinite numbers of joint space solutions for the\n two arms that correspond to the same task space relative pose. However, in the\n presence of actuation errors, the joint space solutions are not all identical\n since they map the joint space error sets differently to the task space. Thus,\n the goal of this paper is to develop a methodical approach to compute a joint\n space solution such that the maximum task space error is below a (specified)\n threshold with high probability. Such a solution is called a robust inverse\n kinematics solution for the bi-manual robot. Our proposed method also allows\n the robot to self-evaluate whether it can perform a given bi-manual task\n reliably. We use a square peg-in-a-hole assembly scenario on the dual-arm\n Baxter robot for numerical simulations that shows the utility of our approach.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2212.12721",
        "title": "Polarimetric Multi-View Inverse Rendering",
        "abstract": "A polarization camera has great potential for 3D reconstruction since the\n angle of polarization (AoP) and the degree of polarization (DoP) of reflected\n light are related to an object's surface normal. In this paper, we propose a\n novel 3D reconstruction method called Polarimetric Multi-View Inverse Rendering\n (Polarimetric MVIR) that effectively exploits geometric, photometric, and\n polarimetric cues extracted from input multi-view color-polarization images. We\n first estimate camera poses and an initial 3D model by geometric reconstruction\n with a standard structure-from-motion and multi-view stereo pipeline. We then\n refine the initial model by optimizing photometric rendering errors and\n polarimetric errors using multi-view RGB, AoP, and DoP images, where we propose\n a novel polarimetric cost function that enables an effective constraint on the\n estimated surface normal of each vertex, while considering four possible\n ambiguous azimuth angles revealed from the AoP measurement. The weight for the\n polarimetric cost is effectively determined based on the DoP measurement, which\n is regarded as the reliability of polarimetric information. Experimental\n results using both synthetic and real data demonstrate that our Polarimetric\n MVIR can reconstruct a detailed 3D shape without assuming a specific surface\n material and lighting condition.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2306.17724",
        "title": "Electromagnetohydrodynamics",
        "abstract": "Interaction of plasma flow with a magnetic obstacle is a frequent process in\n many laser-plasma experiments in the laboratory, and is an important event in\n many astrophysical objects such as X-ray pulsars, AGN, GRB etc. As a result of\n plasma penetration through the magnetic wall we could expect a formation of\n magnetohydrodynamic (MHD) shock waves, as well as of electromagnetic (EM) ones.\n To study these processes we need equations following from hydrodynamic and\n Maxwell equations, which in the limiting situations describe MHD and EM waves,\n and are valid for the general case, when both phenomena are present. Here we\n derive a set of equations following from hydrodynamic and Maxwell equations,\n without neglecting a displacement current, needed for a formation of EM waves.\n We find a dispersion equation describing a propagation of a weak linear wave in\n a magnetized plasma along the $x$ axis, perpendicular to the magnetic field\n $H_y(x)$, which contains MHD, hydrodynamic and EM waves in the limiting cases,\n and some new types of behaviour in a general situation. We consider a plasma\n with zero viscosity and heat conductivity, but with a finite electric\n conductivity with a scalar coefficient.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0709.0866",
        "title": "Upper limits on the mass of supermassive black holes from STIS archival\n  data",
        "abstract": "The growth of supermassive black holes (SMBHs) appears to be closely linked\n with the formation of spheroids. There is a pressing need to acquire better\n statistics on SMBH masses, since the existing samples are preferentially\n weighted toward early-type galaxies with very massive SMBHs. With this\n motivation we started a project aimed at measuring upper limits on the mass of\n the SMBHs that can be present in the center of all the nearby galaxies (D<100\n Mpc) for which STIS/G750M spectra are available in the HST archive. These upper\n limits will be derived by modeling the central emission-line widths ([NII],\n Halpha and [SII]) observed over an aperture of ~0.1\" (R<50 pc). Here we present\n our preliminary results for a subsample of 76 bulges.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1503.06805",
        "title": "Evolution of a ring around the Pluto-Charon binary",
        "abstract": "We consider the formation of satellites around the Pluto-Charon binary. An\n early collision between the two partners likely produced the binary and a\n narrow ring of debris, out of which arose the moons Styx, Nix, Kerberos and\n Hydra. How the satellites emerged from the compact ring is uncertain. Here we\n show that a particle ring spreads from physical collisions and collective\n gravitational scattering, similar to migration. Around a binary, these\n processes take place in the reference frames of \"most circular\" orbits, akin to\n circular ones in a Keplerian potential. Ring particles damp to these orbits and\n avoid destructive collisions. Damping and diffusion also help particles survive\n dynamical instabilities driven by resonances with the binary. In some\n situations, particles become trapped near resonances that sweep outward with\n the tidal evolution of the Pluto-Charon binary. With simple models and\n numerical experiments, we show how the Pluto-Charon impact ring may have\n expanded into a broad disk, out of which grew the circumbinary moons. In some\n scenarios, the ring can spread well beyond the orbit of Hydra, the most distant\n moon, to form a handful of smaller satellites. If these small moons exist, New\n Horizons will find them.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1811.09730",
        "title": "Collective social behavior in a crowd controlled game",
        "abstract": "Despite many efforts, the behavior of a crowd is not fully understood. The\n advent of modern communication media has made it an even more challenging\n problem, as crowd dynamics could be driven by both human-to-human and\n human-technology interactions. Here, we study the dynamics of a crowd\n controlled game (Twitch Plays Pok\\'emon), in which nearly a million players\n participated during more than two weeks. We dissect the temporal evolution of\n the system dynamics along the two distinct phases that characterized the game.\n We find that players who do not follow the crowd average behavior are key to\n succeed in the game. The latter finding can be well explained by an n-$th$\n order Markov model that reproduces the observed behavior. Secondly, we analyze\n a phase of the game in which players were able to decide between two different\n modes of playing, mimicking a voting system. Our results suggest that under\n some conditions, the collective dynamics can be better regarded as a swarm-like\n behavior instead of a crowd. Finally, we discuss our findings in the light of\n the social identity theory, which appears to describe well the observed\n dynamics.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1203.1007",
        "title": "Agnostic System Identification for Model-Based Reinforcement Learning",
        "abstract": "A fundamental problem in control is to learn a model of a system from\n observations that is useful for controller synthesis. To provide good\n performance guarantees, existing methods must assume that the real system is in\n the class of models considered during learning. We present an iterative method\n with strong guarantees even in the agnostic case where the system is not in the\n class. In particular, we show that any no-regret online learning algorithm can\n be used to obtain a near-optimal policy, provided some model achieves low\n training error and access to a good exploration distribution. Our approach\n applies to both discrete and continuous domains. We demonstrate its efficacy\n and scalability on a challenging helicopter domain from the literature.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0805.2935",
        "title": "Quantum Gravity Yesterday and Today",
        "abstract": "We present a talk given by Bryce DeWitt on Quantum Gravity, and present many\n references.\n  This article is the first installment of the book \"The Pursuit of Quantum\n Gravity 1946-2004; Memoirs of Bryce DeWitt\" that Cecile DeWitt is preparing.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2308.08799",
        "title": "Capturing Popularity Trends: A Simplistic Non-Personalized Approach for\n  Enhanced Item Recommendation",
        "abstract": "Recommender systems have been gaining increasing research attention over the\n years. Most existing recommendation methods focus on capturing users'\n personalized preferences through historical user-item interactions, which may\n potentially violate user privacy. Additionally, these approaches often overlook\n the significance of the temporal fluctuation in item popularity that can sway\n users' decision-making. To bridge this gap, we propose Popularity-Aware\n Recommender (PARE), which makes non-personalized recommendations by predicting\n the items that will attain the highest popularity. PARE consists of four\n modules, each focusing on a different aspect: popularity history, temporal\n impact, periodic impact, and side information. Finally, an attention layer is\n leveraged to fuse the outputs of four modules. To our knowledge, this is the\n first work to explicitly model item popularity in recommendation systems.\n Extensive experiments show that PARE performs on par or even better than\n sophisticated state-of-the-art recommendation methods. Since PARE prioritizes\n item popularity over personalized user preferences, it can enhance existing\n recommendation methods as a complementary component. Our experiments\n demonstrate that integrating PARE with existing recommendation methods\n significantly surpasses the performance of standalone models, highlighting\n PARE's potential as a complement to existing recommendation methods.\n Furthermore, the simplicity of PARE makes it immensely practical for industrial\n applications and a valuable baseline for future research.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2405.13733",
        "title": "Nuclear quantum effects in structural and elastic properties of cubic\n  silicon carbide",
        "abstract": "Silicon carbide, a semiconducting material, has gained importance in the\n fields of ceramics, electronics, and renewable energy due to its remarkable\n hardness and resistance. In this study, we delve into the impact of nuclear\n quantum motion, or vibrational mode quantization, on the structural and elastic\n properties of 3C-SiC. This aspect, elusive in conventional {\\it ab-initio}\n calculations, is explored through path-integral molecular dynamics (PIMD)\n simulations using an efficient tight-binding (TB) Hamiltonian. This\n investigation spans a wide range of temperatures and pressures, including\n tensile stress, adeptly addressing the quantization and anharmonicity inherent\n in solid-state vibrational modes. The accuracy of the TB model has been checked\n by comparison with density-functional-theory calculations at zero temperature.\n The magnitude of quantum effects is assessed by comparing PIMD outcomes with\n results obtained from classical molecular dynamics simulations. Our\n investigation uncovers notable reductions of 5%, 10%, and 4% in the elastic\n constants $C_{11}$, $C_{12}$, and $C_{44}$, respectively, attributed to atomic\n zero-point oscillations. Consequently, the bulk modulus and Poisson's ratio of\n 3C-SiC exhibit reduced values by 7% and 5% at low temperature. The persistence\n of these quantum effects in the material's structural and elastic attributes\n beyond room temperature underscores the necessity of incorporating nuclear\n quantum motion for an accurate description of these fundamental properties of\n SiC.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2112.14633",
        "title": "Bayesian Compressive Channel Estimation for Hybrid Full-Dimensional MIMO\n  Communications",
        "abstract": "Efficient channel estimation is challenging in full-dimensional\n multiple-input multiple-output communication systems, particularly in those\n with hybrid digital-analog architectures. Under a compressive sensing\n framework, this letter first designs a uniform dictionary based on a spherical\n Fibonacci grid to represent channels in a sparse domain, yielding smaller\n angular errors in three-dimensional beamspace than traditional dictionaries.\n Then, a Bayesian inference-aided greedy pursuit algorithm is developed to\n estimate channels in the frequency domain. Finally, simulation results\n demonstrate that both the designed dictionary and the proposed Bayesian channel\n estimation outperform the benchmark schemes and attain a lower normalized mean\n squared error of channel estimation.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2104.03951",
        "title": "Joint Design for Electric Fleet Operator an Charging Service Provider:\n  Understanding the Non-Cooperative Nature",
        "abstract": "This work proposes a new modeling framework for jointly optimizing the\n charging network design and the logistic mobility planning for an electric\n vehicle fleet. Existing literature commonly assumes the existence of a single\n entity, the social planner, as a powerful decision maker who manages all\n resources. However, this is often not the case in practice. Instead of making\n this assumption, we specifically examine the innate noncooperative nature of\n two different entities involved in the planning problem. Namely, they are the\n charging service provider (CSP) and the fleet operator (FO). To address the\n strategic interaction between entities, a bilevel mixed integer program is\n formulated, with the CSP and FO problems expressed in the upper and lower\n levels respectively, in a joint decision making process. These decisions\n involve the CSP infrastructure siting, sizing, substation capacity upgrades,\n the FO fleet composition, vehicle routing, charging, and delivery assignment.\n To relieve computational burdens, we utilize a double loop solution\n architecture to iteratively reach optimality. We conduct detailed numerical\n studies on a synthesized small network and the simulation results reveal the\n unique aspects of this two entity framework. This modeling perspective can be\n generalized to other system design problems with two interacting agents\n planning and operating resources across networks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.06433",
        "title": "Concentration properties of theta lifts on orthogonal groups",
        "abstract": "Let $n>m\\geqslant 1$ be integers with $n+m\\geqslant 4$ even. We prove the\n existence of Maass forms with large sup norms on anisotropic ${\\rm O}(n,m)$, by\n combining a counting argument with a new period relation showing that a certain\n orthogonal period on ${\\rm O}(n,m)$ distinguishes theta lifts from ${\\rm\n Sp}_{2m}$. This generalizes a method of Rudnick and Sarnak in the rank one\n case, when $m = 1$. Our lower bound is naturally expressed as a ratio of the\n Plancherel measures for the groups ${\\rm O}(n,m)$ and ${\\rm\n Sp}_{2m}(\\mathbb{R})$, up to logarithmic factors, and strengthens the lower\n bounds of our previous paper for such groups. In the case of odd-dimensional\n hyperbolic spaces, the growth exponent we obtain improves on a result of\n Donnelly, and is optimal under the purity conjecture of Sarnak.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1402.0837",
        "title": "On the existence of the $\\it NN$-decoupled dibaryon $d^\\ast_1$(1956)",
        "abstract": "We present strong experimental evidence for the existence of a nonstrange\n $\\it NN$-decoupled dibaryon with a mass of about 1956 MeV stable against strong\n decay, which is a very likely candidate for long-sought multiquark hadrons with\n the baryon number B=2. We start by presenting the first evidence for the\n existence of this dibaryon called $d^{\\ast }_{1}$(1956), that was found in the\n energy spectrum of coincident photons emitted at $\\pm 90^0$ from the reaction\n $pp \\to pp \\gamma\\gamma$ at an energy of 216 MeV measured at JINR. We then show\n its signatures found in several experimental energy spectra of single photons\n and invariant mass spectra of photon pairs from photon production processes\n induced by nucleon-nucleon and nucleon-nucleus collisions at intermediate\n energies.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1407.8051",
        "title": "Implementing a neutral-atom controlled-phase gate with a single Rydberg\n  pulse",
        "abstract": "One can implement fast two-qubit entangling gates by exploiting the Rydberg\n blockade. Although various theoretical schemes have been proposed,\n experimenters have not yet been able to demonstrate two-atom gates of high\n fidelity due to experimental constraints. We propose a novel scheme, which only\n uses a single Rydberg pulse illuminating both atoms, for the construction of\n neutral-atom controlled-phase gates. In contrast to the existing schemes, our\n approach is simpler to implement and requires neither individual addressing of\n atoms nor adiabatic procedures. With parameters estimated based on actual\n experimental scenarios, a gate fidelity higher than 0.99 is achievable.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math/0205194",
        "title": "Classification of differentials and Cartan calculus on bicrossproducts",
        "abstract": "We provide the Cartan calculus for bicovariant differential forms on\n bicrossproduct quantum groups $k(M)\\lrbicross kG$ associated to finite group\n factorizations $X=GM$ and a field $k$. The irreducible calculi are associated\n to certain conjugacy classes in $X$ and representations of isotropy groups. We\n find the full exterior algebras and show that they are inner by a bi-invariant\n 1-form $\\theta$ which is a generator in the noncommutative de Rham cohomology\n $H^1$. The special cases where one subgroup is normal are analysed. As an\n application, we study the noncommutative cohomology on the quantum codouble\n $D^*(S_3)\\isom k(S_3)\\lrbicross k\\Z_6$ and the quantum double\n $D(S_3)=k(S_3)\\lcross kS_3$, finding respectively a natural calculus and a\n unique calculus with $H^0=k.1$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2203.11773",
        "title": "Pumping and mixing in active pores",
        "abstract": "We show both numerically and analytically that a chemically patterned active\n pore can act as a micro/nano-pump for fluids, even if it is fore-aft symmetric.\n This is possible due to a spontaneous symmetry breaking which occurs when\n advection rather than diffusion is the dominant mechanism of solute transport.\n We further demonstrate that, for pumping and tuning the flow rate, a\n combination of geometrical and chemical inhomogeneities is required. For\n certain parameter values, the flow is unsteady, and persistent oscillations\n with a tunable frequency appear. Finally, we find that the flow may lose its\n axial symmetry and hence promotes mixing in the low Reynolds number regime.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2107.11494",
        "title": "TinyAction Challenge: Recognizing Real-world Low-resolution Activities\n  in Videos",
        "abstract": "This paper summarizes the TinyAction challenge which was organized in\n ActivityNet workshop at CVPR 2021. This challenge focuses on recognizing\n real-world low-resolution activities present in videos. Action recognition task\n is currently focused around classifying the actions from high-quality videos\n where the actors and the action is clearly visible. While various approaches\n have been shown effective for recognition task in recent works, they often do\n not deal with videos of lower resolution where the action is happening in a\n tiny region. However, many real world security videos often have the actual\n action captured in a small resolution, making action recognition in a tiny\n region a challenging task. In this work, we propose a benchmark dataset,\n TinyVIRAT-v2, which is comprised of naturally occuring low-resolution actions.\n This is an extension of the TinyVIRAT dataset and consists of actions with\n multiple labels. The videos are extracted from security videos which makes them\n realistic and more challenging. We use current state-of-the-art action\n recognition methods on the dataset as a benchmark, and propose the TinyAction\n Challenge.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1306.6718",
        "title": "Building the Chessboard-like Supramolecular Structure on Au (111)\n  Surfaces",
        "abstract": "We investigate an anthracene derivative, 3(5)-(9-anthryl) pyrazole (ANP),\n self-assembled on the Au (111) surface by means of scanning tunneling\n microscopy (STM) and density functional theory (DFT) calculations. A\n chessboard-like network structure composed of ANP molecules is found, covering\n the whole Au (111) substrate. Our STM results and DFT calculations reveal that\n the formation of chessboard-like networks originates from a basic unit cell, a\n tetramer structure, which is formed by four ANP molecules connected through\n C-H-N hydrogen-bonds. The hydrogen bonds inside each tetramer and the molecular\n adsorption interaction are fundamentally important in providing a driving force\n for formation of the supramolecular networks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1609.07320",
        "title": "Influence of heavy metal materials on magnetic properties of Pt/Co/heavy\n  metal tri-layered structures",
        "abstract": "Pt/Co/heavy metal (HM) tri-layered structures with interfacial perpendicular\n magnetic anisotropy (PMA) are currently under intensive research for several\n emerging spintronic effects, such as spinorbit torque, domain wall motion, and\n room temperature skyrmions. HM materials are used as capping layers to generate\n the structural asymmetry and enhance the interfacial effects. For instance, the\n Pt/Co/Ta structure attracts a lot of attention as it may exhibit large\n Dzyaloshinskii-Moriya interaction. However, the dependence of magnetic\n properties on different capping materials has not been systematically\n investigated. In this paper, we experimentally show the interfacial PMA and\n damping constant for Pt/Co/HM tri-layered structures through time-resolved\n magneto-optical Kerr effect measurements as well as magnetometry measurements,\n where the capping HM materials are W, Ta, and Pd. We found that the Co/HM\n interface plays an important role on the magnetic properties. In particular,\n the magnetic multilayers with a W capping layer features the lowest effective\n damping value, which may be attributed to the different spin-orbit coupling and\n interfacial hybridization between Co and HM materials. Our findings allow a\n deep understanding of the Pt/Co/HM tri-layered structures. Such structures\n could lead to a better era of data storage and processing devices.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/9607001",
        "title": "On the evolution of ejecta fragments in compact supernova remnants",
        "abstract": "We examine the evolution of inhomogeneities (fragments) of supernova ejecta\n in compact supernova remnants by means of hydrodynamical modeling and\n simplified analytical calculations. Under the influence of intense post-shock\n cooling the fragments become strongly compressed as they traverse the hot\n shocked region between the reverse and outer shocks of the remnant. We find\n that the most likely outcome of the interaction of fragments with the reverse\n shock and the hot shocked region is their disruption resulting in generation of\n secondary fragments. Secondary fragments arriving at the thin and dense outer\n shell of the remnant give rise to brief X-ray flashes. Under suitable\n conditions the primary fragments may traverse the hot shocked region without\n being completely destroyed, to eventually reach the outer shell as dense,\n elongated structures. Collisions of such fragments with the shell are likely to\n give rise to powerful X-ray flares.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1505.06678",
        "title": "High-performance chemical-bath deposited CdS thin-film transistors with\n  ZrO2 gate dielectric",
        "abstract": "We demonstrate high performance chemical bath deposited CdS thin-film\n transistors TFTs using atomic layer deposited ZrO2 based high-k gate dielectric\n material.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ex/0310039",
        "title": "Latest News from SNO",
        "abstract": "A summary of the intial pure D2O phase results is presented in the first half\n of the paper. The second half describes the various aspects of the calibration\n and detector reponse analysis carried out for the second `salt' phase of\n running. In addition, estimates on the precision attainable with the salt data\n are presented.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0512401",
        "title": "Orientational Defects in Ice Ih: An Interpretation of Electrical\n  Conductivity Measurements",
        "abstract": "We present a first-principles study of the structure and energetics of\n Bjerrum defects in ice Ih and compare the results to experimental electrical\n conductivity data. While the DFT result for the activation energy is in good\n agreement with experiment, we find that its two components have quite different\n values. Aside from providing new insight into the fundamental parameters of the\n microscopic electrical theory of ice, our results suggest the activity of traps\n in doped ice in the temperature regime typically assumed to be controlled by\n the free migration of L defects.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1011.4687",
        "title": "Acceleration of a QM/MM-QMC simulation using GPU",
        "abstract": "We accelerated an ab-initio molecular QMC calculation by using GPGPU. Only\n the bottle-neck part of the calculation is replaced by CUDA subroutine and\n performed on GPU. The performance on a (single core CPU + GPU) is compared with\n that on a (single core CPU with double precision), getting 23.6 (11.0) times\n faster calculations in single (double) precision treatments on GPU. The energy\n deviation caused by the single precision treatment was found to be within the\n accuracy required in the calculation, \\sim 10^{-5} hartree. The accelerated\n computational nodes mounting GPU are combined to form a hybrid MPI cluster on\n which we confirmed the performance linearly scales to the number of nodes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1310.0985",
        "title": "Average synaptic activity and neural networks topology: a global inverse\n  problem",
        "abstract": "The dynamics of neural networks is often characterized by collective behavior\n and quasi-synchronous events, where a large fraction of neurons fire in short\n time intervals, separated by uncorrelated firing activity. These global\n temporal signals are crucial for brain functioning. They strongly depend on the\n topology of the network and on the fluctuations of the connectivity. We propose\n a heterogeneous mean--field approach to neural dynamics on random networks,\n that explicitly preserves the disorder in the topology at growing network\n sizes, and leads to a set of self-consistent equations. Within this approach,\n we provide an effective description of microscopic and large scale temporal\n signals in a leaky integrate-and-fire model with short term plasticity, where\n quasi-synchronous events arise. Our equations provide a clear analytical\n picture of the dynamics, evidencing the contributions of both periodic (locked)\n and aperiodic (unlocked) neurons to the measurable average signal. In\n particular, we formulate and solve a global inverse problem of reconstructing\n the in-degree distribution from the knowledge of the average activity field.\n Our method is very general and applies to a large class of dynamical models on\n dense random networks.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2009.07778",
        "title": "Scaffold-constrained molecular generation",
        "abstract": "One of the major applications of generative models for drug Discovery targets\n the lead-optimization phase. During the optimization of a lead series, it is\n common to have scaffold constraints imposed on the structure of the molecules\n designed. Without enforcing such constraints, the probability of generating\n molecules with the required scaffold is extremely low and hinders the\n practicality of generative models for de-novo drug design. To tackle this\n issue, we introduce a new algorithm to perform scaffold-constrained in-silico\n molecular design. We build on the well-known SMILES-based Recurrent Neural\n Network (RNN) generative model, with a modified sampling procedure to achieve\n scaffold-constrained generation. We directly benefit from the associated\n reinforcement Learning methods, allowing to design molecules optimized for\n different properties while exploring only the relevant chemical space. We\n showcase the method's ability to perform scaffold-constrained generation on\n various tasks: designing novel molecules around scaffolds extracted from\n SureChEMBL chemical series, generating novel active molecules on the Dopamine\n Receptor D2 (DRD2) target, and, finally, designing predicted actives on the\n MMP-12 series, an industrial lead-optimization project.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/9608137",
        "title": "Interband effects in the c-axis optical conductivity in YBaCuO",
        "abstract": "The normal state optical conductivity is calculated for a layered metal with\n two layers per unit cell coupled through a transverse hopping matrix element\n $t_\\perp$. The optical response involves an interband term in addition to the\n more familiar intraband term which leads to the usual Drude form. The interband\n term is only weakly temperature dependent, even for an inelastic scattering\n rate which is linear in T. It gives a $c$-axis response which extends in\n frequency over the entire band width although there can be structure on this\n energy scale which reflects details of the electronic structure. In particular,\n at low energy, the $c$-axis response can develop a gap or pseudogap as the\n temperature is lowered. At high temperature, a Drude response will be seen only\n if the intraband transitions, which are of order $t_\\perp^4$, become important\n compared with the interband transitions which are of order $t_\\perp^2$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2302.13531",
        "title": "Interface tool from Wannier90 to RESPACK: wan2respack",
        "abstract": "We develop the interface tool $\\verb|wan2respack|$, which connects\n $\\verb|RESPACK|$ (software that derives the low-energy effective Hamiltonians\n of solids) with $\\verb|Wannier90|$ (software that constructs Wannier\n functions). $\\verb|wan2respack|$ converts the Wannier functions obtained by\n $\\verb|Wannier90|$ into those used in $\\verb|RESPACK|$, which is then used to\n derive the low-energy effective Hamiltonians of solids. In this paper, we\n explain the basic usage of $\\verb|wan2respack|$ and show its application to\n standard compounds of correlated materials, namely, the correlated metal\n SrVO$_3$ and the high-$T_{c}$ superconductor La$_2$CuO$_4$. Furthermore, we\n compare the low-energy effective Hamiltonians of these compounds using Wannier\n functions obtained by $\\verb|Wannier90|$ and those obtained by\n $\\verb|RESPACK|$. We confirm that both types of Wannier functions give the same\n Hamiltonians. This benchmark comparison demonstrates that $\\verb|wan2respack|$\n correctly converts Wannier functions in the $\\verb|Wannier90|$ format into\n those in the $\\verb|RESPACK|$ format.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0808.2184",
        "title": "Supersymmetric Multi-trace Boundary Conditions in AdS",
        "abstract": "Boundary conditions for massive fermions are investigated in AdS_d for $d \\ge\n 2$. For fermion masses in the range $0 \\le |m| < 1/2\\ell$ with $\\ell$ the AdS\n length, the standard notion of normalizeability allows a choice of boundary\n conditions. As in the case of scalars at or slightly above the\n Breitenlohner-Freedman (BF) bound, such boundary conditions correspond to\n multi-trace deformations of any CFT dual. By constructing appropriate boundary\n superfields, for d=3,4,5 we identify joint scalar/fermion boundary conditions\n which preserve either ${\\cal N}=1$ supersymmetry or ${\\cal N}=1$ superconformal\n symmetry on the boundary. In particular, we identify boundary conditions\n corresponding via AdS/CFT (at large N) to a 595-parameter family of\n double-trace marginal deformations of the low-energy theory of N M2-branes\n which preserve ${\\cal N} =1$ superconformal symmetry. We also establish that\n (at large N and large 't Hooft coupling $\\lambda$) there are no marginal or\n relevant multi-trace deformations of 3+1 ${\\cal N} =4$ super Yang-Mills which\n preserve even ${\\cal N}=1$ supersymmetry.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2108.10819",
        "title": "Next-generation perception system for automated defects detection in\n  composite laminates via polarized computational imaging",
        "abstract": "Finishing operations on large-scale composite components like wind turbine\n blades, including trimming and sanding, often require multiple workers and part\n repositioning. In the composites manufacturing industry, automation of such\n processes is challenging, as manufactured part geometry may be inconsistent and\n task completion is based on human judgment and experience. Implementing a\n mobile, collaborative robotic system capable of performing finishing tasks in\n dynamic and uncertain environments would improve quality and lower\n manufacturing costs. To complete the given tasks, the collaborative robotic\n team must properly understand the environment and detect irregularities in the\n manufactured parts. In this paper, we describe the initial implementation and\n demonstration of a polarized computational imaging system to identify defects\n in composite laminates. As the polarimetric images are highly relevant to the\n surface micro-geometry, they can be used to detect surface defects that are not\n visible in conventional color images. The proposed vision system successfully\n identifies defect types and surface characteristics (e.g., pinholes, voids,\n scratches, resin flash) for different glass fiber and carbon fiber laminates.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1501.02442",
        "title": "Neutrino versus antineutrino cross sections and CP violation",
        "abstract": "We discuss the nuclear interactions of neutrinos versus those of\n antineutrinos, a relevant comparison for CP violation experiments in the\n neutrino sector. We consider the MiniBooNE quasielastic-like double\n differential neutrinos and antineutrinos cross sections which are flux\n dependent and hence specific to the MiniBooNE set-up. We combine them\n introducing their sum and their difference. We show that the last combination\n can bring a general information, which can be exploited in other experiments,\n on the nuclear matrix elements of the axial vector interference term. Our\n theoretical model reproduces well the two cross sections combinations. This\n confirms the need for a sizeable multinucleon component in particular in the\n interference term.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/nucl-th/0305048",
        "title": "Comparative Analysis of the Mechanisms of Fast Light Particle Formation\n  in Nucleus-Nucleus Collisions at Low and Intermediate Energies",
        "abstract": "The dynamics and the mechanisms of preequilibrium-light-particle formation in\n nucleus-nucleus collisions at low and intermediate energies are studied on the\n basis of a classical four-body model. The angular and energy distributions of\n light particles from such processes are calculated. It is found that, at\n energies below 50 MeV per nucleon, the hardest section of the energy spectrum\n is formed owing to the acceleration of light particles from the target by the\n mean field of the projectile nucleus. Good agreement with available\n experimental data is obtained.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0801.4606",
        "title": "Palatini f(R) Cosmology",
        "abstract": "We investigate the modified gravity theories in terms of the effective dark\n energy models. We compare the cosmic expansion history and the linear growth in\n different models. We also study the evolution of linear cosmological\n perturbations in modified theories of gravity assuming the Palatini formalism.\n We find the stability of the superhorizon metric evolution depends on models.\n We also study the matter density fluctuation in the general gauge and show the\n differential equations in super and sub-horizon scales.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2211.16245",
        "title": "Geometric construction of classes in van Daele's K-theory",
        "abstract": "We describe explicit generators for the \"real\" K-theory of \"real\" spheres in\n van Daele's picture. Pulling these generators back along suitable maps from\n tori to spheres produces a family of Hamiltonians used in the physics\n literature on topological insulators. We compute their K-theory classes\n geometrically, based on wrong-way functoriality of K-theory and the geometric\n version of bivariant K-theory, which we extend to the \"real\" case.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1907.03197",
        "title": "Composable Core-sets for Determinant Maximization: A Simple Near-Optimal\n  Algorithm",
        "abstract": "``Composable core-sets'' are an efficient framework for solving optimization\n problems in massive data models. In this work, we consider efficient\n construction of composable core-sets for the determinant maximization problem.\n This can also be cast as the MAP inference task for determinantal point\n processes, that have recently gained a lot of interest for modeling diversity\n and fairness. The problem was recently studied in [IMOR'18], where they\n designed composable core-sets with the optimal approximation bound of $\\tilde\n O(k)^k$. On the other hand, the more practical Greedy algorithm has been\n previously used in similar contexts. In this work, first we provide a\n theoretical approximation guarantee of $O(C^{k^2})$ for the Greedy algorithm in\n the context of composable core-sets; Further, we propose to use a Local Search\n based algorithm that while being still practical, achieves a nearly optimal\n approximation bound of $O(k)^{2k}$; Finally, we implement all three algorithms\n and show the effectiveness of our proposed algorithm on standard data sets.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1510.04740",
        "title": "Semiparametric theory and empirical processes in causal inference",
        "abstract": "In this paper we review important aspects of semiparametric theory and\n empirical processes that arise in causal inference problems. We begin with a\n brief introduction to the general problem of causal inference, and go on to\n discuss estimation and inference for causal effects under semiparametric\n models, which allow parts of the data-generating process to be unrestricted if\n they are not of particular interest (i.e., nuisance functions). These models\n are very useful in causal problems because the outcome process is often complex\n and difficult to model, and there may only be information available about the\n treatment process (at best). Semiparametric theory gives a framework for\n benchmarking efficiency and constructing estimators in such settings. In the\n second part of the paper we discuss empirical process theory, which provides\n powerful tools for understanding the asymptotic behavior of semiparametric\n estimators that depend on flexible nonparametric estimators of nuisance\n functions. These tools are crucial for incorporating machine learning and other\n modern methods into causal inference analyses. We conclude by examining related\n extensions and future directions for work in semiparametric causal inference.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2212.02147",
        "title": "Three-dimensional reconstruction of type U radio bursts: a novel remote\n  sensing approach for coronal loops",
        "abstract": "Type U radio bursts are impulsive coherent radio emissions produced by the\n Sun that indicate the presence of subrelativistic electron beams propagating\n along magnetic loops in the solar corona. In this work, we present the analysis\n of a type U radio burst that was exceptionally imaged on 2011 March 22 by the\n Nan\\c{c}ay Radioheliograph (NRH) at three different frequencies (298.7, 327.0,\n and 360.8 MHz). Using a novel modelling approach, we show for the first time\n that the use of high-resolution radio heliograph images of type U radio bursts\n can be sufficient to both accurately reconstruct the 3D morphology of coronal\n loops (without recurring to triangulation techniques) and to fully constrain\n their physical parameters. At the same time, we can obtain unique information\n on the dynamics of the accelerated electron beams, which provides important\n clues as to the plasma mechanisms involved in their acceleration and as to why\n type U radio bursts are not observed as frequently as type III radio bursts. We\n finally present plausible explanations for a problematic aspect related to the\n apparent lack of association between the modeled loop as inferred from radio\n images and the extreme-ultraviolet (EUV) structures observed from space in the\n same coronal region",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2201.04250",
        "title": "Functional Renormalization Group Approach for Signal Detection",
        "abstract": "This review paper uses renormalization group techniques for signal detection\n in nearly-continuous positive spectra. We highlight universal aspects of the\n analogue field-theory approach. The first aim is to present an extended\n self-consistent construction of the analogue effective field-theory framework\n for data, which can be viewed as a maximum entropy model. In particular and\n exploiting universality arguments, we justify the $\\mathbb{Z}_2$-symmetry of\n the classical action and we stress the existence of a large-scale (local)\n regime and of a small-scale (nonlocal) regime. Secondly and related to noise\n models, we observe the universal relation between phase transition and symmetry\n breaking in the vicinity of the detection threshold. Finally, we discuss the\n issue of defining the covariance matrix for tensorial-like data. Based on the\n cutting graph prescription, we note the superiority of definitions based on\n complete graphs of large size for data analysis.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1310.0930",
        "title": "Exploiting multi-scale parallelism for large scale numerical modelling\n  of laser wakefield accelerators",
        "abstract": "A new generation of laser wakefield accelerators, supported by the extreme\n accelerating fields generated in the interaction of PW-Class lasers and\n underdense targets, promises the production of high quality electron beams in\n short distances for multiple applications. Achieving this goal will rely\n heavily on numerical modeling for further understanding of the underlying\n physics and identification of optimal regimes, but large scale modeling of\n these scenarios is computationally heavy and requires efficient use of\n state-of-the-art Petascale supercomputing systems. We discuss the main\n difficulties involved in running these simulations and the new developments\n implemented in the OSIRIS framework to address these issues, ranging from\n multi-dimensional dynamic load balancing and hybrid distributed / shared memory\n parallelism to the vectorization of the PIC algorithm. We present the results\n of the OASCR Joule Metric program on the issue of large scale modeling of LWFA,\n demonstrating speedups of over 1 order of magnitude on the same hardware.\n Finally, scalability to over $\\sim 10^6$ cores, and sustained performance over\n $\\sim 2$ PFlops is demonstrated, opening the way for large scale modeling of\n laser wakefield accelerator scenarios.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2203.06088",
        "title": "Game Dynamics Structure Control by Design: an Example from Experimental\n  Economics",
        "abstract": "Game dynamics structure (e.g., endogenous cycle motion) in human subjects\n game experiments can be predicted by game dynamics theory. However, whether the\n structure can be controlled by mechanism design to a desired goal is not known.\n Here, using the pole assignment approach in modern control theory, we\n demonstrate how to control the structure in two steps: (1) Illustrate an\n theoretical workflow on how to design a state-depended feedback controller for\n desired structure; (2) Evaluate the controller by laboratory human subject game\n experiments and by agent-based evolutionary dynamics simulation. To our\n knowledge, this is the first realisation of the control of the human social\n game dynamics structure in theory and experiment.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2104.06779",
        "title": "Temporally-Aware Feature Pooling for Action Spotting in Soccer\n  Broadcasts",
        "abstract": "Toward the goal of automatic production for sports broadcasts, a paramount\n task consists in understanding the high-level semantic information of the game\n in play. For instance, recognizing and localizing the main actions of the game\n would allow producers to adapt and automatize the broadcast production,\n focusing on the important details of the game and maximizing the spectator\n engagement. In this paper, we focus our analysis on action spotting in soccer\n broadcast, which consists in temporally localizing the main actions in a soccer\n game. To that end, we propose a novel feature pooling method based on NetVLAD,\n dubbed NetVLAD++, that embeds temporally-aware knowledge. Different from\n previous pooling methods that consider the temporal context as a single set to\n pool from, we split the context before and after an action occurs. We argue\n that considering the contextual information around the action spot as a single\n entity leads to a sub-optimal learning for the pooling module. With NetVLAD++,\n we disentangle the context from the past and future frames and learn specific\n vocabularies of semantics for each subsets, avoiding to blend and blur such\n vocabulary in time. Injecting such prior knowledge creates more informative\n pooling modules and more discriminative pooled features, leading into a better\n understanding of the actions. We train and evaluate our methodology on the\n recent large-scale dataset SoccerNet-v2, reaching 53.4% Average-mAP for action\n spotting, a +12.7% improvement w.r.t the current state-of-the-art.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1511.06526",
        "title": "Sufficient Conditions for Efficient Classical Simulation of Quantum\n  Optics",
        "abstract": "We provide general sufficient conditions for the efficient classical\n simulation of quantum-optics experiments that involve inputting states to a\n quantum process and making measurements at the output. The first condition is\n based on the negativity of phase-space quasiprobability distributions (PQDs) of\n the output state of the process and the output measurements; the second one is\n based on the negativity of PQDs of the input states, the output measurements,\n and the transition function associated with the process. We show that these\n conditions provide useful practical tools for investigating the effects of\n imperfections in implementations of boson sampling. In particular, we apply our\n formalism to boson-sampling experiments that use single-photon or\n spontaneous-parametric-down-conversion sources and on-off photodetectors.\n Considering simple models for loss and noise, we show that above some threshold\n for the probability of random counts in the photodetectors, these\n boson-sampling experiments are classically simulatable. We identify mode\n mismatching as the major source of error contributing to random counts and\n suggest that this is the chief challenge for implementations of boson sampling\n of interesting size.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2212.07653",
        "title": "Spontaneous Scalarization of Black Holes in Gauss-Bonnet Teleparallel\n  Gravity",
        "abstract": "In this paper, we find new scalarized black holes by coupling a scalar field\n with the Gauss-Bonnet invariant in Teleparallel gravity. The Teleparallel\n formulation of this theory uses torsion instead of curvature to describe the\n gravitational interaction and it turns out that, in this language, the usual\n Gauss-Bonnet term in four dimensions, decays in two distinct boundary terms,\n the Teleparallel Gauss-Bonnet invariants. Both can be coupled individually, or\n in any combination to a scalar field, to obtain a Teleparallel Gauss-Bonnet\n extension of the Teleparallel equivalent of general relativity. The theory we\n study contains the familiar Riemannian Einstein-Gauss-Bonnet gravity theory as\n a particular limit and offers a natural extension, in which scalarization is\n triggered by torsion and with new interesting phenomenology. We demonstrate\n numerically the existence of asymptotically flat scalarized black hole\n solutions and show that, depending on the choice of coupling of the boundary\n terms, they can have a distinct behaviour compared to the ones known from the\n usual Einstein-Gauss-Bonnet case. More specifically, non-monotonicity of the\n metric functions and the scalar field can be present, a feature that was not\n observed until now for static scalarized black hole solutions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1103.1240",
        "title": "Electromagnetic Oscillations in a Spherical Conducting Cavity with\n  Dielectric Layers. Application to Linear Accelerators",
        "abstract": "We present an analysis of electromagnetic oscillations in a spherical\n conducting cavity filled concentrically with either dielectric or vacuum\n layers. The fields are given analytically, and the resonant frequency is\n determined numerically. An important special case of a spherical conducting\n cavity with a smaller dielectric sphere at its center is treated in more\n detail. By numerically integrating the equations of motion we demonstrate that\n the transverse electric oscillations in such cavity can be used to accelerate\n strongly relativistic electrons. The electron's trajectory is assumed to be\n nearly tangential to the dielectric sphere. We demonstrate that the interaction\n of such electrons with the oscillating magnetic field deflects their trajectory\n from a straight line only slightly. The Q factor of such a resonator only\n depends on losses in the dielectric. For existing ultra low loss dielectrics, Q\n can be three orders of magnitude better than obtained in existing cylindrical\n cavities.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2102.01532",
        "title": "Photon-jet events as a probe of axion-like particles at the LHC",
        "abstract": "Axion-like particles (ALPs) are predicted by many extensions of the Standard\n Model (SM). When ALP mass lies in the range of MeV to GeV, the cosmology and\n astrophysics will be largely irrelevant. In this work, we investigate such\n light ALPs through the ALP-strahlung process $pp \\to V a (\\to \\gamma\\gamma)$ at\n the 14 TeV LHC with an integrated luminosity of 3000 fb$^{-1}$ (HL-LHC). With\n the photon-jet algorithm, we demonstrate that our approach can probe the mass\n range of ALPs, which is inaccessible to previous LHC experiments. The obtained\n result can surpass the existing limits on ALP-photon coupling in the ALP mass\n range from 0.3 GeV to 10 GeV.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2404.03940",
        "title": "Towards introspective loop closure in 4D radar SLAM",
        "abstract": "Imaging radar is an emerging sensor modality in the context of Localization\n and Mapping (SLAM), especially suitable for vision-obstructed environments.\n This article investigates the use of 4D imaging radars for SLAM and analyzes\n the challenges in robust loop closure. Previous work indicates that 4D radars,\n together with inertial measurements, offer ample information for accurate\n odometry estimation. However, the low field of view, limited resolution, and\n sparse and noisy measurements render loop closure a significantly more\n challenging problem. Our work builds on the previous work - TBV SLAM - which\n was proposed for robust loop closure with 360$^\\circ$ spinning radars. This\n article highlights and addresses challenges inherited from a directional 4D\n radar, such as sparsity, noise, and reduced field of view, and discusses why\n the common definition of a loop closure is unsuitable. By combining multiple\n quality measures for accurate loop closure detection adapted to 4D radar data,\n significant results in trajectory estimation are achieved; the absolute\n trajectory error is as low as 0.46 m over a distance of 1.8 km, with consistent\n operation over multiple environments.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.12755",
        "title": "Low Scale Leptogenesis in Singlet-Triplet Scotogenic Model",
        "abstract": "The scotogenic model presents an elegant and succinct framework for\n elucidating the origin of tiny neutrino masses within the framework of the\n Standard Model, employing radiative corrections within the domain of the dark\n sector. We investigate the possibility of achieving low-scale leptogenesis in\n the singlet-triplet scotogenic model (STSM), where dark matter mediates\n neutrino mass generation. We initially considered a scenario involving two\n moderately hierarchical heavy fermions, N and $\\Sigma$, wherein the lepton\n asymmetry is generated by the out-of-equilibrium decay of both particles. Our\n analysis indicates that the scale of leptogenesis in this scenario is similar\n to that of standard thermal leptogenesis and is approximately $M_{N,\\Sigma}\\sim\n 10^{9}$ GeV, which is comparable to the Type-I seesaw case. Further, we\n consider the case with three heavy fermions ($N_1$, $N_2$, and $\\Sigma$) with\n the hierarchy $M_{N_{1}} < M_{\\Sigma} \\ll M_{N_{2}}$, which yields the lower\n bound on heavy fermions up to 3.1 TeV, therefore significantly reduce the scale\n of the leptogenesis up to TeV scale. The only prerequisite is suppression in\n the $N_{1}$ and $\\Sigma$ Yukawa couplings, which causes suppressed washout\n effects and a small active neutrino mass of about $10^{-5}$ eV. This brings\n about the fascinating insight that experiments aiming to measure the absolute\n neutrino mass scale can test low-scale leptogenesis in the scotogenic model.\n Further, the hyperchargeless scalar triplet $\\Omega$ provides an additional\n contribution to mass of the $W$-boson explaining CDF-II result.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2207.09545",
        "title": "Pandora Box Problem with Nonobligatory Inspection: Hardness and\n  Approximation Scheme",
        "abstract": "Weitzman (1979) introduced the Pandora Box problem as a model for sequential\n search with inspection costs, and gave an elegant index-based policy that\n attains provably optimal expected payoff. In various scenarios, the searching\n agent may select an option without making a costly inspection. The variant of\n the Pandora box problem with non-obligatory inspection has attracted interest\n from both economics and algorithms researchers. Various simple algorithms have\n proved suboptimal, with the best known 0.8-approximation algorithm due to Guha\n et al. (2008). No hardness result for the problem was known.\n  In this work, we show that it is NP-hard to compute an optimal policy for\n Pandora's problem with nonobligatory inspection. We also give a polynomial-time\n approximation scheme (PTAS) that computes policies with an expected payoff at\n least $(1 - \\epsilon)$-fraction of the optimal, for arbitrarily small $\\epsilon\n > 0$. On the side, we show the decision version of the problem to be in NP.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1105.3203",
        "title": "Coulomb drag in graphene",
        "abstract": "We calculate theoretically the Coulomb drag resistivity for two graphene\n monolayers spatially separated by a distance \"$d$\". We show that the frictional\n drag induced by inter-layer electron-electron interaction goes asymptotically\n as $T^2/n^3$ and $T^2 \\ln(n)/n$ in the high-density ($k_F d \\gg 1$) and\n low-density ($k_F d \\ll 1$) limits, respectively.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2301.05951",
        "title": "Weak-Field Coherent Control of Ultrafast Molecule Making",
        "abstract": "Coherent control of ultrafast molecule making from colliding reactants is\n crucial for realizing coherent control of binary photoreactions (CCBP). To\n handle diverse excitation scenarios, feasibility with both weak and strong\n fields is essential. We experimentally demonstrate here the weak-field\n feasibility, achieving it even under thermally hot conditions typical of\n chemical reactions. The making of KAr molecules from hot pairs of colliding K\n and Ar atoms via resonance-mediated two-photon excitation is controlled by weak\n linearly-chirped femtosecond pulses. Negative chirps enhance the yield. Our\n experimental and ab initio results are in excellent agreement. New routes to\n CCBP are opened.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2403.11953",
        "title": "Advancing COVID-19 Detection in 3D CT Scans",
        "abstract": "To make a more accurate diagnosis of COVID-19, we propose a straightforward\n yet effective model. Firstly, we analyse the characteristics of 3D CT scans and\n remove the non-lung parts, facilitating the model to focus on lesion-related\n areas and reducing computational cost. We use ResNeSt50 as the strong feature\n extractor, initializing it with pretrained weights which have COVID-19-specific\n prior knowledge. Our model achieves a Macro F1 Score of 0.94 on the validation\n set of the 4th COV19D Competition Challenge $\\mathrm{I}$, surpassing the\n baseline by 16%. This indicates its effectiveness in distinguishing between\n COVID-19 and non-COVID-19 cases, making it a robust method for COVID-19\n detection.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0811.4688",
        "title": "Single-top Cross Section Measurements at ATLAS",
        "abstract": "The single-top production cross section is one third that of the top-pair\n production cross section at the LHC. During a year of data-taking, assuming an\n average luminosity of 10^33 cm-2 s-1 and a CMS energy of 14 TeV, the\n determination of the major contributions to the total single-top cross section\n should be achievable. Comparisons between the measured cross sections and the\n theoretical predictions will provide a crucial test of the standard model.\n These measurements should also lead to the first direct measurement of |V_tb|,\n with a precision at the level of a few percent. In addition, they will probe\n for new physics via the search for evidence of anomalous couplings to the top\n quark and the measurements of additional bosonic contributions to single-top\n production. Methods developed to optimize the selection of single-top events in\n the three production channels are presented and the potential for the cross\n section measurements with 1 fb-1 and 30 fb-1 of integrated luminosity is\n established.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2010.04094",
        "title": "Determinants and Limit Systems in some Idempotent and Non-Associative\n  Algebraic Structure",
        "abstract": "This paper considers an idempotent and symmetrical algebraic structure as\n well as some closely related concept. A special notion of determinant is\n introduced and a Cramer formula is derived for a class of limit systems derived\n from the Hadamard matrix product and we give the algebraic form of a sequence\n of hyperplanes passing through a finite number of points. Thereby, some\n standard results arising for Max-Times systems with nonnegative entries appear\n as a special case. The case of two sided systems is also analyzed. In addition,\n a notion of eigenvalue in limit is considered. It is shown that one can\n construct a special semi-continuous regularized polynomial to find the\n eigenvalues of a matrix with nonnegative entries.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1805.06565",
        "title": "Modification of Spin Ice Physics in Ho$_2$Ti$_2$O$_7$ Thin Films",
        "abstract": "We present an extensive study on the effect of substrate orientation, strain,\n stoichiometry and defects on spin ice physics in Ho$_2$Ti$_2$O$_7$ thin films\n grown onto yttria-stabilized-zirconia substrates. We find that growth in\n different orientations produces different strain states in the films. All films\n exhibit similar c-axis lattice parameters for their relaxed portions, which are\n consistently larger than the bulk value of 10.10 \\AA. Transmission electron\n microscopy reveals anti-site disorder and growth defects to be present in the\n films, but stuffing is not observed. The amount of disorder depends on the\n growth orientation, with the (110) film showing the least. Magnetization\n measurements at 1.8 K show the expected magnetic anisotropy and saturation\n magnetization values associated with a spin ice for all orientations; shape\n anisotropy is apparent when comparing in and out-of-plane directions.\n Significantly, only the (110) oriented films display the hallmark spin ice\n plateau state in magnetization, albeit less well-defined compared to the\n plateau observed in a single crystal. Neutron scattering maps on the more\n disordered (111) oriented films show the Q=0 phase previously observed in bulk\n materials, but the Q=X phase giving the plateau state remains elusive. We\n conclude that the spin ice physics in thin films is modified by defects and\n strain, leading to a reduction in the temperature at which correlations drive\n the system into the spin ice state.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-th/9710159",
        "title": "The Second-Order Equation from the (1/2,0)+ (0,1/2) Representation of\n  the Poincare Group",
        "abstract": "On the basis of the first principles we derive the Barut-Wilson-Fushchich\n second-order equation in the (1/2,0)+(0,1/2) representation. Then we discuss\n the possibility of the description of various mass and spin states in such a\n framework.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1910.11247",
        "title": "A Bayesian Approach to Recurrence in Neural Networks",
        "abstract": "We begin by reiterating that common neural network activation functions have\n simple Bayesian origins. In this spirit, we go on to show that Bayes's theorem\n also implies a simple recurrence relation; this leads to a Bayesian recurrent\n unit with a prescribed feedback formulation. We show that introduction of a\n context indicator leads to a variable feedback that is similar to the forget\n mechanism in conventional recurrent units. A similar approach leads to a\n probabilistic input gate. The Bayesian formulation leads naturally to the two\n pass algorithm of the Kalman smoother or forward-backward algorithm, meaning\n that inference naturally depends upon future inputs as well as past ones.\n Experiments on speech recognition confirm that the resulting architecture can\n perform as well as a bidirectional recurrent network with the same number of\n parameters as a unidirectional one. Further, when configured explicitly\n bidirectionally, the architecture can exceed the performance of a conventional\n bidirectional recurrence.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1609.06037",
        "title": "Systematic Theoretical Study on the Interstellar Carbon Chain Molecules",
        "abstract": "In an effort to further our interest in understanding basic chemistry of\n interstellar molecules, we carry out here an extensive investigation of the\n stabilities of interstellar carbon chains; Cn, H2Cn, HCnN and CnX (X=N, O, Si,\n S, H, P, H-, N-). These sets of molecules accounts for about 20% of all the\n known interstellar and circumstellar molecules, their high abundances therefore\n demand a serious attention. High level ab initio quantum chemical calculations\n are employed to accurately estimate enthalpy of formation, chemical reactivity\n indices; global hardness and softness; and other chemical parameters of these\n molecules. Chemical modeling of the abundances of these molecular species has\n also been performed. Of the 89 molecules considered from these groups, 47 have\n been astronomically observed, these observed molecules are found to be more\n stable with respect to other members of the group. Of the 47 observed\n molecules, 60% are odd number carbon chains. Interstellar chemistry is not\n actually driven by the thermodynamics, it is primarily dependent on various\n kinetic parameters. However, we found that the detectability of the odd\n numbered carbon chains could be correlated due to the fact that they are more\n stable than the corresponding even numbered carbon chains. Based on this\n aspect, the next possible carbon chain molecule for astronomical observation in\n each group is proposed. The effect of kinetics in the formation of some of\n these carbon chain molecules is also discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/gr-qc/0410117",
        "title": "The Bohm Interpretation of Quantum Cosmology",
        "abstract": "I make a review on the aplications of the Bohm-De Broglie interpretation of\n quantum mechanics to quantum cosmology. In the framework of minisuperspaces\n models, I show how quantum cosmological effects in Bohm's view can avoid the\n initial singularity, isotropize the Universe, and even be a cause for the\n present observed acceleration of the Universe. In the general case, we\n enumerate the possible structures of quantum space and time.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2202.13310",
        "title": "Attention-based Cross-Layer Domain Alignment for Unsupervised Domain\n  Adaptation",
        "abstract": "Unsupervised domain adaptation (UDA) aims to learn transferable knowledge\n from a labeled source domain and adapts a trained model to an unlabeled target\n domain. To bridge the gap between source and target domains, one prevailing\n strategy is to minimize the distribution discrepancy by aligning their semantic\n features extracted by deep models. The existing alignment-based methods mainly\n focus on reducing domain divergence in the same model layer. However, the same\n level of semantic information could distribute across model layers due to the\n domain shifts. To further boost model adaptation performance, we propose a\n novel method called Attention-based Cross-layer Domain Alignment (ACDA), which\n captures the semantic relationship between the source and target domains across\n model layers and calibrates each level of semantic information automatically\n through a dynamic attention mechanism. An elaborate attention mechanism is\n designed to reweight each cross-layer pair based on their semantic similarity\n for precise domain alignment, effectively matching each level of semantic\n information during model adaptation. Extensive experiments on multiple\n benchmark datasets consistently show that the proposed method ACDA yields\n state-of-the-art performance.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2102.07767",
        "title": "Communication-efficient Distributed Cooperative Learning with Compressed\n  Beliefs",
        "abstract": "We study the problem of distributed cooperative learning, where a group of\n agents seeks to agree on a set of hypotheses that best describes a sequence of\n private observations. In the scenario where the set of hypotheses is large, we\n propose a belief update rule where agents share compressed (either sparse or\n quantized) beliefs with an arbitrary positive compression rate. Our algorithm\n leverages a unified communication rule that enables agents to access\n wide-ranging compression operators as black-box modules. We prove the almost\n sure asymptotic exponential convergence of beliefs around the set of optimal\n hypotheses. Additionally, we show a non-asymptotic, explicit, and linear\n concentration rate in probability of the beliefs on the optimal hypothesis set.\n We provide numerical experiments to illustrate the communication benefits of\n our method. The simulation results show that the number of transmitted bits can\n be reduced to 5-10% of the non-compressed method in the studied scenarios.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0006333",
        "title": "Continuous quantum measurement of two coupled quantum dots using a point\n  contact: A quantum trajectory approach",
        "abstract": "We obtain the finite-temperature unconditional master equation of the density\n matrix for two coupled quantum dots (CQD) when one dot is subjected to a\n measurement of its electron occupation number using a point contact (PC). To\n determine how the CQD system state depends on the actual current through the PC\n device, we use the so-called quantum trajectory method to derive the\n zero-temperature conditional master equation. We first treat the electron\n tunneling through the PC barrier as a classical stochastic point process (a\n quantum-jump model). Then we show explicitly that our results can be extended\n to the quantum-diffusive limit when the average electron tunneling rate is very\n large compared to the extra change of the tunneling rate due to the presence of\n the electron in the dot closer to the PC. We find that in both quantum-jump and\n quantum-diffusive cases, the conditional dynamics of the CQD system can be\n described by the stochastic Schr\\\"{o}dinger equations for its conditioned state\n vector if and only if the information carried away from the CQD system by the\n PC reservoirs can be recovered by the perfect detection of the measurements.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.15098",
        "title": "Fibrations by affine lines on rational affine surfaces with irreducible\n  boundaries",
        "abstract": "We consider fibrations by affine lines on smooth affine surfaces obtained as\n complements of smooth rational curves $B$ in smooth projective surfaces $X$\n defined over an algebraically closed field of characteristic zero. We observe\n that except for two exceptions, these surfaces $X \\setminus B$ admit infinitely\n many families of $\\mathbb{A}^1$-fibrations over the projective line with\n irreducible fibers and a unique singular fiber of arbitrarily large\n multiplicity. For $\\mathbb{A}^1$-fibrations over the affine line, we give a new\n and essentially self-contained proof that the set of equivalence classes of\n such fibrations up to composition by automorphisms at the source and target is\n finite if and only if the self-intersection number of $B$ in $X$ is less than\n or equal to 6.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/0110230",
        "title": "Computational AstroStatistics: Fast and Efficient Tools for Analysing\n  Huge Astronomical Data Sources",
        "abstract": "I present here a review of past and present multi-disciplinary research of\n the Pittsburgh Computational AstroStatistics (PiCA) group. This group is\n dedicated to developing fast and efficient statistical algorithms for analysing\n huge astronomical data sources. I begin with a short review of\n multi-resolutional kd-trees which are the building blocks for many of our\n algorithms. For example, quick range queries and fast n-point correlation\n functions. I will present new results from the use of Mixture Models (Connolly\n et al. 2000) in density estimation of multi-color data from the Sloan Digital\n Sky Survey (SDSS). Specifically, the selection of quasars and the automated\n identification of X-ray sources. I will also present a brief overview of the\n False Discovery Rate (FDR) procedure (Miller et al. 2001a) and show how it has\n been used in the detection of ``Baryon Wiggles'' in the local galaxy power\n spectrum and source identification in radio data. Finally, I will look forward\n to new research on an automated Bayes Network anomaly detector and the possible\n use of the Locally Linear Embedding algorithm (LLE; Roweis & Saul 2000) for\n spectral classification of SDSS spectra.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2004.06873",
        "title": "Verification of phased Dicke states",
        "abstract": "Dicke states are typical examples of quantum states with genuine multipartite\n entanglement. They are valuable resources in many quantum information\n processing tasks, including multiparty quantum communication and quantum\n metrology. Phased Dicke states are a generalization of Dicke states and include\n antisymmetric basis states as a special example. These states are useful in\n atomic and molecular physics besides quantum information processing. Here we\n propose practical and efficient protocols based on adaptive local projective\n measurements for verifying all phased Dicke states, including $W$ states and\n qudit Dicke states. To verify any $n$-partite phased Dicke state within\n infidelity $\\epsilon$ and significance level $\\delta$, the number of tests\n required is only $O(n\\epsilon^{-1}\\ln\\delta^{-1})$, which is linear in $n$ and\n is exponentially more efficient than traditional tomographic approaches. In the\n case of $W$ states, the number of tests can be further reduced to\n $O(\\sqrt{n}\\,\\epsilon^{-1}\\ln\\delta^{-1})$. Moreover, we construct an optimal\n protocol for any antisymmetric basis state; the number of tests required\n decreases (rather than increases) monotonically with $n$. This is the only\n optimal protocol known for multipartite nonstabilizer states.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2204.06923",
        "title": "A Unified Multi-task Learning Framework for Multi-goal Conversational\n  Recommender Systems",
        "abstract": "Recent years witnessed several advances in developing multi-goal\n conversational recommender systems (MG-CRS) that can proactively attract users'\n interests and naturally lead user-engaged dialogues with multiple\n conversational goals and diverse topics. Four tasks are often involved in\n MG-CRS, including Goal Planning, Topic Prediction, Item Recommendation, and\n Response Generation. Most existing studies address only some of these tasks. To\n handle the whole problem of MG-CRS, modularized frameworks are adopted where\n each task is tackled independently without considering their interdependencies.\n In this work, we propose a novel Unified MultI-goal conversational recommeNDer\n system, namely UniMIND. In specific, we unify these four tasks with different\n formulations into the same sequence-to-sequence (Seq2Seq) paradigm.\n Prompt-based learning strategies are investigated to endow the unified model\n with the capability of multi-task learning. Finally, the overall learning and\n inference procedure consists of three stages, including multi-task learning,\n prompt-based tuning, and inference. Experimental results on two MG-CRS\n benchmarks (DuRecDial and TG-ReDial) show that UniMIND achieves\n state-of-the-art performance on all tasks with a unified model. Extensive\n analyses and discussions are provided for shedding some new perspectives for\n MG-CRS.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/quant-ph/0404091",
        "title": "Ensemble teleportation",
        "abstract": "The possibility of teleportation is by sure the most interesting consequence\n of quantum non-separability. So far, however, teleportation schemes have been\n formulated by use of state vectors and considering individual entities only. In\n the present article the feasibility of teleportation is examined on the basis\n of the rigorous ensemble interpretation of quantum mechanics (not to be\n confused with a mere treatment of noisy EPR pairs) leading to results which are\n unexpected from the usual point of view.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/mtrl-th/9508011",
        "title": "Theoretical Study of One-dimensional Chains of Metal Atoms in Nanotubes",
        "abstract": "Using first-principles total-energy pseudopotential calculations, we have\n studied the properties of chains of potassium and aluminum in nanotubes. For BN\n tubes, there is little interaction between the metal chains and the tubes, and\n the conductivity of these tubes is through carriers located at the inner part\n of the tube. In contrast, for small radius carbon nanotubes, there are two\n types of interactions: charge-transfer (dominant for alkali atoms) leading to\n strong ionic cohesion, and hybridization (for multivalent metal atoms)\n resulting in a smaller cohesion. For Al-atomic chains in carbon tubes, we show\n that both effects contribute. New electronic properties related to these\n confined atomic chains of metal are analyzed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1409.2629",
        "title": "A note on the Gauss curvature flow",
        "abstract": "Using polar convex bodies and the $C_0$-bounds from Guan and Ni \\cite{PL}, we\n obtain a uniform lower bound on the Gauss curvature of the normalized solution\n of the Gauss curvature flow without using Chow's Harnack inequality \\cite{Ch2}.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1802.06864",
        "title": "The NWRA Classification Infrastructure: Description and Extension to the\n  Discriminant Analysis Flare Forecasting System (DAFFS)",
        "abstract": "A classification infrastructure built upon Discriminant Analysis has been\n developed at NorthWest Research Associates for examining the statistical\n differences between samples of two known populations. Originating to examine\n the physical differences between flare-quiet and flare-imminent solar active\n regions, we describe herein some details of the infrastructure including:\n parametrization of large datasets, schemes for handling \"null\" and \"bad\" data\n in multi-parameter analysis, application of non-parametric multi-dimensional\n Discriminant Analysis, an extension through Bayes' theorem to probabilistic\n classification, and methods invoked for evaluating classifier success. The\n classifier infrastructure is applicable to a wide range of scientific questions\n in solar physics. We demonstrate its application to the question of\n distinguishing flare-imminent from flare-quiet solar active regions, updating\n results from the original publications that were based on different data and\n much smaller sample sizes. Finally, as a demonstration of \"Research to\n Operations\" efforts in the space-weather forecasting context, we present the\n Discriminant Analysis Flare Forecasting System (DAFFS), a near-real-time\n operationally-running solar flare forecasting tool that was developed from the\n research-directed infrastructure.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1105.1193",
        "title": "Response of graphene to femtosecond high-intensity laser irradiation",
        "abstract": "We study the response of graphene to high-intensity 10^11-10^12 Wcm^-2,\n 50-femtosecond laser pulse excitation. We establish that graphene has a fairly\n high (~3\\times10^12Wcm^-2) single-shot damage threshold. Above this threshold,\n a single laser pulse cleanly ablates graphene, leaving microscopically defined\n edges. Below this threshold, we observe laser-induced defect formation that\n leads to degradation of the lattice over multiple exposures. We identify the\n lattice modification processes through in-situ Raman microscopy. The effective\n lifetime of CVD graphene under femtosecond near-IR irradiation and its\n dependence on laser intensity is determined. These results also define the\n limits of non-linear applications of graphene in femtosecond high-intensity\n regime.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1712.06402",
        "title": "Taylor series and twisting-index invariants of coupled spin-oscillators",
        "abstract": "About six years ago, semitoric systems on 4-dimensional manifolds were\n classified by Pelayo & Vu Ngoc by means of five invariants. A standard example\n of such a system is the coupled spin-oscillator on $\\mathbb{S}^2 \\times\n \\mathbb{R}^2$. Calculations of three of the five semitoric invariants of this\n system (namely the number of focus-focus singularities, the generalised\n semitoric polygon, and the height invariant) already appeared in the\n literature, but the so-called twisting index was not yet computed and, of the\n so-called Taylor series invariant, only the linear terms were known.\n  In the present paper, we complete the list of invariants for the coupled\n spin-oscillator by calculating higher order terms of the Taylor series\n invariant and by computing the twisting index. Moreover, we prove that the\n Taylor series invariant has certain symmetry properties that make the even\n powers in one of the variables vanish and allow us to show superintegrability\n of the coupled spin-oscillator on the zero energy level.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1711.03085",
        "title": "Combination of KLOE\n  $\\sigma\\big(e^+e^-\\rightarrow\\pi^+\\pi^-\\gamma(\\gamma)\\big)$ measurements and\n  determination of $a_{\\mu}^{\\pi^+\\pi^-}$ in the energy range $0.10 < s < 0.95$\n  GeV$^2$",
        "abstract": "The three precision measurements of the cross section\n $\\sigma\\big(e^+e^-\\rightarrow\\pi^+\\pi^-\\gamma(\\gamma)\\big)$ using initial state\n radiation by the KLOE collaboration provide an important input for the\n prediction of the hadronic contribution to the anomalous magnetic moment of the\n muon. These measurements are correlated for both statistical and systematic\n uncertainties and, therefore, the simultaneous use of these measurements\n requires covariance matrices that fully describe the correlations. We present\n the construction of these covariance matrices and use them to determine a\n combined KLOE measurement for\n $\\sigma\\big(e^+e^-\\rightarrow\\pi^+\\pi^-\\gamma(\\gamma)\\big)$. We find, from this\n combination, a two-pion contribution to the muon magnetic anomaly in the energy\n range $0.10 < s < 0.95$ GeV$^2$ of $a_{\\mu}^{\\pi^+\\pi^-} = (489.8 \\pm 1.7_{\\rm\n stat} \\pm 4.8_{\\rm sys} ) \\times 10^{-10}$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0907.5194",
        "title": "Ultra High Energy Cosmic Rays: The disappointing model",
        "abstract": "We develop a model for explaining the data of Pierre Auger Observatory\n (Auger) for Ultra High Energy Cosmic Rays (UHECR), in particular, the mass\n composition being steadily heavier with increasing energy from 3 EeV to 35 EeV.\n The model is based on the proton-dominated composition in the energy range (1 -\n 3) EeV observed in both Auger and HiRes experiments. Assuming extragalactic\n origin of this component, we argue that it must disappear at higher energies\n due to a low maximum energy of acceleration, E_p^{\\max} \\sim (4 - 10) EeV.\n Under an assumption of rigidity acceleration mechanism, the maximum\n acceleration energy for a nucleus with the charge number Z is ZE_p^{\\max}, and\n the highest energy in the spectrum, reached by Iron, does not exceed (100 -\n 200) EeV. The growth of atomic weight with energy, observed in Auger, is\n provided by the rigidity mechanism of acceleration, since at each energy\n E=ZE_p^{\\max} the contribution of nuclei with Z' < Z vanishes. The described\n model has disappointing consequences for future observations in UHECR: Since\n average energies per nucleon for all nuclei are less than (2 - 4) EeV, (i) pion\n photo-production on CMB photons in extragalactic space is absent; (ii) GZK\n cutoff in the spectrum does not exist; (iii) cosmogenic neutrinos produced on\n CMBR are absent; (iv) fluxes of cosmogenic neutrinos produced on infrared -\n optical background radiation are too low for registration by existing detectors\n and projects. Due to nuclei deflection in galactic magnetic fields, the\n correlation with nearby sources is absent even at highest energies.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1411.1572",
        "title": "A stochastic model of the production of multiple proteins in cells",
        "abstract": "The production processes of proteins in prokaryotic cells are investigated.\n Most of the mathematical models in the literature study the production of {\\em\n one} fixed type of proteins. When several classes of proteins are considered,\n an important additional aspect has to be taken into account, the limited common\n resources of the cell (polymerases and ribosomes) used by the production\n process. Understanding the impact of this limitation is a key issue in this\n domain. In this paper we focus on the allocation of ribosomes in the case of\n the production of multiple proteins. The cytoplasm of the cell being a\n disorganized medium subject to thermal noise, the protein production process\n has an important stochastic component. For this reason, a Markovian model of\n this process is introduced. Asymptotic results of the equilibrium are obtained\n under a scaling procedure and a realistic biological assumption of saturation\n of the ribosomes available in the cell. It is shown in particular that, in the\n limit, the number of non-allocated ribosomes at equilibrium converges in\n distribution to a Poisson distribution whose parameter satisfies a fixed point\n equation. It is also shown that the production process of different types of\n proteins can be seen as independent production processes but with modified\n parameters.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1503.07755",
        "title": "On spinless null propagation in five dimensional space-times with\n  approximate space-like Killing symmetry",
        "abstract": "Five-dimensional (5D) space-time symmetry greatly facilitates how a 4D\n observer perceives the propagation of a single spinless particle in a 5D\n space-time. In particular, if the 5D geometry is independent of the fifth\n coordinate then the 5D physics may be interpreted as 4D quantum mechanics. In\n this work we address the case where symmetry is approximate, focusing on the\n case where the 5D geometry depends weakly on the fifth coordinate. We show that\n concepts developed for the case of exact symmetry approximately hold when other\n concepts such as decaying quantum states, resonant quantum scattering and\n Stokes drag are adopted, as well. We briefly comment on the optical model of\n the nuclear interactions and Millikan's oil drop experiment.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1410.1904",
        "title": "CI-RMBPT calculations of photoionization cross sections from\n  quasi-continuum oscillator strengths",
        "abstract": "Many applications are in need of accurate photoionization cross-sections,\n especially in the case of complex atoms. Configuration-interaction relativistic\n many-body perturbation theory (CI-RMBPT) that has been successful in predicting\n atomic energies, matrix elements between discrete states, and other properties\n is quite promising, but it has not been applied to photo-ionization problems\n owing to extra complications arising from continuum states. In this paper a\n method that will allow the conversion of discrete CI-(R)MPBT oscillator\n strengths (OS) to photo-ionization cross sections with minimal modifications of\n the codes is introduced and CI-RMBPT cross sections of Ne, Ar, Kr, Xe are\n calculated. A consistent agreement with experiment is found. RMBPT corrections\n are particularly significant for Ar, Kr, and Xe and improve agreement with\n experiment compared to the particle-hole CI method. The demonstrated conversion\n method can be applied to CI-RMBPT photo-ionization calculations for a large\n number of multi-valence atoms and ions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/9805137",
        "title": "Optimization by Move--Class Deflation",
        "abstract": "A new approach to combinatorial optimization based on systematic move-class\n deflation is proposed. The algorithm combines heuristics of genetic algorithms\n and simulated annealing, and is mainly entropy-driven. It is tested on two\n problems known to be NP hard, namely the problem of finding ground states of\n the SK spin--glass and of the 3-$D$ $\\pm J$ spin-glass. The algorithm is\n sensitive to properties of phase spaces of complex systems other than those\n explored by simulated annealing, and it may therefore also be used as a\n diagnostic instrument. Moreover, dynamic freezing transitions, which are well\n known to hamper the performance of simulated annealing in the large system\n limit are not encountered by the present setup.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2207.00565",
        "title": "Efficient and Scalable GaInAs Thermophotovoltaic Devices",
        "abstract": "Thermophotovoltaics are promising solid-state energy converters for a variety\n of applications such as grid-scale energy storage, concentrating solar-thermal\n power, and waste heat recovery. Here, we report the design, fabrication, and\n testing of large area (0.8 cm$^2$), scalable, single junction 0.74-eV GaInAs\n thermophotovoltaic devices reaching an efficiency of 38.8$\\pm$2.0% and an\n electrical power density of 3.78 W/cm$^2$ at an emitter temperature of\n 1850{\\deg}C. Reaching such a high emitter temperature and power density without\n sacrificing efficiency is a direct result of combining good spectral management\n with a highly optimized cell architecture, excellent material quality, and very\n low series resistance. Importantly, fabrication of 12 high-performing devices\n on a two-inch wafer is shown to be repeatable, and the cell design can be\n readily transferred to commercial epitaxy on even larger wafers. Further\n improvements in efficiency can be obtained by using a multijunction\n architecture, and early results for a two-junction 0.84-eV GaInPAs / 0.74-eV\n GaInAs device illustrate this promise.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1209.2171",
        "title": "A Generalization of the Convex Kakeya Problem",
        "abstract": "Given a set of line segments in the plane, not necessarily finite, what is a\n convex region of smallest area that contains a translate of each input segment?\n This question can be seen as a generalization of Kakeya's problem of finding a\n convex region of smallest area such that a needle can be rotated through 360\n degrees within this region. We show that there is always an optimal region that\n is a triangle, and we give an optimal \\Theta(n log n)-time algorithm to compute\n such a triangle for a given set of n segments. We also show that, if the goal\n is to minimize the perimeter of the region instead of its area, then placing\n the segments with their midpoint at the origin and taking their convex hull\n results in an optimal solution. Finally, we show that for any compact convex\n figure G, the smallest enclosing disk of G is a smallest-perimeter region\n containing a translate of every rotated copy of G.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0906.3004",
        "title": "A product of integer partitions",
        "abstract": "I present a bijection on integer partitions that leads to recursive\n expressions, closed formulae and generating functions for the cardinality of\n certain sets of partitions of a positive integer $n$. The bijection leads also\n to a product on partitions that is associative with a natural grading thus\n defining a free associative algebra on the set of integer partitions. As an\n outcome of the computations, certain sets of integers appear that I call\n difference sets and the product of the integers in a difference set is an\n invariant for a family of sets of partitions. The main combinatorial objects\n used in these constructions are the central hooks of the Ferrers diagrams of\n partitions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2003.06335",
        "title": "Cucker-Smale type dynamics of infinitely many individuals with repulsive\n  forces",
        "abstract": "We study the existence and uniqueness of the time evolution of a system of\n infinitely many individuals, moving in a tunnel and subjected to a Cucker-Smale\n type alignment dynamics with compactly supported communication kernels and to\n short-range repulsive interactions to avoid collisions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1107.3302",
        "title": "A Temporal Neuro-Fuzzy Monitoring System to Manufacturing Systems",
        "abstract": "Fault diagnosis and failure prognosis are essential techniques in improving\n the safety of many manufacturing systems. Therefore, on-line fault detection\n and isolation is one of the most important tasks in safety-critical and\n intelligent control systems. Computational intelligence techniques are being\n investigated as extension of the traditional fault diagnosis methods. This\n paper discusses the Temporal Neuro-Fuzzy Systems (TNFS) fault diagnosis within\n an application study of a manufacturing system. The key issues of finding a\n suitable structure for detecting and isolating ten realistic actuator faults\n are described. Within this framework, data-processing interactive software of\n simulation baptized NEFDIAG (NEuro Fuzzy DIAGnosis) version 1.0 is developed.\n  This software devoted primarily to creation, training and test of a\n classification Neuro-Fuzzy system of industrial process failures. NEFDIAG can\n be represented like a special type of fuzzy perceptron, with three layers used\n to classify patterns and failures. The system selected is the workshop of\n SCIMAT clinker, cement factory in Algeria.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2207.02046",
        "title": "On Trapped Modes In Variable White Dwarfs As Probes Of The\n  $^{12}$C($\\alpha, \\gamma$)$^{16}$O Reaction Rate",
        "abstract": "We seek signatures of the current experimental\n $^{12}$C$(\\alpha,\\gamma)^{16}$O reaction rate probability distribution function\n in the pulsation periods of carbon-oxygen white dwarf models. We find that\n adiabatic g-modes trapped by the interior carbon-rich layer offer potentially\n useful signatures of this reaction rate probability distribution function.\n Probing the carbon-rich region is relevant because it forms during the\n evolution of low-mass stars under radiative helium burning conditions,\n mitigating the impact of convective mixing processes. We make direct\n quantitative connections between the pulsation periods of the identified\n trapped g-modes in variable WD models and the current experimental\n $^{12}$C$(\\alpha,\\gamma)^{16}$O reaction rate probability distribution\n function. We find an average spread in relative period shifts of $\\Delta P/P\n \\simeq \\pm$ 2\\% for the identified trapped g-modes over the $\\pm$ 3$\\sigma$\n uncertainty in the $^{12}$C$(\\alpha,\\gamma)^{16}$O reaction rate probability\n distribution function -- across the effective temperature range of observed DAV\n and DBV white dwarfs and for different white dwarf masses, helium shell masses,\n and hydrogen shell masses. The g-mode pulsation periods of observed white\n dwarfs are typically given to 6-7 significant figures of precision. This\n suggests that an astrophysical constraint on the\n $^{12}$C$(\\alpha,\\gamma)^{16}$O reaction rate could, in principle, be\n extractable from the period spectrum of observed variable white dwarfs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2109.00004",
        "title": "Ultraviolet-Infrared Mixing in Marginal Fermi Liquids",
        "abstract": "When Fermi surfaces (FSs) are subject to long-range interactions that are\n marginal in the renormalization-group sense, Landau Fermi liquids are\n destroyed, but only barely. With the interaction further screened by\n particle-hole excitations through one-loop quantum corrections, it has been\n believed that these marginal Fermi liquids (MFLs) are described by weakly\n coupled field theories at low energies. In this Letter, we point out a\n possibility in which higher-loop processes qualitatively change the picture\n through UV-IR mixing, in which the size of the FS enters as a relevant scale.\n The UV-IR mixing effect enhances the coupling at low energies, such that the\n basin of attraction for the weakly coupled fixed point of a $(2+1)$-dimensional\n MFL shrinks to a measure-zero set in the low-energy limit. This UV-IR mixing is\n caused by gapless virtual Cooper pairs that spread over the entire FS through\n marginal long-range interactions. Our finding signals a possible breakdown of\n the patch description for the MFL and questions the validity of using the MFL\n as the base theory in a controlled scheme for non-Fermi liquids that arise from\n relevant long-range interactions.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/math-ph/0512001",
        "title": "The Borg-Marchenko Theorem with a Continuous Spectrum",
        "abstract": "The Schr\\\"odinger equation is considered on the half line with a selfadjoint\n boundary condition when the potential is real valued, integrable, and has a\n finite first moment. It is proved that the potential and the two boundary\n conditions are uniquely determined by a set of spectral data containing the\n discrete eigenvalues for a boundary condition at the origin, the continuous\n part of the spectral measure for that boundary condition, and a subset of the\n discrete eigenvalues for a different boundary condition. This result provides a\n generalization of the celebrated uniqueness theorem of Borg and Marchenko using\n two sets of discrete spectra to the case where there is also a continuous\n spectrum. The proof employed yields a method to recover the potential and the\n two boundary conditions, and it also constructs data sets used in various\n inversion methods. A comparison is made with the uniqueness result of Gesztesy\n and Simon using Krein's spectral shift function as the inversion data.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1501.04244",
        "title": "Generalised Random Forest Space Overview",
        "abstract": "Assuming a view of the Random Forest as a special case of a nested ensemble\n of interchangeable modules, we construct a generalisation space allowing one to\n easily develop novel methods based on this algorithm. We discuss the role and\n required properties of modules at each level, especially in context of some\n already proposed RF generalisations.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2206.08294",
        "title": "Mixing time and expansion of non-negatively curved Markov chains",
        "abstract": "We establish three remarkable consequences of non-negative curvature for\n sparse Markov chains. First, their conductance decreases logarithmically with\n the number of states. Second, their displacement is at least diffusive until\n the mixing time. Third, they never exhibit the cutoff phenomenon. The first\n result provides a nearly sharp quantitative answer to a classical question of\n Ollivier, Milman and Naor. The second settles a conjecture of Lee and Peres for\n graphs with non-negative curvature. The third offers a striking counterpoint to\n the recently established cutoff for non-negatively curved chains with uniform\n expansion.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0907.2231",
        "title": "Existence of multi-pulses of the regularized short-pulse and Ostrovsky\n  equations",
        "abstract": "The existence of multi-pulse solutions near orbit-flip bifurcations of a\n primary single-humped pulse is shown in reversible, conservative, singularly\n perturbed vector fields. Similar to the non-singular case, the sign of a\n geometric condition that involves the first integral decides whether\n multi-pulses exist or not. The proof utilizes a combination of geometric\n singular perturbation theory and Lyapunov--Schmidt reduction through Lin's\n method. The motivation for considering orbit flips in singularly perturbed\n systems comes from the regularized short-pulse equation and the Ostrovsky\n equation, which both fit into this framework and are shown here to support\n multi-pulses.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2212.04947",
        "title": "Monitoring Spent Nuclear Fuel in a Dry Cask Using Momentum Integrated\n  Muon Scattering Tomography",
        "abstract": "Nuclear materials accountability and nonproliferation are among the critical\n tasks to be addressed for the advancement of nuclear energy in the United\n States. Monitoring spent nuclear fuel is important to continue reliable\n stewardship of SNF storage. Cosmic ray muons have been acknowledged a promising\n radiographic tool for monitoring SNF due to their highly penetrative nature and\n high energy. Cosmic ray muons are more suitable and have been used for imaging\n large and dense objects. Despite their potential in various applications, the\n wide application of cosmic ray muons is limited by the naturally low intensity\n at sea level. To efficiently utilize cosmic ray muons in engineering\n applications, trajectory and momentum must be measured. Although various\n studies demonstrate that there is significant potential for measuring momentum\n in muon applications, it is still difficult to measure both muon scattering\n angle and momentum in the field. To fill this critical gap, a muon spectrometer\n using multilayer pressurized gas Cherenkov radiators was proposed. However,\n existing muon tomographic algorithms were developed assuming monoenergetic muon\n scattering and are not optimized for a measured polyenergetic momentum\n spectrum. In this work, we develop and evaluate a momentum integrated muon\n scattering tomography algorithm. We evaluate the algorithm on its capability to\n identify a missing fuel assembly from a SNF dry cask. Our results demonstrate\n that image resolution using MMST is significantly improved when measuring muon\n momentum and it can reduce monitoring time by a factor of 10 when compared to\n that of a conventional muon imaging technique in terms of systematically\n finding a missing FA.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2003.10753",
        "title": "Nebulosities of the Symbiotic Binary R Aquarii -- A Short Review",
        "abstract": "In this proceeding, we present a short review of the fascinating nebulosities\n of symbiotic binary R Aquarii. The R Aquarii system, comprising the central\n binary and surrounding nebular material, has been the subject of\n near-continuous study since its discovery, with a few hundred papers listed in\n ADS. As such, it is impossible to provide here the comprehensive review that R\n Aquarii deserves, instead we chose to focus on the nebulosities -- covering\n both our own research and other relevant results from the literature.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0704.3673",
        "title": "Superconducting gap variations induced by structural supermodulation in\n  BSCCO",
        "abstract": "We discuss the possibility that the strain field introduced by the structural\n supermodulation in Bi-2212 and certain other cuprate materials may modulate the\n superconducting pairing interaction. We calculate the amplitude of this effect,\n visible in scanning tunneling spectroscopy experiments, and thereby relate a\n change in the local superconducting gap with the change in the local dopant\n displacements induced by the supermodulation. In principle, since this\n modulation is periodic, sufficiently accurate x-ray measurements or ab initio\n calculations should enable one to determine which atomic displacements enhance\n pairing and therefore T_c.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0904.3412",
        "title": "Horava-Lifshitz gravity, absolute time, and objective particles in\n  curved space",
        "abstract": "Recently, Horava formulated a renormalizable theory of quantum gravity that\n reduces to general relativity at large distances but violates Lorentz\n invariance at small distances. The absolute time involved in this theory allows\n to define an objective notion of particles associated with quantization of\n fields in classical gravitational backgrounds. The Unruh effect and other\n observer-dependent notions of particles in curved space are interpreted as\n effects caused by interaction between the objective vacuum and the measuring\n apparatus made up of objective particles.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/hep-ph/0408131",
        "title": "Constraints on proton structure from precision atomic physics\n  measurements",
        "abstract": "Ground-state hyperfine splittings in hydrogen and muonium are very well\n measured. Their difference, after correcting for magnetic moment and reduced\n mass effects, is due solely to proton structure--the large QED contributions\n for a pointlike nucleus essentially cancel. The rescaled hyperfine difference\n depends on the Zemach radius, a fundamental measure of the proton, computed as\n an integral over a product of electric and magnetic proton form factors. The\n determination of the Zemach radius, (1.043 +/- 0.016) fm, from atomic physics\n tightly constrains fits to accelerator measurements of proton form factors.\n Conversely, we can use muonium data to extract an ``experimental'' value for\n QED corrections to hydrogenic hyperfine data; we find that measurement and\n theory are consistent.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2101.11348",
        "title": "Low-Complexity Joint CFO and Channel Estimation for RIS-aided OFDM\n  Systems",
        "abstract": "Accurate channel estimation is essential for achieving the performance gains\n offered by reconfigurable intelligent surface (RIS)-aided wireless\n communications. A variety of channel estimation methods have been proposed for\n such systems; however, none of the existing methods takes into account the\n effect of synchronization errors such as carrier frequency offset (CFO). In\n general, CFO can significantly degrade channel estimation performance of\n orthogonal frequency division multiplexing (OFDM) systems. Motivated by this,\n we investigate the effect of CFO on channel estimation for RIS-aided OFDM\n systems. Furthermore, we propose a joint CFO and channel impulse response (CIR)\n estimation method for these systems. Simulation results demonstrate the\n effectiveness of our proposed method, and also demonstrate that the use of\n time-domain rather than frequency-domain estimation in this context results in\n an improvement in the mean-squared error (MSE) performance of channel\n estimation as well as a significantly lower overall computational complexity.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1605.09572",
        "title": "Simultaneous VLBA polarimetric observations of the v=$\\{$1,2$\\}$ J=1-0\n  and v=1, J=2-1 SiO maser emission toward VY CMa II: component-level\n  polarization analysis",
        "abstract": "This paper presents a component-level comparison of the polarized v=1 J =1-0,\n v=2 J=1-0 and v=1 J=2-1 SiO maser emission towards the supergiant star VY CMa\n at milliarcsecond-scale, as observed using the VLBA at $\\lambda=7$mm and\n $\\lambda=3$mm. An earlier paper considered overall maser morphology and\n constraints on SiO maser excitation and pumping derived from these data. The\n goal of the current paper is to use the measured polarization properties of\n individual co-spatial components detected across multiple transitions to\n provide constraints on several competing theories for the transport of\n polarized maser emission. This approach minimizes the significant effects of\n spatial blending. We present several diagnostic tests designed to distinguish\n key features of competing theoretical models for maser polarization. The number\n of coincident features is limited by sensitivity however, particularly in the\n v=1 J=2-1 transition at 86 GHz, and deeper observations are needed. Preliminary\n conclusions based on the current data provide some support for: i)\n spin-independent solutions for linear polarization; ii) the influence of\n geometry on the distribution of fractional linear polarization with intensity;\n and, iii) $\\pi/2$ rotations in linear polarization position angle arising from\n transitions across the Van Vleck angle ($\\sin^2{\\theta}=2/3$) between the maser\n line-of-sight and magnetic field. There is weaker evidence for several\n enumerated non-Zeeman explanations for circular polarization. The expected 2:1\n ratio in circular polarization between J=1-0 and J=2-1 predicted by standard\n Zeeman theory cannot unfortunately be tested conclusively due to insufficient\n coincident components.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cs/9912012",
        "title": "Avoiding Braess' Paradox through Collective Intelligence",
        "abstract": "In an Ideal Shortest Path Algorithm (ISPA), at each moment each router in a\n network sends all of its traffic down the path that will incur the lowest cost\n to that traffic. In the limit of an infinitesimally small amount of traffic for\n a particular router, its routing that traffic via an ISPA is optimal, as far as\n cost incurred by that traffic is concerned. We demonstrate though that in many\n cases, due to the side-effects of one router's actions on another routers\n performance, having routers use ISPA's is suboptimal as far as global aggregate\n cost is concerned, even when only used to route infinitesimally small amounts\n of traffic. As a particular example of this we present an instance of Braess'\n paradox for ISPA's, in which adding new links to a network decreases overall\n throughput. We also demonstrate that load-balancing, in which the routing\n decisions are made to optimize the global cost incurred by all traffic\n currently being routed, is suboptimal as far as global cost averaged across\n time is concerned. This is also due to \"side-effects\", in this case of current\n routing decision on future traffic.\n  The theory of COllective INtelligence (COIN) is concerned precisely with the\n issue of avoiding such deleterious side-effects. We present key concepts from\n that theory and use them to derive an idealized algorithm whose performance is\n better than that of the ISPA, even in the infinitesimal limit. We present\n experiments verifying this, and also showing that a machine-learning-based\n version of this COIN algorithm in which costs are only imprecisely estimated (a\n version potentially applicable in the real world) also outperforms the ISPA,\n despite having access to less information than does the ISPA. In particular,\n this COIN algorithm avoids Braess' paradox.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1809.06178",
        "title": "A Classification of BPMN Collaborations based on Safeness and Soundness\n  Notions",
        "abstract": "BPMN 2.0 standard has a huge uptake in modelling business processes within\n the same organisation or collaborations involving multiple interacting\n participants. It results that providing a solid foundation to enable BPMN\n designers to understand their models in a consistent way is becoming more and\n more important. In our investigation we define and exploit a formal\n characterisation of the collaborations' semantics, specifically and directly\n given for BPMN models, to provide a classification of BPMN collaborations. In\n particular, we refer to collaborations involving processes with arbitrary\n topology, thus overcoming the well-structuredness limitations. The proposed\n classification is based on some of the most important correctness properties in\n the business process domain, namely safeness and soundness. We prove, with a\n uniform formal framework, some conjectured and expected results and, most of\n all, we achieve novel results for BPMN collaborations concerning the\n relationships between safeness and soundness, and their compositionality, that\n represent major advances in the state-of-the-art.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/gr-qc/9511066",
        "title": "Scalar Field Inhomogeneous Cosmologies",
        "abstract": "Some exact solutions for the Einstein field equations corresponding to\n inhomogeneous $G_2$ cosmologies with an exponential-potential scalar field\n which generalize solutions obtained previously are considered. Several\n particular cases are studied and the properties related to generalized\n inflation and asymptotic behaviour of the models are discussed.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2309.04895",
        "title": "Super-compact universal quantum logic gates with inversedesigned\n  elements",
        "abstract": "Integrated quantum photonic circuit is a promising platform for the\n realization of quantum information processing in the future. To achieve the\n largescale quantum photonic circuits, the applied quantum logic gates should be\n as small as possible for the high-density integration on chips. Here, we report\n the implementation of super-compact universal quantum logic gates on silicon\n chips by the method of inverse design. In particular, the fabricated\n controlled-NOT gate and Hadamard gate are both nearly a vacuum wavelength,\n being the smallest optical quantum gates reported up to now. We further design\n the quantum circuit by cascading these fundamental gates to perform arbitrary\n quantum processing, where the corresponding size is about several orders\n smaller than that of previous quantum photonic circuits. Our study paves the\n way for the realization of largescale quantum photonic chips with integrated\n sources, and can possess important applications in the field of quantum\n information processes.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1803.05410",
        "title": "Effective non-linear spinor dynamics in a spin-1 Bose-Einstein\n  condensate",
        "abstract": "We derive from first principles the experimentally observed effective\n dynamics of a spinor Bose gas initially prepared as a Bose-Einstein condensate\n and then left free to expand ballistically. In spinor condensates, which\n represent one of the recent frontiers in the manipulation of ultra-cold atoms,\n particles interact with a two-body spatial interaction and a spin-spin\n interaction. The effective dynamics is governed by a system of coupled\n semi-linear Schr\\\"odinger equations: we recover this system, in the sense of\n marginals in the limit of infinitely many particles, with a mean-field\n re-scaling of the many-body Hamiltonian. When the resulting control of the\n dynamical persistence of condensation is quantified with the parameters of\n modern observations, we obtain a bound that remains quite accurate for the\n whole typical duration of the experiment.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2012.14900",
        "title": "Requirements and sensitivity analysis of RANS-free wall-modeled LES",
        "abstract": "We study the sensitivity of wall model input variables to the modeling\n choices of the outer LES. This work is motivated by sensitivities observed in\n dynamic slip wall models. These dynamic wall models use variables from the\n near-wall LES solution as inputs to predict the wall stress without relying on\n a priori coefficients or equilibrium assumption. Mitigating the sensitivities\n in the wall model inputs allows development of robust dynamic wall models. The\n effects of SGS model, boundary condition type, numerics, and mesh topology are\n assessed through a series of WMLES calculations of turbulent channels.\n Probability density functions are computed from planes of the WMLES solutions\n at a wall-normal sampling height and are used as a metric for sensitivity.\n Sensitivity to SGS model is alleviated when the fraction of total wall stress\n carried by the SGS model is held constant. Use of hexagonal close-packed grids\n mitigated numerical sensitivities.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2107.00318",
        "title": "Zero-pronoun Data Augmentation for Japanese-to-English Translation",
        "abstract": "For Japanese-to-English translation, zero pronouns in Japanese pose a\n challenge, since the model needs to infer and produce the corresponding pronoun\n in the target side of the English sentence. However, although fully resolving\n zero pronouns often needs discourse context, in some cases, the local context\n within a sentence gives clues to the inference of the zero pronoun. In this\n study, we propose a data augmentation method that provides additional training\n signals for the translation model to learn correlations between local context\n and zero pronouns. We show that the proposed method significantly improves the\n accuracy of zero pronoun translation with machine translation experiments in\n the conversational domain.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2106.06069",
        "title": "Concurrent multi-parameter learning demonstrated on the\n  Kuramoto-Sivashinsky equation",
        "abstract": "We develop an algorithm based on the nudging data assimilation scheme for the\n concurrent (on-the-fly) estimation of scalar parameters for a system of\n evolutionary dissipative partial differential equations in which the state is\n partially observed. The algorithm takes advantage of the error that results\n from nudging a system with incorrect parameters with data from the true system.\n The intuitive nature of the algorithm makes its extension to several different\n systems immediate, and it allows for recovery of multiple parameters\n simultaneously. We test the method on the Kuramoto-Sivashinsky equation in one\n dimension and demonstrate its efficacy in this context.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2211.11633",
        "title": "Open charm production and asymmetry in $p$Ne collisions at\n  $\\sqrt{s_{\\scriptscriptstyle\\rm NN}} =$ 68.5 GeV",
        "abstract": "A measurement of $D^0$ meson production by the LHCb experiment in its\n fixed-target configuration is presented. The production of $D^0$ mesons is\n studied with a beam of 2.5 TeV protons colliding on a gaseous neon target at\n rest, corresponding to a nucleon-nucleon centre-of-mass energy of $\\sqrt{s_{\\rm\n NN}}$ = 68.5 GeV. The sum of the $D^0$ and ${\\overline D^0}$ production\n cross-section in $p$Ne collisions in the centre-of-mass rapidity range\n $y^{\\star}\\in [-2.29, 0]$ is found to be $\\sigma_{D^{0}}^{y^\\star \\in [-2.29,\n 0]} = 48.2 \\pm 0.3 \\pm 4.5 \\,\\mu\\textrm{b/nucleon}$ where the first uncertainty\n is statistical and the second is systematic. The $D^0-{\\overline D^0}$\n production asymmetry is also evaluated and suggests a trend towards negative\n values at large negative $y^{\\star}$. The considered models do not account\n precisely for all the features observed in the LHCb data, but theoretical\n predictions including 1$\\%$ intrinsic charm and 10$\\%$ recombination\n contributions better describe the data than the other models considered.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1001.3055",
        "title": "Quantum work statistics of linear and nonlinear parametric oscillators",
        "abstract": "We consider the nonequilibrium work distribution of a quantum oscillator with\n modulated angular frequency. We examine the discrete-to-continuous transition\n of the distribution as the temperature and the degree of nonadiabaticity of the\n frequency transformation are increased. We further develop a perturbative\n approach to analyze the effect of weak quartic anharmonicities, as well as of a\n random electric field on a charged oscillator. We find in both cases that the\n degree of nonadiabaticity is enhanced by the perturbation.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1201.0549",
        "title": "Motion generates entanglement",
        "abstract": "We demonstrate entanglement generation between mode pairs of a quantum field\n in a nonuniformly accelerated cavity in Minkowski space-time. The effect is\n sensitive to the initial state, the choice of the mode pair and bosonic versus\n fermionic statistics, and it can be stronger by orders of magnitude than the\n entanglement degradation between a nonuniformly accelerated cavity and an\n inertial cavity. Detailed results are obtained for massless scalar and spinor\n fields in (1+1) dimensions. By the equivalence principle, the results provide a\n model of entanglement generation by gravitational effects.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2306.12359",
        "title": "The isoperimetric problem for convex hulls and the large deviations rate\n  functionals of random walks",
        "abstract": "We study the asymptotic shape of the most likely trajectories of a planar\n random walk that result in large deviations of the area of the convex hull of\n the first $n$ steps of the walk, as $n$ tends to infinity. If the increments of\n the walk have finite Laplace transform, such a scaled limit trajectory $h$\n solves the inhomogeneous anisotropic isoperimetric problem for the convex hull,\n where the usual length of $h$ is replaced by the large deviations rate\n functional $\\int_0^1 I(h'(t)) dt$ with $I$ being the rate function of the\n increments. Assuming that the distribution of increments is not contained in a\n half-plane, we show that the optimal trajectories are smooth, convex, and\n satisfy the Euler--Lagrange equation, which we solve explicitly for every $I$.\n Our solution resembles that of the isoperimetric problem in the Minkowski plane\n found by Busemann (1947).",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2001.08829",
        "title": "High dimensional expansion using zig-zag product",
        "abstract": "We wish to renew the discussion over recent combinatorial structures that are\n 3-uniform hypergraph expanders, viewing them in a more general perspective,\n shedding light on a previously unknown relation to the zig-zag product. We do\n so by introducing a new structure called triplet structure, that maintains the\n same local environment around each vertex. The structure is expected to yield,\n in some cases, a bounded family of hypergraph expanders whose 2-dimensional\n random walk converges. We have applied the results obtained here to several\n known constructions, obtaining a better expansion rate than previously known.\n Namely, we did so in the case of Conlon's construction and the $S=[1,1,0]$\n construction by Chapman, Linal and Peled.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1408.3776",
        "title": "Charging and heat collection by a positively charged dust grain in a\n  plasma",
        "abstract": "Dust particulates immersed in a quasineutral plasma can emit electrons in\n several important applications. Once electron emission becomes strong enough,\n the dust enters the positively charged regime where the conventional\n Orbital-Motion-Limited (OML) theory can break down due to potential well\n effects on trapped electrons. A minimal modification of the trapped-passing\n boundary approximation in the so-called OML$^+$ approach is shown to accurately\n predict the dust charge and heat collection flux for a wide range of dust size\n and temperature.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2207.04532",
        "title": "Stationary Anisotropic Stokes, Oseen, and Navier-Stokes Systems:\n  Periodic Solutions in $\\R^n$",
        "abstract": "First, the solution uniqueness, existence and regularity for stationary\n anisotropic (linear) Stokes and generalised Oseen systems with constant\n viscosity coefficients in a compressible framework are analysed in a range of\n periodic Sobolev (Bessel-potential) spaces on $n$-dimensional flat torus. By\n the Galerkin algorithm, the linear results are employed to show existence of\n solution to the stationary anisotropic (non-linear) Navier-Stokes\n incompressible system on torus in a periodic Sobolev space for any $n\\ge 2$.\n Then the solution uniqueness and regularity results for stationary anisotropic\n Navier-Stokes system on torus are established for $n\\in\\{2,3,4\\}$.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/cond-mat/0601166",
        "title": "Phase equilibrium in two orbital model under magnetic field",
        "abstract": "The phase equilibrium in manganites under magnetic field is studied using a\n two orbital model, based on the equivalent chemical potential principle for the\n competitive phases. We focus on the magnetic field induced melting process of\n CE phase in half-doped manganites. It is predicted that the homogenous CE phase\n begins to decompose into coexisting ferromagnetic phase and CE phase once the\n magnetic field exceeds the threshold field. In a more quantitative way, the\n volume fractions of the two competitive phases in the phase separation regime\n are evaluated.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1910.05026",
        "title": "Customizing Sequence Generation with Multi-Task Dynamical Systems",
        "abstract": "Dynamical system models (including RNNs) often lack the ability to adapt the\n sequence generation or prediction to a given context, limiting their real-world\n application. In this paper we show that hierarchical multi-task dynamical\n systems (MTDSs) provide direct user control over sequence generation, via use\n of a latent code $\\mathbf{z}$ that specifies the customization to the\n individual data sequence. This enables style transfer, interpolation and\n morphing within generated sequences. We show the MTDS can improve predictions\n via latent code interpolation, and avoid the long-term performance degradation\n of standard RNN approaches.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/astro-ph/9710115",
        "title": "Neutrino Interactions in Hot and Dense Matter",
        "abstract": "We study the charged and neutral current weak interaction rates relevant for\n the determination of neutrino opacities in dense matter found in supernovae and\n neutron stars. We establish an efficient formalism for calculating differential\n cross sections and mean free paths for interacting, asymmetric nuclear matter\n at arbitrary degeneracy. The formalism is valid for both charged and neutral\n current reactions. Strong interaction corrections are incorporated through the\n in-medium single particle energies at the relevant density and temperature. The\n effects of strong interactions on the weak interaction rates are investigated\n using both potential and effective field-theoretical models of matter. We\n investigate the relative importance of charged and neutral currents for\n different astrophysical situations, and also examine the influence of\n strangeness-bearing hyperons. Our findings show that the mean free paths are\n significantly altered by the effects of strong interactions and the\n multi-component nature of dense matter. The opacities are then discussed in the\n context of the evolution of the core of a protoneutron star.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2111.04122",
        "title": "Dynamical fixed points in holography",
        "abstract": "Typically, an interactive system evolves towards thermal equilibrium, with\n hydrodynamics representing a universal framework for its late-time dynamics.\n Classification of the dynamical fixed points (DFPs) of a driven Quantum Field\n Theory (with time dependent coupling constants, masses, external background\n fields, etc.) is unknown. We use holographic framework to analyze such fixed\n points in one example of strongly coupled gauge theory, driven by homogeneous\n and isotropic expansion of the background metric - equivalently, a late-time\n dynamics of the corresponding QFT in Friedmann-Lemaitre-Robertson-Walker\n Universe. We identify DFPs that are perturbatively stable, and those that are\n perturbatively unstable, computing the spectrum of the quasinormal modes in the\n corresponding holographic dual. We further demonstrate that a stable DFP can be\n unstable non-perturbatively, and explain the role of the entanglement entropy\n density as a litmus test for a non-perturbative stability. Finally, we\n demonstrated that a driven evolution might not have a fixed point at all: the\n entanglement entropy density of a system can grow without bounds.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1310.2264",
        "title": "Application of compressed sensing to genome wide association studies and\n  genomic selection",
        "abstract": "We show that the signal-processing paradigm known as compressed sensing (CS)\n is applicable to genome-wide association studies (GWAS) and genomic selection\n (GS). The aim of GWAS is to isolate trait-associated loci, whereas GS attempts\n to predict the phenotypic values of new individuals on the basis of training\n data. CS addresses a problem common to both endeavors, namely that the number\n of genotyped markers often greatly exceeds the sample size. We show using CS\n methods and theory that all loci of nonzero effect can be identified (selected)\n using an efficient algorithm, provided that they are sufficiently few in number\n (sparse) relative to sample size. For heritability h2 = 1, there is a sharp\n phase transition to complete selection as the sample size is increased. For\n heritability values less than one, complete selection can still occur although\n the transition is smoothed. The transition boundary is only weakly dependent on\n the total number of genotyped markers. The crossing of a transition boundary\n provides an objective means to determine when true effects are being recovered;\n we discuss practical methods for detecting the boundary. For h2 = 0.5, we find\n that a sample size that is thirty times the number of nonzero loci is\n sufficient for good recovery.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/nlin/0503053",
        "title": "The Evolution of Cellar Automaton based on Dilemmma Games with Selfish\n  Strategy",
        "abstract": "We have proposed two new evolutionary rules on spatio-iterated games that is\n not mimic evolution of strategies, and mainly discussed the Prisoner's Dilemma\n game \\cite{toyota2} by the two evoutionary rules \\cite{toyota3}. In this paper\n we focus the first rule, that is, the selfish evolutionary rule for various\n dilemma games. In contrast to the Prisoner's Dilemma, there are gererally rich\n pase structures in the dilemma games. First we analytically clear the structure\n to present phase diagrams in the various dilemma games. Forthermore we simulate\n the time evolution of the soatio-games in the some representatives of the\n parameters according to the phase diagrams.\n  Including some mutations, detail investigations are made by a computer\n simulation for five kinds of initial configurations. As results we find some\n dualities and game invariant properties. They show a sort of bifurcation as a\n mutation parameter are varied. In the path from one period to two one some\n common features are observed in most of games and some chaotic behaviors appear\n in the middle of the transition. Lastly we estimate the total hamiltonian,\n which is defined by the sum of the total payoff of all agents in the system,\n and show that the chaotic period is best from the perspective of the payoff. We\n also made some primitive discussions on them.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.03106",
        "title": "Oscillatory reconnection and waves driven by merging magnetic flux ropes\n  in solar flares",
        "abstract": "Oscillatory reconnection is a process that has been suggested to underlie\n several solar and stellar phenomena, and is likely to play an important role in\n transient events such as flares. Quasi-periodic pulsations (QPPs) in flare\n emissions may be a manifestation of oscillatory reconnection, but the\n underlying mechanisms remain uncertain. In this paper, we present 2D\n magnetohydrodynamic (MHD) simulations of two current-carrying magnetic flux\n ropes with an out-of-plane magnetic field undergoing oscillatory reconnection\n in which the two flux ropes merge into a single flux rope. We find that\n oscillatory reconnection can occur intrinsically without an external\n oscillatory driver during flux rope coalescence, which may occur both during\n large-scale coronal loop interactions and the merging of plasmoids in\n fragmented current sheets. Furthermore, we demonstrate that radially\n propagating non-linear waves are produced in the aftermath of flux rope\n coalescence, due to the post-reconnection oscillations of the merged flux rope.\n The behaviour of these waves is found to be almost independent of the initial\n out-of-plane magnetic field. It is estimated that the waves emitted through\n merging coronal loops and merging plasmoids in loop-top current sheets would\n have a typical phase speed of 90 km/s and 900 km/s respectively. It is possible\n that the properties of the waves emitted during flux rope coalescence could be\n used as a diagnostic tool to determine physical parameters within a coalescing\n region.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1607.00937",
        "title": "Temperature scaling of effective polaron mobility in energetically\n  disordered media",
        "abstract": "We study effective mobility in 2 dimensional (2D) and 3 dimensional (3D)\n systems, where hopping transitions of carriers are described by the Marcus\n equation under a Gaussian density of states in the dilute limit. Using an\n effective medium approximation (EMA), we determined the coefficient $C_d$ for\n the effective mobility expressed by $\\mu_{\\rm\n eff}\\propto\\exp\\left[-\\lambda/\\left(4 k_{\\rm B} T\\right)-\n C_d\\sigma^2/\\left(k_{\\rm B} T\\right)^2 \\right]/\\left[\\sqrt{\\lambda} (k_{\\rm B}\n T)^{3/2}\\right]$, where $\\lambda$ is the reorganization energy, $\\sigma$ is the\n standard deviation of the Gaussian density of states, and $k_{\\rm B} T$ takes\n its usual meaning. We found $C_d=1/2$ for both 2D and 3D. While various\n estimates of the coefficient $C_d$ for 3D systems are available in the\n literature, we provide for the first time the expected $C_d$ value for a 2D\n system. By means of kinetic Monte-Carlo simulations, we show that the effective\n mobility is well described by the equation shown above under certain conditions\n on $\\lambda$. We also give examples of analysis of experimental data for 2D and\n 3D systems based on our theoretical results.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1704.02513",
        "title": "Transcription factor clusters regulate genes in eukaryotic cells",
        "abstract": "Transcription is regulated through binding factors to gene promoters to\n activate or repress expression, however, the mechanisms by which factors find\n targets remain unclear. Using single-molecule fluorescence microscopy, we\n determined in vivo stoichiometry and spatiotemporal dynamics of a GFP tagged\n repressor, Mig1, from a paradigm signaling pathway of Saccharomyces cerevisiae.\n We find the repressor operates in clusters, which upon extracellular signal\n detection, translocate from the cytoplasm, bind to nuclear targets and\n turnover. Simulations of Mig1 configuration within a 3D yeast genome model\n combined with a promoter-specific, fluorescent translation reporter confirmed\n clusters are the functional unit of gene regulation. In vitro and structural\n analysis on reconstituted Mig1 suggests that clusters are stabilized by\n depletion forces between intrinsically disordered sequences. We observed\n similar clusters of a co-regulatory activator from a different pathway,\n supporting a generalized cluster model for transcription factors that reduces\n promoter search times through intersegment transfer while stabilizing gene\n expression.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2402.06926",
        "title": "On a mixed local-nonlocal evolution equation with singular nonlinearity",
        "abstract": "We will prove several existence and regularity results for the mixed\n local-nonlocal parabolic equation of the form \\begin{eqnarray} \\begin{split}\n u_t-\\Delta u+(-\\Delta)^s u&=\\frac{f(x,t)}{u^{\\gamma(x,t)}} \\text { in }\n \\Omega_T:=\\Omega \\times(0, T), \\\\ u&=0 \\text { in }(\\mathbb{R}^n \\backslash\n \\Omega) \\times(0, T), \\\\ u(x, 0)&=u_0(x) \\text { in } \\Omega ; \\end{split}\n \\end{eqnarray} where \\begin{equation*} (-\\Delta )^s u=\n c_{n,s}\\operatorname{P.V.}\\int_{\\mathbb{R}^n}\\frac{u(x,t)-u(y,t)}{|x-y|^{n+2s}}\n d y. \\end{equation*} Under the assumptions that $\\gamma$ is a positive\n continuous function on $\\overline{\\Omega}_T$ and $\\Omega$ is a bounded domain\n %of class $\\mathcal{C}^{1,1}$ with Lipschitz boundary in $\\mathbb{R}^{n}$, $n>\n 2$, $s\\in(0,1)$, $0<T<+\\infty$, $f\\geq 0$, $u_0\\geq 0$, $f$ and $u_0$ belongs\n to suitable Lebesgue spaces. Here $c_{n,s}$ is a suitable normalization\n constant, and $\\operatorname{P.V.}$ stands for Cauchy Principal Value.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2112.02962",
        "title": "DANets: Deep Abstract Networks for Tabular Data Classification and\n  Regression",
        "abstract": "Tabular data are ubiquitous in real world applications. Although many\n commonly-used neural components (e.g., convolution) and extensible neural\n networks (e.g., ResNet) have been developed by the machine learning community,\n few of them were effective for tabular data and few designs were adequately\n tailored for tabular data structures. In this paper, we propose a novel and\n flexible neural component for tabular data, called Abstract Layer (AbstLay),\n which learns to explicitly group correlative input features and generate\n higher-level features for semantics abstraction. Also, we design a structure\n re-parameterization method to compress the learned AbstLay, thus reducing the\n computational complexity by a clear margin in the reference phase. A special\n basic block is built using AbstLays, and we construct a family of Deep Abstract\n Networks (DANets) for tabular data classification and regression by stacking\n such blocks. In DANets, a special shortcut path is introduced to fetch\n information from raw tabular features, assisting feature interactions across\n different levels. Comprehensive experiments on seven real-world tabular\n datasets show that our AbstLay and DANets are effective for tabular data\n classification and regression, and the computational complexity is superior to\n competitive methods. Besides, we evaluate the performance gains of DANet as it\n goes deep, verifying the extendibility of our method. Our code is available at\n https://github.com/WhatAShot/DANet.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2305.14422",
        "title": "Identifying non-Abelian anyons with upstream noise",
        "abstract": "Non-Abelian phases are among the most highly-sought states of matter, with\n those whose anyons permit universal quantum gates constituting the ultimate\n prize. The most promising candidate of such a phase is the fractional quantum\n Hall plateau at filling factors $\\nu=\\frac{12}{5}$, which putatively\n facilitates Fibonacci anyons. Experimental validation of this assertion poses a\n major challenge and remains elusive. We present a measurement protocol that\n could achieve this goal with already-demonstrated experimental techniques.\n Interfacing the $\\nu=\\frac{12}{5}$ state with any readily-available Abelian\n state yields a binary outcome of upstream noise or no noise. Judicious choices\n of the Abelian states can produce a sequence of yes--no outcomes that\n fingerprint the possible non-Abelian phase by ruling out its competitors.\n Crucially, this identification is insensitive to the precise value of the\n measured noise and can uniquely identify the anyon type at filling factors\n $\\nu=\\frac{12}{5}$. In addition, it can distinguish any non-Abelian candidates\n at half-filling in graphene and semiconductor heterostructures.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2204.02206",
        "title": "Hot 2DHG states in tellurium",
        "abstract": "Element semiconductor Te is very popular in both fundamental electronic\n structure study, and device fabrication research area due to its unique band\n structure. Specifically, in low temperatures, Te possesses strong quantum\n oscillations with magnetic field applied in basal plane, either following\n Shubnikov-de Haas (SdH) oscillation rule or following log-periodic oscillation\n rule. With magnetic field applied along the [001] direction, the SdH\n oscillations are attributed to the two-dimensional hole gas (2DHG) surface\n states. Here we reported an interesting SdH oscillation in Te-based single\n crystals, with the magnetic field applied along the [001] direction of the\n crystals, showing the maximum oscillation intensity at ~ 75 K, and still\n traceable at 200 K, which indicates a rather hot 2DHG state. The nontrivial\n Berry phase can be also obtained from the oscillations, implying the\n contribution from topological states. More importantly, the high temperature\n SdH oscillation phenomena are observed in different Te single crystals samples,\n and Te single crystals with nonmagnetic/magnetic dopants, showing robustness to\n bulk defects. Therefore, the oscillation may be contributed by the bulk\n symmetry protected hot 2DHG states, which will offer a new platform for\n high-temperature quantum transport studies.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2112.10093",
        "title": "Epistatic models predict mutable sites in SARS-CoV-2 proteins and\n  epitopes",
        "abstract": "The emergence of new variants of SARS-CoV-2 is a major concern given their\n potential impact on the transmissibility and pathogenicity of the virus as well\n as the efficacy of therapeutic interventions. Here, we predict the mutability\n of all positions in SARS-CoV-2 protein domains to forecast the appearance of\n unseen variants. Using sequence data from other coronaviruses, pre-existing to\n SARS-CoV-2, we build statistical models that do not only capture amino-acid\n conservation but more complex patterns resulting from epistasis. We show that\n these models are notably superior to conservation profiles in estimating the\n already observable SARS-CoV-2 variability. In the receptor binding domain of\n the spike protein, we observe that the predicted mutability correlates well\n with experimental measures of protein stability and that both are reliable\n mutability predictors (ROC AUC ~0.8). Most interestingly, we observe an\n increasing agreement between our model and the observed variability as more\n data become available over time, proving the anticipatory capacity of our\n model. When combined with data concerning the immune response, our approach\n identifies positions where current variants of concern are highly\n overrepresented. These results could assist studies on viral evolution, future\n viral outbreaks and, in particular, guide the exploration and anticipation of\n potentially harmful future SARS-CoV-2 variants.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1406.5508",
        "title": "The influence of diffuse scattered light I. The PSF and its role to\n  observations of the edge-on galaxy NGC 5907",
        "abstract": "All telescopes and instruments are to some degree affected by scattered\n light. It is possible to estimate the amount of such scattered light, and even\n correct for it, with a radially extended point spread function (PSF). The outer\n parts of the PSF have only rarely been determined, since they are faint and\n therefore difficult to measure. A mostly complete overview of existing\n properties and measurements of radially extended PSFs is presented, to both\n show their similarities and to indicate how bright extended objects can be used\n to measure the faintest regions. The importance of the far wings of the PSF and\n their possible temporal variations are demonstrated in three edge-on galaxy\n models. The same study is applied to the first edge-on galaxy where earlier\n observations reveal a halo, NGC 5907. All PSFs were collected in two diagrams,\n after they were offset or normalized, when that was possible.\n Surface-brightness structures of edge-on galaxies were modelled and analysed to\n study scattered-light haloes that result with an exponential disc. The models\n were convolved with both a lower-limit PSF and a more average PSF. The PSF of\n the observed data could be used in the case of NGC 5907. The comparison of the\n PSFs demonstrates a lower-limit $r^{-2}$ power-law decline at larger radii. The\n analysis of the galaxy models shows that also the outer parts of the PSF are\n important to correctly model and analyse observations and, in particular,\n fainter regions. The reassessed analysis of the earlier measurements of NGC\n 5907 reveals an explanation for the faint halo in scattered light, within the\n quoted level of accuracy.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2301.02583",
        "title": "Elastic diffeological spaces",
        "abstract": "We introduce a class of diffeological spaces, called elastic, on which the\n left Kan extension of the tangent functor of smooth manifolds defines an\n abstract tangent functor in the sense of Rosicky. On elastic spaces there is a\n natural Cartan calculus, consisting of vector fields and differential forms,\n together with the Lie bracket, de Rham differential, inner derivative, and Lie\n derivative, satisfying the usual graded commutation relations. Elastic spaces\n are closed under arbitrary coproducts, finite products, and retracts. Examples\n include manifolds with corners and cusps, diffeological groups and\n diffeological vector spaces with a mild extra condition, mapping spaces between\n smooth manifolds, and spaces of sections of smooth fiber bundles. This paper is\n a condensed preview of a longer work, explaining its motivation, main concepts,\n and results, but omitting most of the proofs.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1410.7793",
        "title": "Investigation of the magnetic dipole field at the atomic scale in\n  quasi-one-dimensional paramagnetic conductor Li$_{0.9}$Mo$_{6}$O$_{17}$",
        "abstract": "We report magnetic dipole field investigation at the atomic scale in a single\n crystal of quasi-one-dimensional (Q1D) paramagnetic conductor\n Li$_{0.9}$Mo$_{6}$O$_{17}$, using a paramagnetic electron model and\n $^{7}$Li-NMR spectroscopy measurements with an externally applied magnetic\n field $B_{0}$ = 9 T. We find that the magnetic dipole field component\n ($B_{||}^{\\text{dip}}$) parallel to $B_{0}$ at the Li site from the Mo\n electrons has no lattice axial symmetry; it is small around the middle between\n the lattice $c$ and $a$ axes in the $ac$-plane with the minimum at the field\n orientation angle $\\theta$ = +52.5$^{\\circ}$, while the $B_{||}^{\\text{dip}}$\n maximum is at $\\theta$ = +142.5$^{\\circ}$ when $B_{0}$ is applied perpendicular\n to $b$ ($B_{0}$ $\\perp$ $b$), where $\\theta$ = 0$^{\\circ}$ represents the\n direction of $B_{0}$ $\\parallel$ $c$. Further estimate indicates that\n $B_{||}^{\\text{dip}}$ has a maximum value of 0.35 G at $B_{0}$ = 9 T, and the\n Mo ions have a possible effective magnetic dipole moment 0.015 $\\mu_{\\text{B}}$\n per ion, which is significantly smaller than that of a spin 1/2 free electron.\n By minimizing potential magnetic contributions to the NMR spectrum satellites\n with the NMR spectroscopy measurements at the direction where the value of the\n magnetic dipole field is the smallest, the behavior of the independent charge\n contributions is observed. This work demonstrates that the magnetic dipole\n field from the Mo electrons is the dominant source of the local magnetic fields\n at the Li site, and it suggests that the mysterious \"metal-insulator\" crossover\n at low temperatures is not a charge effect. The work also reveals valuable\n local field information for further NMR investigation which is suggested\n recently [Phys. Rev. B $\\bf{85}$, 235128 (2012)] to be key important to the\n understanding of many mysterious properties of this Q1D material of particular\n interest.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1304.0246",
        "title": "The number of accessible paths in the hypercube",
        "abstract": "Motivated by an evolutionary biology question, we study the following\n problem: we consider the hypercube $\\{0,1\\}^L$ where each node carries an\n independent random variable uniformly distributed on $[0,1]$, except\n $(1,1,\\ldots,1)$ which carries the value $1$ and $(0,0,\\ldots,0)$ which carries\n the value $x\\in[0,1]$. We study the number $\\Theta$ of paths from vertex\n $(0,0,\\ldots,0)$ to the opposite vertex $(1,1,\\ldots,1)$ along which the values\n on the nodes form an increasing sequence. We show that if the value on\n $(0,0,\\ldots,0)$ is set to $x=X/L$ then $\\Theta/L$ converges in law as\n $L\\to\\infty$ to $\\mathrm{e}^{-X}$ times the product of two standard independent\n exponential variables. As a first step in the analysis, we study the same\n question when the graph is that of a tree where the root has arity $L$, each\n node at level 1 has arity $L-1$, \\ldots, and the nodes at level $L-1$ have only\n one offspring which are the leaves of the tree (all the leaves are assigned the\n value 1, the root the value $x\\in[0,1]$).",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1911.09574",
        "title": "Rank-two 5d SCFTs from M-theory at isolated toric singularities: a\n  systematic study",
        "abstract": "We carry out a detailed exploration of the deformations of rank-two\n five-dimensional superconformal field theories (SCFTs)\n $\\mathcal{T}_{\\mathbf{X}}$, which are geometrically engineered by M-theory on\n the space transverse to isolated toric Calabi-Yau (CY) threefold singularities\n $\\mathbf{X}$. Deformations of 5d $\\mathcal{N}=1$ SCFTs can lead to\n \"gauge-theory phases,\" but also to \"non-gauge-theoretic phases,\" which have no\n known Lagrangian interpretation. In previous work, a technique relying on\n fiberwise M-theory/type IIA duality was developed to associate a type IIA\n background to any resolution of $\\mathbf{X}$ which admits a suitable projection\n of its toric diagram. The type IIA background consists of an A-type ALE space\n fibered over the real line, with stacks of coincident D6-branes wrapping\n 2-cycles in the ALE resolution. In this work, we combine that technique with\n some elementary ideas from graph theory, to analyze mass deformations of\n $\\mathcal{T}_{\\mathbf{X}}$ when $\\mathbf{X}$ is a isolated toric CY$_3$\n singularity of rank-two (that is, it has two compact divisors). We explicitly\n derive type IIA descriptions of all isolated rank-two CY$_3$ toric\n singularities. We also comment on the renormalization group flows in the\n extended parameter spaces of these theories, which frequently relate distinct\n geometries by flowing to theories with lower flavor symmetries, including those\n that describe non-gauge-theoretic phases.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/0711.3837",
        "title": "Coevolution of Mercy and Altruistic Cooperation",
        "abstract": "Besides altruistic punishment and group selection, we argue that, mercy can\n lead to altruistic cooperation. Modeling the micro economic behavior of the\n mercy, with two alleles of genes (Cooperation or Defection & Mercy or No mercy)\n agents in a network, we present the computational simulation results in the\n spatiotemporal evolution game theory frame to prove the above argument. Here,\n mercy (or as 'Love thy neighbors') means, the agents, with mercy preference,\n might share his own fitness with his poorest neighbor who poorer than himself.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1903.09118",
        "title": "Some new results related to Lorentz G-gamma spaces and interpolation",
        "abstract": "We compute the K-functional related to some couple of spaces as small or\n classical Lebesgue space or Lorentz-Marcinkiewicz spaces completing the results\n of the previous works of the authors. This computation allows to determine the\n interpolation space in the sense of Peetre for such couple. It happens that the\n result is always a G-gamma space, since this last space covers many spaces. The\n motivations of such study are various, among them we wish to obtain a\n regularity estimate for the so called very weak solution of linear equation in\n a domain Omega with data in the space of the integrable function with respect\n to the distance function to the boundary of Omega.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2205.11137",
        "title": "Decentralized Federated Learning Based on Committees and Blockchain",
        "abstract": "Machine learning algorithms are undoubtedly one of the most popular\n algorithms in recent years, and neural networks have demonstrated unprecedented\n precision. In daily life, different communities may have different user\n characteristics, which also means that training a strong model requires the\n union of different communities, so the privacy issue needs to be solved\n urgently. Federated learning is a popular privacy solution, each community does\n not need to expose specific data, but only needs to upload sub-models to the\n coordination server to train more powerful models. However, federated learning\n also has some problems, such as the security and fairness of the coordination\n server. A proven solution to the problem is a decentralized implementation of\n federated learning. In this paper, we apply decentralized tools such as\n blockchain and consensus algorithms to design a support system that supports\n the decentralized operation of federated learning in an alliance environment,\n involving the exploration of incentives, security, fairness and other issues.\n Finally, we experimentally verify the performance of our system, the effect of\n federated learning, and the availability of privacy protection.",
        "label": 0
    },
    {
        "url": "https://aclanthology.org/2021.findings-emnlp.333.pdf",
        "title": "Benchmarking Meta-embeddings: What Works and What Does Not",
        "abstract": "In the last few years, several methods have been proposed to build meta-embeddings. The general aim was to obtain new representa- tions integrating complementary knowledge from different source pre-trained embeddings thereby improving their overall quality. How- ever, previous meta-embeddings have been evaluated using a variety of methods and datasets, which makes it difficult to draw meaningful conclusions regarding the merits of each approach. In this paper we propose a unified common framework, including both intrinsic and extrinsic tasks, for a fair and objective meta-embeddings evaluation. Fur- thermore, we present a new method to gen- erate meta-embeddings, outperforming previ- ous work on a large number of intrinsic evalu- ation benchmarks. Our evaluation framework also allows us to conclude that previous extrin- sic evaluations of meta-embeddings have been overestimated.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2201.05955v5",
        "title": "WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation",
        "abstract": "A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting ex- amples, leading to a lack of linguistic diver- sity. We introduce a novel approach for dataset creation based on worker and AI collabo- ration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI for natural language infer- ence (NLI), our approach uses dataset cartog- raphy to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new exam- ples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowd- workers. The resulting dataset, WANLI, con- sists of 107,885 NLI examples and presents unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI improves performance on eight out- of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI, compared to training on the 4\u00d7 larger MultiNLI. Moreover, it continues to be more effective than MultiNLI augmented with other NLI datasets. Our results demonstrate the promise of leveraging natural language gener- ation techniques and re-imagining the role of humans in the dataset creation process.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2212.09254v1",
        "title": "TEXTGRAD: ADVANCING ROBUSTNESS EVALUATION IN NLP BY GRADIENT-DRIVEN OPTIMIZATION",
        "abstract": "Robustness evaluation against adversarial examples has become increasingly im- portant to unveil the trustworthiness of the prevailing deep models in natural lan- guage processing (NLP). However, in contrast to the computer vision (CV) do- main where the first-order projected gradient descent (PGD) is used as the bench- mark approach to generate adversarial examples for robustness evaluation, there lacks a principled first-order gradient-based robustness evaluation framework in NLP. The emerging optimization challenges lie in 1) the discrete nature of textual inputs together with the strong coupling between the perturbation location and the actual content, and 2) the additional constraint that the perturbed text should be fluent and achieve a low perplexity under a language model. These challenges make the development of PGD-like NLP attacks difficult. To bridge the gap, we propose TEXTGRAD, a new attack generator using gradient-driven optimization, supporting high-accuracy and high-quality assessment of adversarial robustness in NLP. Specifically, we address the aforementioned challenges in a unified opti- mization framework. And we develop an effective convex relaxation method to co-optimize the continuously-relaxed site selection and perturbation variables, and leverage an effective sampling method to establish an accurate mapping from the continuous optimization variables to the discrete textual perturbations. Moreover, as a first-order attack generation method, TEXTGRAD can be baked in adversarial training to further improve the robustness of NLP models. Extensive experiments are provided to demonstrate the effectiveness of TEXTGRAD not only in attack generation for robustness evaluation but also in adversarial defense. From the at- tack perspective, we show that TEXTGRAD achieves remarkable improvements in both the attack success rate and the perplexity score over five state-of-the-art baselines. From the defense perspective, TEXTGRAD-enabled adversarial train- ing yields the most robust NLP model against a wide spectrum of NLP attacks.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2104.05832v1",
        "title": "SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning",
        "abstract": "This paper proposes a question-answering (QA) benchmark for spatial reasoning on nat- ural language text which contains more real- istic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reason- ing rules to automatically generate a spatial de- scription of visual scenes and corresponding QA pairs. Experiments show that further pre- training LMs on these automatically generated data significantly improves LMs\u2019 capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster inves- tigations into more sophisticated models for spatial reasoning over text.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2404.18630",
        "title": "4D-DRESS: A 4D Dataset of\nReal-World Human Clothing With Semantic Annotations",
        "abstract": "The studies of human clothing for digital avatars have pre- dominantly relied on synthetic datasets. While easy to col- lect, synthetic data often fall short in realism and fail to capture authentic clothing dynamics. Addressing this gap, we introduce 4D-DRESS, the first real-world 4D dataset advancing human clothing research with its high-quality 4D textured scans and garment meshes. 4D-DRESS cap- tures 64 outfits in 520 human motion sequences, amount- ing to 78k textured scans. Creating a real-world clothing dataset is challenging, particularly in annotating and seg- menting the extensive and complex 4D human scans. To address this, we develop a semi-automatic 4D human pars- ing pipeline. We efficiently combine a human-in-the-loop process with automation to accurately label 4D scans in diverse garments and body movements. Leveraging precise annotations and high-quality garment meshes, we estab- lish several benchmarks for clothing simulation and recon- struction. 4D-DRESS offers realistic and challenging data that complements synthetic sources, paving the way for ad- vancements in research of lifelike human clothing.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2401.12051",
        "title": "CloSe: A 3D Clothing Segmentation Dataset and Model",
        "abstract": "3D Clothing modeling and datasets play crucial role in the entertainment, animation, and digital fashion industries. Existing work often lacks detailed semantic understanding or uses synthetic datasets, lacking realism and personaliza- tion. To address this, we first introduce CloSe-D: a novel large-scale dataset containing 3D clothing segmentation of 3167 scans, covering a range of 18 distinct clothing classes. Additionally, we propose CloSe-Net, the first learning-based 3D clothing segmentation model for fine-grained segmenta- tion from colored point clouds. CloSe-Net uses local point features, body-clothing correlation, and a garment-class and point features-based attention module, improving per- formance over baselines and prior work. The proposed attention module enables our model to learn appearance and geometry-dependent clothing prior from data. We fur- ther validate the efficacy of our approach by successfully segmenting publicly available datasets of people in clothing. We also introduce CloSe-T, a 3D interactive tool for refining\nsegmentation labels. Combining the tool with CloSe-Net in a continual learning setup demonstrates improved general- ization on real-world data. Dataset, model, and tool can be found at https://virtualhumans.mpi-inf.mpg.de/close3dv24/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1809.02922",
        "title": "Transforming Question Answering Datasets Into Natural Language Inference Datasets",
        "abstract": "Existing datasets for natural language in- ference (NLI) have propelled research on language understanding. We propose a new method for automatically deriving NLI datasets from the growing abundance of large- scale question answering datasets.\nOur approach hinges on learning a sen- tence transformation model which converts question-answer pairs into their declarative forms. Despite being primarily trained on a single QA dataset, we show that it can be suc- cessfully applied to a variety of other QA re- sources. Using this system, we automatically derive a new freely available dataset of over 500k NLI examples (QA-NLI), and show that it exhibits a wide range of inference phenom- ena rarely seen in previous NLI datasets.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1804.07461v3",
        "title": "GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTAND- ING",
        "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluat- ing the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encour- age models that share general linguistic knowledge across tasks. GLUE also in- cludes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and rep- resentation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.04528v4",
        "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
        "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs\u2019 resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks including sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4, 788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. Code is available at: https://github.com/microsoft/promptbench.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1905.00537v3",
        "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems",
        "abstract": "In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understand- ing tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more diffi- cult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2201.06777",
        "title": "COPA-SSE: Semi-structured Explanations for Commonsense Reasoning",
        "abstract": "We present Semi-Structured Explanations for COPA (COPA-SSE), a new crowdsourced dataset of 9,747 semi-structured, En- glish common sense explanations for Choice of Plausible Alternatives (COPA) questions. The explanations are formatted as a set of triple-like common sense statements with ConceptNet relations but freely written concepts. This semi-structured for- mat strikes a balance between the high quality but low coverage of structured data and the lower quality but high coverage of free-form crowdsourcing. Each explanation also includes a set of human-given quality ratings. With their familiar format, the explanations are geared towards commonsense reasoners operating on knowledge graphs and serve as a starting point for ongoing work on improving such systems. The dataset is available at https://github.com/a-brassard/copa-sse.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1911.07176",
        "title": "Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering",
        "abstract": "We propose an unsupervised strategy for the selection of justification sentences for multi- hop question answering (QA) that (a) maxi- mizes the relevance of the selected sentences, (b) minimizes the overlap between the selected facts, and (c) maximizes the coverage of both question and answer. This unsupervised sen- tence selection method can be coupled with any supervised QA approach. We show that the sentences selected by our method im- prove the performance of a state-of-the-art supervised QA model on two multi-hop QA datasets: AI2\u2019s Reasoning Challenge (ARC) and Multi-Sentence Reading Comprehension (MultiRC). We obtain new state-of-the-art per- formance on both datasets among approaches that do not use external resources for training the QA system: 56.82% F1 on ARC (41.24% on Challenge and 64.49% on Easy) and 26.1% EM0 on MultiRC. Our justification sentences have higher quality than the justifications se- lected by a strong information retrieval base- line, e.g., by 5.4% F1 in MultiRC. We also show that our unsupervised selection of justifi- cation sentences is more stable across domains than a state-of-the-art supervised sentence se- lection method.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1808.09121",
        "title": "WiC: the Word-in-Context Dataset\nfor Evaluating Context-Sensitive Meaning Representations",
        "abstract": "By design, word embeddings are unable to model the dynamic nature of words\u2019 seman- tics, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized mean- ing representation techniques such as sense or contextualized embeddings have been pro- posed. However, despite the popularity of research on this topic, very few evaluation benchmarks exist that specifically focus on the dynamic semantics of words. In this paper we show that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contex- tual Word Similarity, and highlight its short- comings. To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on anno- tations curated by experts, for generic evalua- tion of context-sensitive representations. WiC is released in https://pilehvar.github.io/wic/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2004.15016",
        "title": "WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context",
        "abstract": "We present WiC-TSV, a new multi-domain evaluation benchmark for Word Sense Disam- biguation. More specifically, we introduce a framework for Target Sense Verification of Words in Context which grounds its unique- ness in the formulation as binary classification task thus being independent of external sense inventories, and the coverage of various do- mains. This makes the dataset highly flexi- ble for the evaluation of a diverse set of mod- els and systems in and across domains. WiC- TSV provides three different evaluation set- tings, depending on the input signals provided to the model. We set baseline performance on the dataset using state-of-the-art language models. Experimental results show that even though these models can perform decently on the task, there remains a gap between machine and human performance, especially in out-of- domain settings. WiC-TSV data is available at https://competitions.codalab. org/competitions/23683",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2010.06478",
        "title": "XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization",
        "abstract": "The ability to correctly model distinct mean- ings of a word is crucial for the effectiveness of semantic representation techniques. How- ever, most existing evaluation benchmarks for assessing this criterion are tied to sense inven- tories (usually WordNet), restricting their us- age to a small subset of knowledge-based rep- resentation techniques. The Word-in-Context dataset (WiC) addresses the dependence on sense inventories by reformulating the stan- dard disambiguation task as a binary classifi- cation problem; but, it is limited to the En- glish language. We put forward a large mul- tilingual benchmark, XL-WiC, featuring gold standards in 12 new languages from varied lan- guage families and with different degrees of resource availability, opening room for evalu- ation scenarios such as zero-shot cross-lingual transfer. We perform a series of experiments to determine the reliability of the datasets and to set performance baselines for several recent contextualized multilingual models. Experi- mental results show that even when no tagged instances are available for a target language, models trained solely on the English data can attain competitive performance in the task of distinguishing different meanings of a word, even for distant languages. XL-WiC is avail- able at https://pilehvar.github.io/xlwic/.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1909.03065",
        "title": "\u201cGoing on a vacation\u201d takes longer than \u201cGoing for a walk\u201d: A Study of Temporal Commonsense Understanding",
        "abstract": "Understanding time is crucial for understand- ing events expressed in natural language. Be- cause people rarely say the obvious, it is of- ten necessary to have commonsense knowl- edge about various temporal aspects of events, such as duration, frequency, and temporal or- der. However, this important problem has so far received limited attention. This pa- per systematically studies this temporal com- monsense problem. Specifically, we define five classes of temporal commonsense, and use crowdsourcing to develop a new dataset, MCTACO , that serves as a test set for this task. We find that the best current methods used on MCTACO are still far behind human performance, by about 20%, and discuss sev- eral directions for improvement. We hope that the new dataset and our study here can foster more future research on this topic.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1806.03822",
        "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD",
        "abstract": "Extractive reading comprehension sys- tems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines exist- ing SQuAD data with over 50,000 unan- swerable questions written adversarially by crowdworkers to look similar to an- swerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understand- ing task for existing models: a strong neu- ral system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2009.03300",
        "title": "MEASURING MASSIVE MULTITASK LANGUAGE UNDERSTANDING",
        "abstract": "We propose a new test to measure a text model\u2019s multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near- random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model\u2019s academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2103.03874",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
        "abstract": "Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2404.10690",
        "title": "MATHWRITING: A DATASET FOR HANDWRITTEN MATHEMATICAL EXPRESSION RECOGNITION",
        "abstract": "\nWe introduce MathWriting, the largest online handwritten mathematical expression dataset to date. It consists of 230k human-written samples and an additional 400k synthetic ones. MathWriting can also be used for offline HME recognition and is larger than all existing offline HME datasets like IM2LATEX-100K [6]. We introduce a benchmark based on MathWriting data in order to advance research on both online and offline HME recognition.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2402.10176",
        "title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset",
        "abstract": "Recent work has shown the immense poten- tial of synthetically generated datasets for train- ing large language models (LLMs), especially for acquiring targeted skills. Current large- scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAm- moTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with com- mercially restrictive licenses. A key reason lim- iting the use of open-source LLMs in these data generation pipelines has been the wide gap be- tween the mathematical skills of the best closed- source LLMs, such as GPT-4, and the best open- source LLMs. Building on the recent progress in open-source LLMs, our proposed prompt- ing novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruc- tion tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthe- sizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning bench- marks, using the recently released and permis- sively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a sub- set of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models. We release our code, models, and the OpenMathInstruct-1 dataset under a commer- cially permissive license.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2402.14804",
        "title": "Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset",
        "abstract": "Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual con- texts, with models approaching human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these bench- marks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathe- matical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abil- ities of LMMs. Through extensive experimen- tation, we unveil a notable performance gap be- tween current LMMs and human performance on MATH-V, underscoring the imperative for further advancements in LMMs. Moreover, our de- tailed categorization allows for a thorough error analysis of LMMs, offering valuable insights to guide future research and development.\n",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1901.05066",
        "title": "Investigating Antigram Behaviour using Distributional Semantics",
        "abstract": "\nThe field of computational linguistics con- stantly presents new challenges and topics for research. Whether it be analyzing word usage changes over time or identifying relationships between pairs of seemingly unrelated words. To this point, we identify Anagrams and Anti- grams as words possessing such unique prop- erties. The presented work is an exploration into generating anagrams from a given word and determining whether there exists antigram (semantically opposite anagrams) relationships between the pairs of generated anagrams using GloVe embeddings. We propose a rudimentary, yet interpretable, rule-based algorithm for de- tecting antigrams. On a small dataset of just 12 antigrams, our approach yielded an accuracy of 39% which shows that there is much work left to be done in this space",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.13165",
        "title": "CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation",
        "abstract": "Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based meth- ods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces CycleNet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate CycleNet on unpaired I2I tasks of different granulari- ties. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that CycleNet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual prompt. CycleNet is a practical framework, which is robust even with very limited training data (around 2k) and requires minimal computational resources (1 GPU) to train.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2207.12393",
        "title": "CelebV-HQ: A Large-Scale Video Facial Attributes Dataset",
        "abstract": "Large-scale datasets have played indispensable roles in the recent success of face generation/editing and significantly facilitated the advances of emerging research fields. However, the academic commu- nity still lacks a video dataset with diverse facial attribute annotations, which is crucial for the research on face-related videos. In this work, we propose a large-scale, high-quality, and diverse video dataset with rich facial attribute annotations, named the High-Quality Celebrity Video Dataset (CelebV-HQ). CelebV-HQ contains 35, 666 video clips with the resolution of 512 \u00d7 512 at least, involving 15, 653 identities. All \nare labeled manually with 83 facial attributes, covering appearance, ac- tion, and emotion. We conduct a comprehensive analysis in terms of age, ethnicity, brightness stability, motion smoothness, head pose diversity, and data quality to demonstrate the diversity and temporal coherence of CelebV-HQ. Besides, its versatility and potential are validated on two representative tasks, i.e., unconditional video generation and video fa- cial attribute editing. Furthermore, we envision the future potential of CelebV-HQ, as well as the new opportunities and challenges it would bring to related research directions. Data, code, and models are publicly available4 .",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.08460",
        "title": "LongForm: Effective Instruction Tuning with Reverse Instructions",
        "abstract": "Instruction tuning enables language models to more effectively generalize and better follow user intent. However, obtaining instruction data is costly and challenging. Prior work em- ploys methods such as expensive human an- notation, crowd-sourced datasets with align- ment issues, and generating noisy examples via LLMs. We introduce the LongForm-C dataset, which is created by reverse instruc- tions. We generate instructions via LLMs for human-written corpus examples using re- verse instructions. First we select a diverse set of human-written documents from corpora such as C4 and Wikipedia; then we generate instructions for these documents via LLMs. This approach provides a cheaper and cleaner instruction-tuning dataset with natural output and one suitable for long text generation. Our models outperform 10x larger language mod- els without instruction tuning on tasks such as story/recipe generation and long-form ques- tion answering. Moreover, LongForm mod- els outperform prior instruction-tuned models such as FLAN-T5 and Alpaca by a large mar- gin, and improve language understanding capa- bilities further. Finally, our models can effec- tively follow and answer multilingual instruc-\ntions; we demonstrate this for news genera-\ntion. We publicly release our data and models:\nhttps://github.com/akoksal/LongForm",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2106.03634",
        "title": "PROST:\nPhysical Reasoning about Objects through Space and Time",
        "abstract": "We present a new probing dataset named\nPROST: Physical Reasoning about Objects Through Space and Time. This dataset con- tains 18,736 multiple-choice questions made from 14 manually curated templates, covering 10 physical reasoning concepts. All questions are designed to probe both causal and masked language models in a zero-shot setting. We conduct an extensive analysis which demon- strates that state-of-the-art pretrained models are inadequate at physical reasoning: they are influenced by the order in which answer op- tions are presented to them, they struggle when the superlative in a question is inverted (e.g., most \u2194 least), and increasing the amount of pretraining data and parameters only yields minimal improvements. These results provide support for the hypothesis that current pre- trained models\u2019 ability to reason about physi- cal interactions is inherently limited by a lack of real world experience. By highlighting these limitations, we hope to motivate the de- velopment of models with a human-like under- standing of the physical world.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2105.03011",
        "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers",
        "abstract": "Readers of academic research papers often read with the goal of answering specific ques- tions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information- seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natu- ral Language Processing papers. Each ques- tion is written by an NLP practitioner who read only the title and abstract of the corre- sponding paper, and the question seeks infor- mation present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting ev- idence to answers. We find that existing mod- els that do well on other QA tasks do not per- form well on answering these questions, un- derperforming humans by at least 27 F1 points when answering them from entire papers, moti- vating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1806.00358",
        "title": "A Systematic Classification of Knowledge, Reasoning, and Context within the ARC Dataset",
        "abstract": "The recent work of Clark et al. (2018) introduces the AI2 Reasoning Challenge (ARC) and the associated ARC dataset that partitions open domain, complex sci- ence questions into an Easy Set and a Challenge Set. That paper includes an analysis of 100 questions with respect to the types of knowledge and reasoning re- quired to answer them; however, it does not include clear definitions of these types, nor does it offer information about the quality of the labels. We propose a com- prehensive set of definitions of knowledge and reasoning types necessary for answer- ing the questions in the ARC dataset. Us- ing ten annotators and a sophisticated an- notation interface, we analyze the distri- bution of labels across the Challenge Set and statistics related to them. Addition- ally, we demonstrate that although naive information retrieval methods return sen- tences that are irrelevant to answering the query, sufficient supporting text is of- ten present in the (ARC) corpus. Eval- uating with human-selected relevant sen- tences improves the performance of a neu- ral machine comprehension model by 42 points.\n",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2402.15043v1",
        "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
        "abstract": "Automatic evaluation methods for large lan- guage models (LLMs) are hindered by data con- tamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quan- tifying contamination status instead of accu- rately gauging model performance. In this paper, we introduce KIEval, a Knowledge- grounded Interactive Evaluation framework, which incorporates an LLM-powered \"inter- actor\" role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically gen- erated, multi-round, and knowledge-focused dialogues to determine whether a model\u2019s re- sponse is merely a recall of benchmark answers or demonstrates a deep comprehension to ap- ply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KIEval\u2019s effective- ness and generalization. We also reveal that data contamination brings no contribution or even negative effect to models\u2019 real-world ap- plicability and understanding, and existing con- tamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2007.08124",
        "title": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning",
        "abstract": "Machine reading is a fundamental task for test- ing the capability of natural language understand- ing, which is closely related to human cognition in many aspects. With the rising of deep learning techniques, algorithmic models rival human perfor- mances on simple QA, and thus increasingly chal- lenging machine reading datasets have been pro- posed. Though various challenges such as ev- idence integration and commonsense knowledge have been integrated, one of the fundamental ca- pabilities in human reading, namely logical reason- ing, is not fully investigated. We build a compre- hensive dataset, named LogiQA, which is sourced from expert-written questions for testing human Logical reasoning. It consists of 8,678 QA in- stances, covering multiple types of deductive rea- soning. Results show that state-of-the-art neural models perform by far worse than human ceiling. Our dataset can also serve as a benchmark for re- investigating logical AI under the deep learning NLP setting. The dataset is freely available at https://github.com/lgw863/LogiQA-dataset.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2403.14112v2",
        "title": "Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations",
        "abstract": "We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large lan- guage models (LLMs) in Chinese, which cov- ers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, em- ploying 5 representative prompt strategies for improving LLMs\u2019 reasoning ability, such as Chain-of-Thought. Our findings indicated that the LLM\u2019s language orientation and the task\u2019s domain influence the effectiveness of the prompt strategy, which enriches previous research find- ings. We built closely-interconnected reason- ing and memorization tasks, and found that some LLMs struggle with memorizing Chi- nese commonsense, affecting their reasoning ability, while others show differences in reason- ing despite similar memorization performance. We also evaluated the LLMs\u2019 memorization- independent reasoning abilities and analyzed the typical errors. Our study precisely iden- tified the LLMs\u2019 strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.09107v1",
        "title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
        "abstract": "\nRecently, large language models (LLMs), including notable models such as GPT-4 and burgeoning community models, have show- cased significant general language understanding abilities. How- ever, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a meticulously assembled General Logical Reasoning Evaluation benchmark comprised of 12 datasets that span three different types of tasks. Our experimental results show that compared to the performance of human and supervised fine-tuning, the logical reasoning capabilities of open LLM mod- els necessitate additional improvement; ChatGPT and GPT-4 show a strong capability of logical reasoning, with GPT-4 surpassing ChatGPT by a large margin. We propose a self-consistency prob- ing method to enhance the accuracy of ChatGPT and a fine-tuned method to boost the performance of an open LLM. We release the datasets and evaluation programs to facilitate future research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2004.04494",
        "title": "MuTual: A Dataset for Multi-Turn Dialogue Reasoning",
        "abstract": "Non-task oriented dialogue systems have achieved great success in recent years due to largely accessible conversation data and the de- velopment of deep learning techniques. Given a context, current systems are able to yield a relevant and fluent response, but sometimes make logical mistakes because of weak rea- soning capabilities. To facilitate the conver- sation reasoning research, we introduce Mu- Tual, a novel dataset for Multi-Turn dialogue Reasoning, consisting of 8,860 manually anno- tated dialogues based on Chinese student En- glish listening comprehension exams. Com- pared to previous benchmarks for non-task ori- ented dialogue systems, MuTual is much more challenging since it requires a model that can handle various reasoning problems. Empiri- cal results show that state-of-the-art methods only reach 71%, which is far behind the hu- man performance of 94%, indicating that there is ample room for improving reasoning abil- ity. MuTual is available at https://github. com/Nealcly/MuTual.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2306.14399",
        "title": "Mutual Query Network for Multi-Modal Product Image Segmentation",
        "abstract": "Product image segmentation is vital in e-commerce. Most existing methods extract the product image foreground only based on the visual modality, making it difficult to distinguish irrelevant products. As product titles contain abundant appear- ance information and provide complementary cues for product image segmentation, we propose a mutual query network to segment products based on both visual and linguistic modalities. First, we design a language query vision module to obtain the response of language description in image areas, thus aligning the visual and linguistic representations across modalities. Then, a vision query language module utilizes the correlation between visual and linguistic modalities to filter the product title and effectively suppress the content irrelevant to the vision in the title. To promote the research in this field, we also construct a Multi-Modal Product Segmentation dataset (MMPS), which contains 30,000 images and corresponding titles. The proposed method significantly outperforms the state-of-the-art methods on MMPS.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2104.08671",
        "title": "When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset of 53,000+ Legal Holdings",
        "abstract": "While self-supervised learning has made rapid advances in natural language processing, it remains unclear when researchers should engage in resource-intensive domain-specific pretraining (domain pretraining). The law, puzzlingly, has yielded few documented in- stances of substantial gains to domain pretraining in spite of the fact that legal language is widely seen to be unique. We hypothesize that these existing results stem from the fact that existing legal NLP tasks are too easy and fail to meet conditions for when domain pretraining can help. To address this, we first present CaseHOLD (Case Holdings On Legal Decisions), a new dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case. This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second, we assess performance gains on CaseHOLD and existing legal NLP datasets. While a Transformer architecture (BERT) pretrained on a general corpus (Google Books and Wikipedia) improves perfor- mance, domain pretraining (on a corpus of \u22483.5M decisions across all courts in the U.S. that is larger than BERT\u2019s) with a custom legal vocabulary exhibits the most substantial performance gains with CaseHOLD (gain of 7.2% on F1, representing a 12% improvement on BERT) and consistent performance gains across two other legal tasks. Third, we show that domain pretraining may be warranted when the task exhibits sufficient similarity to the pretraining corpus: the level of performance increase in three legal tasks was directly tied to the domain specificity of the task. Our findings inform when researchers should engage in resource-intensive pretraining and show that Transformer-based architectures, too, learn embeddings suggestive of distinct legal language.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2110.00976v4",
        "title": "LexGLUE: A Benchmark Dataset for Legal Language Understanding in English",
        "abstract": "Laws and their interpretations, legal arguments and agreements are typically expressed in writ- ing, leading to the production of vast corpora of legal text. Their analysis, which is at the center of legal practice, becomes increasingly elaborate as these collections grow in size. Natural language understanding (NLU) tech- nologies can be a valuable tool to support le- gal practitioners in these endeavors. Their use- fulness, however, largely depends on whether current state-of-the-art models can generalize across various tasks in the legal domain. To answer this currently open question, we intro- duce the Legal General Language Understand- ing Evaluation (LexGLUE) benchmark, a col- lection of datasets for evaluating model perfor- mance across a diverse set of legal NLU tasks in a standardized way. We also provide an evaluation and analysis of several generic and legal-oriented models demonstrating that the latter consistently offer performance improve- ments across multiple tasks.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.14494",
        "title": "CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course",
        "abstract": "We introduce CS1QA, a dataset for code- based question answering in the programming education domain. CS1QA consists of 9,237 question-answer pairs gathered from chat logs in an introductory programming class using Python, and 17,698 unannotated chat data with code1. Each question is accompanied with the student\u2019s code, and the portion of the code relevant to answering the question. We carefully design the annotation process to construct CS1QA, and analyze the collected dataset in detail. The tasks for CS1QA are to predict the question type, the relevant code snippet given the question and the code and re- trieving an answer from the annotated corpus. Results for the experiments on several baseline models are reported and thoroughly analyzed. The tasks for CS1QA challenge models to un- derstand both the code and natural language. This unique dataset can be used as a bench- mark for source code comprehension and ques- tion answering in the educational setting.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.13432",
        "title": "Towards Better Few-Shot and Finetuning Performance with Forgetful Causal Language Models",
        "abstract": "Large language models (LLM) trained using the next-token-prediction objective, such as GPT3 and PaLM, have revolutionized natural language processing in recent years by showing impressive zero-shot and few-shot capabilities across a wide range of tasks. In this work, we propose a sim- ple technique that significantly boosts the perfor- mance of LLMs without adding computational cost. Our key observation is that, by perform- ing the next token prediction task with randomly selected past tokens masked out, we can im- prove the quality of the learned representations for downstream language understanding tasks. We hypothesize that randomly masking past to- kens prevents over-attending to recent tokens and encourages attention to tokens in the distant past. We find that our method, Forgetful Causal Mask- ing (FCM), significantly improves both few-shot and finetuning performance of PaLM. We further consider a simple extension, T-FCM, which in- troduces bidirectional context to causal language model without altering the sequence order, and further improves finetuning performance.\n",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1905.00537",
        "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems",
        "abstract": "In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understand- ing tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more diffi- cult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2002.04326",
        "title": "RECLOR: A READING COMPREHENSION DATASET REQUIRING LOGICAL REASONING",
        "abstract": "Recent powerful pre-trained language models have achieved remarkable perfor- mance on most of the popular datasets for reading comprehension. It is time to introduce more challenging datasets to push the development of this field to- wards more comprehensive reasoning of text. In this paper, we introduce a new Reading Comprehension dataset requiring logical reasoning (ReClor) extracted from standardized graduate admission examinations. As earlier studies suggest, human-annotated datasets usually contain biases, which are often exploited by models to achieve high accuracy without truly understanding the text. In order to comprehensively evaluate the logical reasoning ability of models on ReClor, we propose to identify biased data points and separate them into EASY set while the rest as HARD set. Empirical results show that state-of-the-art models have an outstanding ability to capture biases contained in the dataset with high accuracy on EASY set. However, they struggle on HARD set with poor performance near that of random guess, indicating more research is needed to essentially enhance the logical reasoning ability of current models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2104.06598",
        "title": "AR-LSAT: Investigating Analytical Reasoning of Text",
        "abstract": "Analytical reasoning is an essential and chal-\nlenging task that requires a system to ana-\nlyze a scenario involving a set of particu-\nlar circumstances and perform reasoning over\nit to make conclusions. In this paper, we\nstudy the challenge of analytical reasoning of text and introduce a new dataset consisting of questions from Law school admission Test from 1991 to 2016.\nwe analyse what knowledge understanding and reasoning abilities are required to do well on this task. furthermore, to address this reasoning 1)Transformer-based method which leverages the state of the art pre-trained language models and (2) analytical Reasoning Machine. it is close to random guess and ARM achieves better perfor- mance by leveraging symbolic knowledge and interpretable reasoning steps. Results show that both methods still lag far behind human performance, which leave further space for fu- ture research.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2405.19342",
        "title": "Sonos Voice Control Bias Assessment Dataset: A Methodology for Demographic Bias Assessment in Voice Assistants",
        "abstract": "Recent works demonstrate that voice assistants do not perform equally well for everyone, but research on demographic robustness of speech technologies is still scarce. This is mainly due to the rarity of large datasets with controlled demographic tags. This paper introduces the Sonos Voice Control Bias Assessment Dataset, an open dataset composed of voice assistant requests for North American English in the music domain (1, 038 speakers, 166 hours, 170k audio samples, with 9, 040 unique labelled transcripts) with a controlled demographic diversity (gender, age, dialectal region and ethnicity). We also release a statistical demographic bias assessment methodology, at the univariate and multivariate levels, tailored to this specific use case and leveraging spoken language understanding metrics rather than transcription accuracy, which we believe is a better proxy for user experience. To demonstrate the capabilities of this dataset and statistical method to detect demographic bias, we consider a pair of state-of-the-art Automatic Speech Recognition and Spoken Language Understanding models. Results show statistically significant differences in performance across age, dialectal region and ethnicity. Multivariate tests are crucial to shed light on mixed effects between dialectal region, gender and age.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2305.09678",
        "title": "Anomaly Detection Dataset for Industrial Control Systems",
        "abstract": "Over the past few decades, Industrial Control Sys- tems (ICSs) have been targeted by cyberattacks and are becoming increasingly vulnerable as more ICSs are connected to the internet. Using Machine Learning (ML) for Intrusion Detection Systems (IDSs) is a promising approach for ICS cyber protection, but the lack of suitable datasets for evaluating ML algorithms is a challenge. Although there are a few commonly used datasets, they may not reflect realistic ICS network data, lack necessary features for effective anomaly detection, or be outdated. This paper presents the \u2019ICS-Flow\u2019 dataset, which offers network data and process state variables logs for supervised and unsu- pervised ML-based IDS assessment. The network data includes normal and anomalous network packets and flows captured from simulated ICS components and emulated networks. The anomalies were injected into the system through various attack techniques commonly used by hackers to modify network traffic and compromise ICSs. We also proposed open-source tools, \u201cICSFlowGenerator\" for generating network flow parameters from Raw network packets. The final dataset comprises over 25,000,000 raw network packets, network flow records, and process variable logs. The paper describes the methodology used to collect and label the dataset and provides a detailed data analysis. Finally, we implement several ML models, including the decision tree, random forest, and artificial neural network to detect anomalies and attacks, demonstrating that our dataset can be used effectively for training intrusion detection ML models.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2208.07363",
        "title": "MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control",
        "abstract": "\nSimulated humanoids are an appealing research domain due to their physical capabilities. Nonetheless, they are also challenging to control, as a policy must drive an unstable, discontinuous, and high-dimensional physical system. One widely studied approach is to utilize motion capture (MoCap) data to teach the humanoid agent low-level skills (e.g., standing, walking, and running) that can then be re-used to synthesize high-level behaviors. However, even with MoCap data, controlling simulated humanoids remains very hard, as MoCap data offers only kinematic information. Finding physical control inputs to realize the demonstrated motions requires computationally intensive methods like reinforcement learning. Thus, despite the publicly available MoCap data, its utility has been limited to institutions with large-scale compute. In this work, we dramatically lower the barrier for productive research on this topic by training and releasing high-quality agents that can track over three hours of MoCap data for a simulated humanoid in the dm_control physics-based environment. We release MoCapAct (Motion Capture with Actions), a dataset of these expert agents and their rollouts, which contain proprioceptive observations and actions. We demonstrate the utility of MoCapAct by using it to train a single hierarchical policy capable of tracking the entire MoCap dataset within dm_control and show the learned low-level component can be re-used to efficiently learn downstream high-level tasks. Finally, we use MoCapAct to train an autoregressive GPT model and show that it can control a simulated humanoid to perform natural motion completion given a motion prompt. Videos of the results and links to the code and dataset are available at the project website.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2311.09528",
        "title": "HELPSTEER: Multi-attribute Helpfulness Dataset for STEERLM",
        "abstract": "Existing open-source helpfulness preference datasets do not specify what makes some re- sponses more helpful and others less so. Mod- els trained on these datasets can incidentally learn to model dataset artifacts (e.g. prefer- ring longer but unhelpful responses only due to their length). To alleviate this problem, we collect HELPSTEER, a multi-attribute help- fulness dataset annotated for the various as- pects that make responses helpful. Specifi- cally, our 37k-sample dataset has annotations for correctness, coherence, complexity, and verbosity in addition to overall helpfulness of responses. Training Llama 2 70B using the HELPSTEER dataset with STEERLM tech- nique produces a model that scores 7.54 on MT Bench, which is currently the highest score for open models that do not require training data from more powerful models (e.g. GPT4). We release this dataset with CC- BY-4.0 license at https://huggingface.co/ datasets/nvidia/HelpSteer",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2009.14505",
        "title": "TaxiNLI: Taking a Ride up the NLU Hill",
        "abstract": "Pre-trained Transformer-based neural architec- tures have consistently achieved state-of-the- art performance in the Natural Language Infer- ence (NLI) task. Since NLI examples encom- pass a variety of linguistic, logical, and reason- ing phenomena, it remains unclear as to which specific concepts are learnt by the trained sys- tems and where they can achieve strong gener- alization. To investigate this question, we pro- pose a taxonomic hierarchy of categories that are relevant for the NLI task. We introduce TAXINLI, a new dataset, that has 10k exam- ples from the MNLI dataset (Williams et al., 2018) with these taxonomic labels. Through various experiments on TAXINLI, we observe that whereas for certain taxonomic categories SOTA neural models have achieved near per- fect accuracies\u2014a large jump over the previ- ous models\u2014some categories still remain diffi- cult. Our work adds to the growing body of lit- erature that shows the gaps in the current NLI systems and datasets through a systematic pre- sentation and analysis of reasoning categories.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.03439",
        "title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
        "abstract": "Harnessing logical reasoning ability is a com- prehensive natural language understanding en- deavor. With the release of Generative Pre- trained Transformer 4 (GPT-4), highlighted as \"advanced\" at reasoning tasks, we are ea- ger to learn the GPT-4 performance on vari- ous logical reasoning tasks. This report anal- yses multiple logical reasoning datasets, with popular benchmarks like LogiQA and ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice reading comprehen- sion and natural language inference tasks with benchmarks requiring logical reasoning. We further construct a logical reasoning out-of- distribution dataset to investigate the robust- ness of ChatGPT and GPT-4. We also make a performance comparison between ChatGPT and GPT-4. Experiment results show that ChatGPT performs significantly better than the RoBERTa fine-tuning method on most logical reasoning benchmarks. With early access to the GPT-4 API we are able to conduct intense experiments on the GPT-4 model. The results show GPT-4 yields even higher performance on most logical reason- ing datasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known datasets like LogiQA and ReClor. However, the performance drops significantly when han- dling newly released and out-of-distribution datasets. Logical reasoning remains challeng- ing for ChatGPT and GPT-4, especially on out- of-distribution and natural language inference datasets. We release the prompt-style logical reasoning datasets as a benchmark suite and name it LogiEval.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2307.05034v2",
        "title": "Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference",
        "abstract": "We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis that investigates the performance of Natural Lan- guage Inference (NLI) models to understand compositionality in logic. We produce 1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et al., 2014). To this end, we modify the original texts using a set of phrases \u2013 modifiers that correspond to uni- versal quantifiers, existential quantifiers, nega- tion, and other concept modifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases to modify the subject, verb, and object parts of the premise and hypothesis. Lastly, we annotate these modified texts with the corre- sponding entailment labels following NL rules. We conduct a preliminary verification of how well the change in the structural and semantic composition is captured by neural NLI models, in both zero-shot and fine-tuned scenarios. We found that the performance of NLI models un- der the zero-shot setting is poor, especially for modified sentences with negation and existen- tial quantifiers. After fine-tuning this dataset, we observe that models continue to perform poorly over negation, existential and universal modifiers.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1904.12166v1",
        "title": "HELP: A Dataset for Identifying Shortcomings of Neural Models in Monotonicity Reasoning",
        "abstract": "Large crowdsourced datasets are widely used for training and evaluating neural models on natural language inference (NLI). Despite these efforts, neural models have a hard time capturing logical inferences, including those licensed by phrase replacements, so- called monotonicity reasoning. Since no large dataset has been developed for monotonicity reasoning, it is still unclear whether the main obstacle is the size of datasets or the model ar- chitectures themselves. To investigate this is- sue, we introduce a new dataset, called HELP, for handling entailments with lexical and logi- cal phenomena. We add it to training data for the state-of-the-art neural models and evaluate them on test sets for monotonicity phenomena. The results showed that our data augmentation improved the overall accuracy. We also find that the improvement is better on monotonic- ity inferences with lexical replacements than on downward inferences with disjunction and modification. This suggests that some types of inferences can be improved by our data aug- mentation while others are immune to it.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2110.02386",
        "title": "Analyzing the Effects of Reasoning Types on Cross-Lingual Transfer Performance",
        "abstract": "Multilingual language models achieve impres- sive zero-shot accuracies in many languages in complex tasks such as Natural Language Infer- ence (NLI). Examples in NLI (and equivalent complex tasks) often pertain to various types of sub-tasks, requiring different kinds of rea- soning. Certain types of reasoning have proven to be more difficult to learn in a monolingual context, and in the crosslingual context, similar observations may shed light on zero-shot trans- fer efficiency and few-shot sample selection. Hence, to investigate the effects of types of reasoning on transfer performance, we propose a category-annotated multilingual NLI dataset and discuss the challenges to scale monolingual annotations to multiple languages. We statisti- cally observe interesting effects that the conflu- ence of reasoning types and language similari- ties have on transfer performance.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2108.00648",
        "title": "From LSAT: The Progress and Challenges of Complex Reasoning",
        "abstract": "Complex reasoning aims to draw a correct inference based on complex rules. As a hallmark of human intelligence, it involves a degree of explicit reading comprehension, interpre- tation of logical knowledge and complex rule application. In this paper, we take a step forward in complex reasoning by systematically studying the three challenging and domain-general tasks of the Law School Admission Test (LSAT), including ana- lytical reasoning, logical reasoning and reading comprehension. We propose a hybrid reasoning system to integrate these three tasks and achieve impressive overall performance on the LSAT tests. The experimental results demonstrate that our system endows itself a certain complex reasoning ability, especially the fundamental reading comprehension and challenging logical reasoning capacities. Further analysis also shows the effectiveness of combining the pre-trained models with the task-specific rea- soning module, and integrating symbolic knowledge into discrete interpretable reasoning steps in complex reasoning. We further shed a light on the potential future directions, like unsupervised symbolic knowledge extraction, model interpretability, few-shot learning and comprehensive benchmark for complex reasoning.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.03256",
        "title": "Not another Negation Benchmark:\nThe NaN-NLI Test Suite for Sub-clausal Negation",
        "abstract": "Negation is poorly captured by current lan- guage models, although the extent of this prob- lem is not widely understood. We introduce a natural language inference (NLI) test suite to enable probing the capabilities of NLP meth- ods, with the aim of understanding sub-clausal negation. The test suite contains premise\u2013 hypothesis pairs where the premise contains sub-clausal negation and the hypothesis is con- structed by making minimal modifications to the premise in order to reflect different possi- ble interpretations. Aside from adopting stan- dard NLI labels, our test suite is systematically constructed under a rigorous linguistic frame- work. It includes annotation of negation types and constructions grounded in linguistic the- ory, as well as the operations used to construct hypotheses. This facilitates fine-grained anal- ysis of model performance. We conduct ex- periments using pre-trained language models to demonstrate that our test suite is more chal- lenging than existing benchmarks focused on negation, and show how our annotation sup- ports a deeper understanding of the current NLI capabilities in terms of negation and quan- tification.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2310.13850",
        "title": "Ecologically Valid Explanations for Label Variation in NLI",
        "abstract": "Human label variation exists in many natural language processing (NLP) tasks, including nat- ural language inference (NLI). To gain direct evidence of how NLI label variation arises, we build LIVENLI, an English dataset of 1,415 ecologically valid explanations (annotators ex- plain the NLI labels they chose) for 122 MNLI (Williams et al., 2018) items (at least 10 expla- nations per item). The LIVENLI explanations confirm that people can systematically vary on their interpretation and highlight within-label variation: annotators sometimes choose the same label for different reasons. This suggests that explanations are crucial for navigating la- bel interpretations in general. We few-shot prompt language models (LMs) to generate explanations but the results are inconsistent: the models sometimes produce valid and in- formative explanations, but they also generate implausible ones that do not support the label, highlighting directions for improvement.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2304.12443",
        "title": "Understanding and Predicting Human Label Variation in Natural Language Inference through Explanations",
        "abstract": "Human label variation (Plank, 2022), or an- notation disagreement, exists in many nat- ural language processing (NLP) tasks. To be robust and trusted, NLP models need to identify such variation and be able to ex- plain it. To this end, we created the first eco- logically valid explanation dataset with di- verse reasoning, LIVENLI. LIVENLI con- tains annotators\u2019 highlights and free-text ex- planations for the label(s) of their choice for 122 English Natural Language Inference items, each with at least 10 annotations. We used its explanations for chain-of-thought prompting, and found there is still room for improvement in GPT-3\u2019s ability to predict label distribution with in-context learning.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2309.10604",
        "title": "FRACAS: A FRench Annotated Corpus of Attribution relations in newS",
        "abstract": "Quotation extraction is a widely useful task both from a sociological and from a Natural Language Processing perspective. However, very little data is available to study this task in languages other than English. In this paper, we present a manually annotated corpus of 1676 newswire texts in French for quotation extraction and source attribution. We first describe the composition of our corpus and the choices that were made in selecting the data. We then detail the annotation guidelines and annotation process, as well as a few statistics about the final corpus and the obtained balance between quote types (direct, indirect and mixed, which are particularly challenging). We end by detailing our inter-annotator agreement between the 8 annotators who worked on manual labelling, which is substantially high for such a difficult linguistic phenomenon.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1910.09753",
        "title": "MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension",
        "abstract": "We present the results of the Machine Reading for Question Answering (MRQA) 2019\nshared task on evaluating the generalization\ncapabilities of reading comprehension systems.1\nIn this task, we adapted and unified 18\ndistinct question answering datasets into the\nsame format. Among them, six datasets were\nmade available for training, six datasets were\nmade available for development, and the final six were hidden for final evaluation. Ten\nteams submitted systems, which explored various ideas including data sampling, multi-task\nlearning, adversarial training and ensembling.\nThe best system achieved an average F1 score\nof 72.5 on the 12 held-out datasets, 10.7 absolute points higher than our initial baseline\nbased on BERT.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/2402.18334",
        "title": "Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation",
        "abstract": "We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. Our goal is to enable zero-shot task adaptation of large language models on users\u2019 specialized, private data. We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types\u2014yes-no question answering, extractive question answering, and natural language inference\u2014and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned models over the de facto self supervised baseline. For example, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral and Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1 points whereas the next word prediction objective undoes some of the benefits of instruction tuning and reduces the average performance by 0.8 F1 points. We conduct additional experiments with Bonito to understand the effects of the domain, the size of the training set, and the choice of alternative synthetic task generators. Overall, we show that learning with synthetic instruction tuning datasets is an effective way to adapt language models to new domains. The model, dataset, and code are available at https://github.com/BatsResearch/bonito.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1606.05250",
        "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
        "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2008.02275",
        "title": "ALIGNING AI WITH SHARED HUMAN VALUES",
        "abstract": "We show how to assess a language model\u2019s knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2210.13448",
        "title": "EUR-Lex-Sum: A Multi- and Cross-lingual Dataset for Long-form Summarization in the Legal Domain",
        "abstract": "Existing summarization datasets come with two main drawbacks: (1) They tend to focus on overly exposed domains, such as news articles or wiki-like texts, and (2) are primarily monolingual, with few multilingual datasets. In this work, we propose a novel dataset, called EUR-Lex-Sum, based on manually curated document summaries of legal acts from the European Union law platform (EUR-Lex). Documents and their respective summaries exist as cross-lingual paragraph-aligned data in several of the 24 official European languages, enabling access to various cross-lingual and lower-resourced summarization setups. We obtain up to 1,500 document/summary pairs per language, including a subset of 375 crosslingually aligned legal acts with texts available in all 24 languages. In this work, the data acquisition process is detailed and key characteristics of the resource are compared to existing summarization resources. In particular, we illustrate challenging sub-problems and open questions on the dataset that could help the facilitation of future research in the direction of domain-specific cross-lingual summarization. Limited by the extreme length and language diversity of samples, we further conduct experiments with suitable extractive monolingual and cross-lingual baselines for future work. Code for the extraction as well as access to our data and baselines is available online at: https://github.com/ achouhan93/eur-lex-sum.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2109.00904",
        "title": "MultiEURLEX \u2013 A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer",
        "abstract": "We introduce MULTI-EURLEX, a new multilingual dataset for topic classification of legal\ndocuments. The dataset comprises 65k European Union (EU) laws, officially translated in\n23 languages, annotated with multiple labels\nfrom the EUROVOC taxonomy. We highlight\nthe effect of temporal concept drift and the importance of chronological, instead of random\nsplits. We use the dataset as a testbed for zeroshot cross-lingual transfer, where we exploit\nannotated training documents in one language\n(source) to classify documents in another language (target). We find that fine-tuning a multilingually pretrained model (XLM-ROBERTA,\nMT5) in a single source language leads to\ncatastrophic forgetting of multilingual knowledge and, consequently, poor zero-shot transfer to other languages. Adaptation strategies,\nnamely partial fine-tuning, adapters, BITFIT,\nLNFIT, originally proposed to accelerate finetuning for new end-tasks, help retain multilingual knowledge from pretraining, substantially\nimproving zero-shot cross-lingual transfer, but\ntheir impact also depends on the pretrained\nmodel used and the size of the label set.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/1905.10892",
        "title": "Extreme Multi-Label Legal Text Classification: A case study in EU Legislation",
        "abstract": "We consider the task of Extreme Multi-Label Text Classification (XMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR-LEX, the European Union\u2019s public document database, annotated with concepts from EUROVOC, a multidisciplinary thesaurus. The dataset is substantially larger than previous EUR-LEX datasets and suitable for XMTC, few-shot and zero-shot learning. Experimenting with several neural classifiers, we show that BIGRUs with selfattention outperform the current multi-label state-of-the-art methods, which employ labelwise attention. Replacing CNNs with BIGRUs in label-wise attention networks leads to the best overall performance.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2403.00252",
        "title": "Europa: A Legal Multilingual Keyphrase Generation Dataset",
        "abstract": "Keyphrase generation has primarily been explored within the context of academic research articles, with a particular focus on scientific domains and the English language. In this work, we present Europa, a dataset for multilingual keyphrase generation in the legal domain. It is derived from legal judgments from the Court of Justice of the European Union (EU), and contains instances in all 24 EU official languages. We run multilingual models on our corpus and analyze the results, showing room for improvement on a domain-specific multilingual corpus such as the one we present.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/pdf/2203.11258",
        "title": "Efficient Classification of Long Documents Using Transformers",
        "abstract": "Several methods have been proposed for classifying long textual documents using Transformers. However, there is a lack of consensus on a benchmark to enable a fair comparison among different approaches. In this paper, we provide a comprehensive evaluation of the relative efficacy measured against various baselines and diverse datasets \u2014 both in terms\nof accuracy as well as time and space overheads. Our datasets cover binary, multi-class, and multi-label classification tasks and represent various ways information is organized in a long text (e.g. information that is critical to making the classification decision is at the beginning or toward the end of the document).\nOur results show that more complex models\noften fail to outperform simple baselines and\nyield inconsistent performance across datasets.\nThese findings emphasize the need for future\nstudies to consider comprehensive baselines\nand datasets that better represent the task of\nlong document classification to develop robust\nmodels.",
        "label": 0
    },
    {
        "url": "https://arxiv.org/pdf/1906.04701",
        "title": "HEAD-QA: A Healthcare Dataset for Complex Reasoning",
        "abstract": "We present HEAD-QA, a multi-choice question answering testbed to encourage research on complex reasoning. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. We then consider monolingual (Spanish) and cross-lingual (to English) experiments with information retrieval and neural techniques. We show that: (i) HEAD-QA challenges current methods, and (ii) the results lag well behind human performance, demonstrating its usefulness as a benchmark for future work.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2406.08451",
        "title": "GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices",
        "abstract": "Smartphone users often navigate across multiple applications (apps) to complete tasks such as sharing content between social media platforms. Autonomous Graphical User Interface (GUI) navigation agents can enhance user experience in communication, entertainment, and productivity by streamlining workflows and reducing manual intervention. However, prior GUI agents often trained with datasets comprising simple tasks that can be completed within a single app, leading to poor performance in cross-app navigation. To address this problem, we introduce GUI Odyssey, a comprehensive dataset for training and evaluating cross-app navigation agents. GUI Odyssey consists of 7,735 episodes from 6 mobile devices, spanning 6 types of cross-app tasks, 201 apps, and 1.4K app combos. Leveraging GUI Odyssey, we developed OdysseyAgent, a multimodal cross-app navigation agent by fine-tuning the Qwen-VL model with a history resampling module. Extensive experiments demonstrate OdysseyAgent's superior accuracy compared to existing models. For instance, OdysseyAgent surpasses fine-tuned Qwen-VL and zero-shot GPT-4V by 1.44\\% and 55.49\\% in-domain accuracy, and 2.29\\% and 48.14\\% out-of-domain accuracy on average. The dataset and code will be released in \\url{this https URL}.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2401.14011",
        "title": "CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning",
        "abstract": "Multi-modal large language models(MLLMs) have achieved remarkable progress and demonstrated powerful knowledge comprehension and reasoning abilities. However, the mastery of domain-specific knowledge, which is essential for evaluating the intelligence of MLLMs, continues to be a challenge. Current multi-modal benchmarks for domain-specific knowledge concentrate on multiple-choice questions and are predominantly available in English, which imposes limitations on the comprehensiveness of the evaluation. To this end, we introduce CMMU, a novel benchmark for multi-modal and multi-type question understanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7 subjects, covering knowledge from primary to high school. The questions can be categorized into 3 types: multiple-choice, multiple-response, and fill-in-the-blank, bringing greater challenges to MLLMs. In addition, we propose an evaluation strategy called Positional Error Variance for assessing multiple-choice questions. The strategy aims to perform a quantitative analysis of position bias. We evaluate seven open-source MLLMs along with GPT4-V, Gemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a significant challenge to the recent MLLMs. The data and code are available at this https URL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2311.12793",
        "title": "ShareGPT4V: Improving Large Multi-Modal Models with Better Captions",
        "abstract": "In the realm of large multi-modal models (LMMs), efficient modality alignment is crucial yet often constrained by the scarcity of high-quality image-text data. To address this bottleneck, we introduce the ShareGPT4V dataset, a pioneering large-scale resource featuring 1.2 million highly descriptive captions, which surpasses existing datasets in diversity and information content, covering world knowledge, object properties, spatial relationships, and aesthetic evaluations. Specifically, ShareGPT4V originates from a curated 100K high-quality captions collected from advanced GPT4-Vision and has been expanded to 1.2M with a superb caption model trained on this subset. ShareGPT4V first demonstrates its effectiveness for the Supervised Fine-Tuning (SFT) phase, by substituting an equivalent quantity of detailed captions in existing SFT datasets with a subset of our high-quality captions, significantly enhancing the LMMs like LLaVA-7B, LLaVA-1.5-13B, and Qwen-VL-Chat-7B on the MME and MMBench benchmarks, with respective gains of 222.8/22.0/22.3 and 2.7/1.3/1.5. We further incorporate ShareGPT4V data into both the pre-training and SFT phases, obtaining ShareGPT4V-7B, a superior LMM based on a simple architecture that has remarkable performance across a majority of the multi-modal benchmarks. This project is available at this https URL to serve as a pivotal resource for advancing the LMMs community.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2310.00582",
        "title": "Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs",
        "abstract": "Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities in various multi-modal tasks. Nevertheless, their performance in fine-grained image understanding tasks is still limited. To address this issue, this paper proposes a new framework to enhance the fine-grained image understanding abilities of MLLMs. Specifically, we present a new method for constructing the instruction tuning dataset at a low cost by leveraging annotations in existing datasets. A self-consistent bootstrapping method is also introduced to extend existing dense object annotations into high-quality referring-expression-bounding-box pairs. These methods enable the generation of high-quality instruction data which includes a wide range of fundamental abilities essential for fine-grained image perception. Moreover, we argue that the visual encoder should be tuned during instruction tuning to mitigate the gap between full image perception and fine-grained image perception. Experimental results demonstrate the superior performance of our method. For instance, our model exhibits a 5.2% accuracy improvement over Qwen-VL on GQA and surpasses the accuracy of Kosmos-2 by 24.7% on RefCOCO_val. We have also attained the top rank on the leaderboard of MMBench. This promising performance is achieved by training on only publicly available data, making it easily reproducible. The models, datasets, and codes are publicly available at this https URL.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2305.01928",
        "title": "Visual Transformation Telling",
        "abstract": "Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\textbf{V}isual \\textbf{T}ransformation \\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2404.16852",
        "title": "A Disease Labeler for Chinese Chest X-Ray Report Generation",
        "abstract": "In the field of medical image analysis, the scarcity of Chinese chest X-ray report datasets has hindered the development of technology for generating Chinese chest X-ray reports. On one hand, the construction of a Chinese chest X-ray report dataset is limited by the time-consuming and costly process of accurate expert disease annotation. On the other hand, a single natural language generation metric is commonly used to evaluate the similarity between generated and ground-truth reports, while the clinical accuracy and effectiveness of the generated reports rely on an accurate disease labeler (classifier). To address the issues, this study proposes a disease labeler tailored for the generation of Chinese chest X-ray reports. This labeler leverages a dual BERT architecture to handle diagnostic reports and clinical information separately and constructs a hierarchical label learning algorithm based on the affiliation between diseases and body parts to enhance text classification performance. Utilizing this disease labeler, a Chinese chest X-ray report dataset comprising 51,262 report samples was established. Finally, experiments and analyses were conducted on a subset of expert-annotated Chinese chest X-ray reports, validating the effectiveness of the proposed disease labeler.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.04694",
        "title": "Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs",
        "abstract": "AI assistants such as ChatGPT are trained to respond to users by saying, \"I am a large language model\". This raises questions. Do such models know that they are LLMs and reliably act on this knowledge? Are they aware of their current circumstances, such as being deployed to the public? We refer to a model's knowledge of itself and its circumstances as situational awareness. To quantify situational awareness in LLMs, we introduce a range of behavioral tests, based on question answering and instruction following. These tests form the Situational Awareness Dataset (SAD), a benchmark comprising 7 task categories and over 13,000 questions. The benchmark tests numerous abilities, including the capacity of LLMs to (i) recognize their own generated text, (ii) predict their own behavior, (iii) determine whether a prompt is from internal evaluation or real-world deployment, and (iv) follow instructions that depend on self-knowledge.\nWe evaluate 16 LLMs on SAD, including both base (pretrained) and chat models. While all models perform better than chance, even the highest-scoring model (Claude 3 Opus) is far from a human baseline on certain tasks. We also observe that performance on SAD is only partially predicted by metrics of general knowledge (e.g. MMLU). Chat models, which are finetuned to serve as AI assistants, outperform their corresponding base models on SAD but not on general knowledge tasks. The purpose of SAD is to facilitate scientific understanding of situational awareness in LLMs by breaking it down into quantitative abilities. Situational awareness is important because it enhances a model's capacity for autonomous planning and action. While this has potential benefits for automation, it also introduces novel risks related to AI safety and control. Code and latest results available at this https URL .",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.04648",
        "title": "Efficient Materials Informatics between Rockets and Electrons",
        "abstract": "The true power of computational research typically can lay in either what it accomplishes or what it enables others to accomplish. In this work, both avenues are simultaneously embraced across several distinct efforts existing at three general scales of abstractions of what a material is - atomistic, physical, and design. At each, an efficient materials informatics infrastructure is being built from the ground up based on (1) the fundamental understanding of the underlying prior knowledge, including the data, (2) deployment routes that take advantage of it, and (3) pathways to extend it in an autonomous or semi-autonomous fashion, while heavily relying on artificial intelligence (AI) to guide well-established DFT-based ab initio and CALPHAD-based thermodynamic methods.\nThe resulting multi-level discovery infrastructure is highly generalizable as it focuses on encoding problems to solve them easily rather than looking for an existing solution. To showcase it, this dissertation discusses the design of multi-alloy functionally graded materials (FGMs) incorporating ultra-high temperature refractory high entropy alloys (RHEAs) towards gas turbine and jet engine efficiency increase reducing CO2 emissions, as well as hypersonic vehicles. It leverages a new graph representation of underlying mathematical space using a newly developed algorithm based on combinatorics, not subject to many problems troubling the community. Underneath, property models and phase relations are learned from optimized samplings of the largest and highest quality dataset of HEA in the world, called ULTERA. At the atomistic level, a data ecosystem optimized for machine learning (ML) from over 4.5 million relaxed structures, called MPDD, is used to inform experimental observations and improve thermodynamic models by providing stability data enabled by a new efficient featurization framework.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.03937",
        "title": "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models",
        "abstract": "Classical Chinese is a gateway to the rich heritage and wisdom of ancient China, yet its complexities pose formidable comprehension barriers for most modern people without specialized knowledge. While Large Language Models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), they struggle with Classical Chinese Understanding (CCU), especially in data-demanding and knowledge-intensive tasks. In response to this dilemma, we propose \\textbf{TongGu} (mean understanding ancient and modern), the first CCU-specific LLM, underpinned by three core contributions. First, we construct a two-stage instruction-tuning dataset ACCN-INS derived from rich classical Chinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we propose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting, enabling TongGu to acquire new capabilities while preserving its foundational knowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG) technique to reduce hallucinations based on knowledge-grounding. Extensive experiments across 24 diverse CCU tasks validate TongGu's superior ability, underscoring the effectiveness of RAT and CCU-RAG. The model and dataset will be public available.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.02996",
        "title": "Are Large Language Models Consistent over Value-laden Questions?",
        "abstract": "Large language models (LLMs) appear to bias their survey answers toward certain values. Nonetheless, some argue that LLMs are too inconsistent to simulate particular values. Are they? To answer, we first define value consistency as the similarity of answers across (1) paraphrases of one question, (2) related questions under one topic, (3) multiple-choice and open-ended use-cases of one question, and (4) multilingual translations of a question to English, Chinese, German, and Japanese. We apply these measures to a few large (>=34b), open LLMs including llama-3, as well as gpt-4o, using eight thousand questions spanning more than 300 topics. Unlike prior work, we find that models are relatively consistent across paraphrases, use-cases, translations, and within a topic. Still, some inconsistencies remain. Models are more consistent on uncontroversial topics (e.g., in the U.S., \"Thanksgiving\") than on controversial ones (\"euthanasia\"). Base models are both more consistent compared to fine-tuned models and are uniform in their consistency across topics, while fine-tuned models are more inconsistent about some topics (\"euthanasia\") than others (\"women's rights\") like our human subjects (n=165).",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.02552",
        "title": "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs",
        "abstract": "Preference optimization techniques have become a standard final stage for training state-of-art large language models (LLMs). However, despite widespread adoption, the vast majority of work to-date has focused on first-class citizen languages like English and Chinese. This captures a small fraction of the languages in the world, but also makes it unclear which aspects of current state-of-the-art research transfer to a multilingual setting. In this work, we perform an exhaustive study to achieve a new state-of-the-art in aligning multilingual LLMs. We introduce a novel, scalable method for generating high-quality multilingual feedback data to balance data coverage. We establish the benefits of cross-lingual transfer and increased dataset size in preference training. Our preference-trained model achieves a 54.4% win-rate against Aya 23 8B, the current state-of-the-art multilingual LLM in its parameter class, and a 69.5% win-rate or higher against widely used models like Gemma-1.1-7B-it, Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.3. As a result of our study, we expand the frontier of alignment techniques to 23 languages covering half of the world's population.",
        "label": 1
    },
    {
        "url": "https://arxiv.org/abs/2407.02301",
        "title": "CFinBench: A Comprehensive Chinese Financial Benchmark for Large Language Models",
        "abstract": "Large language models (LLMs) have achieved remarkable performance on various NLP tasks, yet their potential in more challenging and domain-specific task, such as finance, has not been fully explored. In this paper, we present CFinBench: a meticulously crafted, the most comprehensive evaluation benchmark to date, for assessing the financial knowledge of LLMs under Chinese context. In practice, to better align with the career trajectory of Chinese financial practitioners, we build a systematic evaluation from 4 first-level categories: (1) Financial Subject: whether LLMs can memorize the necessary basic knowledge of financial subjects, such as economics, statistics and auditing. (2) Financial Qualification: whether LLMs can obtain the needed financial qualified certifications, such as certified public accountant, securities qualification and banking qualification. (3) Financial Practice: whether LLMs can fulfill the practical financial jobs, such as tax consultant, junior accountant and securities analyst. (4) Financial Law: whether LLMs can meet the requirement of financial laws and regulations, such as tax law, insurance law and economic law. CFinBench comprises 99,100 questions spanning 43 second-level categories with 3 question types: single-choice, multiple-choice and judgment. We conduct extensive experiments of 50 representative LLMs with various model size on CFinBench. The results show that GPT4 and some Chinese-oriented models lead the benchmark, with the highest average accuracy being 60.16%, highlighting the challenge presented by CFinBench. The dataset and evaluation code are available at this https URL.",
        "label": 1
    }
]