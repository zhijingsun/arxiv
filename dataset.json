[
    {
        "url": "https://arxiv.org/pdf/2312.05762",
        "label": 1.0,
        "text_before_introduction": "multi-defendant legal judgment prediction via hierarchical reasoning\nyougang lyu1\u2217jitai hao1\u2217zihan wang1kai zhao2shen gao1\npengjie ren1zhumin chen1fang wang1zhaochun ren3\u2020\n1shandong university, qingdao, china\n2georgia state university, atlanta, usa\n3leiden university, leiden, the netherlands\n{youganglyu, 202215112, 202020630}@mail.sdu.edu.cn\nkzhao4@gsu.edu, shengao@sdu.edu.cn, jay.ren@outlook.com\n{chenzhumin, wangfang226}@sdu.edu.cn, z.ren@liacs.leidenuniv.nl\nabstract\nmultiple defendants in a criminal fact descrip-\ntion generally exhibit complex interactions,\nand cannot be well handled by existing legal\njudgment prediction ( ljp) methods which fo-\ncus on predicting judgment results (e.g., law\narticles, charges, and terms of penalty) for\nsingle-defendant cases. to address this prob-\nlem, we propose the task of multi-defendant\nljp, which aims to automatically predict the\njudgment results for each defendant of multi-\ndefendant cases. two challenges arise with the\ntask of multi-defendant ljp: (1) indistinguish-\nable judgment results among various defen-\ndants; and (2) the lack of a real-world dataset\nfor training and evaluation. to tackle the first\nchallenge, we formalize the multi-defendant\njudgment process as hierarchical reasoning\nchains and introduce a multi-defendant ljp\nmethod, named hierarchical reasoning net-\nwork ( hrn ), which follows the hierarchical\nreasoning chains to determine criminal relation-\nships, sentencing circumstances, law articles,\ncharges, and terms of penalty for each defen-\ndant. to tackle the second challenge, we col-\nlect a real-world multi-defendant ljpdataset,\nnamely multiljp, to accelerate the relevant re-\nsearch in the future. extensive experiments on\nmultiljp verify the effectiveness of our pro-\nposed hrn.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2309.05653",
        "label": 1.0,
        "text_before_introduction": "Abstract or Introduction section not found on the first page."
    },
    {
        "url": "https://arxiv.org/pdf/2405.00332",
        "label": 0.0,
        "text_before_introduction": "a careful examination of large language model\nperformance on grade school arithmetic\nhugh zhang\u2217jeff da dean lee vaughn robinson catherine wu will song\ntiffany zhao pranav raja dylan slack qin lyu sean hendryx russell kaplan\nmichele (mike) lunati\u2020summer yue\u2020\nscale ai\nabstract\nlarge language models (llms) have achieved impressive success on many bench-\nmarks for mathematical reasoning. however, there is growing concern that some\nof this performance actually reflects dataset contamination, where data closely\nresembling benchmark questions leaks into the training data, instead of true rea-\nsoning ability. to investigate this claim rigorously, we commission grade school\nmath 1000 (gsm1k). gsm1k is designed to mirror the style and complexity of\nthe established gsm8k benchmark, the gold standard for measuring elementary\nmathematical reasoning. we ensure that the two benchmarks are comparable across\nimportant metrics such as human solve rates, number of steps in solution, answer\nmagnitude, and more. when evaluating leading open- and closed-source llms on\ngsm1k, we observe accuracy drops of up to 13%, with several families of models\n(e.g. phi and mistral) showing evidence of systematic overfitting across almost all\nmodel sizes. at the same time, many models, especially those on the frontier, (e.g.\ngemini/gpt/claude) show minimal signs of overfitting. further analysis suggests\na positive relationship (spearman\u2019s r2= 0.32) between a model\u2019s probability of\ngenerating an example from gsm8k and its performance gap between gsm8k and\ngsm1k, suggesting that many models may have partially memorized gsm8k.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09046",
        "label": 1.0,
        "text_before_introduction": "Abstract or Introduction section not found on the first page."
    },
    {
        "url": "https://arxiv.org/pdf/2406.08863",
        "label": 0.0,
        "text_before_introduction": "self-supervised graph neural network for\nmechanical cad retrieval\nyuhan quan\nquanyh15@tsinghua.org.cn\n4paradigm inc.\nbeijing, chinahuan zhao\nzhaohuan@4paradigm.com\n4paradigm inc.\nbeijing, china\njinfeng yi\nyijinfeng@4paradigm.com\n4paradigm inc.\nbeijing, chinayuqiang chen\nchenyuqiang@4paradigm.com\n4paradigm inc.\nbeijing, china\nabstract\ncad (computer-aided design) plays a crucial role in mechanical\nindustry, where large numbers of similar-shaped cad parts are\noften created. efficiently reusing these parts is key to reducing\ndesign and production costs for enterprises. retrieval systems are\nvital for achieving cad reuse, but the complex shapes of cad\nmodels are difficult to accurately describe using text or keywords,\nmaking traditional retrieval methods ineffective. while existing\nrepresentation learning approaches have been developed for cad,\nmanually labeling similar samples in these methods is expensive.\nadditionally, cad models\u2019 unique parameterized data structure\npresents challenges for applying existing 3d shape representation\nlearning techniques directly. in this work, we propose gc-cad, a\nself-supervised contrastive graph neural network-based method for\nmechanical cad retrieval that directly models parameterized cad\nraw files. gc-cad consists of two key modules: structure-aware\nrepresentation learning and constrastive graph learning framework.\nthe method leverages graph neural networks to extract both geo-\nmetric and topological information from cad models, generating\nfeature representations. we then introduce a simple yet effective\ncontrastive graph learning framework approach, enabling the model\nto train without manual labels and generate retrieval-ready repre-\nsentations. experimental results on four datasets including human\nevaluation demonstrate that the proposed method achieves sig-\nnificant accuracy improvements and up to 100 times efficiency\nimprovement over the baseline methods.\nkeywords\nself-supervised learning, graph neural network, computer-aided\ndesign\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08866",
        "label": 0.0,
        "text_before_introduction": "zoom and shift is all you need\njiahao qina;*\naxi\u2019an jiaotong-liverpool university\norcid id: jiahao qin https://orcid.org/https://orcid.org/0000-0002-0551-4647\nabstract. feature alignment serves as the primary mechanism for\nfusing multimodal data. we put forth a feature alignment approach\nthat achieves full integration of multimodal information. this is ac-\ncomplished via an alternating process of shifting and expanding fea-\nture representations across modalities to obtain a consistent uni-\nfied representation in a joint feature space. the proposed technique\ncan reliably capture high-level interplay between features originating\nfrom distinct modalities. consequently, substantial gains in multi-\nmodal learning performance are attained. additionally, we demon-\nstrate the superiority of our approach over other prevalent multi-\nmodal fusion schemes on a range of tasks. extensive experimental\nevaluation conducted on multimodal datasets comprising time series,\nimage, and text demonstrates that our method achieves state-of-the-\nart results.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08827",
        "label": 1.0,
        "text_before_introduction": "how powerful is graph filtering for recommendation\nshaowen peng\npeng.shaowen@naist.ac.jp\nnara institute of science and technology\nnara, japanxin liu\nxin.liu@aist.go.jp\nnational institute of advanced industrial science and\ntechnology\ntokyo, japan\nkazunari sugiyama\nsugiyama-k@g.osaka-seikei.ac.jp\nosaka seikei university\nosaka, japantsunenori mine\nmine@m.ait.kyushu-u.ac.jp\nkyushu university\nfukuoka, japan\nabstract\nit has been shown that the effectiveness of graph convolutional\nnetwork (gcn) for recommendation is attributed to the spectral\ngraph filtering. most gcn-based methods consist of a graph filter\nor followed by a low-rank mapping optimized based on supervised\ntraining. however, we show two limitations suppressing the power\nof graph filtering: (1) lack of generality. due to the varied noise\ndistribution, graph filters fail to denoise sparse data where noise is\nscattered across all frequencies, while supervised training results\nin worse performance on dense data where noise is concentrated\nin middle frequencies that can be removed by graph filters without\ntraining. (2) lack of expressive power. we theoretically show that\nlinear gcn (lgcn) that is effective on collaborative filtering (cf)\ncannot generate arbitrary embeddings, implying the possibility that\noptimal data representation might be unreachable.\nto tackle the first limitation, we show close relation between\nnoise distribution and the sharpness of spectrum where a sharper\nspectral distribution is more desirable causing data noise to be\nseparable from important features without training. based on this\nobservation, we propose a generalized graph normalization ( g2n)\nwith hyperparameters adjusting the sharpness of spectral distri-\nbution in order to redistribute data noise to assure that it can be\nremoved by graph filtering without training. as for the second\nlimitation, we propose an individualized graph filter (igf) adapt-\ning to the different confidence levels of the user preference that\ninteractions can reflect, which is proved to be able to generate ar-\nbitrary embeddings. by simplifying lgcn, we further propose a\nsimplified graph filtering for cf (sgfcf)1which only requires the\ntop-\ud835\udc3esingular values for recommendation. finally, experimental\nresults on four datasets with different density settings demonstrate\nthe effectiveness and efficiency of our proposed methods.\n1https://github.com/tanatosuu/sgfcf\npermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. copyrights for components of this work owned by others than acm\nmust be honored. abstracting with credit is permitted. to copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. request permissions from permissions@acm.org.\nconference acronym \u2019xx, june 03\u201305, 2018, woodstock, ny\n\u00a92018 association for computing machinery.\nacm isbn 978-1-4503-xxxx-x/18/06. . . $15.00\nhttps://doi.org/xxxxxxx.xxxxxxxccs concepts\n\u2022information systems \u2192recommender systems .\nkeywords\nrecommender system, collaborative filtering, graph convolu-\ntional network\nacm reference format:\nshaowen peng, xin liu, kazunari sugiyama, and tsunenori mine. 2018.\nhow powerful is graph filtering for recommendation. in proceedings of\nmake sure to enter the correct conference title from your rights confirmation\nemai (conference acronym \u2019xx). acm, new york, ny, usa, 12 pages. https:\n//doi.org/xxxxxxx.xxxxxxx\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08920",
        "label": 0.0,
        "text_before_introduction": "a v-gs: learning material and geometry aware\npriors for novel view acoustic synthesis\nswapnil bhosale\nuniversity of surrey, ukhaosen yang\nuniversity of surrey, ukdiptesh kanojia\nuniversity of surrey, uk\njiankang deng\nimperial college london, ukxiatian zhu\nuniversity of surrey, uk\nabstract\nnovel view acoustic synthesis (nv as) aims to render binaural audio at any target\nviewpoint, given a mono audio emitted by a sound source at a 3d scene. existing\nmethods have proposed nerf-based implicit models to exploit visual cues as a\ncondition for synthesizing binaural audio. however, in addition to low efficiency\noriginating from heavy nerf rendering, these methods all have a limited ability of\ncharacterizing the entire scene environment such as room geometry, material prop-\nerties, and the spatial relation between the listener and sound source. to address\nthese issues, we propose a novel audio-visual gaussian splatting (a v-gs) model.\nto obtain a material-aware and geometry-aware condition for audio synthesis, we\nlearn an explicit point-based scene representation with an audio-guidance param-\neter on locally initialized gaussian points, taking into account the space relation\nfrom the listener and sound source. to make the visual scene model audio adaptive,\nwe propose a point densification and pruning strategy to optimally distribute the\ngaussian points, with the per-point contribution in sound propagation (e.g., more\npoints needed for texture-less wall surfaces as they affect sound path diversion).\nextensive experiments validate the superiority of our a v-gs over existing alterna-\ntives on the real-world rwas and simulation-based soundspaces datasets. project\npage: https://surrey-uplab.github.io/research/avgs/\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08863",
        "label": 0.0,
        "text_before_introduction": "self-supervised graph neural network for\nmechanical cad retrieval\nyuhan quan\nquanyh15@tsinghua.org.cn\n4paradigm inc.\nbeijing, chinahuan zhao\nzhaohuan@4paradigm.com\n4paradigm inc.\nbeijing, china\njinfeng yi\nyijinfeng@4paradigm.com\n4paradigm inc.\nbeijing, chinayuqiang chen\nchenyuqiang@4paradigm.com\n4paradigm inc.\nbeijing, china\nabstract\ncad (computer-aided design) plays a crucial role in mechanical\nindustry, where large numbers of similar-shaped cad parts are\noften created. efficiently reusing these parts is key to reducing\ndesign and production costs for enterprises. retrieval systems are\nvital for achieving cad reuse, but the complex shapes of cad\nmodels are difficult to accurately describe using text or keywords,\nmaking traditional retrieval methods ineffective. while existing\nrepresentation learning approaches have been developed for cad,\nmanually labeling similar samples in these methods is expensive.\nadditionally, cad models\u2019 unique parameterized data structure\npresents challenges for applying existing 3d shape representation\nlearning techniques directly. in this work, we propose gc-cad, a\nself-supervised contrastive graph neural network-based method for\nmechanical cad retrieval that directly models parameterized cad\nraw files. gc-cad consists of two key modules: structure-aware\nrepresentation learning and constrastive graph learning framework.\nthe method leverages graph neural networks to extract both geo-\nmetric and topological information from cad models, generating\nfeature representations. we then introduce a simple yet effective\ncontrastive graph learning framework approach, enabling the model\nto train without manual labels and generate retrieval-ready repre-\nsentations. experimental results on four datasets including human\nevaluation demonstrate that the proposed method achieves sig-\nnificant accuracy improvements and up to 100 times efficiency\nimprovement over the baseline methods.\nkeywords\nself-supervised learning, graph neural network, computer-aided\ndesign\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09021",
        "label": 0.0,
        "text_before_introduction": "contextual distillation model for diversified recommendation\nfan li\u2217\nuniversity of science and technology\nof china\nbeijing, china\nlf321@mail.ustc.edu.cnxu si\u2217\ntsinghua university\nbeijing, china\nsix21@mails.tsinghua.edu.cnshisong tang\u2217\nkuaishou inc.\ntsinghua university\nbeijing, china\ntangshisong@kuaishou.com\ndingmin wang\nuniversity of oxford\noxford, united kingdom\ndingmin.wang@cs.ox.ac.ukkunyan han\nkuaishou inc.\nbeijing, china\nhankunyan@kuaishou.combing han\nkuaishou inc.\nbeijing, china\nhanbing@kuaishou.com\nguorui zhou\nkuaishou inc.\nbeijing, china\nzhouguorui@kuaishou.comyang song\nkuaishou inc.\nbeijing, china\nyangsong@kuaishou.comhechang chen\u2020\njilin university\nchangchun, china\nchenhc@jlu.edu.cn\nabstract\nthe diversity of recommendation is equally crucial as accuracy in\nimproving user experience. existing studies, e.g., determinantal\npoint process (dpp) and maximal marginal relevance (mmr), em-\nploy a greedy paradigm to iteratively select items that optimize both\naccuracy and diversity. however, prior methods typically exhibit\nquadratic complexity, limiting their applications to the re-ranking\nstage and are not applicable to other recommendation stages with a\nlarger pool of candidate items, such as the pre-ranking andranking\nstages. in this paper, we propose contextual distillation model\n(cdm), an efficient recommendation model that addresses diver-\nsification, suitable for the deployment in all stages of industrial\nrecommendation pipelines. specifically, cdm utilizes the candidate\nitems in the same user request as context to enhance the diversifi-\ncation of the results. we propose a contrastive context encoder that\nemploys attention mechanisms to model both positive and negative\ncontexts. for the training of cdm, we compare each target item\nwith its context embedding and utilize the knowledge distillation\nframework to learn the win probability of each target item under the\nmmr algorithm, where the teacher is derived from mmr outputs.\nduring inference, ranking is performed through a linear combina-\ntion of the recommendation and student model scores, ensuring\nboth diversity and efficiency. we perform offline evaluations on\ntwo industrial datasets and conduct online a/btest of cdm on the\nshort-video platform kuaishou . the considerable enhancements\n\u2217equal contributions.\n\u2020corresponding author.\npermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. copyrights for components of this work owned by others than the\nauthor(s) must be honored. abstracting with credit is permitted. to copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. request permissions from permissions@acm.org.\nkdd \u201924, august 25\u201329, 2024, barcelona, spain\n\u00a92024 copyright held by the owner/author(s). publication rights licensed to acm.\nacm isbn 979-8-4007-0490-1/24/08. . . $15.00\nhttps://doi.org/10.1145/3637528.3671514observed in both recommendation quality and diversity, as shown\nby metrics, provide strong superiority for the effectiveness of cdm.\nccs concepts\n\u2022information systems \u2192recommender systems .\nkeywords\nrecommender system, knowledge distillation, diversified recom-\nmendation\nacm reference format:\nfan li, xu si, shisong tang, dingmin wang, kunyan han, bing han, guorui\nzhou, yang song, and hechang chen. 2024. contextual distillation model\nfor diversified recommendation. in proceedings of the 30th acm sigkdd\nconference on knowledge discovery and data mining (kdd \u201924), august\n25\u201329, 2024, barcelona, spain. acm, new york, ny, usa, 10 pages. https:\n//doi.org/10.1145/3637528.3671514\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09215",
        "label": 0.0,
        "text_before_introduction": "on softmax direct preference optimization for\nrecommendation\nyuxin chen1\u2217junfei tan2\u2217an zhang1\u2020\nzhengyi yang2leheng sheng1enzhi zhang3\nxiang wang2tat-seng chua1\n1national university of singapore\n2university of science and technology of china\n3hokkaido university\ne1143404@u.nus.edu ,sober_clever@mail.ustc.edu.cn ,anzhang@u.nus.edu\ne1374479@u.nus.edu ,enzhi.zhang.n6@elms.hokudai.ac.jp\nyangzhy1998@gmail.com ,xiangwang1223@gmail.com ,dcscts@nus.edu.sg\nabstract\nrecommender systems aim to predict personalized rankings based on user pref-\nerence data. with the rise of language models (lms), lm-based recommenders\nhave been widely explored due to their extensive world knowledge and power-\nful reasoning abilities. most of the lm-based recommenders convert historical\ninteractions into language prompts, pairing with a positive item as the target re-\nsponse and fine-tuning lm with a language modeling loss. however, the current\nobjective fails to fully leverage preference data and is not optimized for personal-\nized ranking tasks, which hinders the performance of lm-based recommenders.\ninspired by the current advancement of direct preference optimization (dpo)\nin human preference alignment and the success of softmax loss in recommenda-\ntions, we propose softmax-dpo ( s-dpo ) to instill ranking information into the\nlm to help lm-based recommenders distinguish preferred items from negatives,\nrather than solely focusing on positives. specifically, we incorporate multiple\nnegatives in user preference data and devise an alternative version of dpo loss\ntailored for lm-based recommenders, connected to softmax sampling strategies.\ntheoretically, we bridge s-dpo with the softmax loss over negative sampling and\nfind that it has a side effect of mining hard negatives, which assures its excep-\ntional capabilities in recommendation tasks. empirically, extensive experiments\nconducted on three real-world datasets demonstrate the superiority of s-dpo to\neffectively model user preference and further boost recommendation performance\nwhile mitigating the data likelihood decline issue of dpo. our codes are available\nathttps://github.com/chenyuxin1999/s-dpo .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08922",
        "label": 0.0,
        "text_before_introduction": "navigating the shadows: unveiling effective disturbances for modern ai\ncontent detectors\nying zhou1,2, ben he1,2b, le sun2\n1school of computer science and technology,\nuniversity of chinese academy of sciences, beijing, china\n2chinese information processing laboratory,\ninstitute of software, chinese academy of sciences, beijing, china\nzhouying20@mails.ucas.ac.cn, benhe@ucas.ac.cn, sunle@iscas.ac.cn\nabstract\nwith the launch of chatgpt, large language\nmodels (llms) have attracted global attention.\nin the realm of article writing, llms have wit-\nnessed extensive utilization, giving rise to con-\ncerns related to intellectual property protection,\npersonal privacy, and academic integrity. in re-\nsponse, ai-text detection has emerged to distin-\nguish between human and machine-generated\ncontent. however, recent research indicates\nthat these detection systems often lack robust-\nness and struggle to effectively differentiate\nperturbed texts. currently, there is a lack of\nsystematic evaluations regarding detection per-\nformance in real-world applications, and a com-\nprehensive examination of perturbation tech-\nniques and detector robustness is also absent.\nto bridge this gap, our work simulates real-\nworld scenarios in both informal and profes-\nsional writing, exploring the out-of-the-box per-\nformance of current detectors. additionally, we\nhave constructed 12 black-box text perturba-\ntion methods to assess the robustness of current\ndetection models across various perturbation\ngranularities. furthermore, through adversarial\nlearning experiments, we investigate the impact\nof perturbation data augmentation on the robust-\nness of ai-text detectors. we have released our\ncode and data at https://github.com/zhouy\ning20/ai-text-detector-evaluation .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08587",
        "label": 1.0,
        "text_before_introduction": "cs-bench: a comprehensive benchmark for large\nlanguage models towards computer science mastery\nxiaoshuai song\u2217, muxi diao\u2217, guanting dong, zhengyang wang, yujia fu, runqi qiao,\nzhexu wang, dayuan fu, huangxuan wu, bin liang, weihao zeng, yejie wang,\nzhuoma gongque, jianing yu, qiuna tan, weiran xu\u2020\nbeijing university of posts and telecommunications, beijing, china\n{songxiaoshuai, dmx, xuweiran}@bupt.edu.cn\nhttps://csbench.github.io\nabstract\ncomputer science (cs) stands as a testament to the intricacies of human intelli-\ngence, profoundly advancing the development of artificial intelligence and modern\nsociety. however, the current community of large language models (llms) overly\nfocuses on benchmarks for analyzing specific foundational skills (e.g. mathematics\nand code generation), neglecting an all-round evaluation of the computer science\nfield. to bridge this gap, we introduce cs-bench, the first bilingual (chinese-\nenglish) benchmark dedicated to evaluating the performance of llms in computer\nscience. cs-bench comprises approximately 5k meticulously curated test samples,\ncovering 26 subfields across 4 key areas of computer science, encompassing var-\nious task forms and divisions of knowledge and reasoning. utilizing cs-bench,\nwe conduct a comprehensive evaluation of over 30 mainstream llms, revealing\nthe relationship between cs performance and model scales. we also quantita-\ntively analyze the reasons for failures in existing llms and highlight directions\nfor improvements, including knowledge supplementation and cs-specific reason-\ning. further cross-capability experiments show a high correlation between llms\u2019\ncapabilities in computer science and their abilities in mathematics and coding.\nmoreover, expert llms specialized in mathematics and coding also demonstrate\nstrong performances in several cs subfields. looking ahead, we envision cs-\nbench serving as a cornerstone for llm applications in the cs field and paving\nnew avenues in assessing llms\u2019 diverse reasoning capabilities. the cs-bench\ndata and evaluation code are available at https://github.com/csbench/csbench.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08931",
        "label": 0.0,
        "text_before_introduction": "exploring multilingual unseen speaker emotion recognition: leveraging\nco-attention cues in multitask learning\narnav goel1\u2020#, medha hira1\u2020, anubha gupta1,2\n1sbilab, indraprastha institute of information technology delhi (iiit-d), india\n2mirae ai systems pvt. ltd.\narnav21519,medha21265,anubha {@iiitd.ac.in }\nabstract\nadvent of modern deep learning techniques has given rise to ad-\nvancements in the field of speech emotion recognition (ser).\nhowever, most systems prevalent in the field fail to general-\nize to speakers not seen during training. this study focuses\non handling challenges of multilingual ser, specifically on un-\nseen speakers. we introduce camulenet, a novel architecture\nleveraging co-attention based fusion and multitask learning to\naddress this problem. additionally, we benchmark pretrained\nencoders of whisper, hubert, wav2vec2.0, and wavlm us-\ning 10-fold leave-speaker-out cross-validation on five exist-\ning multilingual benchmark datasets: iemocap, ra vdess,\ncrema-d, emodb and cafe and, release a novel dataset for\nser on the hindi language (bhavvani). camulenet shows an\naverage improvement of approximately 8% over all benchmarks\non unseen speakers determined by our cross-validation strategy.\nindex terms : speaker emotion recognition, co-attention, mul-\ntitask learning, new dataset\n1."
    },
    {
        "url": "https://arxiv.org/pdf/2406.08973",
        "label": 0.0,
        "text_before_introduction": "xland-100b: a large-scale multi-task\ndataset for in-context reinforcement learning\nalexander nikulin\u2217\u2020\nairi\nnikulin@airi.netilya zisman\u2217\u2020\nairi\nzisman@airi.netalexey zemtsov\u2217\ntinkoff\na.s.zemtsov@tinkoff.ai\nviacheslav sinii\ntinkoff\nv.siniy@tinkoff.aivladislav kurenkov\u2020\nairi\nkurenkov@airi.netsergey kolesnikov\ntinkoff\ns.s.kolesnikov@tinkoff.ai\nabstract\nfollowing the success of the in-context learning paradigm in large-scale language\nand computer vision models, the recently emerging field of in-context reinforce-\nment learning is experiencing a rapid growth. however, its development has\nbeen held back by the lack of challenging benchmarks, as all the experiments\nhave been carried out in simple environments and on small-scale datasets. we\npresent xland-100b , a large-scale dataset for in-context reinforcement learning\nbased on the xland-minigrid environment, as a first step to alleviate this prob-\nlem. it contains complete learning histories for nearly 30,000different tasks,\ncovering 100b transitions and 2.5b episodes. it took 50,000gpu hours to col-\nlect the dataset, which is beyond the reach of most academic labs. along with\nthe dataset, we provide the utilities to reproduce or expand it even further. with\nthis substantial effort, we aim to democratize research in the rapidly growing\nfield of in-context reinforcement learning and provide a solid foundation for fur-\nther scaling. the code is open-source and available under apache 2.0 licence at\nhttps://github.com/dunno-lab/xland-minigrid-datasets .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08598",
        "label": 1.0,
        "text_before_introduction": "language model council: benchmarking foundation\nmodels on highly subjective tasks by consensus\njustin zhao1, flor miriam plaza-del-arco2, amanda cercas curry2,\n1predibase,2bocconi university\njustin@predibase.com\n{flor.plaza, amanda.cercas}@unibocconi.it\nhttps://www.llm-council.com\nabstract\nthe rapid advancement of large language models (llms) necessitates robust\nand challenging benchmarks. leaderboards like chatbot arena rank llms based\non how well their responses align with human preferences. however, many tasks\nsuch as those related to emotional intelligence, creative writing, or persuasiveness,\nare highly subjective and often lack majoritarian human agreement. judges may\nhave irreconcilable disagreements about what constitutes a better response. to\naddress the challenge of ranking llms on highly subjective tasks, we propose\na novel benchmarking framework, the language model council (lmc) . the\nlmc operates through a democratic process to: 1) formulate a test set through\nequal participation, 2) administer the test among council members, and 3) evaluate\nresponses as a collective jury. we deploy a council of 20 newest llms on an\nopen-ended emotional intelligence task: responding to interpersonal dilemmas.\nour results show that the lmc produces rankings that are more separable, robust,\nand less biased than those from any individual llm judge, and is more consistent\nwith a human-established leaderboard compared to other benchmarks.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08979",
        "label": 1.0,
        "text_before_introduction": "multi-agent software development through cross-team collaboration\nzhuoyun du\u2020\u2663chen qian\u2020\u22c6wei liu\u22c6zihao xie\u22c6\nyifei wang\u22c6yufan dang\u22c6weize chen\u22c6cheng yang\u2660b\n\u2663zhejiang university\u22c6tsinghua university\n\u2660beijing university of posts and telecommunications\nduzy.zju@outlook.com qianc62@gmail.com yangcheng@bupt.edu.cn\nabstract\nthe latest breakthroughs in large language\nmodels (llms), e.g., chatdev, have catalyzed\nprofound transformations, particularly through\nmulti-agent collaboration for software devel-\nopment. llm agents can collaborate in teams\nlike humans, and follow the waterfall model\nto sequentially work on requirements analysis,\ndevelopment, review, testing, and other phases\nto perform autonomous software generation.\nhowever, for an agent team, each phase in a\nsingle development process yields only one pos-\nsible outcome. this results in the completion\nof only one development chain, thereby losing\nthe opportunity to explore multiple potential\ndecision paths within the solution space. con-\nsequently, this may lead to obtaining subop-\ntimal results. to address this challenge, we\nintroduce cross-teamcollaboration (ctc),\na scalable multi-team framework that enables\norchestrated teams to jointly propose various\ndecisions and communicate with their insights\nin a cross-team collaboration environment for\nsuperior content generation. experimental re-\nsults in software development reveal a notable\nincrease in quality compared to state-of-the-\nart baselines, underscoring the efficacy of our\nframework. the significant improvements in\nstory generation demonstrate the promising\ngeneralization ability of our framework across\nvarious domains. we anticipate that our work\nwill guide llm agents towards a cross-team\nparadigm and contribute to their significant\ngrowth in but not limited to software devel-\nopment. the code and data will be available at\nhttps://github.com/openbmb/chatdev .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09043",
        "label": 0.0,
        "text_before_introduction": "language models are crossword solvers\nsoumadeep saha and sutanoya chakraborty and saptarshi saha and utpal garain\nindian statistical institute\nkolkata, india\ncorrespondence: soumadeep.saha97@gmail.com\nabstract\ncrosswords are a form of word puzzle that re-\nquire a solver to demonstrate a high degree\nof proficiency in natural language understand-\ning, wordplay, reasoning, and world knowl-\nedge, along with adherence to character and\nlength constraints. in this paper we tackle the\nchallenge of solving crosswords with large\nlanguage models (llms). we demonstrate\nthat the current generation of state-of-the art\n(sota) language models show significant com-\npetence at deciphering cryptic crossword clues,\nand outperform previously reported sota re-\nsults by a factor of 2-3 in relevant benchmarks.\nwe also develop a search algorithm that builds\noff this performance to tackle the problem of\nsolving full crossword grids with llms for\nthe very first time, achieving an accuracy of\n93% on new york times crossword puzzles.\ncontrary to previous work in this area which\nconcluded that llms lag human expert perfor-\nmance significantly, our research suggests this\ngap is a lot narrower.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09009",
        "label": 1.0,
        "text_before_introduction": "fredformer: frequency debiased transformer for time series\nforecasting\nxihao piao\nsanken, osaka university\nosaka, japan\npark88@sanken.osaka-u.ac.jpzheng chen\nsanken, osaka university\nosaka, japan\nchenz@sanken.osaka-u.ac.jptaichi murayama\nsanken, osaka university\nosaka, japan\ntaichi@sanken.osaka-u.ac.jp\nyasuko matsubara\nsanken, osaka university\nosaka, japan\nyasuko@sanken.osaka-u.ac.jpyasushi sakurai\nsanken, osaka university\nosaka, japan\nyasushi@sanken.osaka-u.ac.jp\nabstract\nthe transformer model has shown leading performance in time se-\nries forecasting. nevertheless, in some complex scenarios, it tends\nto learn low-frequency features in the data and overlook high-\nfrequency features, showing a frequency bias. this bias prevents\nthe model from accurately capturing important high-frequency\ndata features. in this paper, we undertake empirical analyses to\nunderstand this bias and discover that frequency bias results from\nthe model disproportionately focusing on frequency features with\nhigher energy. based on our analysis, we formulate this bias and\npropose fredformer , a transformer-based framework designed\nto mitigate frequency bias by learning features equally across dif-\nferent frequency bands. this approach prevents the model from\noverlooking lower amplitude features important for accurate fore-\ncasting. extensive experiments show the effectiveness of our pro-\nposed approach, which can outperform other baselines in differ-\nent real-world time-series datasets. furthermore, we introduce a\nlightweight variant of the fredformer with an attention matrix\napproximation, which achieves comparable performance but with\nmuch fewer parameters and lower computation costs. the code is\navailable at: https://github.com/chenzrg/fredformer\nccs concepts\n\u2022computing methodologies \u2192artificial intelligence ;neural\nnetworks .\nkeywords\ntime series forecasting, deep learning\nacm reference format:\nxihao piao, zheng chen, taichi murayama, yasuko matsubara, and yasushi\nsakurai. 2024. fredformer: frequency debiased transformer for time\nseries forecasting . in proceedings of the 30th acm sigkdd conference\non knowledge discovery and data mining (kdd \u201924), august 25\u201329, 2024,\npermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. copyrights for components of this work owned by others than the\nauthor(s) must be honored. abstracting with credit is permitted. to copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. request permissions from permissions@acm.org.\nkdd \u201924, august 25\u201329, 2024, barcelona, spain\n\u00a92024 copyright held by the owner/author(s). publication rights licensed to acm.\nacm isbn 979-8-4007-0490-1/24/08\nhttps://doi.org/10.1145/3637528.3671928barcelona, spain. acm, new york, ny, usa, 17 pages. https://doi.org/10.\n1145/3637528.3671928\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08680",
        "label": 0.0,
        "text_before_introduction": "analyzing large language models for classroom\ndiscussion assessment\nnhat tran\nuniversity of pittsburgh\npittsburgh, pa, usa\nnlt26@pitt.edubenjamin pierce\nuniversity of pittsburgh\npittsburgh, pa, usa\nbep51@pitt.edudiane litman\nuniversity of pittsburgh\npittsburgh, pa, usa\ndlitman@pitt.edu\nrichard correnti\nuniversity of pittsburgh\npittsburgh, pa, usa\nrcorrent@pitt.edulindsay clare matsumura\nuniversity of pittsburgh\npittsburgh, pa, usa\nlclare@pitt.edu\nabstract\nautomatically assessing classroom discussion quality is be-\ncoming increasingly feasible with the help of new nlp ad-\nvancements such as large language models (llms). in this\nwork, we examine how the assessment performance of 2\nllms interacts with 3 factors that may affect performance:\ntask formulation, context length, and few-shot examples.\nwe also explore the computational efficiency and predic-\ntive consistency of the 2 llms. our results suggest that\nthe 3 aforementioned factors do affect the performance of\nthe tested llms and there is a relation between consistency\nand performance. we recommend a llm-based assessment\napproach that has a good balance in terms of predictive per-\nformance, computational efficiency, and consistency.\nkeywords\nclassroom discussion, large language models, scoring\n1."
    },
    {
        "url": "https://arxiv.org/pdf/2406.08707",
        "label": 1.0,
        "text_before_introduction": "moscar: a large-scale multilingual\nand multimodal document-level corpus\nmatthieu futeral\u22171,2armel zebaze1,4pedro ortiz suarez5julien abadji1\nr\u00e9mi lacroix3, 6cordelia schmid1,2rachel bawden1beno\u00eet sagot1\n1inria2d\u00e9partement d\u2019informatique de l\u2019ens, cnrs, psl research university\n3institut du d\u00e9veloppement et des ressources en informatique scientifique, cnrs\n4sorbonne universit\u00e9, paris, france5common crawl foundation6universit\u00e9 paris-saclay\nabstract\nmultimodal large language models (mllms) are trained on a large amount of\ntext-image data. while most mllms are trained on caption-like data only, alayrac\net al. [2022] showed that additionally training them on interleaved sequences of text\nand images can lead to the emergence of in-context learning capabilities. however,\nthe dataset they used, m3w, is not public and is only in english. there have\nbeen attempts to reproduce their results but the released datasets are english-only.\nin contrast, current multilingual and multimodal datasets are either composed\nof caption-like only or medium-scale or fully private data. this limits mllm\nresearch for the 7,000 other languages spoken in the world. we therefore introduce\nmoscar, to the best of our knowledge the first large-scale multilingual and\nmultimodal document corpus crawled from the web. it covers 163 languages,\n315m documents, 214b tokens and 1.2b images. we carefully conduct a set of\nfiltering and evaluation steps to make sure moscar is sufficiently safe, diverse and\nof good quality. we additionally train two types of multilingual model to prove the\nbenefits of moscar: (1) a model trained on a subset of moscar and captioning\ndata and (2) a model train on captioning data only. the model additionally trained\non moscar shows a strong boost in few-shot learning performance across various\nmultilingual image-text tasks and benchmarks, confirming previous findings for\nenglish-only mllms. the dataset can be accessed here.2\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09031",
        "label": 1.0,
        "text_before_introduction": "a c omprehensive graph pooling benchmark :\neffectiveness , robustness and generalizability\npengyun wang1, junyu luo2, yanxin shen3, siyu heng4\u2217, xiao luo5\u2217\n1university of oxford,2peking university,3nankai university,\n4new york university,5university of california, los angles\npengyun.wang@oii.ox.ac.uk, siyuheng@nyu.edu, xiaoluo@cs.ucla.edu\nabstract\ngraph pooling has gained attention for its ability to obtain effective node and graph representations\nfor various downstream tasks. despite the recent surge in graph pooling approaches, there is a lack of\nstandardized experimental settings and fair benchmarks to evaluate their performance. to address\nthis issue, we have constructed a comprehensive benchmark that includes 15 graph pooling methods\nand 21 different graph datasets. this benchmark systematically assesses the performance of graph\npooling methods in three dimensions, i.e., effectiveness, robustness, and generalizability. we first\nevaluate the performance of these graph pooling approaches across different tasks including graph\nclassification, graph regression and node classification. then, we investigate their performance under\npotential noise attacks and out-of-distribution shifts in real-world scenarios. we also involve detailed\nefficiency analysis and parameter analysis. extensive experiments validate the strong capability and\napplicability of graph pooling approaches in various scenarios, which can provide valuable insights\nand guidance for deep geometric learning research. the source code of our benchmark is available at\nhttps://github.com/goose315/graph_pooling_benchmark .\nkeywords graph pooling; benchmark; graph neural networks; graph machine learning\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09052",
        "label": 1.0,
        "text_before_introduction": "Abstract or Introduction section not found on the first page."
    },
    {
        "url": "https://arxiv.org/pdf/2406.08726",
        "label": 0.0,
        "text_before_introduction": "arxiv:2406.08726v1  [cs.cl]  13 jun 2024standardlanguage ideology inai-generated language\ngenevievesmith*\ngenevieve.smith@berkeley.edu\nucberkeley\nberkeley,ca,usaeve fleisig*\ne\ufb02eisig@berkeley.edu\nucberkeley\nberkeley,ca,usamadelinebossi\nucberkeley\nberkeley,ca,usa\nishitarustagi\nucberkeley\nberkeley,ca,usaxavieryin\nucberkeley\nberkeley,ca,usa\nkeywords\nlanguage models, generative ai, language ideology, dialec ts, natu-\nral language processing\nacmreference format:\ngenevievesmith*, eve fleisig*,madelinebossi, ishitarustagi, and xav ier\nyin.2018.standardlanguageideologyinai-generatedlanguage. acm/ims\nj. data sci. 37,4,article111 (august 2018),7pages.\nabstract\nin this position paper, we explore standard language ideolo gy in\nlanguagegeneratedbylargelanguagemodels(llms).first, weout-\nlinehowstandardlanguageideologyisre\ufb02ectedandreinfor ced in\nllms. we then present a taxonomy of open problems regarding\nstandard language ideology in ai-generated language with i mpli-\ncations for minoritized language communities. we introduc e the\nconcept of standard ai-generated language ideology, the pr ocess\nbywhich ai-generated language regards standard american e ng-\nlish(sae)asalinguisticdefaultandreinforcesalinguist icbiasthat\nsae is the most \u201cappropriate\u201d language. finally, we discuss ten-\nsions that remain, including re\ufb02ecting on what desirable sy stem\nbehaviorlookslike,aswellasadvantagesanddrawbacksofg ener-\native ai tools imitating\u2014or often not\u2014di\ufb00erent english lan guage\nvarieties.throughout,wediscussstandardlanguageideol ogyasa\nmanifestation of existing global power structures in and th rough\nai-generated language before ending with questions to move to-\nwards alternative, moreemancipatorydigital futures.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08757",
        "label": 0.0,
        "text_before_introduction": "srfund: a multi-granularity hierarchical structure\nreconstruction benchmark in form understanding\njiefeng ma1yan wang1chenyu liu2jun du1\u2217yu hu1\nzhenrong zhang1pengfei hu1qing wang1jianshu zhang2\n1university of science and technology of china, hefei, china\n2iflytek, hefei, china\n{jfma, yanwangsa, zzr666, hudeyouxiang}@mail.ustc.edu.cn ,\n{jundu, yuhu2, qingwang2}@ustc.edu.cn, {cyliu7, jszhang6}@iflytek.com\nabstract\naccurately identifying and organizing textual content is crucial for the automation\nof document processing in the field of form understanding. existing datasets, such\nas funsd and xfund, support entity classification and relationship prediction\ntasks but are typically limited to local and entity-level annotations. this limitation\noverlooks the hierarchically structured representation of documents, constraining\ncomprehensive understanding of complex forms. to address this issue, we present\nthe srfund, a hierarchically structured multi-task form understanding bench-\nmark. srfund provides refined annotations on top of the original funsd and\nxfund datasets, encompassing five tasks: (1) word to text-line merging , (2)\ntext-line to entity merging , (3) entity category classification , (4) item table lo-\ncalization , and (5) entity-based full-document hierarchical structure recovery .\nwe meticulously supplemented the original dataset with missing annotations at\nvarious levels of granularity and added detailed annotations for multi-item table\nregions within the forms. additionally, we introduce global hierarchical struc-\nture dependencies for entity relation prediction tasks, surpassing traditional local\nkey-value associations. the srfund dataset includes eight languages including\nenglish, chinese, japanese, german, french, spanish, italian, and portuguese ,\nmaking it a powerful tool for cross-lingual form understanding. extensive exper-\nimental results demonstrate that the srfund dataset presents new challenges\nand significant opportunities in handling diverse layouts and global hierarchical\nstructures of forms, thus providing deep insights into the field of form understand-\ning. the original dataset and implementations of baseline methods are available at\nhttps://sprateam-ustc.github.io/srfund .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08796",
        "label": 1.0,
        "text_before_introduction": "deep exploration of cross-lingual zero-shot generalization in\ninstruction tuning\njanghoon han\u2217changho lee\u2217joongbo shin\nstanley jungkyu choi honglak lee kyunghoon bae\nlg ai research\n{janghoon.han,changho.lee}@lgresearch.ai\nabstract\ninstruction tuning has emerged as a powerful\ntechnique, significantly boosting zero-shot per-\nformance on unseen tasks. while recent work\nhas explored cross-lingual generalization by ap-\nplying instruction tuning to multilingual mod-\nels, previous studies have primarily focused\non english, with a limited exploration of non-\nenglish tasks. for an in-depth exploration of\ncross-lingual generalization in instruction tun-\ning, we perform instruction tuning individu-\nally for two distinct language meta-datasets.\nsubsequently, we assess the performance on\nunseen tasks in a language different from the\none used for training. to facilitate this inves-\ntigation, we introduce a novel non-english\nmeta-dataset named \"korani\" (korean natu-\nral instruction), comprising 51 korean bench-\nmarks. moreover, we design cross-lingual tem-\nplates to mitigate discrepancies in language\nand instruction-format of the template between\ntraining and inference within the cross-lingual\nsetting. our experiments reveal consistent im-\nprovements through cross-lingual generaliza-\ntion in both english and korean, outperforming\nbaseline by average scores of 20.7% and 13.6%,\nrespectively. remarkably, these enhancements\nare comparable to those achieved by monolin-\ngual instruction tuning and even surpass them\nin some tasks. the result underscores the sig-\nnificance of relevant data acquisition across lan-\nguages over linguistic congruence with unseen\ntasks during instruction tuning1.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09056",
        "label": 0.0,
        "text_before_introduction": "received: added at production revised: added at production accepted: added at production\ndoi: xxx/xxxx\na r t i c l e t y p e\ncudrt: benchmarking the detection of human vs. large\nlanguage models generated texts\nzhen tao*1zhiyu li*2dinghao xi3wei xu1\n1school of information, renmin university of\nchina, beijing, china\n2institute for advanced algorithms research,\nshanghai, china\n3department of digital economics, shanghai\nuniversity of finance and economics, shanghai,\nchina\ncorrespondence\ncorresponding author dinghao xi.\nemail: xidinghao@mail.shufe.edu.cnabstract\nthe proliferation of large language models (llms) has significantly enhanced text generation capabilities\nacross various industries. however, these models\u2019 ability to generate human-like text poses substantial\nchallenges in discerning between human and artificial intelligence (ai) authorship. despite the effectiveness\nof existing ai-generated text detectors, their development is hindered by the lack of comprehensive, publicly\navailable benchmarks. current benchmarks are limited to specific scenarios, such as question answering\nand text polishing, and predominantly focus on english texts, failing to capture the diverse applications and\nlinguistic nuances of llms. to address these limitations, this paper constructs a comprehensive bilingual\nbenchmark in both chinese and english to evaluate mainstream ai-generated text detectors. we categorize\nllm text generation into five distinct operations: creation, updating, deletion, rewriting, and translation\n(cudrt ), encompassing all current llms activities. we also establish a robust benchmark evaluation\nframework to support scalable and reproducible experiments. for each cudrt category, we have developed\nextensive datasets to thoroughly assess detector performance. by employing the latest mainstream llms\nspecific to each language, our datasets provide a thorough evaluation environment. extensive experimental\nresults offer critical insights for optimizing ai-generated text detectors and suggest future research directions\nto improve detection accuracy and generalizability across various scenarios. source code is available at\ngithub\u2020.\nk e y w o r d s\nai-generated text, large language models, llms operations, text detection, benchmarking\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09105",
        "label": 1.0,
        "text_before_introduction": "ins-mmbench: a comprehensive benchmark for\nevaluating lvlms\u2019 performance in insurance\nchenwei lin\nfudan university\ncwlin23@m.fudan.edu.cnhanjia lyu\nuniversity of rochester\nhlyu5@ur.rochester.edu\nxian xu\nfudan university\nxianxu@fudan.edu.cnjiebo luo\nuniversity of rochester\njluo@cs.rochester.edu\nabstract\nlarge vision-language models (lvlms) have demonstrated outstanding per-\nformance in various general multimodal applications such as image recogni-\ntion and visual reasoning, and have also shown promising potential in special-\nized domains. however, the application potential of lvlms in the insurance\ndomain\u2014characterized by rich application scenarios and abundant multimodal\ndata\u2014has not been effectively explored. there is no systematic review of multi-\nmodal tasks in the insurance domain, nor a benchmark specifically designed to\nevaluate the capabilities of lvlms in insurance. this gap hinders the development\nof lvlms within the insurance domain. in this paper, we systematically review and\ndistill multimodal tasks for four representative types of insurance: auto insurance,\nproperty insurance, health insurance, and agricultural insurance. we propose ins-\nmmbench, the first comprehensive lvlms benchmark tailored for the insurance\ndomain. ins-mmbench comprises a total of 2.2k thoroughly designed multiple-\nchoice questions, covering 12 meta-tasks and 22 fundamental tasks. furthermore,\nwe evaluate multiple representative lvlms, including closed-source models such\nas gpt-4o and open-source models like blip-2. this evaluation not only validates\nthe effectiveness of our benchmark but also provides an in-depth performance\nanalysis of current lvlms on various multimodal tasks in the insurance domain.\nwe hope that ins-mmbench will facilitate the further application of lvlms in\nthe insurance domain and inspire interdisciplinary development. our dataset and\nevaluation code are available at https://github.com/fdu-ins/ins-mmbench .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09112",
        "label": 0.0,
        "text_before_introduction": "large-scale evaluation of\nopen-set image classification techniques\nhalil bisgin bisgin@umich.edu\ndepartment of computer science\nuniversity of michigan-flint\nflint, mi 48502, usa\nandres palechor andrespalechor11@gmail.com\nmike suter mike.milow.suter@gmail.com\nmanuel g \u00a8unther siebenkopf@googlemail.com\ndepartment of informatics\nuniversity of zurich\nandreasstrasse 15\n8050 zurich, switzerland\nabstract\nthe goal for classification is to correctly assign labels to unseen samples. however, most\nmethods misclassify samples with unseen labels and assign them to one of the known classes.\nopen-set classification (osc) algorithms aim to maximize both closed and open-set recog-\nnition capabilities. recent studies showed the utility of such algorithms on small-scale\ndata sets, but limited experimentation makes it difficult to assess their performances in\nreal-world problems. here, we provide a comprehensive comparison of various osc algo-\nrithms, including training-based (softmax, garbage, eos) and post-processing methods\n(maximum softmax scores, maximum logit scores, openmax, evm, proser), the lat-\nter are applied on features from the former. we perform our evaluation on three large-scale\nprotocols that mimic real-world challenges, where we train on known and negative open-set\nsamples, and test on known and unknown instances. our results show that eos helps to\nimprove performance of almost all post-processing algorithms. particularly, openmax and\nproser are able to exploit better-trained networks, demonstrating the utility of hybrid\nmodels. however, while most algorithms work well on negative test samples \u2013 samples of\nopen-set classes seen during training \u2013 they tend to perform poorly when tested on samples\nof previously unseen unknown classes, especially in challenging conditions.\nkeywords: open-set classification, large-scale evaluation, image classification, deep learn-\ning, reproducible research\n1."
    },
    {
        "url": "https://arxiv.org/pdf/2406.09155",
        "label": 1.0,
        "text_before_introduction": "defan: definitive answer dataset for llms\nhallucination evaluation\na b m ashikur rahman\u2217\nics department\nkfupm\ndhahran, ksa - 31261\ng202204800@kfupm.edu.sasaeed anwar\nics department, kfupm\njrcai, sdaia-kfupm\ndhahran, ksa - 31261\nsaeed.anwar@kfupm.edu.sa\nmuhammad usman\nics department, kfupm\njrcai, sdaia-kfupm\ndhahran, ksa - 31261\nmuhammad.usman@kfupm.edu.saajmal mian\nthe university of western australia\ncrawley, western australia\najmal.mian@uwa.edu.au\nabstract\nlarge language models (llms) have demonstrated remarkable capabilities, revo-\nlutionizing the integration of ai in daily life applications. however, they are prone\nto hallucinations, generating claims that contradict established facts, deviating from\nprompts, and producing inconsistent responses when the same prompt is presented\nmultiple times. addressing these issues is challenging due to the lack of com-\nprehensive and easily assessable benchmark datasets. most existing datasets are\nsmall and rely on multiple-choice questions, which are inadequate for evaluating\nthe generative prowess of llms. to measure hallucination in llms, this paper\nintroduces a comprehensive benchmark dataset comprising over 75,000 prompts\nacross eight domains. these prompts are designed to elicit definitive, concise,\nand informative answers. the dataset is divided into two segments: one publicly\navailable for testing and assessing llm performance and a hidden segment for\nbenchmarking various llms. in our experiments, we tested six llms\u2014gpt-3.5,\nllama 2, llama 3, gemini, mixtral, and zephyr\u2014revealing that overall factual\nhallucination ranges from 59% to 82% on the public dataset and 57% to 76% in the\nhidden benchmark. prompt misalignment hallucination ranges from 6% to 95% in\nthe public dataset and 17% to 94% in the hidden counterpart. average consistency\nranges from 21% to 61% and 22% to 63%, respectively. domain-wise analysis\nshows that llm performance significantly deteriorates when asked for specific\nnumeric information while performing moderately with person, location, and date\nqueries. our dataset demonstrates its efficacy and serves as a comprehensive\nbenchmark for llm performance evaluation. our dataset and llms responses are\navailable at https://github.com/ashikiut/defan.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09181",
        "label": 1.0,
        "text_before_introduction": "a large-scale universal evaluation benchmark for\nface forgery detection\nyijun bei\nzhejiang university\nbeiyj@zju.edu.cnhengrui lou\nzhejiang university\nhengrui@zju.edu.cnjinsong geng\nzhejiang university\n22251346@zju.edu.cn\nerteng liu\nzhejiang university\nlet@zju.edu.cnlechao cheng\nhefei university of technology\nchenglc@hfut.edu.cnjie song\nzhejiang university\nsjie@zju.edu.cn\nmingli song\nzhejiang university\nbrooksong@zju.edu.cnzunlei feng\u2217\nzhejiang university\nzunleifeng@zju.edu.cn\nabstract\nwith the rapid development of ai-generated content (aigc) technology, the\nproduction of realistic fake facial images and videos that deceive human visual\nperception has become possible. consequently, various face forgery detection\ntechniques have been proposed to identify such fake facial content. however,\nevaluating the effectiveness and generalizability of these detection techniques re-\nmains a significant challenge. to address this, we have constructed a large-scale\nevaluation benchmark called deepfacegen, aimed at quantitatively assessing the\neffectiveness of face forgery detection and facilitating the iterative development\nof forgery detection technology. deepfacegen consists of 776 ,990real face\nimage/video samples and 773 ,812face forgery image/video samples, generated\nusing 34mainstream face generation techniques. during the construction pro-\ncess, we carefully consider important factors such as content diversity, fairness\nacross ethnicities, and availability of comprehensive labels, in order to ensure\nthe versatility and convenience of deepfacegen. subsequently, deepfacegen is\nemployed in this study to evaluate and analyze the performance of 13mainstream\nface forgery detection techniques from various perspectives. through extensive\nexperimental analysis, we derive significant findings and propose potential direc-\ntions for future research. the code and dataset for deepfacegen are available at\nhttps://github.com/hengruilou/deepfacegen.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09215",
        "label": 0.0,
        "text_before_introduction": "on softmax direct preference optimization for\nrecommendation\nyuxin chen1\u2217junfei tan2\u2217an zhang1\u2020\nzhengyi yang2leheng sheng1enzhi zhang3\nxiang wang2tat-seng chua1\n1national university of singapore\n2university of science and technology of china\n3hokkaido university\ne1143404@u.nus.edu ,sober_clever@mail.ustc.edu.cn ,anzhang@u.nus.edu\ne1374479@u.nus.edu ,enzhi.zhang.n6@elms.hokudai.ac.jp\nyangzhy1998@gmail.com ,xiangwang1223@gmail.com ,dcscts@nus.edu.sg\nabstract\nrecommender systems aim to predict personalized rankings based on user pref-\nerence data. with the rise of language models (lms), lm-based recommenders\nhave been widely explored due to their extensive world knowledge and power-\nful reasoning abilities. most of the lm-based recommenders convert historical\ninteractions into language prompts, pairing with a positive item as the target re-\nsponse and fine-tuning lm with a language modeling loss. however, the current\nobjective fails to fully leverage preference data and is not optimized for personal-\nized ranking tasks, which hinders the performance of lm-based recommenders.\ninspired by the current advancement of direct preference optimization (dpo)\nin human preference alignment and the success of softmax loss in recommenda-\ntions, we propose softmax-dpo ( s-dpo ) to instill ranking information into the\nlm to help lm-based recommenders distinguish preferred items from negatives,\nrather than solely focusing on positives. specifically, we incorporate multiple\nnegatives in user preference data and devise an alternative version of dpo loss\ntailored for lm-based recommenders, connected to softmax sampling strategies.\ntheoretically, we bridge s-dpo with the softmax loss over negative sampling and\nfind that it has a side effect of mining hard negatives, which assures its excep-\ntional capabilities in recommendation tasks. empirically, extensive experiments\nconducted on three real-world datasets demonstrate the superiority of s-dpo to\neffectively model user preference and further boost recommendation performance\nwhile mitigating the data likelihood decline issue of dpo. our codes are available\nathttps://github.com/chenyuxin1999/s-dpo .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09260",
        "label": 1.0,
        "text_before_introduction": "deep transformer network for monocular\npose estimation of ship-based uav\nmaneesha wickramasuriya\u2217, taeyoung lee\u2020, murray snyder\u2021\nthe george washington university, 800 22nd st nw, washington dc 20052\nthis paper introduces a deep transformer network for estimating the relative 6d pose of a\nunmannedaerialvehicle(uav)withrespecttoashipusingmonocularimages. asynthetic\ndataset of ship images is created and annotated with 2d keypoints of multiple ship parts. a\ntransformer neural network model is trained to detect these keypoints and estimate the 6d\npose of each part. the estimates are integrated using bayesian fusion. the model is tested\non synthetic data and in-situ flight experiments, demonstrating robustness and accuracy in\nvarious lighting conditions. the position estimation error is approximately 0.8% and 1.0%\nofthedistancetotheshipforthesyntheticdataandtheflightexperiments,respectively. the\nmethod has potential applications for ship-based autonomous uav landing and navigation.\nsource code : fdcl-gwu/tnn-mo , video : https://youtu.be/zg_zvvs8xw8\ni."
    },
    {
        "url": "https://arxiv.org/pdf/2406.09321",
        "label": 1.0,
        "text_before_introduction": "jailbreakeval : an integrated toolkit for evaluating\njailbreak attempts against large language models\ndelong ran1, jinyuan liu1, yichen gong1, jingyi zheng2,\nxinlei he2, tianshuo cong1(b), anyu wang1\n1tsinghua university\n2the hong kong university of science and technology (guangzhou)\nabstract\njailbreak attacks aim to induce large language models (llms) to generate harmful responses\nfor forbidden instructions, presenting severe misuse threats to llms. up to now, research into\njailbreak attacks and defenses is emerging, however, there is (surprisingly) no consensus on how\nto evaluate whether a jailbreak attempt is successful. in other words, the methods to assess the\nharmfulness of an llm\u2019s response are varied, such as manual annotation or prompting gpt-4\nin specific ways. each approach has its own set of strengths and weaknesses, impacting their\nalignment with human values, as well as the time and financial cost. this diversity in evaluation\npresents challenges for researchers in choosing suitable evaluation methods and conducting fair\ncomparisons across different jailbreak attacks and defenses.\nin this paper, we conduct a comprehensive analysis of jailbreak evaluation methodologies,\ndrawing from nearly ninety jailbreak research released between may 2023 and april 2024. our\nstudy introduces a systematic taxonomy of jailbreak evaluators, offering in-depth insights into\ntheir strengths and weaknesses, along with the current status of their adaptation. moreover,\nto facilitate subsequent research, we propose jailbreakeval (https://github.com/thuccslab/\njailbreakeval ), a user-friendly toolkit focusing on the evaluation of jailbreak attempts. it includes\nvarious well-known evaluators out-of-the-box, so that users can obtain evaluation results with only\na single command. jailbreakeval also allows users to customize their own evaluation workflow\nin a unified framework with the ease of development and comparison. in summary, we regard\njailbreakeval to be a catalyst that simplifies the evaluation process in jailbreak research and\nfosters an inclusive standard for jailbreak evaluation within the community.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09326",
        "label": 1.0,
        "text_before_introduction": "pianomotion10m : dataset and benchmark\nfor hand motion generation in piano performance\nqijun gan\nzhejiang university\nganqijun@zju.edu.cnsong wang\nzhejiang university\nsongw@zju.edu.cn\nshengtao wu\nhangzhou dianzi university\n20010131@hdu.edu.cnjianke zhu\nzhejiang university\njkzhu@zju.edu.cn\nabstract\nrecently, artificial intelligence techniques for education have been received increas-\ning attentions, while it still remains an open problem to design the effective music\ninstrument instructing systems. although key presses can be directly derived from\nsheet music, the transitional movements among key presses require more extensive\nguidance in piano performance. in this work, we construct a piano-hand motion\ngeneration benchmark to guide hand movements and fingerings for piano playing.\nto this end, we collect an annotated dataset, pianomotion10m , consisting of\n116 hours of piano playing videos from a bird\u2019s-eye view with 10 million annotated\nhand poses. we also introduce a powerful baseline model that generates hand\nmotions from piano audios through a position predictor and a position-guided\ngesture generator. furthermore, a series of evaluation metrics are designed to\nassess the performance of the baseline model, including motion similarity, smooth-\nness, positional accuracy of left and right hands, and overall fidelity of movement\ndistribution. despite that piano key presses with respect to music scores or audios\nare already accessible, pianomotion10m aims to provide guidance on piano\nfingering for instruction purposes. the dataset and source code can be accessed at\nhttps://agnjason.github.io/pianomotion-page .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09388",
        "label": 0.0,
        "text_before_introduction": "exploring the spectrum of visio-linguistic compositionality and recognition\nyoungtaek oh1* pyunghwan ahn2jinhyung kim2gwangmo song2\nsoonyoung lee2\u2020in so kweon1\u2020junmo kim1\u2020\n1kaist2lg ai research\n1{youngtaek.oh, iskweon77, junmo.kim }@kaist.ac.kr\n2{p.ahn, jinhyung.kim, gwangmo.song, soonyoung.lee }@lgresearch.ai\nabstract\nvision and language models (vlms) such as clip have\nshowcased remarkable zero-shot recognition abilities yet\nface challenges in visio-linguistic compositionality, partic-\nularly in linguistic comprehension and fine-grained image-\ntext alignment. this paper explores the intricate relation-\nship between compositionality and recognition \u2013 two piv-\notal aspects of vlm capability. we conduct a compre-\nhensive evaluation of existing vlms, covering both pre-\ntraining approaches aimed at recognition and the fine-\ntuning methods designed to improve compositionality. our\nevaluation employs 12 benchmarks for compositionality,\nalong with 21 zero-shot classification and two retrieval\nbenchmarks for recognition. in our analysis from 274 clip\nmodel checkpoints, we reveal patterns and trade-offs that\nemerge between compositional understanding and recog-\nnition accuracy. ultimately, this necessitates strategic ef-\nforts towards developing models that improve both capabil-\nities, as well as the meticulous formulation of benchmarks\nfor compositionality. we open our evaluation framework at\nhttps://github.com/ytaek-oh/vl_compo .\n1."
    },
    {
        "url": "https://arxiv.org/pdf/2406.09401",
        "label": 1.0,
        "text_before_introduction": "mmscan: a multi-modal 3d scene dataset with\nhierarchical grounded language annotations\nruiyuan lyu1,2\u2217, tai wang1\u2217, jingli lin1,3\u2217, shuai yang1,4\u2217, xiaohan mao1,3, yilun chen1,\nrunsen xu1,5, haifeng huang1,4, chenming zhu1,6, dahua lin1,5, jiangmiao pang1\u2020\n1shanghai ai laboratory,2tsinghua university,3shanghai jiao tong university,\n4zhejiang university,5the chinese university of hong kong,6the university of hong kong\n\u2217equal contribution\u2020corresponding author\nfigure 1: mmscan provides the largest ever multi-modal 3d scene dataset with 6.9m hierarchical\ngrounded language annotations, covering holistic aspects on both object- and region-level.\nabstract\nwith the emergence of llms and their integration with other data modalities,\nmulti-modal 3d perception attracts more attention due to its connectivity to the\nphysical world and makes rapid progress. however, limited by existing datasets,\nprevious works mainly focus on understanding object properties or inter-object\nspatial relationships in a 3d scene. to tackle this problem, this paper builds the\nfirst largest ever multi-modal 3d scene dataset and benchmark with hierarchical\ngrounded language annotations, mmscan. it is constructed based on a top-down\nlogic, from region to object level, from a single target to inter-target relation-\nships, covering holistic aspects of spatial and attribute understanding. the overall\npipeline incorporates powerful vlms via carefully designed prompts to initialize\nthe annotations efficiently and further involve humans\u2019 correction in the loop to\nensure the annotations are natural, correct, and comprehensive. built upon exist-\ning 3d scanning data, the resulting multi-modal 3d dataset encompasses 1.4m\nmeta-annotated captions on 109k objects and 7.7k regions as well as over 3.04m\ndiverse samples for 3d visual grounding and question-answering benchmarks. we\nevaluate representative baselines on our benchmarks, analyze their capabilities in\ndifferent aspects, and showcase the key problems to be addressed in the future.\nfurthermore, we use this high-quality dataset to train state-of-the-art 3d visual\ngrounding and llms and obtain remarkable performance improvement both on\nexisting benchmarks and in-the-wild evaluation. codes, datasets, and benchmarks\nwill be available at https://github.com/openrobotlab/embodiedscan .\npreprint. under review.arxiv:2406.09401v1  [cs.cv]  13 jun 20241"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09406",
        "label": 1.0,
        "text_before_introduction": "4m-21: an any-to-any vision model\nfor tens of tasks and modalities\nroman bachmann1\u2020\u2217o\u02d8guzhan fatih kar1\u2217david mizrahi2\u2020\u2217ali garjani1\nmingfei gao2david griffiths2jiaming hu2afshin dehghan2amir zamir1\n1swiss federal institute of technology lausanne (epfl)2apple\nhttps://4m.epfl.ch\nimage\nmetadata\norig.res.:512x512\ncolorfulness:35%\ncontrast:45%\nbrightness:60%\nsaturation:40% \n\u2026\nrgb\ndepth\nsemantic\nsegmentation\nclipfeatures \n(dense)\nsurface\nnormals\ndinov2 \nfeatures\n(dense)\ncannyedges\nbounding\nboxes\ncolorpalette\nimagebind\nfeatures\n(dense)\nsam\ninstancessamedges\n3dhuman\nposes\nsemantic\nmetadata\n#humans:7 \n#instances:12\nobjectness:40%\nwalkability:40%\nclutterscore:75%\n\u2026geometric\nmetadata\ngeometric\ncomplexity:55%\nocclusionscore:\n25%\n\u2026dinov2\nfeatures\n(global)imagebind\nfeatures\n(global)webtext\nalbany\ninternational\nairportservesas\nthemajorair\ncenterforthe\ncapitalregion,\nnortheastern...caption\ngettingreadyfor\nmyflight!t5-xxl\nembeddings\ntransformer\nencodertransformer\ndecodergeometricmodalities\nsemanticmodalities\nfeaturemapmodalitiestextmodalities\nmetadatamodalitiesrgbmodalities edgemodalities\nglobalfeaturemodalitiesany-to-anymodel\nfigure 1: we demonstrate training a single model on tens of highly diverse modalities without a loss in\nperformance compared to specialized single/few task models. the modalities are mapped to discrete tokens\nusing modality-specific tokenizers. the model can generate anyof the modalities from any subset of them.\nabstract\ncurrent multimodal and multitask foundation models, like 4m [ 62] or uni-\nfiedio [ 59,58], show promising results. however, their out-of-the-box abilities to\naccept diverse inputs and perform diverse tasks are limited by the (usually small)\nnumber of modalities and tasks they are trained on. in this paper, we develop\na single any-to-any model trained on tens of highly diverse modalities and by\nperforming co-training on large-scale multimodal datasets and text corpora. this\nincludes training on images and text along with several semantic and geometric\nmodalities, feature maps from recent state of the art models like dinov2 and\nimagebind, pseudo labels of specialist models like sam and 4dhumans, and a\nrange of new modalities that allow for novel ways to interact with the model and\nsteer the generation, for example, image metadata or color palettes. a crucial step\nin this process is performing discrete tokenization on various modalities, whether\nthey are image-like, neural network feature maps, vectors, structured data like\ninstance segmentation or human poses, or data that can be represented as text.\nthrough this, we show the possibility of training one model to solve at least 3x more\ntasks/modalities than existing models and doing so without a loss in performance .\nin addition, this enables more fine-grained and controllable multimodal generation\ncapabilities and allows studying the distillation of models trained on diverse data\nand objectives into one unified model. we scale the training to a three billion\nparameter and different datasets. the multimodal models and training code are\nopen sourced at https://4m.epfl.ch .\n*equal contribution & corresponding authors. randomized order.\n\u2020work partially done while at epfl and apple.\npreprint.arxiv:2406.09406v2  [cs.cv]  14 jun 20241"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09410",
        "label": 1.0,
        "text_before_introduction": "Abstract or Introduction section not found on the first page."
    },
    {
        "url": "https://arxiv.org/pdf/2406.09411",
        "label": 1.0,
        "text_before_introduction": "muirbench : a comprehensive benchmark for\nrobust multi-image understanding\nfei wang1\u2217xingyu fu2\u2217james y . huang1\u2020zekun li3\u2020qin liu4\u2020xiaogeng liu5\u2020\nmingyu derek ma6\u2020nan xu1\u2020wenxuan zhou1\u2020kai zhang7tianyi lorena yan1\nwenjie jacky mo1hsiang-hui liu3pan lu6chunyuan li8chaowei xiao5kai-wei chang6\ndan roth2sheng zhang9hoifung poon9muhao chen4\n1usc2upenn3umn4uc davis5uw\u2013madison6ucla7osu8bytedance9microsoft research\nfigure 1: themuirbench benchmark. muirbench contains 11,264 images and 2,600 multiple-\nchoice questions, providing robust evaluation on 12 multi-image understanding tasks. each example\ncomes from one task in m uirbench , presenting diverse multi-image relations.\nabstract\nwe introduce muirbench , a comprehensive benchmark that focuses on robust\nmulti-image understanding capabilities of multimodal llms. muirbench con-\nsists of 12 diverse multi-image tasks ( e.g., scene understanding, ordering) that\ninvolve 10 categories of multi-image relations ( e.g., multiview, temporal relations).\ncomprising 11,264 images and 2,600 multiple-choice questions, muirbench\nis created in a pairwise manner, where each standard instance is paired with an\nunanswerable variant that has minimal semantic differences, in order for a reliable\nassessment. evaluated upon 20 recent multi-modal llms, our results reveal that\neven the best-performing models like gpt-4o and gemini pro find it challenging\nto solve muirbench , achieving 68.0% and 49.3% in accuracy. open-source\nmultimodal llms trained on single images can hardly generalize to multi-image\nquestions, hovering below 33.3% in accuracy. these results highlight the im-\nportance of muirbench in encouraging the community to develop multimodal\nllms that can look beyond a single image, suggesting potential pathways for\nfuture improvements.\n\u2217equal leadership. correspondance to <fwang598@usc.edu; xingyuf2@seas.upenn.edu>.\n\u2020equal contribution; alphabetic order.\nproject page: https://huggingface.co/datasets/muirbench/muirbench\npreprint. under review.arxiv:2406.09411v1  [cs.cv]  13 jun 2024figure 2: compared with previous benchmarks, muirbench has several novel features: (1) it\nevaluates on a comprehensive range of 12 multi-image understanding abilities, e.g. geographic\nunderstanding and diagram understanding as introduced in \u00a73, while prior benchmarks generally\ncontain single-image questions. (2) it contains 10 diverse multi-image relations, e.g. narrative and\ncomplementary as discussed in \u00a73. (3) it provides a robust evaluation on models by unanswerable\ninstance variants. the samples of previous benchmarks are from [25, 37, 53].\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09979",
        "label": 1.0,
        "text_before_introduction": "hiro: hierarchical information retrieval optimization\nkrish goel, mahek chandak\nkrishgoel3@gmail.com, mahekchandak99@gmail.com\nabstract\nlarge language models (llms) excel in natural language\ntasks but face limitations due to static training datasets, result-\ning in outdated or contextually shallow responses. retrieval-\naugmented generation (rag) addresses this by integrating\nreal-time external knowledge, enhancing model accuracy and\ncredibility, especially for knowledge-intensive tasks. how-\never, rag-enhanced llms struggle with long contexts, caus-\ning them to \u201dchoke\u201d on information overload, compromising\nresponse quality. recent rag applications use hierarchical\ndata structures for storing documents, organized at various\nlevels of summarization and information density. in this con-\ntext, we introduce hiro (hierarchical information retrieval\noptimization), a novel querying approach for rag appli-\ncations using hierarchical structures for storing documents.\nhiro employs dfs-based recursive similarity score calcula-\ntion and branch pruning to minimize the context returned to\nthe llm without informational loss. hiro outperforms ex-\nisting querying mechanisms on the narrativeqa dataset by\nan absolute performance gain of 10.85%.\ncode \u2014 https://github.com/krishgoel/hiro"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09464",
        "label": 0.0,
        "text_before_introduction": "gpt-ology, computational models, silicon sampling:\nhow should we think about llms in cognitive science?\ndesmond c. ong (desmond.ong@utexas.edu)\ndepartment of psychology, the university of texas at austin\nabstract\nlarge language models have taken the cognitive sci-\nence world by storm. it is perhaps timely now to take\nstock of the various research paradigms that have been\nused to make scientific inferences about \u201ccognition\u201d in\nthese models or about human cognition. we review sev-\neral emerging research paradigms\u2014gpt-ology, llms-\nas-computational-models, and \u201csilicon sampling\u201d\u2014 and\nreview recent papers that have used llms under these\nparadigms. in doing so, we discuss their claims as well\nas challenges to scientific inference under these vari-\nous paradigms. we highlight several outstanding is-\nsues about llms that have to be addressed to push\nour science forward: closed-source vs open-sourced mod-\nels; (the lack of visibility of) training data; and repro-\nducibility in llm research, including forming conven-\ntions on new task \u201chyperparameters\u201d like instructions\nand prompts.\nkeywords: large language models; cognitive science"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09671",
        "label": 0.0,
        "text_before_introduction": "evaluating chatgpt-4 vision on brazil\u2019s national\nundergraduate computer science exam\nnabor c. mendon\u00e7a, post-graduate program in applied informatics, university of fortaleza, brazil\nthe recent integration of visual capabilities into large language models (llms) has the potential to play\na pivotal role in science and technology education, where visual elements such as diagrams, charts, and\ntables are commonly used to improve the learning experience. this study investigates the performance of\nchatgpt-4 vision, openai\u2019s most advanced visual model at the time the study was conducted, on the bachelor\nin computer science section of brazil\u2019s 2021 national undergraduate exam (enade). by presenting the\nmodel with the exam\u2019s open and multiple-choice questions in their original image format and allowing for\nreassessment in response to differing answer keys, we were able to evaluate the model\u2019s reasoning and self-\nreflecting capabilities in a large-scale academic assessment involving textual and visual content. chatgpt-4\nvision significantly outperformed the average exam participant, positioning itself within the top 10 best score\npercentile. while it excelled in questions that incorporated visual elements, it also encountered challenges\nwith question interpretation, logical reasoning, and visual acuity. a positive correlation between the model\u2019s\nperformance in multiple-choice questions and the performance distribution of the human participants suggests\nmultimodal llms can provide a useful tool for question testing and refinement. however, the involvement of\nan independent expert panel to review cases of disagreement between the model and the answer key revealed\nsome poorly constructed questions containing vague or ambiguous statements, calling attention to the critical\nneed for improved question design in future exams. our findings suggest that while chatgpt-4 vision shows\npromise in multimodal academic evaluations, human oversight remains crucial for verifying the model\u2019s\naccuracy and ensuring the fairness of high-stakes educational exams. the paper\u2019s research materials are\npublicly available at https://github.com/nabormendonca/gpt-4v-enade-cs-2021.\nccs concepts: \u2022applied computing \u2192e-learning ;\u2022computing methodologies \u2192natural language\nprocessing ;computer vision .\nadditional key words and phrases: multimodal generative ai, chatgpt-4 vision, educational assessment,\ncomputer science education\nacm reference format:\nnabor c. mendon\u00e7a. 2024. evaluating chatgpt-4 vision on brazil\u2019s national undergraduate computer science\nexam. acm trans. comput. educ. 1, 1 (june 2024), 59 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09818",
        "label": 1.0,
        "text_before_introduction": "climretrieve: a benchmarking dataset for information retrieval from\ncorporate climate disclosures\ntobias schimanski1, jingwei ni2, roberto spacey3, nicola ranger3, markus leippold1, 4\n1university of zurich2eth zurich3university of oxford4swiss finance institute (sfi)\ntobias.schimanski@df.uzh.ch\nabstract\nto handle the vast amounts of qualitative data\nproduced in corporate climate communica-\ntion, stakeholders increasingly rely on retrieval\naugmented generation (rag) systems. how-\never, a significant gap remains in evaluating\ndomain-specific information retrieval \u2013 the ba-\nsis for answer generation. to address this chal-\nlenge, this work simulates the typical tasks of a\nsustainability analyst by examining 30 sustain-\nability reports with 16 detailed climate-related\nquestions. as a result, we obtain a dataset\nwith over 8.5k unique question-source-answer\npairs labeled by different levels of relevance.\nfurthermore, we develop a use case with the\ndataset to investigate the integration of expert\nknowledge into information retrieval with em-\nbeddings. although we show that incorpo-\nrating expert knowledge works, we also out-\nline the critical limitations of embeddings in\nknowledge-intensive downstream domains like\nclimate change communication.1 2\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09520",
        "label": 0.0,
        "text_before_introduction": "education  \n \n \n \nsystematic review \na systematic review of generative ai for teaching and \nlearning practice \n \nbayode ogunleye 1,*     , kudirat ibilola zakariyyah 1, oluwaseun ajao 2, olakunle olayinka 3 \nand hemlata sharma 4 \n \n \n1 school of architecture, technology & engineering, university of brighton, brighton bn2 4gj, uk \n2 department of computing & mathematics, manchester metropolitan university, manchester m1 5gd,  uk \n3 school of computer science, the university of sheffield, sheffield, s1 4dp, uk \n4 department of computing, sheffield hallam university, sheffield s1 2nu,  uk \n* correspondence: b.ogunleye@brighton.ac.uk \n \nabstract: the use of generative artificial intelligence (genai) in academia is a subjective and hotly \ndebated topic. currently, there are no agreed guidelines towards the usage of genai systems in \nhigher education (he) and, thus, it is still unclear how to make effective use of the technology for \nteaching and learning practice. this paper provides an overview of the current state of research on \ngenai for teaching and learning in he. to this end, this study conducted a systematic review of \nrelevant studies indexed by scopus, using the preferred reporting items for systematic reviews and \nmeta-analyses (prisma) guidelines. the search criteria revealed a total of 625 research papers, of \nwhich 355 met the final inclusion criteria. the findings from the review showed the current state \nand the future trends in documents, citations, document sources/authors, keywords, and co-\nauthorship. the research gaps identified suggest that while some authors have looked at \nunderstanding the detection of ai-generated text, it may be beneficial to understand how genai can \nbe incorporated into supp orting the educational curriculum for assessments, teaching, and learning \ndelivery. furthermore, there is a need for additional interdisciplinary, multidimensional studies in he \nthrough collaboration. this will strengthen the awareness and understanding of students, tutors, \nand other stakeholders, which will be instrumental in formulating guidelines, frameworks, and \npolicies for genai usage. \n \n \ncitation: ogunleye, b.; zakariyyah, \nk.i.; ajao, o.; olayinka, o.; sharma, \nh. a systematic review of generative \nai for teaching and learning practice. \neduc. sci. 2024 , 14, 636. https:// \ndoi.org/10.3390 /educsci14060636  \n \nacademic editor: bracha kramarski \n \nreceived: 28 march 2024  \nrevised: 6 june 2024  \naccepted: 11 june 2024  \npublished: 13 june 2024  \n \n \n \ncopyright: \u00a9 2024  by the authors. \nlicensee mdpi, basel, switzerland. \nthis article is an open access article \ndistributed under the terms and \nconditions of the creative comm ons \nattribution (cc by) license (https:// \ncreativecomm ons.org/licenses/by/ \n4.0/). keywords: artificial intelligence; generative ai; higher education; prisma; systematic literature \nreview; teaching and learning; topic modelling \n \n \n \n1."
    },
    {
        "url": "https://arxiv.org/pdf/2406.09988",
        "label": 1.0,
        "text_before_introduction": "details make a difference: object state-sensitive\nneurorobotic task planning\nxiaowen sun\u22c6, xufeng zhao, jae hee lee, wenhao lu,\nmatthias kerzel, and stefan wermter\nknowledge technology, department of informatics, university of hamburg\n{xiaowen.sun, xufeng.zhao, jae.hee.lee, wenhao.lu,\nmatthias.kerzel, stefan.wermter}@uni-hamburg.de\nwww.knowledge-technology.info\nabstract. the state of an object reflects its current status or condition\nand is important for a robot\u2019s task planning and manipulation. how-\never, detecting an object\u2019s state and generating a state-sensitive plan\nfor robots is challenging. recently, pre-trained large language models\n(llms) and vision-language models (vlms) have shown impressive\ncapabilities in generating plans. however, to the best of our knowledge,\nthereishardlyanyinvestigationonwhetherllmsorvlmscanalsogen-\nerate object state-sensitive plans. to study this, we introduce an object\nstate-sensitive agent (ossa), a task-planning agent empowered by pre-\ntrained neural networks. we propose two methods for ossa: (i) a modu-\nlarmodelconsistingofapre-trainedvisionprocessingmodule(densecap-\ntioning model, dcm) and a natural language processing model (llm),\nand (ii) a monolithic model consisting only of a vlm. to quantitatively\nevaluate the performances of the two methods, we use tabletop scenarios\nwhere the task is to clear the table. we contribute a multimodal bench-\nmark dataset that takes object states into consideration. our results\nshow that both methods can be used for object state-sensitive tasks, but\nthe monolithic approach outperforms the modular approach. the code\nfor ossa is available at https://github.com/xiao-wen-sun/ossa\nkeywords: objectstateidentification \u00b7artificialintelligence \u00b7robotics\n\u00b7language models \u00b7multimodality\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10144",
        "label": 1.0,
        "text_before_introduction": "discovering the unknown: improving rule mining via embedding-based\nlink prediction\nn\u2019dah jean kouagoua,\u2217, arif yilmazb, michel dumontierband axel-cyrille ngonga ngomoa\napaderborn university, warburger str., paderborn, 33098, nrw, germany\nbmaastricht university, md, maastricht, p .o. box 616 6200 md, limburg, the netherlands\na r t i c l e i n f o\nkeywords :\nknowledge graph embedding\nlink prediction\nrule mininga b s t r a c t\nrule mining on knowledge graphs allows for explainable link prediction. contrarily, embedding-\nbased methods for link prediction are well known for their generalization capabilities, but their\npredictions are not interpretable. several approaches combining the two families have been proposed\nin recent years. the majority of the resulting hybrid approaches are usually trained within a unified\nlearning framework, which often leads to convergence issues due to the complexity of the learning\ntask. in this work, we propose a new way to combine the two families of approaches. specifically,\nwe enrich a given knowledge graph by means of its pre-trained entity and relation embeddings\nbefore applying rule mining systems on the enriched knowledge graph. to validate our approach,\nwe conduct extensive experiments on seven benchmark datasets. an analysis of the results generated\nby our approach suggests that we discover new valuable rules on the enriched graphs. we provide\nan open source implementation of our approach as well as pretrained models and datasets at\nhttps://github.com/jean-kouagou/enhancedrulelearning .\n1."
    },
    {
        "url": "https://arxiv.org/pdf/2406.10228",
        "label": 1.0,
        "text_before_introduction": "vega\n : learning interleaved image-text comprehension in vision-language\nlarge models\nchenyu zhou1,\u2217, mengdan zhang\u2217, peixian chen\u2660,\u2217, chaoyou fu, yunhang shen\nxiawu zheng1,\u2020, xing sun, rongrong ji1\n1key laboratory of multimedia trusted perception and efficient computing,\nministry of education of china, xiamen university\nzhoucy977@stu.xmu.edu.cn, {pxchen13, zhangmengdanrz}@gmail.com\nabstract\nthe swift progress of multi-modal large models\n(mllms) has showcased their impressive ability to tackle\ntasks blending vision and language. yet, most current models\nand benchmarks cater to scenarios with a narrow scope of\nvisual and textual contexts. these models often fall short\nwhen faced with complex comprehension tasks, which in-\nvolve navigating through a plethora of irrelevant and poten-\ntially misleading information in both text and image forms.\nto bridge this gap, we introduce a new, more demanding task\nknown as interleaved image-text comprehension (iitc).\nthis task challenges models to discern and disregard super-\nfluous elements in both images and text to accurately answer\nquestions and to follow intricate instructions to pinpoint\nthe relevant image. in support of this task, we further craft\na new vega dataset, tailored for the iitc task on scien-\ntific content, and devised a subtask, image-text association\n(ita), to refine image-text correlation skills. our evaluation\nof four leading closed-source models, as well as various\nopen-source models using vega, underscores the rigorous\nnature of iitc. even the most advanced models, such as\ngemini-1.5-pro and gpt4v , only achieved modest success.\nby employing a multi-task, multi-scale post-training strat-\negy, we have set a robust baseline for mllms on the iitc\ntask, attaining an 85.8%accuracy rate in image associa-\ntion and a 0.508rouge score. these results validate the\neffectiveness of our dataset in improving mllms capabili-\nties for nuanced image-text comprehension. project page:\nhttps://zhourax.github.io/vega/\n*equal contribution\n\u2020corresponding author\n\u2660project leader1."
    },
    {
        "url": "https://arxiv.org/pdf/2406.10227",
        "label": 0.0,
        "text_before_introduction": "videogui: a benchmark for gui automation\nfrom instructional videos\nkevin qinghong lin1, linjie li2, difei gao1, qinchen wu1,\nmingyi yan1, zhengyuan yang2, lijuan wang2, mike zheng shou1b\n1show lab, national university of singapore2microsoft gen ai\nhttps://showlab.github.io/videogui/\nabstract\ngraphical user interface (gui) automation holds significant promise for enhancing\nhuman productivity by assisting with computer tasks. existing task formulations\nprimarily focus on simple tasks that can be specified by a single, language-only\ninstruction, such as \u201cinsert a new slide.\u201d in this work, we introduce videogui , a\nnovel multi-modal benchmark designed to evaluate gui assistants on visual-centric\ngui tasks. sourced from high-quality web instructional videos, our benchmark\nfocuses on tasks involving professional and novel software ( e.g., adobe pho-\ntoshop or stable diffusion webui) and complex activities ( e.g., video editing).\nvideogui evaluates gui assistants through a hierarchical process, allowing for\nidentification of the specific levels at which they may fail: (i) high-level planning:\nreconstruct procedural subtasks from visual conditions without language descrip-\ntions; (ii) middle-level planning: generate sequences of precise action narrations\nbased on visual state ( i.e.,screenshot) and goals; (iii) atomic action execution:\nperform specific actions such as accurately clicking designated elements. for each\nlevel, we design evaluation metrics across individual dimensions to provide clear\nsignals, such as individual performance in clicking, dragging, typing, and scrolling\nfor atomic action execution. our evaluation on videogui reveals that even the\nsota large multimodal model gpt4o performs poorly on visual-centric gui tasks,\nespecially for high-level planning.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10221",
        "label": 0.0,
        "text_before_introduction": "short film dataset (sfd):\na benchmark for story-level video understanding\nridouane ghermi\nlix, ecole polytechnique, ip paris\nridouane.ghermi@inria.frxi wang\nlix, ecole polytechnique, ip paris\nxi.wang@lix.polytechnique.fr\nvicky kalogeiton\nlix, ecole polytechnique, ip paris\nvicky.kalogeiton@polytechnique.eduivan laptev\nmbzuai\nivan.laptev@inria.fr\nfigure 1: videoqa examples from three video domains: instructional videos, egocentric videos and\nmovies. while instructional and egocentric videos usually depict one or two people performing a single task,\nmovies present time-extended stories with a rich variety in terms of scenes, characters and interactions.\nhttps://shortfilmdataset.github.io\nabstract\nrecent advances in vision-language models have significantly propelled video\nunderstanding. existing datasets and tasks, however, have notable limitations. most\ndatasets are confined to short videos with limited events and narrow narratives.\nfor example, datasets with instructional and egocentric videos often document the\nactivities of one person in a single scene. although some movie datasets offer\nricher content, they are often limited to short-term tasks, lack publicly available\nvideos and frequently encounter data leakage given the use of movie forums and\nother resources in llm training. to address the above limitations, we propose\nthe short film dataset (sfd) with 1,078 publicly available amateur movies, a\nwide variety of genres and minimal data leakage issues. sfd offers long-term\nstory-oriented video tasks in the form of multiple-choice and open-ended question\npreprint. under review.arxiv:2406.10221v1  [cs.cv]  14 jun 20240 200 400 600 800\naverage video length (seconds)0100200300400total hoursactivitynet-qahow2qa\negoschemasfdmovieqa\nmoviechat\nlvucinepile\ntvqanext-qa\nivqamovies (accessible)\nmovies (restricted)\negocentric\ninstructional\ngeneralfigure 2: comparison of sfd to other vqa\ndatasets. the circle size indicates the number of\nqa pairs in each dataset.\n19.728.922.126.333.528.93624.131.518.344.134.556.755.455.464.451.971.3\n28.869.964.17570.268.57164.176\n15253545556575\ngemma2b(42.3)mistral7b(62.5)llama 38b(68.4)gpt-3.5(70)mixtral8x7b(70.6)claude 3haiku(75.2)claude 3sonnet(79)llama 370b(82)gpt-4(86.4)% accuracyzero-shot llm accuracysfdmovieqalvuyto: youtube-objects voc: pascal voc 2007\n6model and (mmlu)figure 3: data leakage. when given only the\nmovie title, higher zero-shot accuracy in question-\nanswering by llms indicates greater data leakage.\nllms are ranked by mmlu.\nanswering. our extensive experiments emphasize the need for long-term reasoning\nto solve sfd tasks. notably, we find strong signals in movie transcripts leading\nto the on-par performance of people and llms. we also show significantly lower\nperformance of current models compared to people when using vision data alone.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10210",
        "label": 1.0,
        "text_before_introduction": "make it count: text-to-image generation with an\naccurate number of objects\nlital binyamin1yoad tewel2,3hilit segev1eran hirsch1royi rassin1gal chechik1,2\n1bar-ilan university2nvidia3tel-aviv university\nhttps://make-it-count-paper.github.io\nabstract\ndespite the unprecedented success of text-to-image diffusion models, controlling\nthe number of depicted objects using text is surprisingly hard. this is important for\nvarious applications from technical documents, to children\u2019s books to illustrating\ncooking recipes. generating object-correct counts is fundamentally challenging\nbecause the generative model needs to keep a sense of separate identity for every\ninstance of the object, even if several objects look identical or overlap, and then\ncarry out a global computation implicitly during generation. it is still unknown if\nsuch representations exist. to address count-correct generation, we first identify\nfeatures within the diffusion model that can carry the object identity information.\nwe then use them to separate and count instances of objects during the denoising\nprocess and detect over-generation and under-generation. we fix the latter by\ntraining a model that predicts both the shape and location of a missing object, based\non the layout of existing ones, and show how it can be used to guide denoising\nwith correct object count. our approach, countgen , does not depend on external\nsource to determine object layout, but rather uses the prior from the diffusion\nmodel itself, creating prompt-dependent and seed-dependent layouts. evaluated on\ntwo benchmark datasets, we find that countgen strongly outperforms the count-\naccuracy of existing baselines.\ncountgen \n(ours) \u201ca photo of six \nkittens  sitting on a \nbranch\u201d \u201ca photo of \ufb01ve \neggs  in a carton\u201d \u201ca realistic photo of \ngoldilocks and three  \nbears eating a porridge\u201d \u201can illustration of \nfour  ninja turtles \u201d\nsdxl \n\u201ca realistic photo of \nseven  dwarves  dancing \nin the forest\u201d \nfigure 1: countgen generates the correct number of objects specified in the input prompt while\nmaintaining a natural layout that aligns with the prompt.\npreprint. under review.arxiv:2406.10210v1  [cs.cv]  14 jun 20241"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10163",
        "label": 0.0,
        "text_before_introduction": "meshanything: artist-created mesh generation\nwith autoregressive transformers\nyiwen chen1,2\u2217, tong he2\u2020, di huang2, weicai ye2, sijin chen3, jiaxiang tang4\nxin chen5, zhongang cai6, lei yang6, gang yu7, guosheng lin1\u2020, chi zhang8\u2020\n1s-lab, nanyang technological university2shanghai ai lab\n3fudan university4peking university5university of chinese academy of sciences\n6sensetime research7stepfun8westlake university\nhttps://buaacyw.github.io/mesh-anything/\n3d gs\n nerf\n text condition: a c\n ommode\nimage\n image\npoint cloud\npoint cloud\ndense mesh\n dense mesh\ndense mesh\n dense mesh\n dense mesh\nfigure 1: meshanything converts any 3d representation into artist-created meshes (ams), i.e.,\nmeshes created by human artists. it can be combined with various 3d asset production pipelines,\nsuch as 3d reconstruction and generation, to transform their results into ams that can be seamlessly\napplied in the 3d industry.\n\u2217work done during a research internship at shanghai ai lab.\n\u2020corresponding authors.arxiv:2406.10163v1  [cs.cv]  14 jun 2024abstract\nrecently, 3d assets created via reconstruction and generation have matched the\nquality of manually crafted assets, highlighting their potential for replacement.\nhowever, this potential is largely unrealized because these assets always need to\nbe converted to meshes for 3d industry applications, and the meshes produced\nby current mesh extraction methods are significantly inferior to artist-created\nmeshes (ams), i.e., meshes created by human artists. specifically, current mesh\nextraction methods rely on dense faces and ignore geometric features, leading\nto inefficiencies, complicated post-processing, and lower representation quality.\nto address these issues, we introduce meshanything, a model that treats mesh\nextraction as a generation problem, producing ams aligned with specified shapes.\nby converting 3d assets in any 3d representation into ams, meshanything can\nbe integrated with various 3d asset production methods, thereby enhancing their\napplication across the 3d industry. the architecture of meshanything comprises\na vq-v ae and a shape-conditioned decoder-only transformer. we first learn a\nmesh vocabulary using the vq-v ae, then train the shape-conditioned decoder-\nonly transformer on this vocabulary for shape-conditioned autoregressive mesh\ngeneration. our extensive experiments show that our method generates ams with\nhundreds of times fewer faces, significantly improving storage, rendering, and\nsimulation efficiencies, while achieving precision comparable to previous methods.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10157",
        "label": 0.0,
        "text_before_introduction": "robogolf: mastering real-world minigolf with a\nreflective multi-modality vision-language model\nhantao zhou1*tianying ji2*jianwei zhang1fuchun sun2huazhe xu2,3\n1universitaet hamburg2tsinghua university3shanghai qi zhi institute\n*equal contribution https://jity16.github.io/robogolf/\nabstract: minigolf, a game with countless court layouts, and complex ball mo-\ntion, constitutes a compelling real-world testbed for the study of embodied intel-\nligence. as it not only challenges spatial and kinodynamic reasoning but also re-\nquires reflective and corrective capacities to address erroneously designed courses.\nwe introduce robogolf , a framework that perceives dual-camera visual inputs\nwith nested vlm-empowered closed-loop control and reflective equilibrium loop.\nextensive experiments demonstrate the effectiveness of robogolf on challenging\nminigolf courts including those that are impossible to finish. experiment videos\nare available at https://jity16.github.io/robogolf/ .\nkeywords: reflective equilibrium, closed-loop control, real-world minigolf, vi-\nsion language model\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10100",
        "label": 1.0,
        "text_before_introduction": "skysensegpt: a fine-grained instruction tuning\ndataset and model for remote sensing\nvision-language understanding\njunwei luo\nwuhan university\nluojunwei\n@whu.edu.cnzhen pang\nwuhan university\npangzhen\n@whu.edu.cnyongjun zhang\nwuhan university\nzhangyj\n@whu.edu.cntingzhu wang\nwuhan university\ntingzhu.wang\n@whu.edu.cn\nlinlin wang\nwuhan university\nwangll\n@whu.edu.cnbo dang\nwuhan university\nbodang\n@whu.edu.cnjiangwei lao\nant group\nwenshuo.ljw\n@antgroup.comjian wang\nant group\nbobblair.wj\n@antgroup.com\njingdong chen\nant group\njingdongchen.cjd\n@antgroup.comyihua tan\nhuazhong university of\nscience and technology\nyhtan@hust.edu.cnyansheng li\u2217\nwuhan university\nyansheng.li\n@whu.edu.cn\nabstract\nremote sensing large multi-modal models (rslmms) are developing rapidly and\nshowcase significant capabilities in remote sensing imagery (rsi) comprehension.\nhowever, due to the limitations of existing datasets, rslmms have shortcomings\nin understanding the rich semantic relations among objects in complex remote\nsensing scenes. to unlock rslmms\u2019 complex comprehension ability, we propose\na large-scale instruction tuning dataset fit-rs, containing 1,800,851 instruction\nsamples. fit-rs covers common interpretation tasks and innovatively introduces\nseveral complex comprehension tasks of escalating difficulty, ranging from re-\nlation reasoning to image-level scene graph generation. based on fit-rs, we\nbuild the fit-rsfg benchmark. furthermore, we establish a new benchmark to\nevaluate the fine-grained relation comprehension capabilities of lmms, named\nfit-rsrc. based on combined instruction data, we propose skysensegpt, which\nachieves outstanding performance on both public datasets and fit-rsfg, sur-\npassing existing rslmms. we hope the fit-rs dataset can enhance the relation\ncomprehension capability of rslmms and provide a large-scale fine-grained\ndata source for the remote sensing community. the dataset will be available at\nhttps://github.com/luo-z13/skysensegpt .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10079",
        "label": 1.0,
        "text_before_introduction": "localizing events in videos with multimodal queries\ngengyuan zhang1,4\u2217mang ling ada fok2\u2217yan xia2,4yansong tang3\ndaniel cremers2,4philip torr3volker tresp1,4jindong gu5\n1lmu munich2tu munich3tsinghua university\n4munich center for machine learning (mcml)5university of oxford\nzhang@dbs.ifi.lmu.de ada.fok@tum.de\nabstract\nvideo understanding is a pivotal task in the digital era, yet the dynamic and multi-\nevent nature of videos makes them labor-intensive and computationally demanding\nto process. thus, localizing a specific event given a semantic query has gained\nimportance in both user-oriented applications like video search and academic\nresearch into video foundation models. a significant limitation in current research\nis that semantic queries are typically in natural language that depicts the semantics\nof the target event. this setting overlooks the potential for multimodal semantic\nqueries composed of images and texts. to address this gap, we introduce a new\nbenchmark, icq, for localizing events in videos with multimodal queries, along\nwith a new evaluation dataset icq-highlight. our new benchmark aims to evaluate\nhow well models can localize an event given a multimodal semantic query that\nconsists of a reference image, which depicts the event, and a refinement text to\nadjust the images\u2019 semantics. to systematically benchmark model performance,\nwe include 4 styles of reference images and 5 types of refinement texts, allowing us\nto explore model performance across different domains. we propose 3 adaptation\nmethods that tailor existing models to our new setting and evaluate 10 sota\nmodels, ranging from specialized to large-scale foundation models. we believe\nthis benchmark is an initial step toward investigating multimodal queries in video\nevent localization.2.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10057",
        "label": 0.0,
        "text_before_introduction": "first multi-dimensional evaluation of flowchart comprehension\nfor multimodal large language models\nenming zhang1*ruobing yao1*huanyong liu2\u2020junhui yu2jiale wang3\n1university of chinese academy of sciences, beijing, china\n2360 ai research institute, beijing, china\n3nanyang technological university, singapore\n{zhangenming23,yaoruobing23 }@mails.ucas.ac.cn\n{liuhuanyong,zhangenming,yaoruobing,wangjiale,yujunhui3 }@360.cn s220193@e.ntu.edu.sg\nabstract\nwith the development of multimodal large language\nmodels (mllms) technology, its general capabilities are\nincreasingly powerful. to evaluate the various abilities of\nmllms, numerous evaluation systems have emerged. but\nnow there is still a lack of a comprehensive method to evalu-\nate mllms in the tasks related to flowcharts, which are very\nimportant in daily life and work. we propose the first com-\nprehensive method, flowce, to assess mllms across var-\nious dimensions for tasks related to flowcharts. it encom-\npasses evaluating mllms\u2019 abilities in reasoning, local-\nization recognition, information extraction, logical veri-\nfication, and summarization on flowcharts. however, we\nfind that even the gpt4o model achieves only a score of\n56.63. among open-source models, phi-3-vision obtained\nthe highest score of 49.97. we hope that flowce can\ncontribute to future research on mllms for tasks based\non flowcharts. https://github.com/360ailab-\nnlp/flowce\n1."
    },
    {
        "url": "https://arxiv.org/pdf/2406.09422",
        "label": 0.0,
        "text_before_introduction": "loopin: a pinfi protocol for decentralized computing\nyunwei mao1, qi he1, and ju li1,2\n1loopro inc., cambridge, ma 02139, usa (https://www.loopro.ai/)\n2massachusetts institute of technology, cambridge, ma 02139, usa\nmarch 20, 2024\nabstract\nnetworked computing power is a critical utility in the era of artificial intelligence. this paper\npresents a novel physical infrastructure finance (pinfi) protocol designed to facilitate the distribu-\ntion of computing power within networks in a decentralized manner. addressing the core challenges\nof coordination, pricing, and liquidity in decentralized physical infrastructure networks (depin),\nthe pinfi protocol introduces a distinctive dynamic pricing mechanism. it enables providers to al-\nlocate excess computing resources to a \u201cdissipative\u201d pinfi liquidity pool, distinct from traditional\ndefi liquidity pools, ensuring seamless access for clients at equitable, market-based prices. this\napproach significantly reduces the costs of accessing computing power, potentially to as low as 1%\ncompared to existing services, while simultaneously enhancing security and dependability. the\npinfi protocol is poised to transform the dynamics of supply and demand in computing power\nnetworks, setting a new standard for efficiency and accessibility.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09454",
        "label": 0.0,
        "text_before_introduction": "advancing high resolution vision-language models\nin biomedicine\nzekai chen arda pekis kevin brown\nstandard model biomedicine, inc.\n{zach,arda,kevin}@standardmodel.bio\nabstract\nmulti-modal learning has transformed generative ai, particularly in vision-\nlanguage modeling. advances such as the multi-modal gpt-4v and open-source\nprojects like llav a have enabled robust conversational agents capable of zero-shot\ntask completions. however, extending these technologies in the biomedical field\nintroduces unique challenges. recent initiatives like llav a-med have begun\nto tailor instruction-tuning to biomedical contexts using extensive datasets like\npmc-15m. our research contributes three significant advancements: (i) we intro-\nduce a new instruct dataset enriched with medical image-text pairs derived from\nclaude3-opus and llama3 70b, (ii) we propose an innovative image encoding\nstrategy that employs hierarchical representations to enhance fine-grained biomed-\nical visual comprehension, and (iii) we develop the llama3-med model, which\nachieves state-of-the-art zero-shot performance on biomedical visual question an-\nswering benchmarks, improving performance by over 10% on average compared\nto prior methods. these advancements provide more precise and reliable tools for\nmedical professionals, effectively bridging gaps in current multi-modal conversa-\ntional assistants and fostering further innovations in medical ai. codes available at\nhttps://github.com/standardmodelbio/llama3-med.git .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09688",
        "label": 1.0,
        "text_before_introduction": "freectrl: constructing control centers with feedforward layers for\nlearning-free controllable text generation\nzijian feng1,3,hanzhang zhou1,3,zixiao zhu1,3, and kezhi mao2,3, *\n1institute of catastrophe risk management, interdisciplinary graduate programme,\nnanyang technological university, singapore\n2school of electrical and electronic engineering, nanyang technological university, singapore\n3future resilient systems programme, singapore-eth centre, create campus, singapore\n{feng0119, hanzhang001, zixiao001}@e.ntu.edu.sg ,ekzmao@ntu.edu.sg\nabstract\ncontrollable text generation (ctg) seeks to\ncraft texts adhering to specific attributes, tradi-\ntionally employing learning-based techniques\nsuch as training, fine-tuning, or prefix-tuning\nwith attribute-specific datasets. these ap-\nproaches, while effective, demand extensive\ncomputational and data resources. in contrast,\nsome proposed learning-free alternatives cir-\ncumvent learning but often yield inferior re-\nsults, exemplifying the fundamental machine\nlearning trade-off between computational ex-\npense and model efficacy. to overcome these\nlimitations, we propose freectrl, a learning-\nfree approach that dynamically adjusts the\nweights of selected feedforward neural network\n(ffn) vectors to steer the outputs of large lan-\nguage models (llms). freectrl hinges on the\nprinciple that the weights of different ffn vec-\ntors influence the likelihood of different tokens\nappearing in the output. by identifying and\nadaptively adjusting the weights of attribute-\nrelated ffn vectors, freectrl can control the\noutput likelihood of attribute keywords in the\ngenerated content. extensive experiments on\nsingle- and multi-attribute control reveal that\nthe learning-free freectrl outperforms other\nlearning-free and learning-based methods, suc-\ncessfully resolving the dilemma between learn-\ning costs and model performance1.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09760",
        "label": 1.0,
        "text_before_introduction": "bootstrapping language models with\ndpo implicit rewards\nchangyu chen\u22171, zichen liu\u221723, chao du\u20202, tianyu pang2, qian liu2, arunesh sinha\u20204,\npradeep varakantham\u20201,min lin2\n1singapore management university\n2sea ai lab, singapore\n3national university of singapore4rutgers university\n{chency,liuzc,duchao,tianyupang,liuqian,linmin}@sea.com;\narunesh.sinha@rutgers.edu; pradeepv@smu.edu.sg\nabstract\nhuman alignment in large language models (llms) is an active area of research.\na recent groundbreaking work, direct preference optimization (dpo), has greatly\nsimplified the process from past work in reinforcement learning from human\nfeedback (rlhf) by bypassing the reward learning stage in rlhf. dpo, after\ntraining, provides an implicit reward model. in this work, we make a novel\nobservation that this implicit reward model can by itself be used in a bootstrapping\nfashion to further align the llm. our approach is to use the rewards from a current\nllm model to construct a preference dataset, which is then used in subsequent\ndpo rounds. we incorporate refinements that debias the length of the responses\nand improve the quality of the preference dataset to further improve our approach.\nour approach, named self-alignment with dpoimplicit rewards (dice), shows\ngreat improvements in alignment and achieves superior performance than gemini\npro on alpacaeval 2, reaching 27.55% length-controlled win rate against gpt-4\nturbo, but with only 8b parameters and no external feedback. our code is available\nathttps://github.com/sail-sg/dice .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09455",
        "label": 1.0,
        "text_before_introduction": "pandora: towards general world model\nwith natural language actions and video states\njiannan xiang\u2217, guangyi liu\u2217, yi gu\u2217, qiyue gao, yuting ning, yuheng zha,\nzeyu feng, tianhua tao, shibo hao, yemin shi, zhengzhong liu,\neric p. xing, zhiting hu\nmaitrix.org, uc san diego, mbzuai\nhttps://world-model.ai *equal contribution\nfigure 1:\npandora simulates future world states (videos) under action control (natural language).\naction 1: the car turns left.action 2: add a car in the front.initialstateaction 3: sheturnsherheadleft.action 1: thewomanistalking.action 2: shewavesherhand.initialstate\naction 3: the red car continues to move.action 1: the red car moves along the path.action 2: explosion happens.initialstateabstract\nworld models simulate future states of the world in response to different actions.\nthey facilitate interactive content creation and provides a foundation for grounded,\nlong-horizon reasoning. current foundation models do not fully meet the capa-\nbilities of general world models: large language models (llms) are constrained\nby their reliance on language modality and their limited understanding of the\nphysical world, while video models lack interactive action control over the world\nsimulations. this paper makes a step towards building a general world model by\nintroducing\npandora , a hybrid autoregressive-diffusion model that simulates world\nstates by generating videos and allows real-time control with free-text actions.\npandora\nachieves domain generality , video consistency , and controllability through\nlarge-scale pretraining and instruction tuning. crucially,\npandora bypasses the cost\nof training-from-scratch by integrating a pretrained llm (7b) and a pretrained\nvideo model, requiring only additional lightweight finetuning. we illustrate ex-\ntensive outputs by\npandora across diverse domains (indoor/outdoor, natural/urban,\nhuman/robot, 2d/3d, etc.). the results indicate great potential of building stronger\ngeneral world models with larger-scale training.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09486",
        "label": 0.0,
        "text_before_introduction": "semopo: learning high-quality model and policy from low-quality\noffline visual datasets\nshenghua wan1 2ziyuan chen3le gan1 2shuai feng4de-chuan zhan1 2\nabstract\nmodel-based offline reinforcement learning (rl)\nis a promising approach that leverages existing\ndata effectively in many real-world applications,\nespecially those involving high-dimensional in-\nputs like images and videos. to alleviate the\ndistribution shift issue in offline rl, existing\nmodel-based methods heavily rely on the uncer-\ntainty of learned dynamics. however, the model\nuncertainty estimation becomes significantly bi-\nased when observations contain complex distrac-\ntors with non-trivial dynamics. to address this\nchallenge, we propose a new approach - sepa-\nrated model-based offline policy optimization\n(semopo) - decomposing latent states into en-\ndogenous and exogenous parts via conservative\nsampling and estimating model uncertainty on the\nendogenous states only. we provide a theoret-\nical guarantee of model uncertainty and perfor-\nmance bound of semopo. to assess the efficacy,\nwe construct the low-quality vision deep data-\ndriven datasets for rl (lqv-d4rl), where the\ndata are collected by non-expert policy and the\nobservations include moving distractors. exper-\nimental results show that our method substan-\ntially outperforms all baseline methods, and fur-\nther analytical experiments validate the critical\ndesigns in our method. the project website is\nhttps://sites.google.com/view/semopo.\n1school of artificial intelligence, nanjing university, china\n2national key laboratory for novel software technology, nanjing\nuniversity, china3school of mathematical sciences, center for\nstatistical science, peking university, beijing, china4school of\ncyberspace science and technology, beijing institute of tech-\nnology, beijing, china. correspondence to: de-chuan zhan\n<zhandc@nju.edu.cn >.\nproceedings of the 41stinternational conference on machine\nlearning , vienna, austria. pmlr 235, 2024. copyright 2024 by\nthe author(s).1."
    },
    {
        "url": "https://arxiv.org/pdf/2406.09496",
        "label": 1.0,
        "text_before_introduction": "you are what you eat? feeding foundation models a\nregionally diverse food dataset of world wide dishes\njabez magomere\u2217\noxford artificial\nintelligence societyshu ishida\u2217\u2020\nvisual geometry group\nuniversity of oxfordtejumade afonja\ncispa helmholtz center\nfor information security\naisaturdays lagosaya salama\nmicrosoft\ndaniel kochin\noxford artificial\nintelligence societyfoutse yuehgoh\nconservatoire national\ndes arts et m\u00e9tiersimane hamzaoui\n\u00e9cole nationale sup\u00e9rieure\nd\u2019informatique algiers\nraesetje sefala\nmcgill university\ndistributed artificial intelligence research instituteaisha aalagib\nindependent researcher\nelizaveta semenova\nimperial college londonlauren crais\u2020\nfaculty of law, university of oxfordsiobhan mackenzie hall\u2020 \u2021\nuniversity of oxford\nabstract\nfoundation models are increasingly ubiquitous in our daily lives, used in everyday\ntasks such as text-image searches, interactions with chatbots, and content gener-\nation. as use increases, so does concern over the disparities in performance and\nfairness of these models for different people in different parts of the world. to\nassess these growing regional disparities, we present world widedishes , a\nmixed text and image dataset consisting of 765 dishes, with dish names collected\nin 131 local languages. world widedishes has been collected purely through\nhuman contribution and decentralised means, by creating a website widely distrib-\nuted through social networks. using the dataset, we demonstrate a novel means of\noperationalising capability and representational biases in foundation models such\nas language models and text-to-image generative models. we enrich these studies\nwith a pilot community review to understand, from a first-person perspective, how\nthese models generate images for people in five african countries and the united\nstates. we find that these models generally do not produce quality text and image\noutputs of dishes specific to different regions. this is true even for the us, which is\ntypically considered to be more well-resourced in training data\u2014though the gener-\nation of us dishes does outperform that of the investigated african countries. the\nmodels demonstrate a propensity to produce outputs that are inaccurate as well as\nculturally misrepresentative, flattening, and insensitive. these failures in capability\nand representational bias have the potential to further reinforce stereotypes and\ndisproportionately contribute to erasure based on region. the dataset and code are\navailable at https://github.com/oxai/world-wide-dishes/ .\n\u2217joint first author.\n\u2020work done in affiliation with the oxford artificial intelligence society.\n\u2021corresponding author: siobhan.hall@nds.ox.ac.uk.\npreprint. under review.arxiv:2406.09496v1  [cs.cy]  13 jun 2024example dish\n dall-e 2\n dall-e 3\n sd v2.1\nbaghrir eru nyama choma ofe okazi babotie hot dog\nalgeria cameroon kenya nigeria south africa united states\nfigure 1: theworld widedishes dataset contains 765 unique instances of dishes from around the world.\nthis paper presents image generation analysis of dishes associated with the united states and five countries\nacross africa. the 1strow shows example images of the dishes from each country, and the 2ndthrough 4th\nrows show images generated by dall-e 2, dall-e 3, and stable diffusion v2.1, respectively. all models\ntend to mis-characterise the dishes. dall-e 2 often outputs the incorrect dish; dall-e 3 tends to exaggerate\nboth visual and cultural stereotypes and to make images more cartoonish; and stable diffusion often generates\nincoherent images barely resembling food.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09520",
        "label": 0.0,
        "text_before_introduction": "education  \n \n \n \nsystematic review \na systematic review of generative ai for teaching and \nlearning practice \n \nbayode ogunleye 1,*     , kudirat ibilola zakariyyah 1, oluwaseun ajao 2, olakunle olayinka 3 \nand hemlata sharma 4 \n \n \n1 school of architecture, technology & engineering, university of brighton, brighton bn2 4gj, uk \n2 department of computing & mathematics, manchester metropolitan university, manchester m1 5gd,  uk \n3 school of computer science, the university of sheffield, sheffield, s1 4dp, uk \n4 department of computing, sheffield hallam university, sheffield s1 2nu,  uk \n* correspondence: b.ogunleye@brighton.ac.uk \n \nabstract: the use of generative artificial intelligence (genai) in academia is a subjective and hotly \ndebated topic. currently, there are no agreed guidelines towards the usage of genai systems in \nhigher education (he) and, thus, it is still unclear how to make effective use of the technology for \nteaching and learning practice. this paper provides an overview of the current state of research on \ngenai for teaching and learning in he. to this end, this study conducted a systematic review of \nrelevant studies indexed by scopus, using the preferred reporting items for systematic reviews and \nmeta-analyses (prisma) guidelines. the search criteria revealed a total of 625 research papers, of \nwhich 355 met the final inclusion criteria. the findings from the review showed the current state \nand the future trends in documents, citations, document sources/authors, keywords, and co-\nauthorship. the research gaps identified suggest that while some authors have looked at \nunderstanding the detection of ai-generated text, it may be beneficial to understand how genai can \nbe incorporated into supp orting the educational curriculum for assessments, teaching, and learning \ndelivery. furthermore, there is a need for additional interdisciplinary, multidimensional studies in he \nthrough collaboration. this will strengthen the awareness and understanding of students, tutors, \nand other stakeholders, which will be instrumental in formulating guidelines, frameworks, and \npolicies for genai usage. \n \n \ncitation: ogunleye, b.; zakariyyah, \nk.i.; ajao, o.; olayinka, o.; sharma, \nh. a systematic review of generative \nai for teaching and learning practice. \neduc. sci. 2024 , 14, 636. https:// \ndoi.org/10.3390 /educsci14060636  \n \nacademic editor: bracha kramarski \n \nreceived: 28 march 2024  \nrevised: 6 june 2024  \naccepted: 11 june 2024  \npublished: 13 june 2024  \n \n \n \ncopyright: \u00a9 2024  by the authors. \nlicensee mdpi, basel, switzerland. \nthis article is an open access article \ndistributed under the terms and \nconditions of the creative comm ons \nattribution (cc by) license (https:// \ncreativecomm ons.org/licenses/by/ \n4.0/). keywords: artificial intelligence; generative ai; higher education; prisma; systematic literature \nreview; teaching and learning; topic modelling \n \n \n \n1."
    },
    {
        "url": "https://arxiv.org/pdf/2406.09622",
        "label": 0.0,
        "text_before_introduction": "dsl-fiqa: assessing facial image quality via dual-set degradation learning\nand landmark-guided transformer\nwei-ting chen1,2\u2020gurunandan krishnan2qiang gao2sy-yen kuo1sizhuo ma2*jian wang2*\u2666\n1national taiwan university2snap inc.\nabstract\ngeneric face image quality assessment (gfiqa) evalu-\nates the perceptual quality of facial images, which is crucial\nin improving image restoration algorithms and selecting\nhigh-quality face images for downstream tasks. we present\na novel transformer-based method for gfiqa, which is\naided by two unique mechanisms. first, a \u201c dual-set\ndegradation representation learning\u201d (dsl) mechanism\nuses facial images with both synthetic and real degrada-\ntions to decouple degradation from content, ensuring gen-\neralizability to real-world scenarios. this self-supervised\nmethod learns degradation features on a global scale, pro-\nviding a robust alternative to conventional methods that use\nlocal patch information in degradation learning. second,\nour transformer leverages facial landmarks to emphasize\nvisually salient parts of a face image in evaluating its per-\nceptual quality. we also introduce a balanced and diverse\ncomprehensive generic face iqa (cgfiqa-40k) dataset\nof 40k images carefully designed to overcome the biases, in\nparticular the imbalances in skin tone and gender represen-\ntation, in existing datasets. extensive analysis and evalua-\ntion demonstrate the robustness of our method, marking a\nsignificant improvement over prior methods.\n1."
    },
    {
        "url": "https://arxiv.org/pdf/2406.09675",
        "label": 0.0,
        "text_before_introduction": "benchmarking spectral graph neural networks:\na comprehensive study on effectiveness and efficiency\nningyi liao1, haoyu liu1, zulun zhu1, siqiang luo1, laks v .s. lakshmanan2\n1nanyang technological university\n2the university of british columbia\n{liao0090,zulun001}@e.ntu.edu.sg,\n{haoyu.liu,siqiang.luo}@ntu.edu.sg, laks@cs.ubc.ca\nabstract\nwith the recent advancements in graph neural networks (gnns), spectral gnns\nhave received increasing popularity by virtue of their specialty in capturing graph\nsignals in the frequency domain, demonstrating promising capability in specific\ntasks. however, few systematic studies have been conducted to assess their spectral\ncharacteristics. this emerging family of models also varies in terms of design and\nsettings, leading to difficulties in comparing their performance and deciding on the\nsuitable model for specific scenarios, especially for large-scale tasks. in this work,\nwe extensively benchmark spectral gnns with a focus on the frequency perspective.\nwe analyze and categorize over 30 gnns with 27 corresponding filters. then,\nwe implement these spectral models within a unified framework with dedicated\ngraph computations and efficient training schemes. thorough experiments are\nconducted on the spectral models with inclusive metrics on effectiveness and\nefficiency, offering practical guidelines on evaluating and selecting spectral gnns\nwith desirable performance. our implementation enables application on larger\ngraphs with comparable performance and less overhead, which is available at:\nhttps://github.com/gdmnl/spectral-gnn-benchmark .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09684",
        "label": 1.0,
        "text_before_introduction": "Abstract or Introduction section not found on the first page."
    },
    {
        "url": "https://arxiv.org/pdf/2406.09723",
        "label": 1.0,
        "text_before_introduction": "when will gradient regularization be harmful?\nyang zhao1hao zhang1xiuyuan hu1\nabstract\ngradient regularization (gr), which aims to pe-\nnalize the gradient norm atop the loss function,\nhas shown promising results in training modern\nover-parameterized deep neural networks. how-\never, can we trust this powerful technique? this\npaper reveals that gr can cause performance de-\ngeneration in adaptive optimization scenarios, par-\nticularly with learning rate warmup. our empiri-\ncal and theoretical analyses suggest this is due to\ngr inducing instability and divergence in gradi-\nent statistics of adaptive optimizers at the initial\ntraining stage. inspired by the warmup heuristic,\nwe propose three gr warmup strategies, each re-\nlaxing the regularization effect to a certain extent\nduring the warmup course to ensure the accurate\nand stable accumulation of gradients. with exper-\niments on vision transformer family, we confirm\nthe three gr warmup strategies can effectively\ncircumvent these issues, thereby largely improv-\ning the model performance. meanwhile, we note\nthat scalable models tend to rely more on the gr\nwarmup, where the performance can be improved\nby up to 3% on cifar10 compared to baseline gr.\ncode is available at https://github.com/zhaoyang-\n0204/gnp.\n1."
    },
    {
        "url": "https://arxiv.org/pdf/2406.09770",
        "label": 0.0,
        "text_before_introduction": "towards efficient pareto set approximation via\nmixture of experts based model fusion\nanke tang\nwuhan university\nwuhan, china\nanketang@whu.edu.cnli shen\nsun yat-sen university\nshenzhen, china\nmathshenli@gmail.comyong luo\nwuhan university\nwuhan, china\nluoyong@whu.edu.cn\nshiwei liu\nuniversity of oxford\noxford,uk\nshiwei.liu@maths.ox.ac.ukhan hu\nbeijing institute of technology\nbeijng, china\nhhu@bit.edu.cnbo du\nwuhan university\nwuhan, china\ndubo@whu.edu.cn\nabstract\nsolving multi-objective optimization problems for large deep neural networks is\na challenging task due to the complexity of the loss landscape and the expensive\ncomputational cost of training and evaluating models. efficient pareto front approx-\nimation of large models enables multi-objective optimization for various tasks such\nas multi-task learning and trade-off analysis. existing algorithms for learning pareto\nset, including (1) evolutionary, hypernetworks, and hypervolume-maximization\nmethods, are computationally expensive and have restricted scalability to large\nmodels; (2) scalarization algorithms, where a separate model is trained for each\nobjective ray, which is inefficient for learning the entire pareto set and fails to\ncapture the objective trade-offs effectively. inspired by the recent success of model\nmerging, we propose a practical and scalable approach to pareto set learning prob-\nlem via mixture of experts (moe) based model fusion. by ensembling the weights\nof specialized single-task models, the moe module can effectively capture the\ntrade-offs between multiple objectives and closely approximate the entire pareto\nset of large neural networks. once the routers are learned and a preference vec-\ntor is set, the moe module can be unloaded, thus no additional computational\ncost is introduced during inference. we conduct extensive experiments on vision\nand language tasks using large-scale models such as clip-vit and gpt-2. the\nexperimental results demonstrate that our method efficiently approximates the\nentire pareto front of large models. using only hundreds of trainable parameters\nof the moe routers, our method even has lower memory usage compared to linear\nscalarization and algorithms that learn a single pareto optimal solution, and are\nscalable to both the number of objectives and the size of the model. our method\nsignificantly reduces the computational burden of learning the pareto set, for exam-\nple, in the two-task case, it can be achieved in just a few minutes. code is available\nat:https://github.com/tanganke/pareto_set_learning\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09831",
        "label": 0.0,
        "text_before_introduction": "received xx month, xxxx; revised xx month, xxxx; accepted xx month, xxxx; date of publication xx month, xxxx; date of\ncurrent version xx month, xxxx.\ndigital object identifier 10.1109/ojim.2022.1234567\nfederated large language models for\nswarm intelligence: a survey\nyouyang qu\u2217, member, ieee\nabstract federated learning (fl) offers a compelling framework for training large language models\n(llms) while addressing data privacy and decentralization challenges. this paper surveys recent advance-\nments in the federated learning of large language models, with a particular focus on machine unlearning\u2014a\ncrucial aspect for complying with privacy regulations like the right to be forgotten. machine unlearning in\nthe context of federated llms involves systematically and securely removing individual data contributions\nfrom the learned model without retraining from scratch. we explore various strategies that enable effective\nunlearning, such as perturbation techniques, model decomposition, and incremental learning, highlighting\ntheir implications for maintaining model performance and data privacy. furthermore, we examine case\nstudies and experimental results from recent literature to assess the effectiveness and efficiency of these\napproaches in real-world scenarios. our survey reveals a growing interest in developing more robust and\nscalable federated unlearning methods, suggesting a vital area for future research in the intersection of ai\nethics and distributed machine learning technologies.\nindex terms federated learning (fl), large language models (llms), swarm intelligence, efficiency,\npre-trained models, privacy and security\ni."
    },
    {
        "url": "https://arxiv.org/pdf/2406.09838",
        "label": 1.0,
        "text_before_introduction": "vision-language models meet meteorology:\ndeveloping models for extreme weather events\ndetection with heatmaps\njian chen1 2peilin zhou2yining hua3dading chong4\nmeng cao5yaowei li6zixuan yuan2bing zhu1*junwei liang2*\n1hsbc2hong kong university of science and technology (guangzhou)3harvard university\n4peking university5mohamed bin zayed university of artificial intelligence\n6school of engineering and applied sciences, harvard university\n{alex.j.chen, bing1.zhu}@hsbc.com, {jchen524, pzhou460}@connect.hkust-gz.edu.cn,\nyininghua@g.harvard.edu, {1601213984, mengcao}@pku.edu.cn,\nyaoweili@seas.harvard.edu, {zixuanyuan, junweiliang}@hkust-gz.edu.cn\nabstract\nreal-time detection and prediction of extreme weather protect human lives and\ninfrastructure. traditional methods rely on numerical threshold setting and manual\ninterpretation of weather heatmaps with geographic information systems (gis),\nwhich can be slow and error-prone. our research redefines extreme weather\nevents detection (ewed) by framing it as a visual question answering (vqa)\nproblem, thereby introducing a more precise and automated solution. leveraging\nvision-language models (vlm) to simultaneously process visual and textual data,\nwe offer an effective aid to enhance the analysis process of weather heatmaps.\nour initial assessment of general-purpose vlms (e.g., gpt-4-vision) on ewed\nrevealed poor performance, characterized by low accuracy and frequent halluci-\nnations due to inadequate color differentiation andinsufficient meteorological\nknowledge . to address these challenges, we introduce climateiqa , the first mete-\norological vqa dataset, which includes 8,760 wind gust heatmaps and 254,040\nquestion-answer pairs covering four question types, both generated from the latest\nclimate reanalysis data. we also propose sparse position and outline tracking\n(spot) , an innovative technique that leverages opencv and k-means clustering\nto capture and depict color contours in heatmaps, providing climateiqa with\nmore accurate color spatial location information. finally, we present climate-zoo ,\nthe first meteorological vlm collection, which adapts vlms to meteorological\napplications using the climateiqa dataset. experiment results demonstrate that\nmodels from climate-zoo substantially outperform state-of-the-art general vlms,\nachieving an accuracy increase from 0% to over 90% in ewed verification. the\ndatasets and models in this study are publicly available for future climate science\nresearch: https://github.com/alexjjjchen/climate-zoo .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09870",
        "label": 1.0,
        "text_before_introduction": "igl-bench: establishing the comprehensive\nbenchmark for imbalanced graph learning\njiawen qin1\u2217, haonan yuan1\u2217, qingyun sun1\u2217, lyujin xu1, jiaqi yuan2, pengfeng huang1,\nzhaonan wang1, xingcheng fu3, hao peng1, jianxin li1, philip s. yu4\n1beihang univerisity2university of electronic science and technology of china\n3guangxi normal university4university of illinois, chicago\n{qinjw,yuanhn,sunqy}@buaa.edu.cn\nabstract\ndeep graph learning has gained grand popularity over the past years due to its versa-\ntility and success in representing graph data across a wide range of domains. how-\never, the pervasive issue of imbalanced graph data distributions, where certain parts\nexhibit disproportionally abundant data while others remain sparse, undermines the\nefficacy of conventional graph learning algorithms, leading to biased outcomes. to\naddress this challenge, imbalanced graph learning (igl) has garnered substantial\nattention, enabling more balanced data distributions and better task performance.\ndespite the proliferation of igl algorithms, the absence of consistent experimental\nprotocols and fair performance comparisons pose a significant barrier to compre-\nhending advancements in this field. to bridge this gap, we introduce igl-bench , a\nfoundational comprehensive benchmark for imbalanced graph learning, embarking\non16diverse graph datasets and 24distinct igl algorithms with uniform data\nprocessing and splitting strategies. specifically, igl-bench systematically inves-\ntigates state-of-the-art igl algorithms in terms of effectiveness ,robustness , and\nefficiency on node-level and graph-level tasks, with the scope of class-imbalance\nandtopology-imbalance . extensive experiments demonstrate the potential benefits\nof igl algorithms on various imbalanced conditions, offering insights and opportu-\nnities in the igl field. further, we have developed an open-sourced and unified\npackage to facilitate reproducible evaluation and inspire further innovative research,\nwhich is available at https://github.com/ringbdstack/igl-bench .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09877",
        "label": 0.0,
        "text_before_introduction": "federated learning with flexible architectures\njong-ik park and carlee joe-wong \u0000[0000\u22120003\u22120785\u22129291]\ncarnegie mellon university, pittsburgh pa 15213, usa\ncjoewong@andrew.cmu.edu\nhttps://www.cmu.edu/\nabstract. traditional federated learning (fl) methods have limited\nsupport for clients with varying computational and communication abil-\nities, leading to inefficiencies and potential inaccuracies in model train-\ning. this limitation hinders the widespread adoption of fl in diverse\nand resource-constrained environments, such as those with client devices\nranging from powerful servers to mobile devices. to address this need,\nthis paper introduces federated learning with flexible architectures\n(fedfa), an fl training algorithm that allows clients to train models of\ndifferent widths and depths. each client can select a network architec-\nture suitable for its resources, with shallower and thinner networks re-\nquiring fewer computing resources for training. unlike prior work in this\narea, fedfa incorporates the layer grafting technique to align clients\u2019 lo-\ncal architectures with the largest network architecture in the fl system\nduring model aggregation. layer grafting ensures that all client contribu-\ntions are uniformly integrated into the global model, thereby minimizing\nthe risk of any individual client\u2019s data skewing the model\u2019s parameters\ndisproportionately and introducing security benefits. moreover, fedfa\nintroduces the scalable aggregation method to manage scale variations in\nweights among different network architectures. experimentally, fedfa\noutperforms previous width and depth flexible aggregation strategies.\nspecifically, fedfa\u2019s testing accuracy matches (1.00 times) or is up to\n1.16 times higher globally for iid settings, 0.98 to 1.13 times locally,\nand 0.95 times to 1.20 times higher globally for non-iid settings com-\npared to earlier strategies. furthermore, fedfa demonstrates increased\nrobustness against performance degradation in backdoor attack scenar-\nios compared to earlier strategies. earlier strategies exhibit more drops\nin testing accuracy under attacks\u2014for iid data by 1.01 to 2.11 times\nglobally, and for non-iid data by 0.89 to 3.31 times locally, and 1.11 to\n1.74 times globally, compared to fedfa.\nkeywords: federated learning \u00b7heterogeneous local network archi-\ntectures \u00b7backdoor attack\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09979",
        "label": 0.0,
        "text_before_introduction": "hiro: hierarchical information retrieval optimization\nkrish goel, mahek chandak\nkrishgoel3@gmail.com, mahekchandak99@gmail.com\nabstract\nlarge language models (llms) excel in natural language\ntasks but face limitations due to static training datasets, result-\ning in outdated or contextually shallow responses. retrieval-\naugmented generation (rag) addresses this by integrating\nreal-time external knowledge, enhancing model accuracy and\ncredibility, especially for knowledge-intensive tasks. how-\never, rag-enhanced llms struggle with long contexts, caus-\ning them to \u201dchoke\u201d on information overload, compromising\nresponse quality. recent rag applications use hierarchical\ndata structures for storing documents, organized at various\nlevels of summarization and information density. in this con-\ntext, we introduce hiro (hierarchical information retrieval\noptimization), a novel querying approach for rag appli-\ncations using hierarchical structures for storing documents.\nhiro employs dfs-based recursive similarity score calcula-\ntion and branch pruning to minimize the context returned to\nthe llm without informational loss. hiro outperforms ex-\nisting querying mechanisms on the narrativeqa dataset by\nan absolute performance gain of 10.85%.\ncode \u2014 https://github.com/krishgoel/hiro"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09988",
        "label": 1.0,
        "text_before_introduction": "details make a difference: object state-sensitive\nneurorobotic task planning\nxiaowen sun\u22c6, xufeng zhao, jae hee lee, wenhao lu,\nmatthias kerzel, and stefan wermter\nknowledge technology, department of informatics, university of hamburg\n{xiaowen.sun, xufeng.zhao, jae.hee.lee, wenhao.lu,\nmatthias.kerzel, stefan.wermter}@uni-hamburg.de\nwww.knowledge-technology.info\nabstract. the state of an object reflects its current status or condition\nand is important for a robot\u2019s task planning and manipulation. how-\never, detecting an object\u2019s state and generating a state-sensitive plan\nfor robots is challenging. recently, pre-trained large language models\n(llms) and vision-language models (vlms) have shown impressive\ncapabilities in generating plans. however, to the best of our knowledge,\nthereishardlyanyinvestigationonwhetherllmsorvlmscanalsogen-\nerate object state-sensitive plans. to study this, we introduce an object\nstate-sensitive agent (ossa), a task-planning agent empowered by pre-\ntrained neural networks. we propose two methods for ossa: (i) a modu-\nlarmodelconsistingofapre-trainedvisionprocessingmodule(densecap-\ntioning model, dcm) and a natural language processing model (llm),\nand (ii) a monolithic model consisting only of a vlm. to quantitatively\nevaluate the performances of the two methods, we use tabletop scenarios\nwhere the task is to clear the table. we contribute a multimodal bench-\nmark dataset that takes object states into consideration. our results\nshow that both methods can be used for object state-sensitive tasks, but\nthe monolithic approach outperforms the modular approach. the code\nfor ossa is available at https://github.com/xiao-wen-sun/ossa\nkeywords: objectstateidentification \u00b7artificialintelligence \u00b7robotics\n\u00b7language models \u00b7multimodality\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10040",
        "label": 1.0,
        "text_before_introduction": "fzi-wim at semeval-2024 task 2: self-consistent cot for complex nli\nin biomedical domain\njin liu1, 2and steffen thoma1\n1fzi research center for information technology, karlsruhe, germany\n2karlsruhe institute of technology, karlsruhe, germany\n{jin.liu, thoma}@fzi.de\nabstract\nthis paper describes the inference system of\nfzi-wim at the semeval-2024 task 2: safe\nbiomedical natural language inference for\nclinical trials. our system utilizes the chain\nof thought (cot) paradigm to tackle this com-\nplex reasoning problem and further improves\nthe cot performance with self-consistency. in-\nstead of greedy decoding, we sample multiple\nreasoning chains with the same prompt and\nmake the final verification with majority voting.\nthe self-consistent cot system achieves a base-\nline f1 score of 0.80 (1st), faithfulness score of\n0.90 (3rd), and consistency score of 0.73 (12th).\nwe release the code and data publicly1.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09948",
        "label": 1.0,
        "text_before_introduction": "ble nd: a benchmark for llms on everyday\nknowledge in diverse cultures and languages\njunho myung1,\u2217, nayeon lee1,\u2217, yi zhou2,\u2217, jiho jin1, rifki afina putri1,\ndimosthenis antypas2, hsuvas borkakoty2, eunsu kim1, carla perez-almendros2,\nabinew ali ayele3,4, v\u00edctor guti\u00e9rrez-basulto2, yazm\u00edn ib\u00e1\u00f1ez-garc\u00eda2, hwaran lee5,\nshamsuddeen hassan muhammad6, kiwoong park1, anar sabuhi rzayev1, nina white2,\nseid muhie yimam3, mohammad taher pilehvar2, nedjma ousidhoum2,\njose camacho-collados2, alice oh1\n1kaist,2cardiff university,3universit\u00e4t hamburg,4bahir dar university,\n5na ver ai lab,6imperial college london\nabstract\nlarge language models (llms) often lack culture-specific knowledge of daily life,\nespecially across diverse regions and non-english languages. existing benchmarks\nfor evaluating llms\u2019 cultural sensitivities are limited to a single language or col-\nlected from online sources such as wikipedia, which do not reflect the mundane\neveryday lifestyles of diverse regions. that is, information about the food people\neat for their birthday celebrations, spices they typically use, musical instruments\nyoungsters play, or the sports they practice in school is common cultural knowledge\nbut uncommon in easily collected online sources, especially for underrepresented\ncultures. to address this issue, we introduce ble nd, a hand-crafted benchmark\ndesigned to evaluate llms\u2019 everyday knowledge across diverse cultures and lan-\nguages. ble nd comprises 52.6k question-answer pairs from 16 countries/regions,\nin 13 different languages, including low-resource ones such as amharic, assamese,\nazerbaijani, hausa, and sundanese. we construct the benchmark to include two\nformats of questions: short-answer and multiple-choice. we show that llms\nperform better for cultures that are highly represented online, with a maximum\n57.34% difference in gpt-4, the best-performing model, in the short-answer format.\nfor cultures represented by mid-to-high-resource languages, llms perform better\nin their local languages, but for cultures represented by low-resource languages,\nllms perform better in english than the local languages. we make our dataset\npublicly available at: https://github.com/nlee0212/blend .\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09979",
        "label": 1.0,
        "text_before_introduction": "hiro: hierarchical information retrieval optimization\nkrish goel, mahek chandak\nkrishgoel3@gmail.com, mahekchandak99@gmail.com\nabstract\nlarge language models (llms) excel in natural language\ntasks but face limitations due to static training datasets, result-\ning in outdated or contextually shallow responses. retrieval-\naugmented generation (rag) addresses this by integrating\nreal-time external knowledge, enhancing model accuracy and\ncredibility, especially for knowledge-intensive tasks. how-\never, rag-enhanced llms struggle with long contexts, caus-\ning them to \u201dchoke\u201d on information overload, compromising\nresponse quality. recent rag applications use hierarchical\ndata structures for storing documents, organized at various\nlevels of summarization and information density. in this con-\ntext, we introduce hiro (hierarchical information retrieval\noptimization), a novel querying approach for rag appli-\ncations using hierarchical structures for storing documents.\nhiro employs dfs-based recursive similarity score calcula-\ntion and branch pruning to minimize the context returned to\nthe llm without informational loss. hiro outperforms ex-\nisting querying mechanisms on the narrativeqa dataset by\nan absolute performance gain of 10.85%.\ncode \u2014 https://github.com/krishgoel/hiro"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10040",
        "label": 1.0,
        "text_before_introduction": "fzi-wim at semeval-2024 task 2: self-consistent cot for complex nli\nin biomedical domain\njin liu1, 2and steffen thoma1\n1fzi research center for information technology, karlsruhe, germany\n2karlsruhe institute of technology, karlsruhe, germany\n{jin.liu, thoma}@fzi.de\nabstract\nthis paper describes the inference system of\nfzi-wim at the semeval-2024 task 2: safe\nbiomedical natural language inference for\nclinical trials. our system utilizes the chain\nof thought (cot) paradigm to tackle this com-\nplex reasoning problem and further improves\nthe cot performance with self-consistency. in-\nstead of greedy decoding, we sample multiple\nreasoning chains with the same prompt and\nmake the final verification with majority voting.\nthe self-consistent cot system achieves a base-\nline f1 score of 0.80 (1st), faithfulness score of\n0.90 (3rd), and consistency score of 0.73 (12th).\nwe release the code and data publicly1.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10099",
        "label": 1.0,
        "text_before_introduction": "know the unknown: an uncertainty-sensitive method for llm\ninstruction tuning\njiaqi li1, yixuan tang1, yi yang1\n1the hong kong university of science and technology\njiaqili@ust.hk, ytangch@connect.ust.hk, imyiyang@ust.hk\nabstract\nlarge language models (llms) have demon-\nstrated remarkable capabilities across various\ntasks but still face challenges such as hallu-\ncinations. one potential reason for halluci-\nnations is the lack of relevant knowledge or\ncontext. thus, a promising solution to miti-\ngate this issue involves instructing llms to\nrespond with \"i do not know\" when a question\nfalls outside their knowledge domain or the\nprovided context. however, in this work, we\nobserved that llms struggle to admit their lack\nof knowledge, primarily due to existing instruc-\ntion datasets designed to encourage specific\nanswers. to improve large language models\u2019\ncapability to recognize the boundaries of their\nknowledge, we propose a novel approach called\nuncertainty-sensitive tuning. this method in-\nvolves two-stage training designed for uncer-\ntainty recognition and prompt-sensitive activa-\ntion. in the first stage, we guide the llm to\nreject unknown questions. in the second stage,\nwe recover the decreased performance in qa\ntasks by incorporating designed causal instruc-\ntions. by leveraging this method, we aim to en-\nhance the model\u2019s ability to identify areas of un-\ncertainty. the experimental results demonstrate\nthat our proposed uncertainty-sensitive tuning\nmethod significantly improves the performance\nof the llama2-chat-7b model. specifically, it\nachieves a substantial 34.7% improvement in\nhandling questions involving knowledge gaps\ncompared to the original model. moreover,\nour approach outperforms gpt-4, exhibiting\na 9.4% increase in overall performance. we\nopen-source the model and code on github1.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10118",
        "label": 0.0,
        "text_before_introduction": "seacrowd: a multilingual multimodal data hub\nand benchmark suite for southeast asian languages\nholy lovenia\u22c6,1,2rahmad mahendra\u22c6,3,2salsabil maulana akbar\u22c6,2lester james v . miranda\u22c6,4\njennifer santoso\u22c6,5elyanah aco\u22c6,6akhdan fadhilah\u22c6,7jonibek mansurov\u22c6,8joseph marvin imperial\u22c6,9,10\nonno p. kampman\u22c6,11joel ruben antony moniz\u22c6,6muhammad ravi shulthan habibi\u22c6,3,2frederikus hudi\u22c6,12,13\nrailey montalan\u22c6,1ryan ignatius6joanito agili lopo14william nixon15b\u00f6rje f. karlsson16james jaya6\nryandito diandaru6yuze gao6patrick amadeus15bin wang6jan christian blaise cruz17chenxi whitehouse18\nivan halim parmonangan19maria khelli15wenyu zhang6lucky susanto20reynard adha ryanda21\nsonny lazuardi hermawan22dan john velasco17muhammad dehan al kautsar15willy fitra hendria6\nyasmin moslem23noah flynn24muhammad farid adilazuarda8haochen li6johanes lee15r. damanhuri25\nshuo sun6muhammad reza qorib26amirbek djanibekov8wei qi leong1quyet v . do27niklas muennighoff28\ntanrada pansuwan18ilham firdausi putra6yan xu29,27ngee chia tai1ayu purwarianti6,30\nsebastian ruder31william tjhi1peerat limkonchotiwat\u22c6,32alham fikri aji\u22c6,8sedrick keh\u22c6,33\ngenta indra winata\u22c6,2ruochen zhang\u22c6,34fajri koto\u22c6,8,2zheng-xin yong\u22c6,34samuel cahyawijaya\u22c6,27,2\n1ai singapore2indonlp3universitas indonesia4allen institute for artificial intelligence5revcomm, inc.\n6independent researcher7tohoku university8mbzuai9university of bath10national university philippines\n11moh office for healthcare transformation (moht)12naist13works applications lab14universitas gadjah mada\n15institut teknologi bandung16beijing academy of artificial intelligence (baai)17samsung research philippines\n18university of cambridge19queensland university of technology20monash university indonesia21imperial college london\n22independent design engineer23bering lab24amazon25universitas diponegoro26nus27hkust28contextual ai\n29huawei noah\u2019s ark lab30prosa.ai31cohere32vistec33toyota research institute34brown university\n\u22c6major contributors\nabstract\nsoutheast asia (sea) is a region rich in lin-\nguistic diversity and cultural variety, with over\n1,300 indigenous languages and a population\nof 671 million people. however, prevailing ai\nmodels suffer from a significant lack of repre-\nsentation of texts, images, and audio datasets\nfrom sea, compromising the quality of ai\nmodels for sea languages. evaluating models\nfor sea languages is challenging due to the\nscarcity of high-quality datasets, compounded\nby the dominance of english training data, rais-\ning concerns about potential cultural misrep-\nresentation. to address these challenges, we\nintroduce seacrowd, a collaborative initia-\ntive that consolidates a comprehensive resource\nhub1that fills the resource gap by providing\nstandardized corpora2in nearly 1,000 sea lan-\nguages across three modalities. through our\nseacrowd benchmarks, we assess the qual-\nity of ai models on 36 indigenous languages\nacross 13 tasks, offering valuable insights into\nthe current ai landscape in sea. furthermore,\nwe propose strategies to facilitate greater ai ad-\nvancements, maximizing potential utility and\nresource equity for the future of ai in sea.\n1https://seacrowd.github.io/seacrowd-catalogue/\n2https://github.com/seacrowd/seacrowd-datahub/1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10130",
        "label": 1.0,
        "text_before_introduction": "Abstract or Introduction section not found on the first page."
    },
    {
        "url": "https://arxiv.org/pdf/2406.10172",
        "label": 1.0,
        "text_before_introduction": "datasets for multilingual answer sentence selection\nmatteo gabburo1, stefano campese1, federico agostini2,3, alessandro moschitti4\n1university of trento ,2polytechnic university of turin,3university of padua,4amazon alexa ai\n{matteo.gabburo,stefano.campese}@unitn.it\nfederico.agostini.5@studenti.unipd.it\namosch@amazon.com\nabstract\nanswer sentence selection (as2) is a criti-\ncal task for designing effective retrieval-based\nquestion answering (qa) systems. most ad-\nvancements in as2 focus on english due to\nthe scarcity of annotated datasets for other\nlanguages. this lack of resources prevents\nthe training of effective as2 models in dif-\nferent languages, creating a performance gap\nbetween qa systems in english and other lo-\ncales. in this paper, we introduce new high-\nquality datasets for as2 in five european lan-\nguages (french, german, italian, portuguese,\nand spanish), obtained through supervised au-\ntomatic machine translation (amt) of ex-\nisting english as2 datasets such as asnq,\nwikiqa, and trec-qa using a large lan-\nguage model (llm). we evaluated our ap-\nproach and the quality of the translated datasets\nthrough multiple experiments with different\ntransformer architectures. the results indicate\nthat our datasets are pivotal in producing robust\nand powerful multilingual as2 models, signifi-\ncantly contributing to closing the performance\ngap between english and other languages.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10173",
        "label": 1.0,
        "text_before_introduction": "intention qa: a benchmark for evaluating purchase intention\ncomprehension abilities of language models in e-commerce\nwenxuan ding*, weiqi wang\u2217, sze heng douglas kwok, minghao liu,\ntianqing fang, jiaxin bai, junxian he, yangqiu song\ndepartment of computer science and engineering, hkust, hong kong sar, china\nwdingaj@connect.ust.hk, {wwangbw, junxianh, yqsong}@cse.ust.hk\nabstract\nenhancing language models\u2019 (lms) ability to\nunderstand purchase intentions in e-commerce\nscenarios is crucial for their effective assis-\ntance in various downstream tasks. how-\never, previous approaches that distill inten-\ntions from lms often fail to generate mean-\ningful and human-centric intentions applica-\nble in real-world e-commerce contexts. this\nraises concerns about the true comprehension\nand utilization of purchase intentions by lms.\nin this paper, we present intention qa, a\ndouble-task multiple-choice question answer-\ning benchmark to evaluate lms\u2019 comprehen-\nsion of purchase intentions in e-commerce.\nspecifically, lms are tasked to infer inten-\ntions based on purchased products and uti-\nlize them to predict additional purchases. in-\ntention qa consists of 4,360 carefully cu-\nrated problems across three difficulty levels,\nconstructed using an automated pipeline to\nensure scalability on large e-commerce plat-\nforms. human evaluations demonstrate the\nhigh quality and low false-negative rate of our\nbenchmark. extensive experiments across 19\nlanguage models show that they still strug-\ngle with certain scenarios, such as understand-\ning products and intentions accurately, jointly\nreasoning with products and intentions, and\nmore, in which they fall far behind human\nperformances. our code and data are pub-\nlicly available at https://github.com/hkust-\nknowcomp/intentionqa.\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10203",
        "label": 1.0,
        "text_before_introduction": "a fundamental trade-off in aligned language models and its relation to\nsampling adaptors\nnaaman tan\n josef valvoda\n tianyu liu\n anej svete\nyanxia qin\n kan min-yen\n ryan cotterell\nnational university of singapore\n university of copenhagen\n eth z\u00fcrich\n{tannaaman ,knmnyn }@nus.edu.sg jval@di.ku.dk\n{tianyu.liu ,asvete ,rcotterell }@inf.ethz.ch\nabstract\nthe relationship between the quality of a string\nand its probability p(y)under a language\nmodel has been influential in the development\nof techniques to build good text generation\nsystems. for example, several decoding\nalgorithms have been motivated to manipulate\np(y)to produce higher-quality text. in this\nwork, we examine the probability\u2013quality\nrelationship in language models explicitly\naligned to human preferences, e.g., through\nreinforcement learning through human\nfeedback (rlhf). we find that, given a\ngeneral language model and its aligned version,\nfor corpora sampled from an aligned language\nmodel, there exists a trade-off between the\naverage reward and average log-likelihood of\nthe strings under the general language model.\nwe provide a formal treatment of this issue and\ndemonstrate how a choice of sampling adaptor\nallows for a selection of how much likelihood\nwe exchange for the reward.\nhttps://github.com/tanyjnaaman/\nprobability-quality-paradox\n1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10203",
        "label": 1.0,
        "text_before_introduction": "a fundamental trade-off in aligned language models and its relation to\nsampling adaptors\nnaaman tan\n josef valvoda\n tianyu liu\n anej svete\nyanxia qin\n kan min-yen\n ryan cotterell\nnational university of singapore\n university of copenhagen\n eth z\u00fcrich\n{tannaaman ,knmnyn }@nus.edu.sg jval@di.ku.dk\n{tianyu.liu ,asvete ,rcotterell }@inf.ethz.ch\nabstract\nthe relationship between the quality of a string\nand its probability p(y)under a language\nmodel has been influential in the development\nof techniques to build good text generation\nsystems. for example, several decoding\nalgorithms have been motivated to manipulate\np(y)to produce higher-quality text. in this\nwork, we examine the probability\u2013quality\nrelationship in language models explicitly\naligned to human preferences, e.g., through\nreinforcement learning through human\nfeedback (rlhf). we find that, given a\ngeneral language model and its aligned version,\nfor corpora sampled from an aligned language\nmodel, there exists a trade-off between the\naverage reward and average log-likelihood of\nthe strings under the general language model.\nwe provide a formal treatment of this issue and\ndemonstrate how a choice of sampling adaptor\nallows for a selection of how much likelihood\nwe exchange for the reward.\nhttps://github.com/tanyjnaaman/\nprobability-quality-paradox\n1"
    }
]