[
    {
        "url": "https://arxiv.org/pdf/2406.08920",
        "label": 0,
        "title": "a v-gs: learning material and geometry aware priors for novel view acoustic synthesis swapnil bhosale",
        "abstract": "novel view acoustic synthesis (nv as) aims to render binaural audio at any target viewpoint, given a mono audio emitted by a sound source at a 3d scene. existing methods have proposed nerf-based implicit models to exploit visual cues as a condition for synthesizing binaural audio. however, in addition to low efficiency originating from heavy nerf rendering, these methods all have a limited ability of characterizing the entire scene environment such as room geometry, material prop- erties, and the spatial relation between the listener and sound source. to address these issues, we propose a novel audio-visual gaussian splatting (a v-gs) model. to obtain a material-aware and geometry-aware condition for audio synthesis, we learn an explicit point-based scene representation with an audio-guidance param- eter on locally initialized gaussian points, taking into account the space relation from the listener and sound source. to make the visual scene model audio adaptive, we propose a point densification and pruning strategy to optimally distribute the gaussian points, with the per-point contribution in sound propagation (e.g., more points needed for texture-less wall surfaces as they affect sound path diversion). extensive experiments validate the superiority of our a v-gs over existing alterna- tives on the real-world rwas and simulation-based soundspaces datasets. project page: https://surrey-uplab.github.io/research/avgs/ "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09021",
        "label": 0,
        "title": "contextual distillation model for diversified recommendation fan li",
        "abstract": "the diversity of recommendation is equally crucial as accuracy in improving user experience. existing studies, e.g., determinantal point process (dpp) and maximal marginal relevance (mmr), em- ploy a greedy paradigm to iteratively select items that optimize both accuracy and diversity. however, prior methods typically exhibit quadratic complexity, limiting their applications to the re-ranking stage and are not applicable to other recommendation stages with a larger pool of candidate items, such as the pre-ranking andranking stages. in this paper, we propose contextual distillation model (cdm), an efficient recommendation model that addresses diver- sification, suitable for the deployment in all stages of industrial recommendation pipelines. specifically, cdm utilizes the candidate items in the same user request as context to enhance the diversifi- cation of the results. we propose a contrastive context encoder that employs attention mechanisms to model both positive and negative contexts. for the training of cdm, we compare each target item with its context embedding and utilize the knowledge distillation framework to learn the win probability of each target item under the mmr algorithm, where the teacher is derived from mmr outputs. during inference, ranking is performed through a linear combina- tion of the recommendation and student model scores, ensuring both diversity and efficiency. we perform offline evaluations on two industrial datasets and conduct online a/btest of cdm on the short-video platform kuaishou . the considerable enhancements \u2217equal contributions. \u2020corresponding author. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. kdd \u201924, august 25\u201329, 2024, barcelona, spain \u00a92024 copyright held by the owner/author(s). publication rights licensed to acm. acm isbn 979-8-4007-0490-1/24/08. . . $15.00 https://doi.org/10.1145/3637528.3671514observed in both recommendation quality and diversity, as shown by metrics, provide strong superiority for the effectiveness of cdm. ccs concepts \u2022information systems \u2192recommender systems . keywords recommender system, knowledge distillation, diversified recom- mendation acm reference format: fan li, xu si, shisong tang, dingmin wang, kunyan han, bing han, guorui zhou, yang song, and hechang chen. 2024. contextual distillation model for diversified recommendation. in proceedings of the 30th acm sigkdd conference on knowledge discovery and data mining (kdd \u201924), august 25\u201329, 2024, barcelona, spain. acm, new york, ny, usa, 10 pages. https: //doi.org/10.1145/3637528.3671514 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09215",
        "label": 0,
        "title": "on softmax direct preference optimization for recommendation yuxin chen1",
        "abstract": "recommender systems aim to predict personalized rankings based on user pref- erence data. with the rise of language models (lms), lm-based recommenders have been widely explored due to their extensive world knowledge and power- ful reasoning abilities. most of the lm-based recommenders convert historical interactions into language prompts, pairing with a positive item as the target re- sponse and fine-tuning lm with a language modeling loss. however, the current objective fails to fully leverage preference data and is not optimized for personal- ized ranking tasks, which hinders the performance of lm-based recommenders. inspired by the current advancement of direct preference optimization (dpo) in human preference alignment and the success of softmax loss in recommenda- tions, we propose softmax-dpo ( s-dpo ) to instill ranking information into the lm to help lm-based recommenders distinguish preferred items from negatives, rather than solely focusing on positives. specifically, we incorporate multiple negatives in user preference data and devise an alternative version of dpo loss tailored for lm-based recommenders, connected to softmax sampling strategies. theoretically, we bridge s-dpo with the softmax loss over negative sampling and find that it has a side effect of mining hard negatives, which assures its excep- tional capabilities in recommendation tasks. empirically, extensive experiments conducted on three real-world datasets demonstrate the superiority of s-dpo to effectively model user preference and further boost recommendation performance while mitigating the data likelihood decline issue of dpo. our codes are available athttps://github.com/chenyuxin1999/s-dpo . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08587",
        "label": 1,
        "title": "cs-bench: a comprehensive benchmark for large language models towards computer science mastery xiaoshuai song",
        "abstract": "computer science (cs) stands as a testament to the intricacies of human intelli- gence, profoundly advancing the development of artificial intelligence and modern society. however, the current community of large language models (llms) overly focuses on benchmarks for analyzing specific foundational skills (e.g. mathematics and code generation), neglecting an all-round evaluation of the computer science field. to bridge this gap, we introduce cs-bench, the first bilingual (chinese- english) benchmark dedicated to evaluating the performance of llms in computer science. cs-bench comprises approximately 5k meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing var- ious task forms and divisions of knowledge and reasoning. utilizing cs-bench, we conduct a comprehensive evaluation of over 30 mainstream llms, revealing the relationship between cs performance and model scales. we also quantita- tively analyze the reasons for failures in existing llms and highlight directions for improvements, including knowledge supplementation and cs-specific reason- ing. further cross-capability experiments show a high correlation between llms\u2019 capabilities in computer science and their abilities in mathematics and coding. moreover, expert llms specialized in mathematics and coding also demonstrate strong performances in several cs subfields. looking ahead, we envision cs- bench serving as a cornerstone for llm applications in the cs field and paving new avenues in assessing llms\u2019 diverse reasoning capabilities. the cs-bench data and evaluation code are available at https://github.com/csbench/csbench. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08973",
        "label": 1,
        "title": "xland-100b: a large-scale multi-task dataset for in-context reinforcement learning alexander nikulin",
        "abstract": "following the success of the in-context learning paradigm in large-scale language and computer vision models, the recently emerging field of in-context reinforce- ment learning is experiencing a rapid growth. however, its development has been held back by the lack of challenging benchmarks, as all the experiments have been carried out in simple environments and on small-scale datasets. we present xland-100b , a large-scale dataset for in-context reinforcement learning based on the xland-minigrid environment, as a first step to alleviate this prob- lem. it contains complete learning histories for nearly 30,000different tasks, covering 100b transitions and 2.5b episodes. it took 50,000gpu hours to col- lect the dataset, which is beyond the reach of most academic labs. along with the dataset, we provide the utilities to reproduce or expand it even further. with this substantial effort, we aim to democratize research in the rapidly growing field of in-context reinforcement learning and provide a solid foundation for fur- ther scaling. the code is open-source and available under apache 2.0 licence at https://github.com/dunno-lab/xland-minigrid-datasets . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08598",
        "label": 1,
        "title": "language model council: benchmarking foundation models on highly subjective tasks by consensus justin zhao1",
        "abstract": "the rapid advancement of large language models (llms) necessitates robust and challenging benchmarks. leaderboards like chatbot arena rank llms based on how well their responses align with human preferences. however, many tasks such as those related to emotional intelligence, creative writing, or persuasiveness, are highly subjective and often lack majoritarian human agreement. judges may have irreconcilable disagreements about what constitutes a better response. to address the challenge of ranking llms on highly subjective tasks, we propose a novel benchmarking framework, the language model council (lmc) . the lmc operates through a democratic process to: 1) formulate a test set through equal participation, 2) administer the test among council members, and 3) evaluate responses as a collective jury. we deploy a council of 20 newest llms on an open-ended emotional intelligence task: responding to interpersonal dilemmas. our results show that the lmc produces rankings that are more separable, robust, and less biased than those from any individual llm judge, and is more consistent with a human-established leaderboard compared to other benchmarks. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08979",
        "label": 1,
        "title": "multi-agent software development through cross-team collaboration zhuoyun du\u2020\u2663chen qian\u2020",
        "abstract": "the latest breakthroughs in large language models (llms), e.g., chatdev, have catalyzed profound transformations, particularly through multi-agent collaboration for software devel- opment. llm agents can collaborate in teams like humans, and follow the waterfall model to sequentially work on requirements analysis, development, review, testing, and other phases to perform autonomous software generation. however, for an agent team, each phase in a single development process yields only one pos- sible outcome. this results in the completion of only one development chain, thereby losing the opportunity to explore multiple potential decision paths within the solution space. con- sequently, this may lead to obtaining subop- timal results. to address this challenge, we introduce cross-teamcollaboration (ctc), a scalable multi-team framework that enables orchestrated teams to jointly propose various decisions and communicate with their insights in a cross-team collaboration environment for superior content generation. experimental re- sults in software development reveal a notable increase in quality compared to state-of-the- art baselines, underscoring the efficacy of our framework. the significant improvements in story generation demonstrate the promising generalization ability of our framework across various domains. we anticipate that our work will guide llm agents towards a cross-team paradigm and contribute to their significant growth in but not limited to software devel- opment. the code and data will be available at https://github.com/openbmb/chatdev . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09043",
        "label": 0,
        "title": "language models are crossword solvers soumadeep saha and sutanoya chakraborty and saptarshi saha and utpal gar",
        "abstract": "crosswords are a form of word puzzle that re- quire a solver to demonstrate a high degree of proficiency in natural language understand- ing, wordplay, reasoning, and world knowl- edge, along with adherence to character and length constraints. in this paper we tackle the challenge of solving crosswords with large language models (llms). we demonstrate that the current generation of state-of-the art (sota) language models show significant com- petence at deciphering cryptic crossword clues, and outperform previously reported sota re- sults by a factor of 2-3 in relevant benchmarks. we also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with llms for the very first time, achieving an accuracy of 93% on new york times crossword puzzles. contrary to previous work in this area which concluded that llms lag human expert perfor- mance significantly, our research suggests this gap is a lot narrower. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09009",
        "label": 0,
        "title": "fredformer: frequency debiased transformer for time series forecasting xihao piao",
        "abstract": "the transformer model has shown leading performance in time se- ries forecasting. nevertheless, in some complex scenarios, it tends to learn low-frequency features in the data and overlook high- frequency features, showing a frequency bias. this bias prevents the model from accurately capturing important high-frequency data features. in this paper, we undertake empirical analyses to understand this bias and discover that frequency bias results from the model disproportionately focusing on frequency features with higher energy. based on our analysis, we formulate this bias and propose fredformer , a transformer-based framework designed to mitigate frequency bias by learning features equally across dif- ferent frequency bands. this approach prevents the model from overlooking lower amplitude features important for accurate fore- casting. extensive experiments show the effectiveness of our pro- posed approach, which can outperform other baselines in differ- ent real-world time-series datasets. furthermore, we introduce a lightweight variant of the fredformer with an attention matrix approximation, which achieves comparable performance but with much fewer parameters and lower computation costs. the code is available at: https://github.com/chenzrg/fredformer ccs concepts \u2022computing methodologies \u2192artificial intelligence ;neural networks . keywords time series forecasting, deep learning acm reference format: xihao piao*, zheng chen*, taichi murayama, yasuko matsubara, and ya- sushi sakurai. 2024. fredformer: frequency debiased transformer for * indicates corresponding authors. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. kdd \u201924, august 25\u201329, 2024, barcelona, spain \u00a92024 copyright held by the owner/author(s). publication rights licensed to acm. acm isbn 979-8-4007-0490-1/24/08 https://doi.org/10.1145/3637528.3671928time series forecasting . in proceedings of the 30th acm sigkdd con- ference on knowledge discovery and data mining (kdd \u201924), august 25\u2013 29, 2024, barcelona, spain. acm, new york, ny, usa, 18 pages. https: //doi.org/10.1145/3637528.3671928 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08680",
        "label": 0,
        "title": "analyzing large language models for classroom discussion assessment nhat tran",
        "abstract": "automatically assessing classroom discussion quality is be- coming increasingly feasible with the help of new nlp ad- vancements such as large language models (llms). in this work, we examine how the assessment performance of 2 llms interacts with 3 factors that may affect performance: task formulation, context length, and few-shot examples. we also explore the computational efficiency and predic- tive consistency of the 2 llms. our results suggest that the 3 aforementioned factors do affect the performance of the tested llms and there is a relation between consistency and performance. we recommend a llm-based assessment approach that has a good balance in terms of predictive per- formance, computational efficiency, and consistency. keywords classroom discussion, large language models, scoring 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08707",
        "label": 1,
        "title": "moscar: a large-scale multilingual and multimodal document-level corpus matthieu futeral",
        "abstract": "multimodal large language models (mllms) are trained on a large amount of text-image data. while most mllms are trained on caption-like data only, alayrac et al. [2022] showed that additionally training them on interleaved sequences of text and images can lead to the emergence of in-context learning capabilities. however, the dataset they used, m3w, is not public and is only in english. there have been attempts to reproduce their results but the released datasets are english-only. in contrast, current multilingual and multimodal datasets are either composed of caption-like only or medium-scale or fully private data. this limits mllm research for the 7,000 other languages spoken in the world. we therefore introduce moscar, to the best of our knowledge the first large-scale multilingual and multimodal document corpus crawled from the web. it covers 163 languages, 315m documents, 214b tokens and 1.2b images. we carefully conduct a set of filtering and evaluation steps to make sure moscar is sufficiently safe, diverse and of good quality. we additionally train two types of multilingual model to prove the benefits of moscar: (1) a model trained on a subset of moscar and captioning data and (2) a model train on captioning data only. the model additionally trained on moscar shows a strong boost in few-shot learning performance across various multilingual image-text tasks and benchmarks, confirming previous findings for english-only mllms. the dataset can be accessed here.2 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09031",
        "label": 1,
        "title": "a c omprehensive graph pooling benchmark : effectiveness",
        "abstract": "graph pooling has gained attention for its ability to obtain effective node and graph representations for various downstream tasks. despite the recent surge in graph pooling approaches, there is a lack of standardized experimental settings and fair benchmarks to evaluate their performance. to address this issue, we have constructed a comprehensive benchmark that includes 15 graph pooling methods and 21 different graph datasets. this benchmark systematically assesses the performance of graph pooling methods in three dimensions, i.e., effectiveness, robustness, and generalizability. we first evaluate the performance of these graph pooling approaches across different tasks including graph classification, graph regression and node classification. then, we investigate their performance under potential noise attacks and out-of-distribution shifts in real-world scenarios. we also involve detailed efficiency analysis and parameter analysis. extensive experiments validate the strong capability and applicability of graph pooling approaches in various scenarios, which can provide valuable insights and guidance for deep geometric learning research. the source code of our benchmark is available at https://github.com/goose315/graph_pooling_benchmark . keywords graph pooling; benchmark; graph neural networks; graph machine learning "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09052",
        "label": 0,
        "title": "data-free generative replay for class-incremental learning on imbalanced data sohaib younis",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.08726",
        "label": 0,
        "title": "standardlanguage ideology inai-generated language genevievesmith",
        "abstract": "in this position paper, we explore standard language ideolo gy in languagegeneratedbylargelanguagemodels(llms).first, weout- linehowstandardlanguageideologyisre\ufb02ectedandreinfor ced in llms. we then present a taxonomy of open problems regarding standard language ideology in ai-generated language with i mpli- cations for minoritized language communities. we introduc e the concept of standard ai-generated language ideology, the pr ocess bywhich ai-generated language regards standard american e ng- lish(sae)asalinguisticdefaultandreinforcesalinguist icbiasthat sae is the most \u201cappropriate\u201d language. finally, we discuss ten- sions that remain, including re\ufb02ecting on what desirable sy stem behaviorlookslike,aswellasadvantagesanddrawbacksofg ener- ative ai tools imitating\u2014or often not\u2014di\ufb00erent english lan guage varieties.throughout,wediscussstandardlanguageideol ogyasa manifestation of existing global power structures in and th rough ai-generated language before ending with questions to move to- wards alternative, moreemancipatorydigital futures. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08757",
        "label": 1,
        "title": "srfund: a multi-granularity hierarchical structure reconstruction benchmark in form understanding jiefeng ma1y",
        "abstract": "accurately identifying and organizing textual content is crucial for the automation of document processing in the field of form understanding. existing datasets, such as funsd and xfund, support entity classification and relationship prediction tasks but are typically limited to local and entity-level annotations. this limitation overlooks the hierarchically structured representation of documents, constraining comprehensive understanding of complex forms. to address this issue, we present the srfund, a hierarchically structured multi-task form understanding bench- mark. srfund provides refined annotations on top of the original funsd and xfund datasets, encompassing five tasks: (1) word to text-line merging , (2) text-line to entity merging , (3) entity category classification , (4) item table lo- calization , and (5) entity-based full-document hierarchical structure recovery . we meticulously supplemented the original dataset with missing annotations at various levels of granularity and added detailed annotations for multi-item table regions within the forms. additionally, we introduce global hierarchical struc- ture dependencies for entity relation prediction tasks, surpassing traditional local key-value associations. the srfund dataset includes eight languages including english, chinese, japanese, german, french, spanish, italian, and portuguese , making it a powerful tool for cross-lingual form understanding. extensive exper- imental results demonstrate that the srfund dataset presents new challenges and significant opportunities in handling diverse layouts and global hierarchical structures of forms, thus providing deep insights into the field of form understand- ing. the original dataset and implementations of baseline methods are available at https://sprateam-ustc.github.io/srfund . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08796",
        "label": 1,
        "title": "deep exploration of cross-lingual zero-shot generalization in instruction tuning janghoon han",
        "abstract": "instruction tuning has emerged as a powerful technique, significantly boosting zero-shot per- formance on unseen tasks. while recent work has explored cross-lingual generalization by ap- plying instruction tuning to multilingual mod- els, previous studies have primarily focused on english, with a limited exploration of non- english tasks. for an in-depth exploration of cross-lingual generalization in instruction tun- ing, we perform instruction tuning individu- ally for two distinct language meta-datasets. subsequently, we assess the performance on unseen tasks in a language different from the one used for training. to facilitate this inves- tigation, we introduce a novel non-english meta-dataset named \"korani\" (korean natu- ral instruction), comprising 51 korean bench- marks. moreover, we design cross-lingual tem- plates to mitigate discrepancies in language and instruction-format of the template between training and inference within the cross-lingual setting. our experiments reveal consistent im- provements through cross-lingual generaliza- tion in both english and korean, outperforming baseline by average scores of 20.7% and 13.6%, respectively. remarkably, these enhancements are comparable to those achieved by monolin- gual instruction tuning and even surpass them in some tasks. the result underscores the sig- nificance of relevant data acquisition across lan- guages over linguistic congruence with unseen tasks during instruction tuning1. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09105",
        "label": 1,
        "title": "ins-mmbench: a comprehensive benchmark for evaluating lvlms\u2019 performance in insurance chenwei lin",
        "abstract": "large vision-language models (lvlms) have demonstrated outstanding per- formance in various general multimodal applications such as image recogni- tion and visual reasoning, and have also shown promising potential in special- ized domains. however, the application potential of lvlms in the insurance domain\u2014characterized by rich application scenarios and abundant multimodal data\u2014has not been effectively explored. there is no systematic review of multi- modal tasks in the insurance domain, nor a benchmark specifically designed to evaluate the capabilities of lvlms in insurance. this gap hinders the development of lvlms within the insurance domain. in this paper, we systematically review and distill multimodal tasks for four representative types of insurance: auto insurance, property insurance, health insurance, and agricultural insurance. we propose ins- mmbench, the first comprehensive lvlms benchmark tailored for the insurance domain. ins-mmbench comprises a total of 2.2k thoroughly designed multiple- choice questions, covering 12 meta-tasks and 22 fundamental tasks. furthermore, we evaluate multiple representative lvlms, including closed-source models such as gpt-4o and open-source models like blip-2. this evaluation not only validates the effectiveness of our benchmark but also provides an in-depth performance analysis of current lvlms on various multimodal tasks in the insurance domain. we hope that ins-mmbench will facilitate the further application of lvlms in the insurance domain and inspire interdisciplinary development. our dataset and evaluation code are available at https://github.com/fdu-ins/ins-mmbench . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09112",
        "label": 0,
        "title": "large-scale evaluation of open-set image classification techniques halil bisgin bisgin",
        "abstract": "the goal for classification is to correctly assign labels to unseen samples. however, most methods misclassify samples with unseen labels and assign them to one of the known classes. open-set classification (osc) algorithms aim to maximize both closed and open-set recog- nition capabilities. recent studies showed the utility of such algorithms on small-scale data sets, but limited experimentation makes it difficult to assess their performances in real-world problems. here, we provide a comprehensive comparison of various osc algo- rithms, including training-based (softmax, garbage, eos) and post-processing methods (maximum softmax scores, maximum logit scores, openmax, evm, proser), the lat- ter are applied on features from the former. we perform our evaluation on three large-scale protocols that mimic real-world challenges, where we train on known and negative open-set samples, and test on known and unknown instances. our results show that eos helps to improve performance of almost all post-processing algorithms. particularly, openmax and proser are able to exploit better-trained networks, demonstrating the utility of hybrid models. however, while most algorithms work well on negative test samples \u2013 samples of open-set classes seen during training \u2013 they tend to perform poorly when tested on samples of previously unseen unknown classes, especially in challenging conditions. keywords: open-set classification, large-scale evaluation, image classification, deep learn- ing, reproducible research 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09155",
        "label": 1,
        "title": "defan: definitive answer dataset for llms hallucination evaluation a b m ashikur rahman",
        "abstract": "large language models (llms) have demonstrated remarkable capabilities, revo- lutionizing the integration of ai in daily life applications. however, they are prone to hallucinations, generating claims that contradict established facts, deviating from prompts, and producing inconsistent responses when the same prompt is presented multiple times. addressing these issues is challenging due to the lack of com- prehensive and easily assessable benchmark datasets. most existing datasets are small and rely on multiple-choice questions, which are inadequate for evaluating the generative prowess of llms. to measure hallucination in llms, this paper introduces a comprehensive benchmark dataset comprising over 75,000 prompts across eight domains. these prompts are designed to elicit definitive, concise, and informative answers. the dataset is divided into two segments: one publicly available for testing and assessing llm performance and a hidden segment for benchmarking various llms. in our experiments, we tested six llms\u2014gpt-3.5, llama 2, llama 3, gemini, mixtral, and zephyr\u2014revealing that overall factual hallucination ranges from 59% to 82% on the public dataset and 57% to 76% in the hidden benchmark. prompt misalignment hallucination ranges from 6% to 95% in the public dataset and 17% to 94% in the hidden counterpart. average consistency ranges from 21% to 61% and 22% to 63%, respectively. domain-wise analysis shows that llm performance significantly deteriorates when asked for specific numeric information while performing moderately with person, location, and date queries. our dataset demonstrates its efficacy and serves as a comprehensive benchmark for llm performance evaluation. our dataset and llms responses are available at https://github.com/ashikiut/defan. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12793",
        "label": 0,
        "title": "chatglm: a family of large language models from glm-130b to glm-4 all tools team glm",
        "abstract": "we introduce chatglm, an evolving family of large language models that we have been developing over time. this report primarily focuses on the glm-4 language series, which includes glm-4, glm-4-air, and glm-4-9b. they represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of chatglm. to date, the glm-4 models are pre-trained on ten trillions of tokens mostly in chinese and english, along with a small set of corpus from 24 languages, and aligned primarily for chinese and english usage. the high-quality alignment is achieved via a multi-stage post- training process, which involves supervised fine-tuning and learning from human feedback. evaluations show that glm-4 1) closely rivals or outperforms gpt-4 in terms of general metrics such as mmlu, gsm8k, math, bbh, gpqa, and humaneval, 2) gets close to gpt-4-turbo in instruction following as measured by ifeval, 3) matches gpt-4 turbo (128k) and claude 3 for long context tasks, and 4) outperforms gpt-4 in chinese alignments as measured by alignbench. the glm- 4 all tools model is further aligned to understand user intent and autonomously decide when and which tool(s) to use\u2014including web browser, python interpreter, text-to-image model, and user-defined functions\u2014to effectively complete complex tasks. in practical applications, it matches and even surpasses gpt-4 all tools in tasks like accessing online information via web browsing and solving math problems using python interpreter. over the course, we have open-sourced a series of models, including chatglm-6b (three generations), glm-4-9b (128k, 1m), glm-4v-9b, webglm, and codegeex, attracting over 10 million downloads on hugging face in the year 2023 alone. the open models can be accessed through https://github.com/thudm andhttps://huggingface.co/thudm . *team glm: aohan zeng, bin xu, bowen wang, chenhui zhang, da yin, diego rojas, guanyu feng, hanlin zhao, hanyu lai, hao yu, hongning wang, jiadai sun, jiajie zhang, jiale cheng, jiayi gui, jie tang, jing zhang, juanzi li, lei zhao, lindong wu, lucen zhong, mingdao liu, minlie huang, peng zhang, qinkai zheng, rui lu, shuaiqi duan, shudan zhang, shulin cao, shuxun yang, weng lam tam, wenyi zhao, xiao liu, xiao xia, xiaohan zhang, xiaotao gu, xin lv, xinghan liu, xinyi liu, xinyue yang, xixuan song, xunkai zhang, yifan an, yifan xu, yilin niu, yuantao yang, yueyan li, yushi bai, yuxiao dong, zehan qi, zhaoyu wang, zhen yang, zhengxiao du, zhenyu hou, zihan wang. \u2020team members are listed alphabetically by first name. preprint. under review.arxiv:2406.12793v1  [cs.cl]  18 jun 2024glmmar. 2021glm-10bjun. 2021glm-130bcodegeex-13baug. 2022glm-proembeddingcharacterglmjun. 2023chatglm2-6bchatglm2-6b-32kcodegeex2-6bglm-4(0116)glm-4vcogview3jan. 2024 chatglm-130bmar. 2023chatglm-6bvisualglm-6bglm-3-turbooct. 2023chatglm3-6bchatglm3-6b-32kcogvlm-17b apisopen llmsopenvlmsglm-4(0520)glm-4-air(0605)jun. 2024glm-4-9bglm-4-9b-chatglm-4-9b-chat-1mglm-4v-9bcogvlm2-19bglm-4alltools webglmcodegeexcode interpreter agentmodelsautowebglm cogviewapr. 2022cogview2cogvideo(dec.)cogagent (may) (may)(may)(128k) (32k)(128k) (jul.)oct. 2022glm-130b(32k)(aug.)glm-10bmglm-1bfigure 1: the timeline of the glm family of language, code, vision, and agent models. the focus of this report is primarily on the language models, i.e., chatglm. the apis are publicly available at https://bigmodel.cn and open models can be accessed through https://github.com/thudm . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12822",
        "label": 0,
        "title": "is it good data for multilingual instruction tuning or just bad multilingual evaluation for large language mod",
        "abstract": "large language models, particularly multilin- gual ones, are designed, claimed, and expected to cater to native speakers of varied languages. we hypothesise that the current practices of fine-tuning and evaluating these models may mismatch this intention owing to a heavy re- liance on translation, which can introduce trans- lation artefacts and defects. it remains un- known whether the nature of the instruction data has an impact on the model output; on the other hand, it remains questionable whether translated test sets can capture such nuances. due to the often coupled practices of using translated data in both stages, such imperfec- tions could have been overlooked. this work investigates these issues by using controlled na- tive or translated data during instruction tuning and evaluation stages and observing model re- sults. experiments on eight base models and eight different benchmarks reveal that native or generation benchmarks display a notable dif- ference between native and translated instruc- tion data especially when model performance is high, whereas other types of test sets cannot. finally, we demonstrate that regularization is beneficial to bridging this gap on structured but not generative tasks.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12845",
        "label": 0,
        "title": "interpretable preferences via multi-objective reward modeling and mixture-of-experts haoxiang wang",
        "abstract": "reinforcement learning from human feedback (rlhf) has emerged as the primary method foraligninglargelanguagemodels(llms)withhumanpreferences. therlhfprocesstypically starts by training a reward model (rm) using human preference data. conventional rms are trained on pairwise responses to the same user request, with relative ratings indicating which response humans prefer. the trained rm serves as a proxy for human preferences. however, due to the black-box nature of rms, their outputs lack interpretability, as humans cannot intuitively understand why an rm thinks a response is good or not. as rms act as human preference proxies, it is desirable for them to be human-interpretable to ensure that their internal decision processes are consistent with human preferences and to prevent reward hacking in llm alignment. to build rms with interpretable preferences, we propose a two- stage approach: i) train an absolute-rating multi-objective reward model (armorm) with multi-dimensional absolute-rating data, each dimension corresponding to a human-interpretable objective (e.g., honesty, verbosity, safety); ii) employ a mixture-of-experts (moe) strategy with a gating network that automatically selects the most suitable reward objectives based on the context. we efficiently trained an armorm with llama-3 8b and a gating network consisting of a shallow mlp on top of the armorm. our trained model, armorm-llama3-8b , obtains state- of-the-art performance on rewardbench, a benchmark evaluating rms for language modeling. notably, the performance of our model surpasses the llm-as-a-judge method with gpt-4 judges by a margin, and approaches the performance of the much larger nemotron-4 340b reward model. our code and model are released at https://github.com/rlhflow/rlhf-rewar d-modeling . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12043",
        "label": 0,
        "title": "grade score: quantifying llm performance in option selection dmitri iourovitski",
        "abstract": "this study introduces the \u201dgrade score\u201d, a novel metric designed to evaluate the consistency and fairness of large language models (llms) when used as multiple-choice judges with respect to order bias and choice consistency. the grade score combines entropy, which measures order bias, and mode frequency, which assesses choice sta- bility, offering insights into llms\u2019 reliability and impartiality. the study explores techniques such as prompt engineering and option sam- pling strategies to optimize the grade score, demonstrating their effec- tiveness in enhancing llms\u2019 performance. results showcase varying performances among llms with respect to prompts and highlight the positive impact of including irrelevant options. the study also identi- fies an emergent behavior in instruction-following models, where they adapt to instructions targeting specific biases, demonstrating their adaptability. the grade score facilitates comparisons between llms and encourages ongoing research towards optimizing their decision- making processes, with potential implications for improving their re- liability and fairness in various applications. all code is available on github1 \u2217email: dmitri.io@utexas.edu 1https://github.com/iodmitri/gradelab 1arxiv:2406.12043v2  [cs.ai]  20 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12072",
        "label": 1,
        "title": "dtgb: a comprehensive benchmark for dynamic text-attributed graphs jiasheng zhang1",
        "abstract": "dynamic text-attributed graphs (dytags) are prevalent in various real-world scenarios, where each node and edge are associated with text descriptions, and both the graph structure and text descriptions evolve over time. despite their broad applicability, there is a notable scarcity of benchmark datasets tailored to dytags, which hinders the potential advancement in many research fields. to address this gap, we introduce dynamic text-attributed graph benchmark ( dtgb ), a collection of large-scale, time-evolving graphs from diverse domains, with nodes and edges enriched by dynamically changing text attributes and categories. to facilitate the use of dtgb, we design standardized evaluation procedures based on four real-world use cases: future link prediction, destination node retrieval, edge classification, and textual relation generation. these tasks require models to understand both dynamic graph structures and natural language, highlighting the unique challenges posed by dytags. moreover, we conduct extensive benchmark experiments on dtgb, evaluating 7 popular dynamic graph learning algorithms and their variants of adapting to text attributes with llm embeddings, along with 6 powerful large language models (llms). our results show the limitations of existing models in handling dytags. our analysis also demonstrates the utility of dtgb in investigating the incorporation of structural and textual dynamics. the proposed dtgb fosters research on dytags and their broad applications. it offers a comprehensive benchmark for evaluating and advancing models to handle the interplay between dynamic graph structures and natural language. the dataset and source code are available at https://github.com/zjs123/dtgb . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12843",
        "label": 0,
        "title": "can go ais be adversarially robust? tom tseng far ai",
        "abstract": "prior work found that superhuman go ais like katago can be defeated by simple adversarial strategies. in this paper, we study if simple defenses can improve katago\u2019s worst-case performance. we test three natural defenses: adversarial training on hand-constructed positions, iterated adversarial training, and changing the network architecture. we find that some of these defenses are able to protect against previously discovered attacks. unfortunately, we also find that none of these defenses are able to withstand adaptive attacks. in particular, we are able to train new adversaries that reliably defeat our defended agents by causing them to blunder in ways humans would not. our results suggest that building robust ai systems is challenging even in narrow domains such as go. for interactive examples of attacks and a link to our codebase, see https://goattack.far.ai/ . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09401",
        "label": 1,
        "title": "mmscan: a multi-modal 3d scene dataset with hierarchical grounded language annotations ruiyuan lyu1",
        "abstract": "with the emergence of llms and their integration with other data modalities, multi-modal 3d perception attracts more attention due to its connectivity to the physical world and makes rapid progress. however, limited by existing datasets, previous works mainly focus on understanding object properties or inter-object spatial relationships in a 3d scene. to tackle this problem, this paper builds the first largest ever multi-modal 3d scene dataset and benchmark with hierarchical grounded language annotations, mmscan. it is constructed based on a top-down logic, from region to object level, from a single target to inter-target relation- ships, covering holistic aspects of spatial and attribute understanding. the overall pipeline incorporates powerful vlms via carefully designed prompts to initialize the annotations efficiently and further involve humans\u2019 correction in the loop to ensure the annotations are natural, correct, and comprehensive. built upon exist- ing 3d scanning data, the resulting multi-modal 3d dataset encompasses 1.4m meta-annotated captions on 109k objects and 7.7k regions as well as over 3.04m diverse samples for 3d visual grounding and question-answering benchmarks. we evaluate representative baselines on our benchmarks, analyze their capabilities in different aspects, and showcase the key problems to be addressed in the future. furthermore, we use this high-quality dataset to train state-of-the-art 3d visual grounding and llms and obtain remarkable performance improvement both on existing benchmarks and in-the-wild evaluation. codes, datasets, and benchmarks will be available at https://github.com/openrobotlab/embodiedscan . preprint. under review.arxiv:2406.09401v1  [cs.cv]  13 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09406",
        "label": 1,
        "title": "4m-21: an any-to-any vision model for tens of tasks and modalities roman bachmann1\u2020",
        "abstract": "current multimodal and multitask foundation models, like 4m [ 62] or uni- fiedio [ 59,58], show promising results. however, their out-of-the-box abilities to accept diverse inputs and perform diverse tasks are limited by the (usually small) number of modalities and tasks they are trained on. in this paper, we develop a single any-to-any model trained on tens of highly diverse modalities and by performing co-training on large-scale multimodal datasets and text corpora. this includes training on images and text along with several semantic and geometric modalities, feature maps from recent state of the art models like dinov2 and imagebind, pseudo labels of specialist models like sam and 4dhumans, and a range of new modalities that allow for novel ways to interact with the model and steer the generation, for example, image metadata or color palettes. a crucial step in this process is performing discrete tokenization on various modalities, whether they are image-like, neural network feature maps, vectors, structured data like instance segmentation or human poses, or data that can be represented as text. through this, we show the possibility of training one model to solve at least 3x more tasks/modalities than existing models and doing so without a loss in performance . in addition, this enables more fine-grained and controllable multimodal generation capabilities and allows studying the distillation of models trained on diverse data and objectives into one unified model. we scale the training to a three billion parameter and different datasets. the multimodal models and training code are open sourced at https://4m.epfl.ch . *equal contribution & corresponding authors. randomized order. \u2020work partially done while at epfl and apple. preprint.arxiv:2406.09406v2  [cs.cv]  14 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09410",
        "label": 1,
        "title": "1 scene graph generation in large-size vhr satellite imagery: a large-scale dataset and a",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09411",
        "label": 0,
        "title": "muirbench : a comprehensive benchmark for robust multi-image understanding fei wang1",
        "abstract": "we introduce muirbench , a comprehensive benchmark that focuses on robust multi-image understanding capabilities of multimodal llms. muirbench con- sists of 12 diverse multi-image tasks ( e.g., scene understanding, ordering) that involve 10 categories of multi-image relations ( e.g., multiview, temporal relations). comprising 11,264 images and 2,600 multiple-choice questions, muirbench is created in a pairwise manner, where each standard instance is paired with an unanswerable variant that has minimal semantic differences, in order for a reliable assessment. evaluated upon 20 recent multi-modal llms, our results reveal that even the best-performing models like gpt-4o and gemini pro find it challenging to solve muirbench , achieving 68.0% and 49.3% in accuracy. open-source multimodal llms trained on single images can hardly generalize to multi-image questions, hovering below 33.3% in accuracy. these results highlight the im- portance of muirbench in encouraging the community to develop multimodal llms that can look beyond a single image, suggesting potential pathways for future improvements. \u2217equal leadership. correspondance to <fwang598@usc.edu; xingyuf2@seas.upenn.edu>. \u2020equal contribution; alphabetic order. project page: https://huggingface.co/datasets/muirbench/muirbench preprint. under review.arxiv:2406.09411v1  [cs.cv]  13 jun 2024figure 2: compared with previous benchmarks, muirbench has several novel features: (1) it evaluates on a comprehensive range of 12 multi-image understanding abilities, e.g. geographic understanding and diagram understanding as introduced in \u00a73, while prior benchmarks generally contain single-image questions. (2) it contains 10 diverse multi-image relations, e.g. narrative and complementary as discussed in \u00a73. (3) it provides a robust evaluation on models by unanswerable instance variants. the samples of previous benchmarks are from [25, 37, 53]. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09979",
        "label": 1,
        "title": "hiro: hierarchical information retrieval optimization krish goel",
        "abstract": "large language models (llms) excel in natural language tasks but face limitations due to static training datasets, result- ing in outdated or contextually shallow responses. retrieval- augmented generation (rag) addresses this by integrating real-time external knowledge, enhancing model accuracy and credibility, especially for knowledge-intensive tasks. how- ever, rag-enhanced llms struggle with long contexts, caus- ing them to \u201dchoke\u201d on information overload, compromising response quality. recent rag applications use hierarchical data structures for storing documents, organized at various levels of summarization and information density. in this con- text, we introduce hiro (hierarchical information retrieval optimization), a novel querying approach for rag appli- cations using hierarchical structures for storing documents. hiro employs dfs-based recursive similarity score calcula- tion and branch pruning to minimize the context returned to the llm without informational loss. hiro outperforms ex- isting querying mechanisms on the narrativeqa dataset by an absolute performance gain of 10.85%. code \u2014 https://github.com/krishgoel/hir"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09464",
        "label": 1,
        "title": "gpt-ology",
        "abstract": "large language models have taken the cognitive sci- ence world by storm. it is perhaps timely now to take stock of the various research paradigms that have been used to make scientific inferences about \u201ccognition\u201d in these models or about human cognition. we review sev- eral emerging research paradigms\u2014gpt-ology, llms- as-computational-models, and \u201csilicon sampling\u201d\u2014 and review recent papers that have used llms under these paradigms. in doing so, we discuss their claims as well as challenges to scientific inference under these vari- ous paradigms. we highlight several outstanding is- sues about llms that have to be addressed to push our science forward: closed-source vs open-sourced mod- els; (the lack of visibility of) training data; and repro- ducibility in llm research, including forming conven- tions on new task \u201chyperparameters\u201d like instructions and prompts. keywords: large language models; cognitive scienc"
    },
    {
        "url": "https://arxiv.org/pdf/2406.17038",
        "label": 0,
        "title": "mode ling: a novel dataset for testing linguistic reasoning in language models nathan a. chi1",
        "abstract": "we introduce mode ling1, a novel benchmark of linguistics olympiad-style puzzles which tests few-shot reasoning in ai systems. solving these puzzles necessitates inferring aspects of a language\u2019s grammatical structure from a small number of examples. such puzzles provide a natural testbed for language models, as they require compositional generalization and few- shot inductive reasoning. consisting solely of new puzzles written specifically for this work, mode ling has no risk of appearing in the training data of existing ai systems: this ame- liorates the risk of data leakage, a potential con- founder for many prior evaluations of reason- ing. evaluating several large open source lan- guage models and gpt on our benchmark, we observe non-negligible accuracy, demonstrat- ing few-shot emergent reasoning ability which cannot merely be attributed to shallow mem- orization. however, imperfect model perfor- mance suggests that mode ling can be used to measure further progress in linguistic reason- ing. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09520",
        "label": 0,
        "title": "education",
        "abstract": ": the use of generative artificial intelligence (genai) in academia is a subjective and hotly  debated topic. currently, there are no agreed guidelines towards the usage of genai systems in  higher education (he) and, thus, it is still unclear how to make effective use of the technology for  teaching and learning practice. this paper provides an overview of the current state of research on  genai for teaching and learning in he. to this end, this study conducted a systematic review of  relevant studies indexed by scopus, using the preferred reporting items for systematic reviews and  meta-analyses (prisma) guidelines. the search criteria revealed a total of 625 research papers, of  which 355 met the final inclusion criteria. the findings from the review showed the current state  and the future trends in documents, citations, document sources/authors, keywords, and co- authorship. the research gaps identified suggest that while some authors have looked at  understanding the detection of ai-generated text, it may be beneficial to understand how genai can  be incorporated into supp orting the educational curriculum for assessments, teaching, and learning  delivery. furthermore, there is a need for additional interdisciplinary, multidimensional studies in he  through collaboration. this will strengthen the awareness and understanding of students, tutors,  and other stakeholders, which will be instrumental in formulating guidelines, frameworks, and  policies for genai usage.      citation: ogunleye, b.; zakariyyah,  k.i.; ajao, o.; olayinka, o.; sharma,  h. a systematic review of generative  ai for teaching and learning practice.  educ. sci. 2024 , 14, 636. https://  doi.org/10.3390 /educsci14060636     academic editor: bracha kramarski    received: 28 march 2024   revised: 6 june 2024   accepted: 11 june 2024   published: 13 june 2024         copyright: \u00a9 2024  by the authors.  licensee mdpi, basel, switzerland.  this article is an open access article  distributed under the terms and  conditions of the creative comm ons  attribution (cc by) license (https://  creativecomm ons.org/licenses/by/  4.0/). keywords: artificial intelligence; generative ai; higher education; prisma; systematic literature  review; teaching and learning; topic modelling        1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09988",
        "label": 1,
        "title": "details make a difference: object state-sensitive neurorobotic task planning xiaowen sun",
        "abstract": ". the state of an object reflects its current status or condition and is important for a robot\u2019s task planning and manipulation. how- ever, detecting an object\u2019s state and generating a state-sensitive plan for robots is challenging. recently, pre-trained large language models (llms) and vision-language models (vlms) have shown impressive capabilities in generating plans. however, to the best of our knowledge, thereishardlyanyinvestigationonwhetherllmsorvlmscanalsogen- erate object state-sensitive plans. to study this, we introduce an object state-sensitive agent (ossa), a task-planning agent empowered by pre- trained neural networks. we propose two methods for ossa: (i) a modu- larmodelconsistingofapre-trainedvisionprocessingmodule(densecap- tioning model, dcm) and a natural language processing model (llm), and (ii) a monolithic model consisting only of a vlm. to quantitatively evaluate the performances of the two methods, we use tabletop scenarios where the task is to clear the table. we contribute a multimodal bench- mark dataset that takes object states into consideration. our results show that both methods can be used for object state-sensitive tasks, but the monolithic approach outperforms the modular approach. the code for ossa is available at https://github.com/xiao-wen-sun/ossa keywords: objectstateidentification \u00b7artificialintelligence \u00b7robotics \u00b7language models \u00b7multimodality "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10144",
        "label": 0,
        "title": "discovering the unknown: improving rule mining via embedding-based link prediction n\u2019dah jean kouagoua",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10228",
        "label": 1,
        "title": "vega  : learning interleaved image-text comprehension in vision-language large models",
        "abstract": "the swift progress of multi-modal large models (mllms) has showcased their impressive ability to tackle tasks blending vision and language. yet, most current models and benchmarks cater to scenarios with a narrow scope of visual and textual contexts. these models often fall short when faced with complex comprehension tasks, which in- volve navigating through a plethora of irrelevant and poten- tially misleading information in both text and image forms. to bridge this gap, we introduce a new, more demanding task known as interleaved image-text comprehension (iitc). this task challenges models to discern and disregard super- fluous elements in both images and text to accurately answer questions and to follow intricate instructions to pinpoint the relevant image. in support of this task, we further craft a new vega dataset, tailored for the iitc task on scien- tific content, and devised a subtask, image-text association (ita), to refine image-text correlation skills. our evaluation of four leading closed-source models, as well as various open-source models using vega, underscores the rigorous nature of iitc. even the most advanced models, such as gemini-1.5-pro and gpt4v , only achieved modest success. by employing a multi-task, multi-scale post-training strat- egy, we have set a robust baseline for mllms on the iitc task, attaining an 85.8%accuracy rate in image associa- tion and a 0.508rouge score. these results validate the effectiveness of our dataset in improving mllms capabili- ties for nuanced image-text comprehension. project page: https://zhourax.github.io/vega/ *equal contribution \u2020corresponding author \u2660project leader1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10221",
        "label": 0,
        "title": "short film dataset (sfd): a benchmark for story-level video understanding ridouane ghermi",
        "abstract": "recent advances in vision-language models have significantly propelled video understanding. existing datasets and tasks, however, have notable limitations. most datasets are confined to short videos with limited events and narrow narratives. for example, datasets with instructional and egocentric videos often document the activities of one person in a single scene. although some movie datasets offer richer content, they are often limited to short-term tasks, lack publicly available videos and frequently encounter data leakage given the use of movie forums and other resources in llm training. to address the above limitations, we propose the short film dataset (sfd) with 1,078 publicly available amateur movies, a wide variety of genres and minimal data leakage issues. sfd offers long-term story-oriented video tasks in the form of multiple-choice and open-ended question preprint. under review.arxiv:2406.10221v1  [cs.cv]  14 jun 20240 200 400 600 800 average video length (seconds)0100200300400total hoursactivitynet-qahow2qa egoschemasfdmovieqa moviechat lvucinepile tvqanext-qa ivqamovies (accessible) movies (restricted) egocentric instructional generalfigure 2: comparison of sfd to other vqa datasets. the circle size indicates the number of qa pairs in each dataset. 19.728.922.126.333.528.93624.131.518.344.134.556.755.455.464.451.971.3 28.869.964.17570.268.57164.176 15253545556575 gemma2b(42.3)mistral7b(62.5)llama 38b(68.4)gpt-3.5(70)mixtral8x7b(70.6)claude 3haiku(75.2)claude 3sonnet(79)llama 370b(82)gpt-4(86.4)% accuracyzero-shot llm accuracysfdmovieqalvuyto: youtube-objects voc: pascal voc 2007 6model and (mmlu)figure 3: data leakage. when given only the movie title, higher zero-shot accuracy in question- answering by llms indicates greater data leakage. llms are ranked by mmlu. answering. our extensive experiments emphasize the need for long-term reasoning to solve sfd tasks. notably, we find strong signals in movie transcripts leading to the on-par performance of people and llms. we also show significantly lower performance of current models compared to people when using vision data alone. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10210",
        "label": 1,
        "title": "make it count: text-to-image generation with an accurate number of objects lital binyamin1yoad tewel2",
        "abstract": "despite the unprecedented success of text-to-image diffusion models, controlling the number of depicted objects using text is surprisingly hard. this is important for various applications from technical documents, to children\u2019s books to illustrating cooking recipes. generating object-correct counts is fundamentally challenging because the generative model needs to keep a sense of separate identity for every instance of the object, even if several objects look identical or overlap, and then carry out a global computation implicitly during generation. it is still unknown if such representations exist. to address count-correct generation, we first identify features within the diffusion model that can carry the object identity information. we then use them to separate and count instances of objects during the denoising process and detect over-generation and under-generation. we fix the latter by training a model that predicts both the shape and location of a missing object, based on the layout of existing ones, and show how it can be used to guide denoising with correct object count. our approach, countgen , does not depend on external source to determine object layout, but rather uses the prior from the diffusion model itself, creating prompt-dependent and seed-dependent layouts. evaluated on two benchmark datasets, we find that countgen strongly outperforms the count- accuracy of existing baselines. countgen  (ours) \u201ca photo of six  kittens  sitting on a  branch\u201d \u201ca photo of \ufb01ve  eggs  in a carton\u201d \u201ca realistic photo of  goldilocks and three   bears eating a porridge\u201d \u201can illustration of  four  ninja turtles \u201d sdxl  \u201ca realistic photo of  seven  dwarves  dancing  in the forest\u201d  figure 1: countgen generates the correct number of objects specified in the input prompt while maintaining a natural layout that aligns with the prompt. preprint. under review.arxiv:2406.10210v1  [cs.cv]  14 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10163",
        "label": 1,
        "title": "meshanything: artist-created mesh generation with autoregressive transformers yiwen chen1",
        "abstract": "recently, 3d assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement. however, this potential is largely unrealized because these assets always need to be converted to meshes for 3d industry applications, and the meshes produced by current mesh extraction methods are significantly inferior to artist-created meshes (ams), i.e., meshes created by human artists. specifically, current mesh extraction methods rely on dense faces and ignore geometric features, leading to inefficiencies, complicated post-processing, and lower representation quality. to address these issues, we introduce meshanything, a model that treats mesh extraction as a generation problem, producing ams aligned with specified shapes. by converting 3d assets in any 3d representation into ams, meshanything can be integrated with various 3d asset production methods, thereby enhancing their application across the 3d industry. the architecture of meshanything comprises a vq-v ae and a shape-conditioned decoder-only transformer. we first learn a mesh vocabulary using the vq-v ae, then train the shape-conditioned decoder- only transformer on this vocabulary for shape-conditioned autoregressive mesh generation. our extensive experiments show that our method generates ams with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10157",
        "label": 0,
        "title": "robogolf: mastering real-world minigolf with a reflective multi-modality vision-language model hantao zhou1",
        "abstract": ": minigolf, a game with countless court layouts, and complex ball mo- tion, constitutes a compelling real-world testbed for the study of embodied intel- ligence. as it not only challenges spatial and kinodynamic reasoning but also re- quires reflective and corrective capacities to address erroneously designed courses. we introduce robogolf , a vlm-based framework that perceives dual-camera vi- sual inputs with nested vlm-empowered closed-loop control and reflective equi- librium loop. extensive experiments demonstrate the effectiveness of robogolf on challenging minigolf courts including those that are impossible to finish. ex- periment videos are available at https://jity16.github.io/robogolf/ . keywords: reflective equilibrium, closed-loop control, real-world minigolf, vi- sion language model "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10100",
        "label": 0,
        "title": "skysensegpt: a fine-grained instruction tuning dataset and model for remote sensing vision-language understand",
        "abstract": "remote sensing large multi-modal models (rslmms) are developing rapidly and showcase significant capabilities in remote sensing imagery (rsi) comprehension. however, due to the limitations of existing datasets, rslmms have shortcomings in understanding the rich semantic relations among objects in complex remote sensing scenes. to unlock rslmms\u2019 complex comprehension ability, we propose a large-scale instruction tuning dataset fit-rs, containing 1,800,851 instruction samples. fit-rs covers common interpretation tasks and innovatively introduces several complex comprehension tasks of escalating difficulty, ranging from re- lation reasoning to image-level scene graph generation. based on fit-rs, we build the fit-rsfg benchmark. furthermore, we establish a new benchmark to evaluate the fine-grained relation comprehension capabilities of lmms, named fit-rsrc. based on combined instruction data, we propose skysensegpt, which achieves outstanding performance on both public datasets and fit-rsfg, sur- passing existing rslmms. we hope the fit-rs dataset can enhance the relation comprehension capability of rslmms and provide a large-scale fine-grained data source for the remote sensing community. the dataset will be available at https://github.com/luo-z13/skysensegpt . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10079",
        "label": 0,
        "title": "localizing events in videos with multimodal queries gengyuan zhang1",
        "abstract": "video understanding is a pivotal task in the digital era, yet the dynamic and multi- event nature of videos makes them labor-intensive and computationally demanding to process. thus, localizing a specific event given a semantic query has gained importance in both user-oriented applications like video search and academic research into video foundation models. a significant limitation in current research is that semantic queries are typically in natural language that depicts the semantics of the target event. this setting overlooks the potential for multimodal semantic queries composed of images and texts. to address this gap, we introduce a new benchmark, icq, for localizing events in videos with multimodal queries, along with a new evaluation dataset icq-highlight. our new benchmark aims to evaluate how well models can localize an event given a multimodal semantic query that consists of a reference image, which depicts the event, and a refinement text to adjust the images\u2019 semantics. to systematically benchmark model performance, we include 4 styles of reference images and 5 types of refinement texts, allowing us to explore model performance across different domains. we propose 3 adaptation methods that tailor existing models to our new setting and evaluate 10 sota models, ranging from specialized to large-scale foundation models. we believe this benchmark is an initial step toward investigating multimodal queries in video event localization1. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09422",
        "label": 1,
        "title": "loopin: a pinfi protocol for decentralized computing yunwei mao1",
        "abstract": "networked computing power is a critical utility in the era of artificial intelligence. this paper presents a novel physical infrastructure finance (pinfi) protocol designed to facilitate the distribu- tion of computing power within networks in a decentralized manner. addressing the core challenges of coordination, pricing, and liquidity in decentralized physical infrastructure networks (depin), the pinfi protocol introduces a distinctive dynamic pricing mechanism. it enables providers to al- locate excess computing resources to a \u201cdissipative\u201d pinfi liquidity pool, distinct from traditional defi liquidity pools, ensuring seamless access for clients at equitable, market-based prices. this approach significantly reduces the costs of accessing computing power, potentially to as low as 1% compared to existing services, while simultaneously enhancing security and dependability. the pinfi protocol is poised to transform the dynamics of supply and demand in computing power networks, setting a new standard for efficiency and accessibility. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09454",
        "label": 1,
        "title": "advancing high resolution vision-language models in biomedicine zekai chen arda pekis kevin brown",
        "abstract": "multi-modal learning has transformed generative ai, particularly in vision- language modeling. advances such as the multi-modal gpt-4v and open-source projects like llav a have enabled robust conversational agents capable of zero-shot task completions. however, extending these technologies in the biomedical field introduces unique challenges. recent initiatives like llav a-med have begun to tailor instruction-tuning to biomedical contexts using extensive datasets like pmc-15m. our research contributes three significant advancements: (i) we intro- duce a new instruct dataset enriched with medical image-text pairs derived from claude3-opus and llama3 70b, (ii) we propose an innovative image encoding strategy that employs hierarchical representations to enhance fine-grained biomed- ical visual comprehension, and (iii) we develop the llama3-med model, which achieves state-of-the-art zero-shot performance on biomedical visual question an- swering benchmarks, improving performance by over 10% on average compared to prior methods. these advancements provide more precise and reliable tools for medical professionals, effectively bridging gaps in current multi-modal conversa- tional assistants and fostering further innovations in medical ai. codes available at https://github.com/standardmodelbio/llama3-med.git . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09688",
        "label": 0,
        "title": "freectrl: constructing control centers with feedforward layers for learning-free controllable text generation ",
        "abstract": "controllable text generation (ctg) seeks to craft texts adhering to specific attributes, tradi- tionally employing learning-based techniques such as training, fine-tuning, or prefix-tuning with attribute-specific datasets. these ap- proaches, while effective, demand extensive computational and data resources. in contrast, some proposed learning-free alternatives cir- cumvent learning but often yield inferior re- sults, exemplifying the fundamental machine learning trade-off between computational ex- pense and model efficacy. to overcome these limitations, we propose freectrl, a learning- free approach that dynamically adjusts the weights of selected feedforward neural network (ffn) vectors to steer the outputs of large lan- guage models (llms). freectrl hinges on the principle that the weights of different ffn vec- tors influence the likelihood of different tokens appearing in the output. by identifying and adaptively adjusting the weights of attribute- related ffn vectors, freectrl can control the output likelihood of attribute keywords in the generated content. extensive experiments on single- and multi-attribute control reveal that the learning-free freectrl outperforms other learning-free and learning-based methods, suc- cessfully resolving the dilemma between learn- ing costs and model performance1. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09760",
        "label": 1,
        "title": "bootstrapping language models with dpo implicit rewards changyu chen",
        "abstract": "human alignment in large language models (llms) is an active area of research. a recent groundbreaking work, direct preference optimization (dpo), has greatly simplified the process from past work in reinforcement learning from human feedback (rlhf) by bypassing the reward learning stage in rlhf. dpo, after training, provides an implicit reward model. in this work, we make a novel observation that this implicit reward model can by itself be used in a bootstrapping fashion to further align the llm. our approach is to use the rewards from a current llm model to construct a preference dataset, which is then used in subsequent dpo rounds. we incorporate refinements that debias the length of the responses and improve the quality of the preference dataset to further improve our approach. our approach, named self-alignment with dpoimplicit rewards (dice), shows great improvements in alignment and achieves superior performance than gemini pro on alpacaeval 2, reaching 27.55% length-controlled win rate against gpt-4 turbo, but with only 8b parameters and no external feedback. our code is available athttps://github.com/sail-sg/dice . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09455",
        "label": 0,
        "title": "pandora: towards general world model with natural language actions and video states jiannan xiang",
        "abstract": "world models simulate future states of the world in response to different actions. they facilitate interactive content creation and provides a foundation for grounded, long-horizon reasoning. current foundation models do not fully meet the capa- bilities of general world models: large language models (llms) are constrained by their reliance on language modality and their limited understanding of the physical world, while video models lack interactive action control over the world simulations. this paper makes a step towards building a general world model by introducing pandora , a hybrid autoregressive-diffusion model that simulates world states by generating videos and allows real-time control with free-text actions. pandora achieves domain generality , video consistency , and controllability through large-scale pretraining and instruction tuning. crucially, pandora bypasses the cost of training-from-scratch by integrating a pretrained llm (7b) and a pretrained video model, requiring only additional lightweight finetuning. we illustrate ex- tensive outputs by pandora across diverse domains (indoor/outdoor, natural/urban, human/robot, 2d/3d, etc.). the results indicate great potential of building stronger general world models with larger-scale training. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09486",
        "label": 1,
        "title": "semopo: learning high-quality model and policy from low-quality offline visual datasets shenghua wan1 2ziyuan ",
        "abstract": "model-based offline reinforcement learning (rl) is a promising approach that leverages existing data effectively in many real-world applications, especially those involving high-dimensional in- puts like images and videos. to alleviate the distribution shift issue in offline rl, existing model-based methods heavily rely on the uncer- tainty of learned dynamics. however, the model uncertainty estimation becomes significantly bi- ased when observations contain complex distrac- tors with non-trivial dynamics. to address this challenge, we propose a new approach - sepa- rated model-based offline policy optimization (semopo) - decomposing latent states into en- dogenous and exogenous parts via conservative sampling and estimating model uncertainty on the endogenous states only. we provide a theoret- ical guarantee of model uncertainty and perfor- mance bound of semopo. to assess the efficacy, we construct the low-quality vision deep data- driven datasets for rl (lqv-d4rl), where the data are collected by non-expert policy and the observations include moving distractors. exper- imental results show that our method substan- tially outperforms all baseline methods, and fur- ther analytical experiments validate the critical designs in our method. the project website is https://sites.google.com/view/semopo. 1school of artificial intelligence, nanjing university, china 2national key laboratory for novel software technology, nanjing university, china3school of mathematical sciences, center for statistical science, peking university, beijing, china4school of cyberspace science and technology, beijing institute of tech- nology, beijing, china. correspondence to: de-chuan zhan <zhandc@nju.edu.cn >. proceedings of the 41stinternational conference on machine learning , vienna, austria. pmlr 235, 2024. copyright 2024 by the author(s).1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09496",
        "label": 0,
        "title": "you are what you eat? feeding foundation models a regionally diverse food dataset of world wide dishes jabez m",
        "abstract": "foundation models are increasingly ubiquitous in our daily lives, used in everyday tasks such as text-image searches, interactions with chatbots, and content gener- ation. as use increases, so does concern over the disparities in performance and fairness of these models for different people in different parts of the world. to assess these growing regional disparities, we present world widedishes , a mixed text and image dataset consisting of 765 dishes, with dish names collected in 131 local languages. world widedishes has been collected purely through human contribution and decentralised means, by creating a website widely distrib- uted through social networks. using the dataset, we demonstrate a novel means of operationalising capability and representational biases in foundation models such as language models and text-to-image generative models. we enrich these studies with a pilot community review to understand, from a first-person perspective, how these models generate images for people in five african countries and the united states. we find that these models generally do not produce quality text and image outputs of dishes specific to different regions. this is true even for the us, which is typically considered to be more well-resourced in training data\u2014though the gener- ation of us dishes does outperform that of the investigated african countries. the models demonstrate a propensity to produce outputs that are inaccurate as well as culturally misrepresentative, flattening, and insensitive. these failures in capability and representational bias have the potential to further reinforce stereotypes and disproportionately contribute to erasure based on region. the dataset and code are available at https://github.com/oxai/world-wide-dishes/ . \u2217joint first author. \u2020work done in affiliation with the oxford artificial intelligence society. \u2021corresponding author: siobhan.hall@nds.ox.ac.uk. preprint. under review.arxiv:2406.09496v1  [cs.cy]  13 jun 2024example dish  dall-e 2  dall-e 3  sd v2.1 baghrir eru nyama choma ofe okazi babotie hot dog algeria cameroon kenya nigeria south africa united states figure 1: theworld widedishes dataset contains 765 unique instances of dishes from around the world. this paper presents image generation analysis of dishes associated with the united states and five countries across africa. the 1strow shows example images of the dishes from each country, and the 2ndthrough 4th rows show images generated by dall-e 2, dall-e 3, and stable diffusion v2.1, respectively. all models tend to mis-characterise the dishes. dall-e 2 often outputs the incorrect dish; dall-e 3 tends to exaggerate both visual and cultural stereotypes and to make images more cartoonish; and stable diffusion often generates incoherent images barely resembling food. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.18346",
        "label": 0,
        "title": "ai a lignment through reinforcement learning from human feedback ? contradictions and limitations adam dahlgre",
        "abstract": "this paper critically evaluates the attempts to align arti\ufb01 cial intelligence (ai) systems, especially large language models (llms), with human values and intenti ons through reinforcement learn- ing from feedback (rlxf) methods, involving either human fe edback (rlhf) or ai feedback (rlaif). speci\ufb01cally, we show the shortcomings of the broad ly pursued alignment goals of honesty, harmlessness, and helpfulness. through a multidisciplina ry sociotechnical critique, we examine both the theoretical underpinnings and practical implemen tations of rlxf techniques, revealing sig- ni\ufb01cant limitations in their approach to capturing the comp lexities of human ethics and contributing to ai safety. we highlight tensions and contradictions inhe rent in the goals of rlxf. in addition, we discuss ethically-relevant issues that tend to be neglecte d in discussions about alignment and rlxf, among which the trade-offs between user-friendliness and d eception, \ufb02exibility and interpretability, and system safety. we conclude by urging researchers and pra ctitioners alike to critically assess the sociotechnical rami\ufb01cations of rlxf, advocating for a more nuanced and re\ufb02ective approach to its application in ai development. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09622",
        "label": 1,
        "title": "dsl-fiqa: assessing facial image quality via dual-set degradation learning and landmark-guided transformer wei",
        "abstract": "generic face image quality assessment (gfiqa) evalu- ates the perceptual quality of facial images, which is crucial in improving image restoration algorithms and selecting high-quality face images for downstream tasks. we present a novel transformer-based method for gfiqa, which is aided by two unique mechanisms. first, a \u201c dual-set degradation representation learning\u201d (dsl) mechanism uses facial images with both synthetic and real degrada- tions to decouple degradation from content, ensuring gen- eralizability to real-world scenarios. this self-supervised method learns degradation features on a global scale, pro- viding a robust alternative to conventional methods that use local patch information in degradation learning. second, our transformer leverages facial landmarks to emphasize visually salient parts of a face image in evaluating its per- ceptual quality. we also introduce a balanced and diverse comprehensive generic face iqa (cgfiqa-40k) dataset of 40k images carefully designed to overcome the biases, in particular the imbalances in skin tone and gender represen- tation, in existing datasets. extensive analysis and evalua- tion demonstrate the robustness of our method, marking a significant improvement over prior methods. 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09675",
        "label": 1,
        "title": "benchmarking spectral graph neural networks: a comprehensive study on effectiveness and efficiency ningyi liao",
        "abstract": "with the recent advancements in graph neural networks (gnns), spectral gnns have received increasing popularity by virtue of their specialty in capturing graph signals in the frequency domain, demonstrating promising capability in specific tasks. however, few systematic studies have been conducted to assess their spectral characteristics. this emerging family of models also varies in terms of design and settings, leading to difficulties in comparing their performance and deciding on the suitable model for specific scenarios, especially for large-scale tasks. in this work, we extensively benchmark spectral gnns with a focus on the frequency perspective. we analyze and categorize over 30 gnns with 27 corresponding filters. then, we implement these spectral models within a unified framework with dedicated graph computations and efficient training schemes. thorough experiments are conducted on the spectral models with inclusive metrics on effectiveness and efficiency, offering practical guidelines on evaluating and selecting spectral gnns with desirable performance. our implementation enables application on larger graphs with comparable performance and less overhead, which is available at: https://github.com/gdmnl/spectral-gnn-benchmark . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09723",
        "label": 0,
        "title": "when will gradient regularization be harmful? yang zhao1hao zhang1xiuyuan hu1 abstract",
        "abstract": "gradient regularization (gr), which aims to pe- nalize the gradient norm atop the loss function, has shown promising results in training modern over-parameterized deep neural networks. how- ever, can we trust this powerful technique? this paper reveals that gr can cause performance de- generation in adaptive optimization scenarios, par- ticularly with learning rate warmup. our empiri- cal and theoretical analyses suggest this is due to gr inducing instability and divergence in gradi- ent statistics of adaptive optimizers at the initial training stage. inspired by the warmup heuristic, we propose three gr warmup strategies, each re- laxing the regularization effect to a certain extent during the warmup course to ensure the accurate and stable accumulation of gradients. with exper- iments on vision transformer family, we confirm the three gr warmup strategies can effectively circumvent these issues, thereby largely improv- ing the model performance. meanwhile, we note that scalable models tend to rely more on the gr warmup, where the performance can be improved by up to 3% on cifar10 compared to baseline gr. code is available at https://github.com/zhaoyang- 0204/gnp. 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09770",
        "label": 0,
        "title": "towards efficient pareto set approximation via mixture of experts based model fusion anke tang",
        "abstract": "solving multi-objective optimization problems for large deep neural networks is a challenging task due to the complexity of the loss landscape and the expensive computational cost of training and evaluating models. efficient pareto front approx- imation of large models enables multi-objective optimization for various tasks such as multi-task learning and trade-off analysis. existing algorithms for learning pareto set, including (1) evolutionary, hypernetworks, and hypervolume-maximization methods, are computationally expensive and have restricted scalability to large models; (2) scalarization algorithms, where a separate model is trained for each objective ray, which is inefficient for learning the entire pareto set and fails to capture the objective trade-offs effectively. inspired by the recent success of model merging, we propose a practical and scalable approach to pareto set learning prob- lem via mixture of experts (moe) based model fusion. by ensembling the weights of specialized single-task models, the moe module can effectively capture the trade-offs between multiple objectives and closely approximate the entire pareto set of large neural networks. once the routers are learned and a preference vec- tor is set, the moe module can be unloaded, thus no additional computational cost is introduced during inference. we conduct extensive experiments on vision and language tasks using large-scale models such as clip-vit and gpt-2. the experimental results demonstrate that our method efficiently approximates the entire pareto front of large models. using only hundreds of trainable parameters of the moe routers, our method even has lower memory usage compared to linear scalarization and algorithms that learn a single pareto optimal solution, and are scalable to both the number of objectives and the size of the model. our method significantly reduces the computational burden of learning the pareto set, for exam- ple, in the two-task case, it can be achieved in just a few minutes. code is available at:https://github.com/tanganke/pareto_set_learning "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09831",
        "label": 0,
        "title": "received xx month",
        "abstract": "federated learning (fl) offers a compelling framework for training large language models (llms) while addressing data privacy and decentralization challenges. this paper surveys recent advance- ments in the federated learning of large language models, with a particular focus on machine unlearning\u2014a crucial aspect for complying with privacy regulations like the right to be forgotten. machine unlearning in the context of federated llms involves systematically and securely removing individual data contributions from the learned model without retraining from scratch. we explore various strategies that enable effective unlearning, such as perturbation techniques, model decomposition, and incremental learning, highlighting their implications for maintaining model performance and data privacy. furthermore, we examine case studies and experimental results from recent literature to assess the effectiveness and efficiency of these approaches in real-world scenarios. our survey reveals a growing interest in developing more robust and scalable federated unlearning methods, suggesting a vital area for future research in the intersection of ai ethics and distributed machine learning technologies. index terms federated learning (fl), large language models (llms), swarm intelligence, efficiency, pre-trained models, privacy and security i"
    },
    {
        "url": "https://arxiv.org/pdf/2406.09838",
        "label": 1,
        "title": "vision-language models meet meteorology: developing models for extreme weather events detection with heatmaps",
        "abstract": "real-time detection and prediction of extreme weather protect human lives and infrastructure. traditional methods rely on numerical threshold setting and manual interpretation of weather heatmaps with geographic information systems (gis), which can be slow and error-prone. our research redefines extreme weather events detection (ewed) by framing it as a visual question answering (vqa) problem, thereby introducing a more precise and automated solution. leveraging vision-language models (vlm) to simultaneously process visual and textual data, we offer an effective aid to enhance the analysis process of weather heatmaps. our initial assessment of general-purpose vlms (e.g., gpt-4-vision) on ewed revealed poor performance, characterized by low accuracy and frequent halluci- nations due to inadequate color differentiation andinsufficient meteorological knowledge . to address these challenges, we introduce climateiqa , the first mete- orological vqa dataset, which includes 8,760 wind gust heatmaps and 254,040 question-answer pairs covering four question types, both generated from the latest climate reanalysis data. we also propose sparse position and outline tracking (spot) , an innovative technique that leverages opencv and k-means clustering to capture and depict color contours in heatmaps, providing climateiqa with more accurate color spatial location information. finally, we present climate-zoo , the first meteorological vlm collection, which adapts vlms to meteorological applications using the climateiqa dataset. experiment results demonstrate that models from climate-zoo substantially outperform state-of-the-art general vlms, achieving an accuracy increase from 0% to over 90% in ewed verification. the datasets and models in this study are publicly available for future climate science research: https://github.com/alexjjjchen/climate-zoo . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09870",
        "label": 1,
        "title": "igl-bench: establishing the comprehensive benchmark for imbalanced graph learning jiawen qin1",
        "abstract": "deep graph learning has gained grand popularity over the past years due to its versa- tility and success in representing graph data across a wide range of domains. how- ever, the pervasive issue of imbalanced graph data distributions, where certain parts exhibit disproportionally abundant data while others remain sparse, undermines the efficacy of conventional graph learning algorithms, leading to biased outcomes. to address this challenge, imbalanced graph learning (igl) has garnered substantial attention, enabling more balanced data distributions and better task performance. despite the proliferation of igl algorithms, the absence of consistent experimental protocols and fair performance comparisons pose a significant barrier to compre- hending advancements in this field. to bridge this gap, we introduce igl-bench , a foundational comprehensive benchmark for imbalanced graph learning, embarking on16diverse graph datasets and 24distinct igl algorithms with uniform data processing and splitting strategies. specifically, igl-bench systematically inves- tigates state-of-the-art igl algorithms in terms of effectiveness ,robustness , and efficiency on node-level and graph-level tasks, with the scope of class-imbalance andtopology-imbalance . extensive experiments demonstrate the potential benefits of igl algorithms on various imbalanced conditions, offering insights and opportu- nities in the igl field. further, we have developed an open-sourced and unified package to facilitate reproducible evaluation and inspire further innovative research, which is available at https://github.com/ringbdstack/igl-bench . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09877",
        "label": 0,
        "title": "federated learning with flexible architectures jong-ik park and carlee joe-wong \u0000[0000\u22120003\u22120785\u22129291] carnegi",
        "abstract": ". traditional federated learning (fl) methods have limited support for clients with varying computational and communication abil- ities, leading to inefficiencies and potential inaccuracies in model train- ing. this limitation hinders the widespread adoption of fl in diverse and resource-constrained environments, such as those with client devices ranging from powerful servers to mobile devices. to address this need, this paper introduces federated learning with flexible architectures (fedfa), an fl training algorithm that allows clients to train models of different widths and depths. each client can select a network architec- ture suitable for its resources, with shallower and thinner networks re- quiring fewer computing resources for training. unlike prior work in this area, fedfa incorporates the layer grafting technique to align clients\u2019 lo- cal architectures with the largest network architecture in the fl system during model aggregation. layer grafting ensures that all client contribu- tions are uniformly integrated into the global model, thereby minimizing the risk of any individual client\u2019s data skewing the model\u2019s parameters disproportionately and introducing security benefits. moreover, fedfa introduces the scalable aggregation method to manage scale variations in weights among different network architectures. experimentally, fedfa outperforms previous width and depth flexible aggregation strategies. specifically, fedfa\u2019s testing accuracy matches (1.00 times) or is up to 1.16 times higher globally for iid settings, 0.98 to 1.13 times locally, and 0.95 times to 1.20 times higher globally for non-iid settings com- pared to earlier strategies. furthermore, fedfa demonstrates increased robustness against performance degradation in backdoor attack scenar- ios compared to earlier strategies. earlier strategies exhibit more drops in testing accuracy under attacks\u2014for iid data by 1.01 to 2.11 times globally, and for non-iid data by 0.89 to 3.31 times locally, and 1.11 to 1.74 times globally, compared to fedfa. keywords: federated learning \u00b7heterogeneous local network archi- tectures \u00b7backdoor attack "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10040",
        "label": 1,
        "title": "fzi-wim at semeval-2024 task 2: self-consistent cot for complex nli in biomedical domain jin liu1",
        "abstract": "this paper describes the inference system of fzi-wim at the semeval-2024 task 2: safe biomedical natural language inference for clinical trials. our system utilizes the chain of thought (cot) paradigm to tackle this com- plex reasoning problem and further improves the cot performance with self-consistency. in- stead of greedy decoding, we sample multiple reasoning chains with the same prompt and make the final verification with majority voting. the self-consistent cot system achieves a base- line f1 score of 0.80 (1st), faithfulness score of 0.90 (3rd), and consistency score of 0.73 (12th). we release the code and data publicly1. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09948",
        "label": 1,
        "title": "ble nd: a benchmark for llms on everyday knowledge in diverse cultures and languages junho myung1",
        "abstract": "large language models (llms) often lack culture-specific knowledge of daily life, especially across diverse regions and non-english languages. existing benchmarks for evaluating llms\u2019 cultural sensitivities are limited to a single language or col- lected from online sources such as wikipedia, which do not reflect the mundane everyday lifestyles of diverse regions. that is, information about the food people eat for their birthday celebrations, spices they typically use, musical instruments youngsters play, or the sports they practice in school is common cultural knowledge but uncommon in easily collected online sources, especially for underrepresented cultures. to address this issue, we introduce ble nd, a hand-crafted benchmark designed to evaluate llms\u2019 everyday knowledge across diverse cultures and lan- guages. ble nd comprises 52.6k question-answer pairs from 16 countries/regions, in 13 different languages, including low-resource ones such as amharic, assamese, azerbaijani, hausa, and sundanese. we construct the benchmark to include two formats of questions: short-answer and multiple-choice. we show that llms perform better for cultures that are highly represented online, with a maximum 57.34% difference in gpt-4, the best-performing model, in the short-answer format. for cultures represented by mid-to-high-resource languages, llms perform better in their local languages, but for cultures represented by low-resource languages, llms perform better in english than the local languages. we make our dataset publicly available at: https://github.com/nlee0212/blend . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10099",
        "label": 0,
        "title": "know the unknown: an uncertainty-sensitive method for llm instruction tuning jiaqi li1",
        "abstract": "large language models (llms) have demon- strated remarkable capabilities across various tasks but still face challenges such as hallu- cinations. one potential reason for halluci- nations is the lack of relevant knowledge or context. thus, a promising solution to miti- gate this issue involves instructing llms to respond with \"i do not know\" when a question falls outside their knowledge domain or the provided context. however, in this work, we observed that llms struggle to admit their lack of knowledge, primarily due to existing instruc- tion datasets designed to encourage specific answers. to improve large language models\u2019 capability to recognize the boundaries of their knowledge, we propose a novel approach called uncertainty-sensitive tuning. this method in- volves two-stage training designed for uncer- tainty recognition and prompt-sensitive activa- tion. in the first stage, we guide the llm to reject unknown questions. in the second stage, we recover the decreased performance in qa tasks by incorporating designed causal instruc- tions. by leveraging this method, we aim to en- hance the model\u2019s ability to identify areas of un- certainty. the experimental results demonstrate that our proposed uncertainty-sensitive tuning method significantly improves the performance of the llama2-chat-7b model. specifically, it achieves a substantial 34.7% improvement in handling questions involving knowledge gaps compared to the original model. moreover, our approach outperforms gpt-4, exhibiting a 9.4% increase in overall performance. we open-source the model and code on github1. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10118",
        "label": 1,
        "title": "seacrowd: a multilingual multimodal data hub and benchmark suite for southeast asian languages holy lovenia",
        "abstract": "southeast asia (sea) is a region rich in lin- guistic diversity and cultural variety, with over 1,300 indigenous languages and a population of 671 million people. however, prevailing ai models suffer from a significant lack of repre- sentation of texts, images, and audio datasets from sea, compromising the quality of ai models for sea languages. evaluating models for sea languages is challenging due to the scarcity of high-quality datasets, compounded by the dominance of english training data, rais- ing concerns about potential cultural misrep- resentation. to address these challenges, we introduce seacrowd, a collaborative initia- tive that consolidates a comprehensive resource hub1that fills the resource gap by providing standardized corpora2in nearly 1,000 sea lan- guages across three modalities. through our seacrowd benchmarks, we assess the qual- ity of ai models on 36 indigenous languages across 13 tasks, offering valuable insights into the current ai landscape in sea. furthermore, we propose strategies to facilitate greater ai ad- vancements, maximizing potential utility and resource equity for the future of ai in sea. 1https://seacrowd.github.io/seacrowd-catalogue/ 2https://github.com/seacrowd/seacrowd-datahub/"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10130",
        "label": 0,
        "title": "published as a conference paper at iclr 2024 thedevil is in the neurons : interpreting and mitigating social b",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.10172",
        "label": 1,
        "title": "datasets for multilingual answer sentence selection matteo gabburo1",
        "abstract": "answer sentence selection (as2) is a criti- cal task for designing effective retrieval-based question answering (qa) systems. most ad- vancements in as2 focus on english due to the scarcity of annotated datasets for other languages. this lack of resources prevents the training of effective as2 models in dif- ferent languages, creating a performance gap between qa systems in english and other lo- cales. in this paper, we introduce new high- quality datasets for as2 in five european lan- guages (french, german, italian, portuguese, and spanish), obtained through supervised au- tomatic machine translation (amt) of ex- isting english as2 datasets such as asnq, wikiqa, and trec-qa using a large lan- guage model (llm). we evaluated our ap- proach and the quality of the translated datasets through multiple experiments with different transformer architectures. the results indicate that our datasets are pivotal in producing robust and powerful multilingual as2 models, signifi- cantly contributing to closing the performance gap between english and other languages. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10173",
        "label": 1,
        "title": "intention qa: a benchmark for evaluating purchase intention comprehension abilities of language models in e-co",
        "abstract": "enhancing language models\u2019 (lms) ability to understand purchase intentions in e-commerce scenarios is crucial for their effective assis- tance in various downstream tasks. how- ever, previous approaches that distill inten- tions from lms often fail to generate mean- ingful and human-centric intentions applica- ble in real-world e-commerce contexts. this raises concerns about the true comprehension and utilization of purchase intentions by lms. in this paper, we present intention qa, a double-task multiple-choice question answer- ing benchmark to evaluate lms\u2019 comprehen- sion of purchase intentions in e-commerce. specifically, lms are tasked to infer inten- tions based on purchased products and uti- lize them to predict additional purchases. in- tention qa consists of 4,360 carefully cu- rated problems across three difficulty levels, constructed using an automated pipeline to ensure scalability on large e-commerce plat- forms. human evaluations demonstrate the high quality and low false-negative rate of our benchmark. extensive experiments across 19 language models show that they still strug- gle with certain scenarios, such as understand- ing products and intentions accurately, jointly reasoning with products and intentions, and more, in which they fall far behind human performances. our code and data are pub- licly available at https://github.com/hkust- knowcomp/intentionqa. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.10203",
        "label": 0,
        "title": "a fundamental trade-off in aligned language models and its relation to sampling adaptors naaman tan",
        "abstract": "the relationship between the quality of a string and its probability p(y)under a language model has been influential in the development of techniques to build good text generation systems. for example, several decoding algorithms have been motivated to manipulate p(y)to produce higher-quality text. in this work, we examine the probability\u2013quality relationship in language models explicitly aligned to human preferences, e.g., through reinforcement learning through human feedback (rlhf). we find that, given a general language model and its aligned version, for corpora sampled from an aligned language model, there exists a trade-off between the average reward and average log-likelihood of the strings under the general language model. we provide a formal treatment of this issue and demonstrate how a choice of sampling adaptor allows for a selection of how much likelihood we exchange for the reward. https://github.com/tanyjnaaman/ probability-quality-paradox "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11873",
        "label": 0,
        "title": "logic-based explainability: past",
        "abstract": ". in recent years, the impact of machine learning (ml) and ar- ti\ufb01cial intelligence (ai) in society has been absolutely re markable. this impact is expected to continue in the foreseeable future. ho wever, the adoption of ai/ml is also a cause of grave concern. the operat ion of the most advances ai/ml models is often beyond the grasp of hu man decision makers. as a result, decisions that impact humans m ay not be understood and may lack rigorous validation. explainable a i (xai) is concerned with providing human decision-makers with under standable explanations for the predictions made by ml models. as a resu lt, xai is a cornerstone of trustworthy ai. despite its strategic impo rtance, most work on xai lacks rigor, and so its use in high-risk or safety- critical domains serves to foster distrust instead of contributing t o build much- needed trust. logic-based xai has recently emerged as a rigo rous alter- native to those other non-rigorous methods of xai. this pape r provides a technical survey of logic-based xai, its origins, the curr ent topics of re- search, and emerging future topics of research. the paper al so highlights the many myths that pervade non-rigorous approaches for xai . keywords: explainable ai \u00b7 symbolic ai \u00b7 formal explainability \u00b7 cer- ti\ufb01cation "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11888",
        "label": 0,
        "title": "neural logic programs and neural nets christian anti \u00b4c christian.antic",
        "abstract": ". neural-symbolic integration aims to combine the connecti onist subsymbolic with the logical symbolic approach to arti\ufb01cial intelligence. in this paper , we \ufb01rst de\ufb01ne the answer set semantics of (boolean) neural nets and then introduce from \ufb01rst principl es a class of neural logic programs and show that nets and programs are equivalent. 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.11897",
        "label": 1,
        "title": "a benchmark for maximum cut: towards standardization of the evaluation of learned heuristics for combinatorial",
        "abstract": "recently, there has been much work on the design of general heuristics for graph- based, combinatorial optimization problems via the incorporation of graph neural networks (gnns) to learn distribution-specific solution structures. however, there is a lack of consistency in the evaluation of these heuristics, in terms of the baselines and instances chosen, which makes it difficult to assess the relative performance of the algorithms. in this paper, we propose an open-source benchmark suite maxcut- bench dedicated to the np-hard maximum cut problem in both its weighted and unweighted variants, based on a careful selection of instances curated from diverse graph datasets. the suite offers a unified interface to various heuristics, both traditional and machine learning-based. next, we use the benchmark in an attempt to systematically corroborate or reproduce the results of several, popular learning- based approaches, including s2v-dqn [ 31], eco-dqn [ 4], among others, in terms of three dimensions: objective value ,generalization , and scalability . our empirical results show that several of the learned heuristics fail to outperform a naive greedy algorithm, and that only one of them consistently outperforms tabu search, a simple, general heuristic based upon local search. furthermore, we find that the performance of eco-dqn remains the same or is improved if the gnn is replaced by a simple linear regression on a subset of the features that are related to tabu search. code, data, and pretrained models are available at: https://github.com/ankurnath/maxcut-bench . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11911",
        "label": 0,
        "title": "a notion of complexity for theory of mind via discrete world models x. angelo huang1",
        "abstract": "theory of mind (tom) can be used to assess the capabilities of large language models (llms) in complex scenarios where social reasoning is required. while the research community has proposed many tom benchmarks, their hard- ness varies greatly, and their complexity is not well defined. this work proposes a framework to measure the complexity of tom tasks. we quantify a problem\u2019s complexity as the number of states necessary to solve it correctly. our complexity measure also accounts for spurious states of a tom problem designed to make it apparently harder. we use our method to as- sess the complexity of five widely adopted tom benchmarks. on top of this framework, we de- sign a prompting technique that augments the information available to a model with a descrip- tion of how the environment changes with the agents\u2019 interactions. we name this technique discrete world models (dwm) and show how it elicits superior performance on tom tasks. https://github.com/flecart/ complexity-tom-dwm "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11915",
        "label": 1,
        "title": "minicodeprops: a minimal benchmark for proving code properties evan lohn",
        "abstract": "neural networks have shown initial promise in automating mathematical theorem proving in proof assistants such as lean. the same proof assistants can be used to verify the correctness of code by pairing code with specifications and proofs that the specifications hold. automating the writing of code, specifications, and proofs could lower the cost of verification, or, ambitiously, enable a machine learning sys- tem to output provably correct code. however, it remains unclear whether current neural theorem provers can automatically verify even relatively simple programs. we present minicodeprops , a benchmark of 177 program specifications in the lean proof assistant, aimed at the subproblem of automatically generating a proof for a provided program and specification. minicodeprops contains specifications about simple, self-contained programs (e.g., lists, natural numbers, binary trees) with varied proof difficulty. despite its simplicity, minicodeprops is challenging for current llm-based provers, which succeed in proving about 25 percent of the specifications. we publicly release minicodeprops as a benchmark for furthering automated theorem proving in the context of formally verified code. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11920",
        "label": 1,
        "title": "job-sdf: a multi-granularity dataset for job skill demand forecasting and benchmarking xi chen1",
        "abstract": [
            null,
            null
        ]
    },
    {
        "url": "https://arxiv.org/pdf/2406.11928",
        "label": 0,
        "title": "flexcare: leveraging cross-task synergy for flexible multimodal healthcare prediction muhao xu",
        "abstract": "multimodal electronic health record (ehr) data can offer a holistic assessment of a patient\u2019s health status, supporting various predic- tive healthcare tasks. recently, several studies have embraced the multitask learning approach in the healthcare domain, exploiting the inherent correlations among clinical tasks to predict multi- ple outcomes simultaneously. however, existing methods necessi- tate samples to possess complete labels for all tasks, which places heavy demands on the data and restricts the flexibility of the model. meanwhile, within a multitask framework with multimodal in- puts, how to comprehensively consider the information disparity among modalities and among tasks still remains a challenging prob- lem. to tackle these issues, a unified healthcare prediction model, also named by flexcare , is proposed to flexibly accommodate in- complete multimodal inputs, promoting the adaption to multiple healthcare tasks. the proposed model breaks the conventional par- adigm of parallel multitask prediction by decomposing it into a series of asynchronous single-task prediction. specifically, a task- agnostic multimodal information extraction module is presented to capture decorrelated representations of diverse intra- and inter- modality patterns. taking full account of the information disparities between different modalities and different tasks, we present a task- guided hierarchical multimodal fusion module that integrates the \u2217corresponding author. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. kdd \u201924, august 25\u201329, 2024, barcelona, spain \u00a92024 copyright held by the owner/author(s). publication rights licensed to acm. acm isbn 979-8-4007-0490-1/24/08 https://doi.org/10.1145/3637528.3671974refined modality-level representations into an individual patient- level representation. experimental results on multiple tasks from mimic-iv/mimic-cxr/mimic-note datasets demonstrate the effectiveness of the proposed method. additionally, further anal- ysis underscores the feasibility and potential of employing such a multitask strategy in the healthcare domain. the source code is available at https://github.com/mhxu1998/flexcare. ccs concepts \u2022applied computing \u2192health informatics ;\u2022information systems\u2192data mining . keywords electronic health record, healthcare prediction, multimodal data, multitask learning acm reference format: muhao xu, zhenfeng zhu, youru li, shuai zheng, yawei zhao, kunlun he, and yao zhao. 2024. flexcare: leveraging cross-task synergy for flexible multimodal healthcare prediction. in proceedings of the 30th acm sigkdd conference on knowledge discovery and data mining (kdd \u201924), august 25\u201329, 2024, barcelona, spain. acm, new york, ny, usa, 11 pages. https: //doi.org/10.1145/3637528.3671974 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11931",
        "label": 0,
        "title": "deepseek-coder-v2: breaking the barrier of closed-source models in code intelligence qihao zhu",
        "abstract": "we present deepseek-coder-v2, an open-source mixture-of-experts (moe) code language model that achieves performance comparable to gpt4-turbo in code-specific tasks. specifically, deepseek-coder-v2 is further pre-trained from an intermediate checkpoint of deepseek-v2 with additional 6 trillion tokens. through this continued pre-training, deepseek-coder-v2 substantially enhances the coding and mathematical reasoning capabilities of deepseek-v2, while maintaining comparable performance in general language tasks. compared to deepseek- coder-33b, deepseek-coder-v2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. additionally, deepseek-coder- v2 expands its support for programming languages from 86 to 338, while extending the context length from 16k to 128k. in standard benchmark evaluations, deepseek-coder-v2 achieves superior performance compared to closed-source models such as gpt4-turbo, claude 3 opus, and gemini 1.5 pro in coding and math benchmarks. humaneval mbpp+ math gsm8k5060708090100accuracy (%)90.2 76.275.794.9 88.2 72.273.493.7 83.5 74.6 67.790.8 84.9 72.0 60.195.0 81.7 69.0 50.493.0 81.1 68.2 aider livecodebench swe-bench01020304050607080 73.7 43.4 12.763.9 45.7 18.357.1 34.1 18.768.4 34.6 11.749.2 28.751.1 31.0 2.7deepseek-coder-v2 gpt-4-turbo-0409 gemini-1.5-pro claude-3-opus llama-3-70b codestral figure 1|the performance of deepseek-coder-v2 on math and code benchmarks. *core contributorsarxiv:2406.11931v1  [cs.se]  17 jun 20241"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12020",
        "label": 0,
        "title": "when box meets graph neural network in tag-aware recommendation fake lin",
        "abstract": "last year has witnessed the re-flourishment of tag-aware recom- mender systems supported by the llm-enriched tags. unfortu- nately, though large efforts have been made, current solutions may fail to describe the diversity and uncertainty inherent in user prefer- ences with only tag-driven profiles. recently, with the development of geometry-based techniques, e.g., box embedding, diversity of user preferences now could be fully modeled as the range within a box in high dimension space. however, defect still exists as these approaches are incapable of capturing high-order neighbor signals, i.e., semantic-rich multi-hop relations within the user-tag-item tripartite graph, which severely limits the effectiveness of user modeling. to deal with this challenge, in this paper, we propose a novel algorithm, called boxgnn, to perform the message aggrega- tion via combination of logical operations, thereby incorporating high-order signals. specifically, we first embed users, items, and tags as hyper-boxes rather than simple points in the representation space, and define two logical operations to facilitate the subsequent process. next, we perform the message aggregation mechanism via the combination of logical operations, to obtain the corresponding high-order box representations. finally, we adopt a volume-based learning objective with gumbel smoothing techniques to refine \u2217corresponding author. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. kdd \u201924, august 25\u201329, 2024, barcelona, spain \u00a92024 copyright held by the owner/author(s). publication rights licensed to acm. acm isbn 979-8-4007-0490-1/24/08 https://doi.org/10.1145/3637528.3671973the representation of boxes. extensive experiments on two pub- licly available datasets and one llm-enhanced e-commerce dataset have validated the superiority of boxgnn compared with various state-of-the-art baselines. the code is released online1. ccs concepts \u2022information systems \u2192recommender systems . keywords tag-aware recommendation, box embedding, graph neural net- works acm reference format: fake lin, ziwei zhao, xi zhu, da zhang, shitian shen, xueying li, tong xu, suojuan zhang, and enhong chen. 2024. when box meets graph neural network in tag-aware recommendation. in proceedings of the 30th acm sigkdd conference on knowledge discovery and data mining (kdd \u201924), august 25\u201329, 2024, barcelona, spain. acm, new york, ny, usa, 11 pages. https://doi.org/10.1145/3637528.3671973 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.11978",
        "label": 0,
        "title": "dialogue action tokens: steering language models in goal-directed dialogue with a multi-turn planner kenneth l",
        "abstract": "we present an approach called dialogue action tokens (dat) that adapts language model agents to plan goal-directed dialogues. the core idea is to treat each utterance as an action, thereby converting dialogues into games where existing approaches such as reinforcement learning can be applied. specifically, we freeze a pretrained language model and train a small planner model that predicts a continuous action vector, used for controlled generation in each round. this design avoids the problem of language degradation under reward optimization. when evaluated on the sotopia platform for social simulations, the dat-steered llama model surpasses gpt-4\u2019s performance. we also apply dat to steer an attacker language model in a novel multi-turn red-teaming setting, revealing a potential new attack surface. code: https://github.com/likenneth/dialogue_action_token . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12465",
        "label": 0,
        "title": "rigl: a unified reciprocal approach for tracing the independent and group learning processes xiaoshan yu",
        "abstract": "in the realm of education, both independent learning and group learning are esteemed as the most classic paradigms. the former allows learners to self-direct their studies, while the latter is typi- cally characterized by teacher-directed scenarios. recent studies in the field of intelligent education have leveraged deep tempo- ral models to trace the learning process, capturing the dynamics of students\u2019 knowledge states, and have achieved remarkable per- formance. however, existing approaches have primarily focused on modeling the independent learning process, with the group learning paradigm receiving less attention. moreover, the recip- rocal effect between the two learning processes, especially their combined potential to foster holistic student development, remains inadequately explored. to this end, in this paper, we propose rigl , a unified reciprocal model to trace knowledge states at both the individual and group levels, drawing from the independent and group learning processes. specifically, we first introduce a time \u2217work was done at career science lab, boss zhipin supervised by chuan qin. \u2020corresponding authors. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. kdd \u201924, august 25\u201329, 2024, barcelona, spain \u00a92024 association for computing machinery. acm isbn 978-x-xxxx-xxxx-x/yy/mm. . . $15.00 https://doi.org/xxxxxxx.xxxxxxxframe-aware reciprocal embedding module to concurrently model both student and group response interactions across various time frames. subsequently, we employ reciprocal enhanced learning modeling to fully exploit the comprehensive and complementary information between the two behaviors. furthermore, we design a relation-guided temporal attentive network, comprised of dynamic graph modeling coupled with a temporal self-attention mechanism. it is used to delve into the dynamic influence of individual and group interactions throughout the learning processes, which is crafted to explore the dynamic intricacies of both individual and group interactions during the learning sequences. conclusively, we introduce a bias-aware contrastive learning module to bolster the stability of the model\u2019s training. extensive experiments on four real-world educational datasets clearly demonstrate the effec- tiveness of the proposed rigl model. our codes are available at https://github.com/labyrinthineleo/rigl. ccs concepts \u2022information systems \u2192data mining ;\u2022applied computing \u2192collaborative learning . keywords intelligent education, knowledge tracing, group learning, reciprocal effect, dynamic graph neural network acm reference format: xiaoshan yu, chuan qin, dazhong shen, shangshang yang, haiping ma, hengshu zhu, and xingyi zhang. 2024. rigl: a unified reciprocal ap- proach for tracing the independent and group learning processes . inarxiv:2406.12465v1  [cs.cy]  18 jun 2024kdd \u201924, august 25\u201329, 2024, barcelona, spain xiaoshan yu et al. figure 1: an illustrative example of the holistic knowledge tracing (hkt) task. the top and bottom halves indicate the individual and group learning processes, respectively, which are organized in time frames, and the radar chart in the middle represents the knowledge proficiency levels of both. proceedings of the 30th acm sigkdd conference on knowledge discovery and data mining(kdd \u201924), august 25\u201329, 2024, barcelona, spain. acm, new york, ny, usa, 12 pages. https://doi.org/xxxxxxx.xxxxxxx "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12080",
        "label": 0,
        "title": "a hierarchical 3d gaussian representation for real-time rendering of very large datasets bernhard kerbl",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12095",
        "label": 0,
        "title": "distillnerf: perceiving 3d scenes from single-glance images by distilling neural fields and foundation model f",
        "abstract": "we propose distillnerf, a self-supervised learning framework addressing the challenge of understanding 3d environments from limited 2d observations in au- tonomous driving. our method is a generalizable feedforward model that predicts a rich neural scene representation from sparse, single-frame multi-view camera inputs, and is trained self-supervised with differentiable rendering to reconstruct rgb, depth, or feature images. our first insight is to exploit per-scene optimized neural radiance fields (nerfs) by generating dense depth and virtual camera targets for training, thereby helping our model to learn 3d geometry from sparse non-overlapping image inputs. second, to learn a semantically rich 3d representa- tion, we propose distilling features from pre-trained 2d foundation models, such as clip or dinov2, thereby enabling various downstream tasks without the need for costly 3d human annotations. to leverage these two insights, we introduce a novel model architecture with a two-stage lift-splat-shoot encoder and a parameterized sparse hierarchical voxel representation. experimental results on the nuscenes dataset demonstrate that distillnerf significantly outperforms existing compara- ble self-supervised methods for scene reconstruction, novel view synthesis, and depth estimation; and it allows for competitive zero-shot 3d semantic occupancy prediction, as well as open-world scene understanding through distilled foundation model features. demos and code will be available at https://distillnerf.github.io/. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12208",
        "label": 0,
        "title": "knowledge fusion by evolving weights of language models guodong du1jing li1",
        "abstract": "fine-tuning pre-trained language models, par- ticularly large language models, demands ex- tensive computing resources and can result in varying performance outcomes across different domains and datasets. this paper examines the approach of integrating multiple models from diverse training scenarios into a unified model. this unified model excels across various data domains and exhibits the ability to generalize well on out-of-domain data. we propose a knowledge fusion method named evolver , in- spired by evolutionary algorithms, which does not need further training or additional training data. specifically, our method involves aggre- gating the weights of different language mod- els into a population and subsequently gener- ating offspring models through mutation and crossover operations. these offspring models are then evaluated against their parents, allow- ing for the preservation of those models that show enhanced performance on development datasets. importantly, our model evolving strat- egy can be seamlessly integrated with existing model merging frameworks, offering a versa- tile tool for model enhancement. experimental results on mainstream language models (i.e., encoder-only, decoder-only, encoder-decoder) reveal that evolver outperforms previous state- of-the-art models by large margins. the code is publicly available at https://github.com/ duguodong7/model-evolution . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12211",
        "label": 0,
        "title": "pcie lam solution for ego4d looking at me challenge kanokphan lertniphonphan lenovo research",
        "abstract": "this report presents our team\u2019s \u2019pcie lam\u2019 solution for the ego4d looking at me challenge at cvpr2024. the main goal of the challenge is to accurately determine if a person in the scene is looking at the camera wearer, based on a video where the faces of social partners have been lo- calized. our proposed solution, internlstm, consists of an internvl image encoder and a bi-lstm network. the in- ternvl extracts spatial features, while the bi-lstm extracts temporal features. however, this task is highly challenging due to the distance between the person in the scene and the camera movement, which results in significant blurring in the face image. to address the complexity of the task, we implemented a gaze smoothing filter to eliminate noise or spikes from the output. our approach achieved the 1stposi- tion in the looking at me challenge with 0.81 map and 0.93 accuracy rate. code is available at https://github. com/kanokphanl/ego4d_lam_internlstm 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12214",
        "label": 1,
        "title": "is your hd map constructor reliable under sensor corruptions? xiaoshuai hao1mengchuan wei1yifan yang1haimei zh",
        "abstract": "driving systems often rely on high-definition (hd) maps for precise environmental information, which is crucial for planning and navigation. while current hd map constructors perform well under ideal conditions, their resilience to real-world challenges, e.g., adverse weather and sensor failures, is not well understood, rais- ing safety concerns. this work introduces mapbench , the first comprehensive benchmark designed to evaluate the robustness of hd map construction methods against various sensor corruptions. our benchmark encompasses a total of 29types of corruptions that occur from cameras and lidar sensors. extensive evaluations across 31hd map constructors reveal significant performance degradation of ex- isting methods under adverse weather conditions and sensor failures, underscoring critical safety concerns. we identify effective strategies for enhancing robustness, including innovative approaches that leverage multi-modal fusion, advanced data augmentation, and architectural techniques. these insights provide a pathway for developing more reliable hd map construction methods, which are essential for the advancement of autonomous driving technology. the benchmark toolkit and affiliated code and model checkpoints have been made publicly accessible. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12219",
        "label": 0,
        "title": "pcie egohandpose solution for egoexo4d hand pose challenge feng chen lenovo research",
        "abstract": "this report presents our team\u2019s \u2019pcie egohandpose\u2019 solution for the egoexo4d hand pose challenge at cvpr2024. the main goal of the challenge is to accurately estimate hand poses, which involve 21 3d joints, using an rgb egocentric video images provided for the task. this task is particularly challenging due to the subtle movements and occlusions. to handle the complexity of the task, we propose the hand pose vision transformer (hp-vit). the hp-vit comprises a vit backbone and transformer head to estimate joint positions in 3d, utilizing mpjpe and rle loss function. our approach achieved the 1stposition in the hand pose challenge with 25.51 mpjpe and 8.49 pa- mpjpe. code is available at https://github.com/ kanokphanl/pcie_egohandpose 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12235",
        "label": 1,
        "title": "holmes-vad : towards unbiased and explainable video anomaly detection via multi-modal llm huaxin zhang1",
        "abstract": "towards open-ended video anomaly detection (v ad), existing methods often exhibit biased detection when faced with challenging or unseen events and lack interpretability. to address these drawbacks, we propose holmes-v ad, a novel framework that leverages precise temporal supervision and rich multimodal in- structions to enable accurate anomaly localization and comprehensive explanations. firstly, towards unbiased and explainable v ad system, we construct the first large- scale multimodal v ad instruction-tuning benchmark, i.e.,vad-instruct50k . this dataset is created using a carefully designed semi-automatic labeling paradigm. efficient single-frame annotations are applied to the collected untrimmed videos, which are then synthesized into high-quality analyses of both abnormal and normal video clips using a robust off-the-shelf video captioner and a large language model (llm). building upon the vad-instruct50k dataset, we develop a customized solution for interpretable video anomaly detection. we train a lightweight temporal sampler to select frames with high anomaly response and fine-tune a multimodal large language model (llm) to generate explanatory content. extensive experimen- tal results validate the generality and interpretability of the proposed holmes-vad , establishing it as a novel interpretable technique for real-world video anomaly analysis. to support the community, our benchmark and model will be publicly available at https://github.com/pipixin321/holmesvad . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12246",
        "label": 0,
        "title": "trol: traversal of layers for large language and vision models byung-kwan lee kaist",
        "abstract": "large language and vision models (llvms) have been driven by the generalization power of large language models (llms) and the advent of visual instruction tuning. along with scaling them up directly, these models enable llvms to showcase powerful vision language (vl) performances by covering di- verse tasks via natural language instructions. however, existing open-source llvms that perform comparably to closed-source llvms such as gpt-4v are often considered too large (e.g., 26b, 34b, and 110b parameters), hav- ing a larger number of layers. these large models demand costly, high-end resources for both training and inference. to address this issue, we present a new efficient llvm fam- ily with 1.8b, 3.8b, and 7b llm model sizes, traversal oflayers (  trol ), which enables the reuse of layers in a token-wise manner. this layer traversing technique simulates the effect of looking back and retracing the answering stream while increasing the number of forward propagation layers without physically adding more layers. we demonstrate that  trol employs a simple layer traversing approach yet efficiently outperforms the open-source llvms with larger model sizes and rivals the performances of the closed-source llvms with substantial sizes. code is available in https://github.com/byungkwanlee/trol. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12256",
        "label": 0,
        "title": "symmetric multi-similarity loss for epic-kitchens-100 multi-instance retrieval challeng e 2024 xiaoqi wang yi ",
        "abstract": "in this report, we present our champion solution for epic-kitchens-100 multi-instance retrieval challenge in cvpr 2024. essentially, this challenge differs from traditional visual-text retrieval tasks by providing a cor - relation matrix that acts as a set of soft labels for video- text clip combinations. however, existing loss functions have not fully exploited this information. motivated by this, we propose a novel loss function, symmetric multi- similarity loss, which offers a more precise learning ob- jective. together with tricks and ensemble learning, the model achieves 63.76% average map and 74.25% average ndcg on the public leaderboard, demonstrating the effec- tiveness of our approach. our code will be released at: https://github.com/xqwang14/sms-loss . 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12321",
        "label": 0,
        "title": "automatic benchmarking of large multimodal models via iterative experiment programming alessandro conti1",
        "abstract": "assessing the capabilities of large multimodal models (lmms) often requires the creation of ad-hoc evaluations. currently, building new benchmarks requires tremendous amounts of manual work for each specific analysis. this makes the evaluation process tedious and costly. in this paper, we present ape x,auto- matic programming of experiments , the first framework for automatic benchmarking of lmms. given a research question expressed in natural language, ape xleverages a large language model (llm) and a library of pre-specified tools to generate a set of experiments for the model at hand, and progressively compile a scientific report. the report drives the testing procedure: based on the current status of the investigation, ape xchooses which experiments to perform and whether the results are sufficient to draw conclusions. finally, the llm refines the report, presenting the results to the user in natural language. thanks to its modularity, our framework is flexible and extensible as new tools become available. empirically, ape xreproduces the findings of existing studies while allowing for arbitrary anal- yses and hypothesis testing. code is available at https://github.com/altndrr/apex. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12384",
        "label": 1,
        "title": "vrsbench: a versatile vision-language benchmark dataset for remote sensing image understanding xiang li jian d",
        "abstract": "we introduce a new benchmark designed to advance the development of general- purpose, large-scale vision-language models for remote sensing images. although several vision-language datasets in remote sensing have been proposed to pursue this goal, existing datasets are typically tailored to single tasks, lack detailed object information, or suffer from inadequate quality control. exploring these improvement opportunities, we present a versatile vision-language bench mark forremote sensing image understanding, termed vrsbench . this benchmark comprises 29,614 images, with 29,614 human-verified detailed captions, 52,472 object references, and 123,221 question-answer pairs. it facilitates the training and evaluation of vision-language models across a broad spectrum of remote sensing image understanding tasks. we further evaluated state-of-the-art models on this benchmark for three vision-language tasks: image captioning, visual grounding, and visual question answering. our work aims to significantly contribute to the development of advanced vision-language models in the field of remote sensing. the data and code can be accessed at https://vrsbench.github.io . the high-resolution  aerial  image  from  googleearth  shows  a waterfront  scene  with residential  areas  and  harbor  facilities . three  distinct  harbors  can be seen,  one located  on the left side and another  on the right  side of the image . between  them,  there  are homes  with different  colored  rooftops,  green  lawns,  and  driveways . a ship is docked  in the central  part of the bottom  edge,  and the water  body  exhibits  gentle   ripples . various  small  vehicles  are scattered  throughout  the residential  area,  parked  near the houses .question : how  many  harbors  are visible? answer : 3 question : what  is the object  located  furthest  to the top? answer : small vehicle question : are the visible  vehicles  near water? answer : noobject referring detailed captioningvisual question answer 1 3 4 572 0 6object  id=1:the small  vehicle  that is the farthest  to the top. object  id=4:the harbor  located  on the left side of the scene  with multiple  docks  extending  into the water . object  id=7:the harbor  situated  on the right  side of the image  with a large  dock  area. figure 1: examples of an image and corresponding annotations in vrsbench dataset. our annotations include object referring, visual question answering, and detailed captions. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12407",
        "label": 0,
        "title": "looc: localizing organs using occupancy networks and body surface depth images pit henrich1and",
        "abstract": ". we introduce a novel method employing occupancy networks for the precise localization of 67 anatomical structures from single depth images captured from the exterior of the human body. this method considers the anatomical diversity across individuals. our contributions include the application of occupancy networks for occluded structure localization, a robust method for estimating anatomical positions from depth images, and the creation of detailed, individualized 3d anatom- ical atlases. this approach promises improvements in medical imaging and automated diagnostic procedures by offering accurate, non-invasive localization of critical anatomical features. keywords: localization of anatomical structures \u00b7patient-individual 3d atlas \u00b7occupancy networks. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12448",
        "label": 1,
        "title": "journal of machine learning for biomedical imaging 2024:012 vol. 2",
        "abstract": "the emergence of clinical data warehouses (cdws), which contain the medical data of millions of patients, has paved the way for vast data sharing for research. the quality of mris gathered in cdws differs greatly from what is observed in research settings and reflects a certain clinical reality. consequently, a significant proportion of these images turns out to be unusable due to their poor quality. given the massive volume of mris contained in cdws, the manual rating of image quality is impossible. thus, it is necessary to develop an automated solution capable of effectively identifying corrupted images in cdws. \u00a92024 . license: cc-by 4.0 https://doi.org/10.59275/j.melba.2024-7fgdarxiv:2406.12448v1  [eess.iv]  18 jun 2024automated mri quality assessment in clinical data warehouses: this study presents an innovative transfer learning method for automated quality con- trol of 3d gradient echo t1-weighted brain mris within a cdw, leveraging artefact sim- ulation. we first intentionally corrupt images from research datasets by inducing poorer contrast, adding noise and introducing motion artefacts. subsequently, three artefact- specific models are pre-trained using these corrupted images to detect distinct types of artefacts. finally, the models are generalised to routine clinical data through a transfer learning technique, utilising 3660 manually annotated images. the overall image quality is inferred from the results of the three models, each designed to detect a specific type of artefact. our method was validated on an independent test set of 385 3d gradient echo t1-weighted mris. our proposed approach achieved excellent results for the detection of bad quality mris, with a balanced accuracy of over 87%, surpassing our previous approach by 3.5 percent points. additionally, we achieved a satisfactory balanced accuracy of 79% for the detection of moderate quality mris, outperforming our previous performance by 5 percent points. our framework provides a valuable tool for exploiting the potential of mris in cdws. keywords: clinical data warehouse, deep learning, transfer learning, quality control, mri 1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12463",
        "label": 0,
        "title": "journal of l atex class files",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12496",
        "label": 0,
        "title": "reparameterizable dual-resolution network for real-time semantic segmentation guoyu yang",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12638",
        "label": 0,
        "title": "efficient and long-tailed generalization for pre-trained vision-language model jiang-xin shi",
        "abstract": "pre-trained vision-language models like clip have shown powerful zero-shot inference ability via image-text matching and prove to be strong few-shot learners in various downstream tasks. however, in real-world scenarios, adapting clip to downstream tasks may encounter the following challenges: 1) data may exhibit long-tailed data distributions and might not have abundant samples for all the classes; 2) there might be emerging tasks with new classes that contain no samples at all. to overcome them, we propose a novel framework to achieve efficient and long-tailed generalization, which can be termed as candle . during the training process, we propose compensating logit-adjusted loss to encourage large mar- gins of prototypes and alleviate imbalance both within the base classes and between the base and new classes. for efficient adap- tation, we treat the clip model as a black box and leverage the extracted features to obtain visual and textual prototypes for predic- tion. to make full use of multi-modal information, we also propose cross-modal attention to enrich the features from both modalities. for effective generalization, we introduce virtual prototypes for new classes to make up for their lack of training images. candle achieves state-of-the-art performance over extensive experiments on 11 diverse datasets while substantially reducing the training time, demonstrating the superiority of our approach. the source code is available at https://github.com/shijxcs/candle. ccs concepts \u2022computing methodologies \u2192supervised learning . \u2217equal contribution. \u2020corresponding author. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. kdd \u201924, august 25\u201329, 2024, barcelona, spain. \u00a92024 copyright held by the owner/author(s). publication rights licensed to acm. acm isbn 979-8-4007-0490-1/24/08 https://doi.org/10.1145/3637528.3671945keywords long-tail learning, vision-language model, new class generalization acm reference format: jiang-xin shi, chi zhang, tong wei, and yu-feng li. 2024. efficient and long-tailed generalization for pre-trained vision-language model. in pro- ceedings of the 30th acm sigkdd conference on knowledge discovery and data mining (kdd \u201924), august 25\u201329, 2024, barcelona, spain. acm, new york, ny, usa, 11 pages. https://doi.org/10.1145/3637528.3671945 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12649",
        "label": 0,
        "title": "probabilistic conceptual explainers: trustworthy conceptual explanations for vision foundation models hengyi w",
        "abstract": "vision transformers (vits) have emerged as a significant area of focus, particularly for their ca- pacity to be jointly trained with large language models and to serve as robust vision foundation models. yet, the development of trustworthy ex- planation methods for vits has lagged, partic- ularly in the context of post-hoc interpretations of vit predictions. existing sub-image selection approaches, such as feature-attribution and con- ceptual models, fall short in this regard. this pa- per proposes five desiderata for explaining vits \u2013 faithfulness, stability, sparsity, multi-level struc- ture, and parsimony \u2013 and demonstrates the in- adequacy of current methods in meeting these criteria comprehensively. we introduce a varia- tional bayesian explanation framework, dubbed probabilistic concept explainers (pace), which models the distributions of patch embeddings to provide trustworthy post-hoc conceptual explana- tions. our qualitative analysis reveals the distri- butions of patch-level concepts, elucidating the effectiveness of vits by modeling the joint distri- bution of patch embeddings and vit\u2019s predictions. moreover, these patch-level explanations bridge the gap between image-level and dataset-level ex- planations, thus completing the multi-level struc- ture of pace. through extensive experiments on both synthetic and real-world datasets, we demon- strate that pace surpasses state-of-the-art meth- ods in terms of the defined desiderata1. *equal contribution1department of computer science, rutgers university, new jersey, usa. correspondence to: hengyi wang <hengyi.wang@rutgers.edu >. proceedings of the 41stinternational conference on machine learning , vienna, austria. pmlr 235, 2024. copyright 2024 by the author(s). 1code will soon be available at https://github.com/wang-ml- lab/interpretable-foundation-models1"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12671",
        "label": 1,
        "title": "geobench: benchmarking and analyzing monocular geometry estimation models yongtao ge1",
        "abstract": "recent advances in discriminative and generative pretraining have yielded geometry estimation models with strong generalization capabilities. while discriminative monocular geometry estimation methods rely on large-scale fine-tuning data to achieve zero-shot generalization, several generative-based paradigms show the potential of achieving impressive generalization performance on unseen scenes by leveraging pre-trained diffusion models and fine-tuning on even a small scale of synthetic training data. frustratingly, these models are trained with different recipes on different datasets, making it hard to find out the critical factors that determine the evaluation performance. besides, the current widely used geometry evaluation benchmarks have two main drawbacks that may prevent the development of the field, i.e.,limited scene diversity andunfavorable label quality . to resolve the above issues, (1) we build fair and strong baselines in a unified codebase for evaluating and analyzing the state-of-the-art (sota) geometry estimation models in terms of both different finetuning paradigms and training recipes; (2) we evaluate monocular geometry estimators on more challenging benchmarks for geometry estimation task with diverse scenes and high-quality annotations. our results reveal that pre-trained using large data, discriminative models such as dinov2, can outperform generative counterparts with a small amount of high-quality synthetic training data under the same training configuration, which suggests that fine-tuning data quality is a more important factor than the data scale and model architecture. our observation also raises a question: if simply fine-tuning a general vision model such as dinov2 using a small amount of synthetic depth data produces sota results, do we really need complex models, e.g., marigold [ koh+24] and depthfm [ gfp+24] for depth estimation? we believe that this work can propel advancements in geometry estimation tasks and a wide range of other downstream vision tasks. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12742",
        "label": 1,
        "title": "benchmarking multi-image understanding in vision and language models: perception",
        "abstract": "the advancement of large language models (llms) has significantly broadened the scope of applications in natural language processing, with multi-modal llms extending these capabilities to integrate and interpret visual data. however, existing benchmarks for visual language models (vlms) predominantly focus on single- image inputs, neglecting the crucial aspect of multi-image understanding. in this paper, we introduce a multi-image relational benchmark mirb , designed to evaluate vlms\u2019 ability to compare, analyze, and reason across multiple images. our benchmark encompasses four categories: perception, visual world knowledge, reasoning, and multi-hop reasoning. through a comprehensive evaluation of a wide range of open-source and closed-source models, we demonstrate that while open-source vlms were shown to approach the performance of gpt-4v in single- image tasks, a significant performance gap remains in multi-image reasoning tasks. our findings also reveal that even the state-of-the-art gpt-4v model struggles with our benchmark, underscoring the need for further research and development in this area. we believe our contribution of mirb could serve as a testbed for developing the next-generation multi-modal models. preprint. under review.arxiv:2406.12742v1  [cs.cv]  18 jun 2024"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12769",
        "label": 0,
        "title": "published as a conference paper at iclr 2024 latent intuitive physics : learning to transfer hidden physics fr",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": "https://arxiv.org/pdf/2406.12847",
        "label": 0,
        "title": "journal of l atex class files",
        "abstract": "Abstract or Introduction section not found on the first page"
    },
    {
        "url": " https://arxiv.org/pdf/2406.11911",
        "label": 0,
        "title": "a notion of complexity for theory of mind via discrete world models x. angelo huang1",
        "abstract": [
            null,
            null
        ]
    },
    {
        "url": "https://arxiv.org/pdf/2406.11944",
        "label": 0,
        "title": "transcoders find interpretable llm feature circuits jacob dunefsky",
        "abstract": "a key goal in mechanistic interpretability is circuit analysis: finding sparse sub- graphs of models corresponding to specific behaviors or capabilities. however, mlp sublayers make fine-grained circuit analysis on transformer-based language models difficult. in particular, interpretable features\u2014such as those found by sparse autoencoders (saes)\u2014are typically linear combinations of extremely many neu- rons, each with its own nonlinearity to account for. circuit analysis in this setting thus either yields intractably large circuits or fails to disentangle local and global behavior. to address this we explore transcoders , which seek to faithfully approx- imate a densely activating mlp layer with a wider, sparsely-activating mlp layer. we successfully train transcoders on language models with 120m, 410m, and 1.4b parameters, and find them to perform at least on par with saes in terms of sparsity, faithfulness, and human-interpretability. we then introduce a novel method for using transcoders to perform weights-based circuit analysis through mlp sublay- ers. the resulting circuits neatly factorize into input-dependent and input-invariant terms. finally, we apply transcoders to reverse-engineer unknown circuits in the model, and we obtain novel insights regarding the \u201cgreater-than circuit\u201d in gpt2- small. our results suggest that transcoders can prove effective in decomposing model computations involving mlps into interpretable circuits. code is available athttps://github.com/jacobdunefsky/transcoder_circuits . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12036",
        "label": 1,
        "title": "medcalc-bench : evaluating large language models for medical calculations nikhil khandekar",
        "abstract": "as opposed to evaluating computation and logic-based reasoning, current bench- marks for evaluating large language models (llms) in medicine are primarily focused on question-answering involving domain knowledge and descriptive rea- soning. while such qualitative capabilities are vital to medical diagnosis, in real- world scenarios, doctors frequently use clinical calculators that follow quantitative equations and rule-based reasoning paradigms for evidence-based decision support. to this end, we propose medcalc-bench , a first-of-its-kind dataset focused on evaluating the medical calculation capability of llms. medcalc-bench contains an evaluation set of over 1000 manually reviewed instances from 55 dif- ferent medical calculation tasks. each instance in medcalc-bench consists of a patient note, a question requesting to compute a specific medical value, a ground truth answer, and a step-by-step explanation showing how the answer is obtained. while our evaluation results show the potential of llms in this area, none of them are effective enough for clinical settings. common issues include extracting the incorrect entities, not using the correct equation or rules for a calculation task, or incorrectly performing the arithmetic for the computation. we hope our study highlights the quantitative knowledge and reasoning gaps in llms within medical settings, encouraging future improvements of llms for various clinical calculation tasks.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12045",
        "label": 1,
        "title": "\u03c4-bench: a benchmark for t ool-a gent-u ser interaction in real-world domains shunyu yao",
        "abstract": "existing benchmarks do not test language agents on their interaction with human users or ability to follow domain-specific rules, both of which are vital for deploying them in real world applications. we propose \u03c4-bench, a benchmark emulating dynamic conversations between a user (simulated by language models) and a language agent provided with domain-specific api tools and policy guidelines. we employ an efficient and faithful evaluation process that compares the database state at the end of a conversation with the annotated goal state. we also propose a new metric (pass^k) to evaluate the reliability of agent behavior over multiple trials. our experiments show that even state-of-the-art function calling agents (like gpt-4o ) succeed on <50% of the tasks, and are quite inconsistent (pass^8 < 25% in retail). our findings point to the need for methods that can improve the ability of agents to act consistently and follow rules reliably. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12050",
        "label": 0,
        "title": "learn beyond the answer: training language models with reflection for mathematical reasoning zhihan zhang\u00001\u2020",
        "abstract": "supervised fine-tuning enhances the problem- solving abilities of language models across var- ious mathematical reasoning tasks. to maxi- mize such benefits, existing research focuses onbroadening the training set with various data augmentation techniques, which is effective for standard single-round question-answering set- tings. our work introduces a novel technique aimed at cultivating a deeper understanding of the training problems at hand, enhancing perfor- mance not only in standard settings but also in more complex scenarios that require reflective thinking. specifically, we propose reflective augmentation , a method that embeds prob- lem reflection into each training instance. it trains the model to consider alternative perspec- tives and engage with abstractions and analo- gies, thereby fostering a thorough comprehen- sion through reflective reasoning. extensive experiments validate the achievement of our aim, underscoring the unique advantages of our method and its complementary nature relative to existing augmentation techniques.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12052",
        "label": 0,
        "title": "uniglm: training one unified language model for text-attributed graphs yi fang1",
        "abstract": "representation learning on text-attributed graphs (tags), where nodes are represented by textual descriptions, is crucial for textual and relational knowledge systems and recom- mendation systems. currently, state-of-the-art embedding methods for tags primarily focus on fine-tuning language models (e.g., bert) using structure-aware training signals. while effective, these methods are tailored for individ- ual tag and cannot generalize across various graph scenarios. given the shared textual space, leveraging multiple tags for joint fine-tuning, aligning text and graph structure from different aspects, would be more beneficial. motivated by this, we introduce a novel unified graph language model ( uniglm ) framework, the first graph embedding model that generalizes well to both in-domain and cross-domain tags. specifically, uniglm is trained over multiple tags with different domains and scales using self-supervised contrastive learning. uniglm includes an adaptive positive sample selection technique for identifying structurally similar nodes and a lazy contrastive module that is devised to accelerate training by minimizing repetitive encoding calculations. extensive empirical results across 9 benchmark tags demonstrate uniglm\u2019s efficacy against lead- ing embedding baselines in terms of general- ization (various downstream tasks and back- bones) and transfer learning (in and out of domain scenarios). the code is available at https://github.com/nyushcs/uniglm . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12060",
        "label": 0,
        "title": "not eliminate but aggregate: post-hoc control over mixture-of-experts to address shortcut shifts in natural la",
        "abstract": "recent models for natural language under- standing are inclined to exploit simple pat- terns in datasets, commonly known as short- cuts. these shortcuts hinge on spurious cor- relations between labels and latent features existing in the training data. at inference time, shortcut-dependent models are likely to generate erroneous predictions under dis- tribution shifts, particularly when some la- tent features are no longer correlated with the labels. to avoid this, previous stud- ies have trained models to eliminate the re- liance on shortcuts. in this study, we ex- plore a different direction: pessimistically aggregating the predictions of a mixture-of- experts, assuming each expert captures rela- tively different latent features. the exper- imental results demonstrate that our post- hoc control over the experts significantly en- hances the model\u2019s robustness to the distri- bution shift in shortcuts. besides, we show that our approach has some practical advan- tages. we also analyze our model and pro- vide results to support the assumption.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12066",
        "label": 1,
        "title": "language models are surprisingly fragile to drug names in biomedical benchmarks jack gallifant1",
        "abstract": "medical knowledge is context-dependent and requires consistent reasoning across various natural language expressions of semantically equivalent phrases. this is particularly cru- cial for drug names, where patients often use brand names like advil or tylenol instead of their generic equivalents. to study this, we create a new robustness dataset, rabbits , to evaluate performance differences on medical benchmarks after swapping brand and generic drug names using physician expert annotations. we assess both open-source and api-based llms on medqa and medmcqa, revealing a consistent performance drop ranging from 1-10%. furthermore, we identify a potential source of this fragility as the contamination of test data in widely used pre-training datasets.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12091",
        "label": 0,
        "title": "is poisoning a real threat to llm alignment ? m aybe more so than you think a p reprint",
        "abstract": "recent advancements in reinforcement learning with human feedback (rlhf) have significantly impacted the alignment of large language models (llms). the sensitivity of reinforcement learning algorithms such as proximal policy optimization (ppo) has led to new line work on direct policy optimization (dpo), which treats rlhf in a supervised learning framework. the increased practical use of these rlhf methods warrants an analysis of their vulnerabilities. in this work, we investigate the vulnerabilities of dpo to poisoning attacks under different scenarios and compare the effectiveness of preference poisoning, a first of its kind. we comprehensively analyze dpo\u2019s vulnerabilities under different types of attacks, i.e., backdoor and non-backdoor attacks, and different poisoning methods across a wide array of language models, i.e., llama 7b, mistral 7b, and gemma 7b. we find that unlike ppo-based methods, which, when it comes to backdoor attacks, require at least 4% of the data to be poisoned to elicit harmful behavior, we exploit the true vulnerabilities of dpo more simply so we can poison the model with only as much as 0.5% of the data. we further investigate the potential reasons behind the vulnerability and how well this vulnerability translates into backdoor vs non-backdoor attacks. implementation of the paper is publically available athttps://github.com/pankayaraj/rlhfpoisoning . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12128",
        "label": 1,
        "title": "ai \u2018news\u2019 content farms are easy to make and hard to detect: a case study in italian giovanni puccetti\u03b1",
        "abstract": "large language models (llms) are increas- ingly used as \u2018content farm\u2019 models (cfms), to generate synthetic text that could pass for real news articles. this is already happening even for languages that do not have high-quality monolingual llms. we show that fine-tuning llama (v1), mostly trained on english, on as lit- tle as 40k italian news articles, is sufficient for producing news-like texts that native speakers of italian struggle to identify as synthetic. we investigate three llms and three methods of detecting synthetic texts (log-likelihood, de- tectgpt, and supervised classification), finding that they all perform better than human raters, but they are all impractical in the real world (requiring either access to token likelihood in- formation or a large dataset of cfm texts). we also explore the possibility of creating a proxy cfm: an llm fine-tuned on a similar dataset to one used by the real \u2018content farm\u2019. we find that even a small amount of fine-tuning data suf- fices for creating a successful detector, but we need to know which base llm is used, which is a major challenge. our results suggest that there are currently no practical methods for detecting synthetic news- like texts \u2018in the wild\u2019, while generating them is too easy. we highlight the urgency of more nlp research on this problem. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12131",
        "label": 0,
        "title": "gram 2vec: an interpretable document vectorizer peter zeng\u25b33",
        "abstract": "we present gram 2vec, a grammatical style embedding algorithm that embeds documents into a higher dimensional space by extracting the normalized relative frequencies of gram- matical features present in the text. compared to neural approaches, gram 2vecoffers in- herent interpretability based on how the feature vectors are generated. in our demo, we present a way to visualize a mapping of authors to doc- uments based on their gram 2vecvectors and highlight the ability to drop or add features to view which authors make certain linguistic choices. next, we use authorship attribution as an application to show how gram 2veccan explain why a document is attributed to a cer- tain author, using cosine similarities between thegram 2vecfeature vectors to calculate the distances between candidate documents and a query document. our gram 2veccode is on https://github.com/eric-sclafani/gram2vec, and a corresponding demo video is available at https://youtu.be/y8vj31d7woi "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12158",
        "label": 0,
        "title": "llms are prone to fallacies in causal inference nitish joshi1abulhair saparov1yixin wang2he he1 1new york",
        "abstract": "recent work shows that causal facts can be ef- fectively extracted from llms through prompt- ing, facilitating the creation of causal graphs for causal inference tasks. however, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which the model can memorize. thus, this work investi- gates: can llms infer causal relations from other relational data in text? to disentangle the role of memorized causal facts vs inferred causal relations, we finetune llms on syn- thetic data containing temporal, spatial and counterfactual relations, and measure whether the llm can then infer causal relations. we find that: (a) llms are susceptible to inferring causal relations from the order of two entity mentions in text (e.g. x mentioned before y implies x causes y); (b) if the order is ran- domized, llms still suffer from the post hoc fallacy , i.e. x occurs before y (temporal re- lation) implies x causes y . we also find that while llms can correctly deduce the absence of causal relations from temporal and spatial re- lations, they have difficulty inferring causal re- lations from counterfactuals, questioning their understanding of causality. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12182",
        "label": 1,
        "title": "aqulia-med llm: pioneering full-process open-source medical language models lulu zhao1",
        "abstract": "recently, both closed-source llms and open- source communities have made significant strides, outperforming humans in various gen- eral domains. however, their performance in specific professional fields such as medicine, especially within the open-source community, remains suboptimal due to the complexity of medical knowledge. we propose aquila-med, a bilingual medical llm based on aquila, ad- dressing these challenges through continue pre- training, supervised fine-tuning (sft), and re- inforcement learning from human feedback (rlhf). we construct a large-scale chinese and english medical dataset for continue pre- training and a high-quality sft dataset, cover- ing extensive medical specialties. addition- ally, we develop a high-quality direct pref- erence optimization (dpo) dataset for fur- ther alignment. aquila-med achieves no- table results across single-turn, multi-turn di- alogues, and medical multiple-choice ques- tions, demonstrating the effectiveness of our approach. we open-source the datasets and the entire training process, contributing valu- able resources to the research community. our models and datasets will released at https://huggingface.co/baai/aquilamed-rl. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.12277",
        "label": 1,
        "title": "what matters in learning facts in language models? multifaceted knowledge probing with diverse multi-prompt da",
        "abstract": "large language models (llms) face issues in handling factual knowledge, making it vital to evaluate their true ability to understand facts. in this study, we introduce knowledge prob- ing frameworks, belief(-icl), to evaluate the knowledge understanding ability of both encoder-based and decoder-based pre-trained lms (plms) from diverse perspectives. be- liefs utilize a multi-prompt dataset to evaluate plm\u2019s accuracy, consistency, and reliability in factual knowledge understanding. to en- able a more reliable evaluation with beliefs, we semi-automatically create myriadlama, which has massively diverse prompts. we vali- date the effectiveness of beliefs in correctly and comprehensively evaluating plm\u2019s factual understanding ability via extensive evaluations with recent llms. we then investigate key fac- tors in learning facts in llms, and reveal the limitation of the prompt-based knowledge prob- ing. the dataset is anonymously publicized.1 "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09181",
        "label": 1,
        "title": "a large-scale universal evaluation benchmark for face forgery detection yijun bei",
        "abstract": "with the rapid development of ai-generated content (aigc) technology, the production of realistic fake facial images and videos that deceive human visual perception has become possible. consequently, various face forgery detection techniques have been proposed to identify such fake facial content. however, evaluating the effectiveness and generalizability of these detection techniques re- mains a significant challenge. to address this, we have constructed a large-scale evaluation benchmark called deepfacegen, aimed at quantitatively assessing the effectiveness of face forgery detection and facilitating the iterative development of forgery detection technology. deepfacegen consists of 776 ,990real face image/video samples and 773 ,812face forgery image/video samples, generated using 34mainstream face generation techniques. during the construction pro- cess, we carefully consider important factors such as content diversity, fairness across ethnicities, and availability of comprehensive labels, in order to ensure the versatility and convenience of deepfacegen. subsequently, deepfacegen is employed in this study to evaluate and analyze the performance of 13mainstream face forgery detection techniques from various perspectives. through extensive experimental analysis, we derive significant findings and propose potential direc- tions for future research. the code and dataset for deepfacegen are available at https://github.com/hengruilou/deepfacegen. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.17186",
        "label": 1,
        "title": "clerc : a dataset for legal case retrieval and retrieval-augmented analysis generation abe bohan hou\u2663orion wel",
        "abstract": "legal professionals need to write analyses that rely on citations to relevant precedents, i.e., pre- vious case decisions. intelligence systems as- sisting legal professionals in writing such doc- uments provide great benefits but are challeng- ing to design. such systems need to help lo- cate, summarize, and reason over salient prece- dents in order to be useful. to enable systems for such tasks, we work with legal profession- als to transform a large open-source legal cor- pus into a dataset1supporting two important backbone tasks: information retrieval (ir) and retrieval-augmented generation (rag). this dataset clerc (caselawevaluation and retrieval corpus), is constructed for training and evaluating models on their ability to (1) find corresponding citations for a given piece of legal analysis and to (2) compile the text of these citations (as well as previous context) into a cogent analysis that supports a reasoning goal. we benchmark state-of-the-art models on clerc , showing that current approaches still struggle: gpt-4o generates analyses with the highest rouge f-scores but hallucinates the most, while zero-shot ir models only achieve 48.3% recall@1000. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09321",
        "label": 1,
        "title": "jailbreakeval : an integrated toolkit for evaluating jailbreak attempts against large language models delong r",
        "abstract": "jailbreak attacks aim to induce large language models (llms) to generate harmful responses for forbidden instructions, presenting severe misuse threats to llms. up to now, research into jailbreak attacks and defenses is emerging, however, there is (surprisingly) no consensus on how to evaluate whether a jailbreak attempt is successful. in other words, the methods to assess the harmfulness of an llm\u2019s response are varied, such as manual annotation or prompting gpt-4 in specific ways. each approach has its own set of strengths and weaknesses, impacting their alignment with human values, as well as the time and financial cost. this diversity in evaluation presents challenges for researchers in choosing suitable evaluation methods and conducting fair comparisons across different jailbreak attacks and defenses. in this paper, we conduct a comprehensive analysis of jailbreak evaluation methodologies, drawing from nearly ninety jailbreak research released between may 2023 and april 2024. our study introduces a systematic taxonomy of jailbreak evaluators, offering in-depth insights into their strengths and weaknesses, along with the current status of their adaptation. moreover, to facilitate subsequent research, we propose jailbreakeval (https://github.com/thuccslab/ jailbreakeval ), a user-friendly toolkit focusing on the evaluation of jailbreak attempts. it includes various well-known evaluators out-of-the-box, so that users can obtain evaluation results with only a single command. jailbreakeval also allows users to customize their own evaluation workflow in a unified framework with the ease of development and comparison. in summary, we regard jailbreakeval to be a catalyst that simplifies the evaluation process in jailbreak research and fosters an inclusive standard for jailbreak evaluation within the community. "
    },
    {
        "url": "https://arxiv.org/pdf/2406.08673",
        "label": 1,
        "title": "helpsteer2: open-source dataset for training top-performing reward models zhilin wang",
        "abstract": "high-quality preference datasets are essential for traini ng reward models that can effectively guide large language models (llms) in generati ng high-quality re- sponses aligned with human preferences. as llms become stro nger and better aligned, permissively licensed preference datasets, such as open assistant, hh- rlhf, and helpsteer need to be updated to remain effective fo r reward model- ing. methods that distil preference data from proprietary l lms such as gpt-4 have restrictions on commercial usage imposed by model prov iders. to improve upon both generated responses and attribute labeling quali ty, we release help- steer2, a permissively licensed preference dataset (cc-by -4.0). using a power- ful internal base model trained on helpsteer2, we are able to achieve the sota score (92.0%) on reward-bench\u2019s primary dataset, outperfo rming currently listed open and proprietary models, as of june 12th, 2024. notably, helpsteer2 consists of only ten thousand response pairs, an order of magnitude fe wer than existing preference datasets (e.g., hh-rlhf), which makes it highly ef\ufb01cient for train- ing reward models. our extensive experiments demonstrate t hat reward models trained with helpsteer2 are effective in aligning llms. in p articular, we propose steerlm 2.0, a model alignment approach that can effectivel y make use of the rich multi-attribute score predicted by our reward models. helpsteer2 is avail- able athttps://huggingface.co/datasets/nvidia/helpsteer2 and code is available at https://github.com/nvidia/nemo-aligner . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09326",
        "label": 1,
        "title": "pianomotion10m : dataset and benchmark for hand motion generation in piano performance qijun gan",
        "abstract": "recently, artificial intelligence techniques for education have been received increas- ing attentions, while it still remains an open problem to design the effective music instrument instructing systems. although key presses can be directly derived from sheet music, the transitional movements among key presses require more extensive guidance in piano performance. in this work, we construct a piano-hand motion generation benchmark to guide hand movements and fingerings for piano playing. to this end, we collect an annotated dataset, pianomotion10m , consisting of 116 hours of piano playing videos from a bird\u2019s-eye view with 10 million annotated hand poses. we also introduce a powerful baseline model that generates hand motions from piano audios through a position predictor and a position-guided gesture generator. furthermore, a series of evaluation metrics are designed to assess the performance of the baseline model, including motion similarity, smooth- ness, positional accuracy of left and right hands, and overall fidelity of movement distribution. despite that piano key presses with respect to music scores or audios are already accessible, pianomotion10m aims to provide guidance on piano fingering for instruction purposes. the dataset and source code can be accessed at https://agnjason.github.io/pianomotion-page . "
    },
    {
        "url": "https://arxiv.org/pdf/2406.09388",
        "label": 1,
        "title": "exploring the spectrum of visio-linguistic compositionality and recognition youngtaek oh1",
        "abstract": "vision and language models (vlms) such as clip have showcased remarkable zero-shot recognition abilities yet face challenges in visio-linguistic compositionality, partic- ularly in linguistic comprehension and fine-grained image- text alignment. this paper explores the intricate relation- ship between compositionality and recognition \u2013 two piv- otal aspects of vlm capability. we conduct a compre- hensive evaluation of existing vlms, covering both pre- training approaches aimed at recognition and the fine- tuning methods designed to improve compositionality. our evaluation employs 12 benchmarks for compositionality, along with 21 zero-shot classification and two retrieval benchmarks for recognition. in our analysis from 274 clip model checkpoints, we reveal patterns and trade-offs that emerge between compositional understanding and recog- nition accuracy. ultimately, this necessitates strategic ef- forts towards developing models that improve both capabil- ities, as well as the meticulous formulation of benchmarks for compositionality. we open our evaluation framework at https://github.com/ytaek-oh/vl_compo . 1"
    }
]